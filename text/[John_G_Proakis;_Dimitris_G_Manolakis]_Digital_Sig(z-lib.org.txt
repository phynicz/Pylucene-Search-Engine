









































Digital Signal Processing: Pearson New International Edition




Digital Signal Processing
John G. Proakis   Dimitris K. Manolakis

Fourth Edition



Pearson Education Limited
Edinburgh Gate
Harlow
Essex CM20 2JE
England and Associated Companies throughout the world

Visit us on the World Wide Web at: www.pearsoned.co.uk

© Pearson Education Limited 2014 

All rights reserved. No part of this publication may be reproduced, stored in a retrieval system, or transmitted 
in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, without either the 
prior written permission of the publisher or a licence permitting restricted copying in the United Kingdom 
issued by the Copyright Licensing Agency Ltd, Saffron House, 6–10 Kirby Street, London EC1N 8TS.

All trademarks used herein are the property of their respective owners. The use of any trademark 
in this text does not vest in the author or publisher any trademark ownership rights in such 
trademarks, nor does the use of such trademarks imply any affi liation with or endorsement of this 
book by such owners. 

British Library Cataloguing-in-Publication Data
A catalogue record for this book is available from the British Library

 Printed in the United States of America

ISBN 10: 1-292-02573-5
ISBN 13: 978-1-292-02573-5

ISBN 10: 1-292-02573-5
ISBN 13: 978-1-292-02573-5



Table of  Contents

P E A R S O N  C U S T O M  L I B R A R Y  

I

1. Introduction

1

1John G. Proakis/Dimitris G. Manolakis

2. Discrete-Time Signals and Systems

43

43John G. Proakis/Dimitris G. Manolakis

3. The z-Transform and Its Application to the Analysis of LTI Systems

151

151John G. Proakis/Dimitris G. Manolakis

4. Frequency Analysis of Signals

229

229John G. Proakis/Dimitris G. Manolakis

5. Frequency-Domain Analysis of LTI Systems

307

307John G. Proakis/Dimitris G. Manolakis

6. Sampling and Reconstruction of Signals

395

395John G. Proakis/Dimitris G. Manolakis

7. The Discrete Fourier Transform: Its Properties and Applications

461

461John G. Proakis/Dimitris G. Manolakis

8. Efficient Computation of the DFT: Fast Fourier Transform Algorithms

523

523John G. Proakis/Dimitris G. Manolakis

9. Implementation of Discrete-Time Systems

577

577John G. Proakis/Dimitris G. Manolakis

10. Design of Digital Filters

669

669John G. Proakis/Dimitris G. Manolakis

11. Multirate Digital Signal Processing

767

767John G. Proakis/Dimitris G. Manolakis

12. Linear Prediction and Optimum Linear Filters

841

841John G. Proakis/Dimitris G. Manolakis

13. Adaptive Filters

899

899John G. Proakis/Dimitris G. Manolakis



II

14. Appendix: Random Number Generators

981

981John G. Proakis/Dimitris G. Manolakis

15. Appendix: Tables of Transition Coefficients for the Design of Lnear-Phase FIR Filters

987

987John G. Proakis/Dimitris G. Manolakis

16. References and Bibliography

993

993John G. Proakis/Dimitris G. Manolakis

1007

1007Index



Introduction

Digital signal processing is an area of science and engineering that has developed
rapidly over the past 40 years. This rapid development is a result of the significant
advances in digital computer technology and integrated-circuit fabrication. The dig-
ital computers and associated digital hardware of four decades ago were relatively
large and expensive and, as a consequence, their use was limited to general-purpose
non-real-time (off-line) scientific computations and business applications. The rapid
developments in integrated-circuit technology, starting with medium-scale integra-
tion (MSI) and progressing to large-scale integration (LSI), and now, very-large-scale
integration (VLSI) of electronic circuits has spurred the development of powerful,
smaller, faster, and cheaper digital computers and special-purpose digital hardware.
These inexpensive and relatively fast digital circuits have made it possible to construct
highly sophisticated digital systems capable of performing complex digital signal pro-
cessing functions and tasks, which are usually too difficult and/or too expensive to
be performed by analog circuitry or analog signal processing systems. Hence many
of the signal processing tasks that were conventionally performed by analog means
are realized today by less expensive and often more reliable digital hardware.

We do not wish to imply that digital signal processing is the proper solution for
all signal processing problems. Indeed, for many signals with extremely wide band-
widths, real-time processing is a requirement. For such signals, analog or, perhaps,
optical signal processing is the only possible solution. However, where digital cir-
cuits are available and have sufficient speed to perform the signal processing, they
are usually preferable.

Not only do digital circuits yield cheaper and more reliable systems for signal
processing, they have other advantages as well. In particular, digital processing
hardware allows programmable operations. Through software, one can more eas-

From Chapter 1 of Digital Signal Processing: Principles, Algorithms, and Applications, Fourth Edition.
John G. Proakis, Dimitris G. Manolakis. Copyright © 2007 by Pearson Education, Inc. All rights reserved.

1



ily modify the signal processing functions to be performed by the hardware. Thus
digital hardware and associated software provide a greater degree of flexibility in
system design. Also, there is often a higher order of precision achievable with digital
hardware and software compared with analog circuits and analog signal processing
systems. For all these reasons, there has been an explosive growth in digital signal
processing theory and applications over the past three decades.

Signals, Systems, and Signal Processing

A signal is defined as any physical quantity that varies with time, space, or any other
independent variable or variables. Mathematically, we describe a signal as a function
of one or more independent variables. For example, the functions

s1(t) = 5t
s2(t) = 20t2

describe two signals, one that varies linearly with the independent variable t (time)
and a second that varies quadratically with t . As another example, consider the
function

s(x, y) = 3x + 2xy + 10y2

This function describes a signal of two independent variables x and y that could
represent the two spatial coordinates in a plane.

Introduction

We begin by introducing some of with the process of converting an analog signal 
to digital form suitable for digital processing. As we shall see, digital processing of 
analog signals has some drawbacks. First, and foremost, conversion of an analog sig-
nal to digital form, accomplished by sampling the signal and quantizing the samples, 
results in a distortion that prevents us from reconstructing the original analog signal 
from the quantized samples. Control of the amount of this distortion is achieved by 
proper choice of the sampling rate and the precision in the quantization process. 
Second, there are finite precision effectsthat must be considered in the digital pro-
cessing of the quantized samples. 

(1.2)

1

(1.1)

The signals described by (1.1) and (1.2) belong to a class of signals that are pre-
cisely defined by specifying the functional dependence on the independent variable. 
However, there are cases where such a functional relationship is unknown or too 
highly complicated to be of any practical use.

For example, a speech signal (see Fig. 1.1) cannot be described functionally by 
expressions such as (1.1). In general, a segment of speech may be represented to

2



Example of a speech signal.

a high degree of accuracy as a sum of several sinusoids of different amplitudes and
frequencies, that is, as

N∑
i=1

Ai(t) sin[2πFi(t)t + θi(t)]

where {Ai(t)}, {Fi(t)}, and {θi(t)} are the sets of (possibly time-varying) amplitudes,
frequencies, and phases, respectively, of the sinusoids. In fact, one way to interpret
the information content or message conveyed by any short time segment of the speech
signal is to measure the amplitudes, frequencies, and phases contained in the short
time segment of the signal.

Another example of a natural signal is an electrocardiogram (ECG). Such a
signal provides a doctor with information about the condition of the patient’s heart.
Similarly, an electroencephalogram (EEG) signal provides information about the
activity of the brain.

Speech, electrocardiogram, and electroencephalogram signals are examples of
information-bearing signals that evolve as functions of a single independent variable,
namely, time. An example of a signal that is a function of two independent variables
is an image signal. The independent variables in this case are the spatial coordinates.
These are but a few examples of the countless number of natural signals encountered
in practice.

Associated with natural signals are the means by which such signals are gener-
ated. For example, speech signals are generated by forcing air through the vocal
cords. Images are obtained by exposing a photographic film to a scene or an object.
Thus signal generation is usually associated with a system that responds to a stimulus
or force. In a speech signal, the system consists of the vocal cords and the vocal tract,
also called the vocal cavity. The stimulus in combination with the system is called a
signal source. Thus we have speech sources, images sources, and various other types
of signal sources.

A system may also be defined as a physical device that performs an operation on
a signal. For example, a filter used to reduce the noise and interference corrupting a
desired information-bearing signal is called a system. In this case the filter performs
some operation(s) on the signal, which has the effect of reducing (filtering) the noise
and interference from the desired information-bearing signal.

Introduction

Figure 1.1

(1.3)

3



When we pass a signal through a system, as in filtering, we say that we have
processed the signal. In this case the processing of the signal involves filtering the
noise and interference from the desired signal. In general, the system is characterized
by the type of operation that it performs on the signal. For example, if the operation
is linear, the system is called linear. If the operation on the signal is nonlinear, the
system is said to be nonlinear, and so forth. Such operations are usually referred to
as signal processing.

For our purposes, it is convenient to broaden the definition of a system to include
not only physical devices, but also software realizations of operations on a signal. In
digital processing of signals on a digital computer, the operations performed on a
signal consist of a number of mathematical operations as specified by a software
program. In this case, the program represents an implementation of the system in
software. Thus we have a system that is realized on a digital computer by means of
a sequence of mathematical operations; that is, we have a digital signal processing
system realized in software. For example, a digital computer can be programmed
to perform digital filtering. Alternatively, the digital processing on the signal may
be performed by digital hardware (logic circuits) configured to perform the desired
specified operations. In such a realization, we have a physical device that performs
the specified operations. In a broader sense, a digital system can be implemented as
a combination of digital hardware and software, each of which performs its own set
of specified operations.

Introduction

Basic Elements of a Digital Signal Processing System1.1

This text deals with the processing of signals by digital means, either in soft-
ware or in hardware. Since many of the signals encountered in practice are analog, 
we must also consider the problem of converting an analog siganl into a digital 
signal for processing. Thus we deal primarily with digital systems. The operations 
performed by such a system can usually be specified mathematically. The method 
or set of rules for implementing the system by a program that performs the corre-
sponding mathematical operations is called an algorithm. Usually, there are many 

hardware, to perform the desired operations and computations. In practice, we have 
an interest in devising algorithms that are computationally efficient, fast, and eas-
ily implemented. Thus a major topic in the study of digital signal processing is the 
discussion of efficient algorithms for performing such operations as filtering, cor-
relation, and spectral analysis.

Most of the signals encountered in science and engineering are analog in nature. 
That is, the signals are functions of a continuous variable, such as time or space, and 
usually take on values in a continuous range. Such signals may be processed directly 
by appropriate analog systems (such as filters, frequency analyzers, or frequency 
multipliers) for the purpose of changing their characteristics or extracting some de-
sired information. In such a case we say that the signal has been processed directly 
in its analog form, as illustrated in Fig. 1.2. Both the input signal and the output 
signal are in analog form.

ways or algorithms by  which a system can be implemented, either in software or in 

4



Analog signal processing.

Analog
input
signal

Analog
output
signal

Analog
signal

processor

The digital signal processor may be a large programmable digital computer or a
small microprocessor programmed to perform the desired operations on the input
signal. It may also be a hardwired digital processor configured to perform a specified
set of operations on the input signal. Programmable machines provide the flexibility
to change the signal processing operations through a change in the software, whereas
hardwired machines are difficult to reconfigure. Consequently, programmable signal
processors are in very common use. On the other hand, when signal processing
operations are well defined, a hardwired implementation of the operations can be
optimized, resulting in a cheaper signal processor and, usually, one that runs faster
than its programmable counterpart. In applications where the digital output from
the digital signal processor is to be given to the user in analog form, such as in speech
communications, we must provide another interface from the digital domain to the
analog domain. Such an interface is called a digital-to-analog (D/A) converter. Thus
the signal is provided to the user in analog form, as illustrated in the block diagram of

Advantages of Digital over Analog Signal Processing

There are many reasons why digital signal processing of an analog signal may be
preferable to processing the signal directly in the analog domain, as mentioned briefly
earlier. First, a digital programmable system allows flexibility in reconfiguring the
digital signal processing operations simply by changing the program. Reconfigu-

Analog
input
signal

A/D
converter

D/A
converter

Analog
output
signal

Digital
signal

processor

Digital
input
signal

Digital
output
signal

Introduction

Figure 1.2

1.2

Block diagram of a digital signal processing system.Figure 1.3

Digital signal processing provides an alternative method for processing the 
analog signal, as illustrated in Fig. 1.3. To perform the processing digitally,  there is a 
need for an interface between the analog signal and the digital processor. This inter-
face is called an  analog-to-digital (A/D)  converter. The output of the A/D converter 
is a digital signal that is appropriate as an input to the digital processor.

Fig. 1.3. However, there are other practical applications involving signal analysis, 
where the desired information is conveyed in digital form and no D/A converter 
is required. For example, in the digital processing of radar signals, the information 
extracted from the radar signal, such as the position of the aircraft and its speed, 
may simply be printed on paper. There is no need for a D/A converter in this case.

5



ration of an analog system usually implies a redesign of the hardware followed by
testing and verification to see that it operates properly.

Accuracy considerations also play an important role in determining the form
of the signal processor. Tolerances in analog circuit components make it extremely
difficult for the system designer to control the accuracy of an analog signal process-
ing system. On the other hand, a digital system provides much better control of
accuracy requirements. Such requirements, in turn, result in specifying the accuracy
requirements in the A/D converter and the digital signal processor, in terms of word
length, floating-point versus fixed-point arithmetic, and similar factors.

Digital signals are easily stored on magnetic media (tape or disk) without de-
terioration or loss of signal fidelity beyond that introduced in the A/D conversion.
As a consequence, the signals become transportable and can be processed off-line
in a remote laboratory. The digital signal processing method also allows for the im-
plementation of more sophisticated signal processing algorithms. It is usually very
difficult to perform precise mathematical operations on signals in analog form but
these same operations can be routinely implemented on a digital computer using
software.

In some cases a digital implementation of the signal processing system is cheaper
than its analog counterpart. The lower cost may be due to the fact that the digital
hardware is cheaper, or perhaps it is a result of the flexibility for modifications pro-
vided by the digital implementation.

As a consequence of these advantages, digital signal processing has been applied
in practical systems covering a broad range of disciplines. We cite, for example, the
application of digital signal processing techniques in speech processing and signal
transmission on telephone channels, in image processing and transmission, in seis-
mology and geophysics, in oil exploration, in the detection of nuclear explosions, in
the processing of signals received from outer space, and in a vast variety of other
applications.

As already indicated, however, digital implementation has its limitations. One
practical limitation is the speed of operation of A/D converters and digital signal
processors. We shall see that signals having extremely wide bandwidths require
fast-sampling-rate A/D converters and fast digital signal processors. Hence there
are analog signals with large bandwidths for which a digital processing approach is
beyond the state of the art of digital hardware.

2 Classification of Signals

The methods we use in processing a signal or in analyzing the response of a sys-
tem to a signal depend heavily on the characteristic attributes of the specific signal.
There are techniques that apply only to specific families of signals. Consequently,
any investigation in signal processing should start with a classification of the signals
involved in the specific application.

Multichannel and Multidimensional Signals

Introduction

2.1

As explained in Section 1, a signal is described by a function of one or more in de-
pendent variables. The value of the function (i.e., the dependent variable) can be

6



a real-valued scalar quantity, a complex-valued quantity, or perhaps a vector. For
example, the signal

s1(t) = A sin 3πt
is a real-valued signal. However, the signal

s2(t) = Aej3πt = A cos 3πt + jA sin 3πt

is complex valued.
In some applications, signals are generated by multiple sources or multiple sen-

three components of a vector signal that represents the ground acceleration due to
an earthquake. This acceleration is the result of three basic types of elastic waves.
The primary (P) waves and the secondary (S) waves propagate within the body of

Three components of ground acceleration measured a few kilometers from
the epicenter of an earthquake. (From Earthquakes, by B. A. Bold, ©1988 by
W. H. Freeman and Company. Reprinted with permission of the publisher.)

Introduction

sors. Such signals, in turn, can be represented in vector form. Figure 2.1 shows the

Figure 2.1

7



Example of a
two-dimensional signal.

I(x1, y1)

x10
x

y1

y

rock and are longitudinal and transversal, respectively. The third type of elastic wave
is called the surface wave, because it propagates near the ground surface. If sk(t),
k = 1, 2, 3, denotes the electrical signal from the kth sensor as a function of time,
the set of p = 3 signals can be represented by a vector S3(t), where

S3(t) =
[
s1(t)

s2(t)

s3(t)

]

We refer to such a vector of signals as a multichannel signal.In electrocardiography,
for example, 3-lead and 12-lead electrocardiograms (ECG) are often used in practice,
which result in 3-channel and 12-channel signals.

Let us now turn our attention to the independent variable(s). If the signal is
a function of a single independent variable, the signal is called a one-dimensional
signal. On the other hand, a signal is called M -dimensional if its value is a function
of M independent variables.

the intensity or brightness I (x, y) at each point is a function of two independent vari-
ables. On the other hand, a black-and-white television picture may be represented
as I (x, y, t) since the brightness is a function of time. Hence the TV picture may
be treated as a three-dimensional signal. In contrast, a color TV picture may be de-
scribed by three intensity functions of the form Ir(x, y, t), Ig(x, y, t), and Ib(x, y, t),
corresponding to the brightness of the three principal colors (red, green, blue) as
functions of time. Hence the color TV picture is a three-channel, three-dimensional
signal, which can be represented by the vector

I(x, y, t) =
[
Ir (x, y, t)

Ig(x, y, t)

Ib(x, y, t)

]

Introduction

Figure 2.2

The picture shown in Fig. 2.2 is an example of a two-dimensional signal, since

8



plex-valued signals and we refer to them simply as signals. In mathematical terms
these signals are described by a function of a single independent variable. Although
the independent variable need not be time, it is common practice to use t as the
independent variable. In many cases the signal processing operations and algorithms
developed in this text for one-dimensional, single-channel signals can be extended
to multichannel and multidimensional signals.

Continuous-Time Versus Discrete-Time Signals

Signals can be further classified into four different categories depending on the char-
acteristics of the time (independent) variable and the values they take. Continuous-
time signals or analog signals are defined for every value of time and they take
on values in the continuous interval (a, b), where a can be −∞ and b can be ∞.
Mathematically, these signals can be described by functions of a continuous vari-

1 2
−|t| ,

−∞ < t <∞ are examples of analog signals. Discrete-time signalsare defined only
at certain specific values of time. These time instants need not be equidistant, but in
practice they are usually taken at equally spaced intervals for computational conve-
nience and mathematical tractability. The signal x(tn) = e−|tn| , n = 0, ±1, ±2, . . .
provides an example of a discrete-time signal. If we use the index n of the discrete-
time instants as the independent variable, the signal value becomes a function of an
integer variable (i.e., a sequence of numbers). Thus a discrete-time signal can be rep-
resented mathematically by a sequence of real or complex numbers. To emphasize
the discrete-time nature of a signal, we shall denote such a signal as x(n) instead of
x(t). If the time instants tn are equally spaced (i.e., tn = nT ), the notation x(nT ) is
also used. For example, the sequence

x(n) =
{

0.8n, if n ≥ 0
0, otherwise

In applications, discrete-time signals may arise in two ways:

1. By selecting values of an analog signal at discrete-time instants. This process
All measur-

ing instruments that take measurements at a regular interval of time provide

0−1

1

x(n)

1 2 3 4 5 6 7 n

…

…

Graphical representation of the discrete time signal
x(n) = 0.8n for n > 0 and x(n) = 0 for n < 0.

Introduction

2.2

able. The speech waveform in Fig. 1.1 and the signals x (t) = cosπt , x (t) = e

(2.1)

is a discrete-time signal, which is represented graphically as in Fig. 2.3.

is called sampling and is discussed in more detail in Section 4.

Figure 2.3

In this text we deal mainly with single-channel, one-dimensional real- or com-

9



1770
0

100

200

1790 1810 1830 1850 1870

N
um

be
r 

of
 s

un
sp

ot
s

Year

Wölfer annual sunspot numbers (1770–1869).

by sampling the analog signal x(t) = 0.8t , t ≥ 0 and x(t) = 0, t < 0 once every
second.

2. By accumulating a variable over a period of time. For example, counting the
number of cars using a given street every hour, or recording the value of gold
every day, results in discrete-time signals.
Wölfer sunspot numbers. Each sample of this discrete-time signal provides the
number of sunspots observed during an interval of 1 year.

The values of a continuous-time or discrete-time signal can be continuous or discrete.
If a signal takes on all possible values on a finite or an infinite range, it is said to be
a continuous-valued signal. Alternatively, if the signal takes on values from a finite
set of possible values, it is said to be a discrete-valued signal. Usually, these values
are equidistant and hence can be expressed as an integer multiple of the distance
between two successive values. A discrete-time signal having a set of discrete values

possible values.
In order for a signal to be processed digitally, it must be discrete in time and its

values must be discrete (i.e., it must be a digital signal). If the signal to be processed
is in analog form, it is converted to a digital signal by sampling the analog signal
at discrete instants in time, obtaining a discrete-time signal, and then by quantizing
its values to a set of discrete values, as described later in the chapter. The process

Introduction

Figure 2.4

discrete-time signals. For example, the signal x(n) in Fig. 2.3 can be obtained

Figure 2.4 shows a graph of the

Continuous-Valued Versus Discrete-Valued Signals2.3

is called a digital signal. Figure 2.5 shows a digital signal that takes on one of four

10



−1 0 1 2 3 4 5 6 7 8 n

x(n)

… …

Digital signal with four different amplitude values.

of converting a continuous-valued signal into a discrete-valued signal, called quan-
tization, is basically an approximation process. It may be accomplished simply by
rounding or truncation. For example, if the allowable signal values in the digital
signal are integers, say 0 through 15, the continuous-value signal is quantized into
these integer values. Thus the signal value 8.58 will be approximated by the value 8
if the quantization process is performed by truncation or by 9 if the quantization
process is performed by rounding to the nearest integer. An explanation of the
analog-to-digital conversion process is given later in the chapter.

Deterministic Versus Random Signals

The mathematical analysis and processing of signals requires the availability of a
mathematical description for the signal itself. This mathematical description, often
referred to as the signal model, leads to another important classification of signals.
Any signal that can be uniquely described by an explicit mathematical expression,
a table of data, or a well-defined rule is called deterministic. This term is used to
emphasize the fact that all past, present, and future values of the signal are known
precisely, without any uncertainty.

In many practical applications, however, there are signals that either cannot be
described to any reasonable degree of accuracy by explicit mathematical formulas,
or such a description is too complicated to be of any practical use. The lack of such a
relationship implies that such signals evolve in time in an unpredictable manner. We
refer to these signals as random.The output of a noise generator, the seismic signal

The mathematical framework for the theoretical analysis of random signals is

It should be emphasized at this point that the classification of a real-world signal
as deterministic or random is not always clear. Sometimes, both approaches lead to
meaningful results that provide more insight into signal behavior. At other times,
the wrong classification may lead to erroneous results, since some mathematical
tools may apply only to deterministic signals while others may apply only to random
signals. This will become clearer as we examine specific mathematical tools.

Introduction

Figure 2.5

2.4

of Fig. 2.1, and the speech signal in Fig. 1.1 are examples of random signals.

provided by the theory of probability and stochastic processes.

11



The Concept of Frequency in Continuous-Time and
Discrete-Time Signals

The concept of frequency is familiar to students in engineering and the sciences. This
concept is basic in, for example, the design of a radio receiver, a high-fidelity system,
or a spectral filter for color photography. From physics we know that frequency is
closely related to a specific type of periodic motion called harmonic oscillation, which
is described by sinusoidal functions. The concept of frequency is directly related to
the concept of time. Actually, it has the dimension of inverse time. Thus we should
expect that the nature of time (continuous or discrete) would affect the nature of the

xa(t) = A cos(�t + θ), −∞ < t <∞

The subscript a used with x(t) denotes an analog signal. This
signal is completely characterized by three parameters: A is the amplitude of the
sinusoid, � is the frequency in radians per second (rad/s), and θ is the phase in
radians. Instead of �, we often use the frequency F in cycles per second or hertz
(Hz), where

� = 2πF

xa(t) = A cos(2πF t + θ), −∞ < t <∞

Example of an analog
sinusoidal signal.

xa(t) = A cos(2Ft + θ)

0

A

A cos θ

t

Tp = 1/F

Introduction

3

3.1

shown in Fig. 3.1.

(3.1)

(3.2)

(3.3)

We will use both forms, (3.1) and (3.3), in representing sinusoidal signals.

Figure 3.1

A simpleharmonic oscillationismathematically describedby the following continuous-

frequency accordingly.

time sinusoidal signal:

Continuous-Time Sinusoidal Signals

In terms of F , (3.1) can be written as

12



ties:

A1. For every fixed value of the frequency F , xa(t) is periodic. Indeed, it can easily
be shown, using elementary trigonometry, that

xa(t + Tp) = xa(t)

where Tp = 1/F is the fundamental period of the sinusoidal signal.
A2. Continuous-time sinusoidal signals with distinct (different) frequencies are

themselves distinct.

A3. Increasing the frequency F results in an increase in the rate of oscillation of
the signal, in the sense that more periods are included in a given time interval.

We observe that for F = 0, the value Tp = ∞ is consistent with the fundamental
relation F = 1/Tp . Due to continuity of the time variable t , we can increase the
frequency F , without limit, with a corresponding increase in the rate of oscillation.

The relationships we have described for sinusoidal signals carry over to the class
of complex exponential signals

xa(t) = Aej(�t+θ)

This can easily be seen by expressing these signals in terms of sinusoids using the
Euler identity

e±jφ = cosφ ± j sinφ

By definition, frequency is an inherently positive physical quantity. This is ob-
vious if we interpret frequency as the number of cycles per unit time in a periodic
signal. However, in many cases, only for mathematical convenience, we need to

may be expressed as

xa(t) = A cos(�t + θ) = A2 e
j (�t+θ) + A

2
e−j (�t+θ)

two equal-amplitude complex-conjugate exponential signals, sometimes called pha-
As time progresses the phasors rotate in opposite di-

rections with angular frequencies ±� radians per second. Since a positive frequency
corresponds to counterclockwise uniform angular motion, a negative frequency
simply corresponds to clockwise angular motion.

For mathematical convenience, we use both negative and positive frequencies

∞.

Introduction

The analog sinusoidal signal in (3.3) is characterized by the following proper-

(3.4)

(3.5)

introduce negative frequencies. To see this we recall that the sinusoidal signal (3.1)

(3.6)

which follows from (3.5). Note that a sinusoidal signal can be obtained by adding

sors, illustrated in Fig. 3.2.

throughout this text. Hence the frequency range for analog sinusoids is −∞ < F <

13



Representation of a
cosine function by a pair
of complex-conjugate
exponentials (phasors).

Ωt + θ

Ω

Ωt + θ

Ω

A/2

A/2

Re

Im

Discrete-Time Sinusoidal Signals

A discrete-time sinusoidal signal may be expressed as

x(n) = A cos(ωn+ θ), −∞ < n <∞
where n is an integer variable, called the sample number, A is the amplitude of the
sinusoid, ω is the frequency in radians per sample, and θ is the phase in radians.

ω ≡ 2πf

x(n) = A cos(2πf n+ θ), −∞ < n <∞
The frequency f has dimensions of cycles per sample.
consider the sampling of analog sinusoids, we relate the frequency variable f of a
discrete-time sinusoid to the frequency F in cycles per second for the analog sinusoid.

frequency ω = = 112 cycles per sample) and phase
θ = π/3.

Example of a discrete-time
sinusoidal signal (ω = π/6
and θ = π/3).

A

−A

x(n) = A cos(ωn + θ)

0 n

… …

Introduction

Figure 3.2

3.2

(3.7)

If instead of ω we use the frequency variable f defined by

(3.8)

(3.9)

the relation (3.7) becomes

In Section 4, where we

For the moment we consider the discrete-time sinusoid in (3.7) independently of

π/6 radians per sample (f
the continuous-time sinusoid given in (3.1). Figure 3.3 shows a sinusoid with

Figure 3.3

14



In contrast to continuous-time sinusoids, the discrete-time sinusoids are charac-
terized by the following properties:

B1. A discrete-time sinusoid is periodic only if its frequency f is a rational number.

By definition, a discrete-time signal x(n) is periodic with period N(N > 0) if and
only if

x(n+N) = x(n) for all n

The proof of the periodicity property is simple. For a sinusoid with frequency f0
to be periodic, we should have

cos[2πf0(N + n)+ θ ] = cos(2πf0n+ θ)
This relation is true if and only if there exists an integer k such that

2πf0N = 2kπ
or, equivalently,

f0 = k
N

f0 can be expressed as the ratio of two integers (i.e., f0 is rational).
To determine the fundamental period N of a periodic sinusoid, we express its

frequency f0
prime. Then the fundamental period of the sinusoid is equal to N . Observe that a
small change in frequency can result in a large change in the period. For example,
note that f1 = 31/60 implies that N1 = 60, whereas f2 = 30/60 results in N2 = 2.
B2. Discrete-time sinusoids whose frequencies are separated by an integer multiple

of 2π are identical.

To prove this assertion, let us consider the sinusoid cos(ω0n + θ). It easily follows
that

cos[(ω0 + 2π)n+ θ ] = cos(ω0n+ 2πn+ θ) = cos(ω0n+ θ)
As a result, all sinusoidal sequences

xk(n) = A cos(ωkn+ θ), k = 0, 1, 2, . . .
where

ωk = ω0 + 2kπ, −π ≤ ω0 ≤ π
are indistinguishable (i.e., identical). Any sequence resulting from a sinusoid with a
frequency |ω| > π , or |f | > 12 , is identical to a sequence obtained from a sinusoidal
signal with frequency |ω| < π . Because of this similarity, we call the sinusoid having
the frequency |ω| > π an alias of a corresponding sinusoid with frequency |ω| < π .
Thus we regard frequencies in the range −π ≤ ω ≤ π , or − 12 ≤ f ≤ 12 , as unique

Introduction

(3.10)

(3.11)

According to (3.11), a discrete-time sinusoidal signal is periodic only if its frequency

as in (3.11) and cancel common factors so that k and N are relatively

(3.12)

(3.13)

The smallest value of N for which (3.10) is true is called the fundamental period.

15



and all frequencies |ω| > π , or |f | > 12 , as aliases. The reader should notice the
difference between discrete-time sinusoids and continuous-time sinusoids, where
the latter result in distinct signals for � or F in the entire range −∞ < � < ∞ or
−∞ < F <∞.
B3. The highest rate of oscillation in a discrete-time sinusoid is attained when ω = π

(or ω = −π ) or, equivalently, f = 12 (or f = − 12 ).
To illustrate this property, let us investigate the characteristics of the sinusoidal

signal sequence
x(n) = cosω0n

when the frequency varies from 0 to π . To simplify the argument, we take values
of ω0 = 0, π/8, π/4, π/2, π corresponding to f = 0, 116 , 18 , 14 , 12 , which result in

note that the period of the sinusoid decreases as the frequency increases. In fact, we
can see that the rate of oscillation increases as the frequency increases.

To see what happens for π ≤ ω0 ≤ 2π , we consider the sinusoids with frequencies
ω1 = ω0 and ω2 = 2π − ω0 . Note that as ω1 varies from π to 2π , ω2 varies from π
to 0. It can be easily seen that

x1(n) = A cosω1n = A cosω0n
x2(n) = A cosω2n = A cos(2π − ω0)n

= A cos(−ω0n) = x1(n)
x(n)

ω0 = 0

n

x(n)
1

ω0 = 8

n

π

ω0 = 2
π

x(n)

1

n

ω0 = 4
π

x(n)
1

n

ω0 = πx(n)

1

n

… …

… …… …

Signal x(n) = cosω0n for various values of the frequency ω0.

Introduction

periodic sequences having periods N = ∞, 16, 8, 4, 2, as depicted in Fig. 3.4. We

(3.14)

Figure 3.4

16



Hence ω2 is an alias of ω1 . If we had used a sine function instead of a cosine function,
the result would basically be the same, except for a 180◦ phase difference between
the sinusoids x1(n) and x2(n). In any case, as we increase the relative frequency ω0 of
a discrete-time sinusoid from π to 2π , its rate of oscillation decreases. For ω0 = 2π
the result is a constant signal, as in the case for ω0 = 0. Obviously, for ω0 = π
(or f = 12 ) we have the highest rate of oscillation.

As for the case of continuous-time signals, negative frequencies can be intro-
duced as well for discrete-time signals. For this purpose we use the identity

x(n) = A cos(ωn+ θ) = A
2
ej (ωn+θ) + A

2
e−j (ωn+θ)

Since discrete-time sinusoidal signals with frequencies that are separated by an
integer multiple of 2π are identical, it follows that the frequencies in any interval
ω1 ≤ ω ≤ ω1 + 2π constitute all the existing discrete-time sinusoids or complex
exponentials. Hence the frequency range for discrete-time sinusoids is finite with
duration 2π . Usually, we choose the range 0 ≤ ω ≤ 2π or −π ≤ ω ≤ π (0 ≤ f ≤ 1,
− 12 ≤ f ≤ 12 ), which we call the fundamental range.

Harmonically Related Complex Exponentials

Sinusoidal signals and complex exponentials play a major role in the analysis of
signals and systems. In some cases we deal with sets of harmonically related complex
exponentials (or sinusoids). These are sets of periodic complex exponentials with
fundamental frequencies that are multiples of a single positive frequency. Although
we confine our discussion to complex exponentials, the same properties clearly hold
for sinusoidal signals. We consider harmonically related complex exponentials in
both continuous time and discrete time.

Continuous-time exponentials. The basic signals for continuous-time, harmoni-
cally related exponentials are

sk(t) = ejk�0t = ej2πkF0t k = 0,±1,±2, . . .
We note that for each value of k , sk(t) is periodic with fundamental period 1/(kF0) =
Tp/k or fundamental frequency kF0 . Since a signal that is periodic with period Tp/k
is also periodic with period k(Tp/k) = Tp for any positive integer k , we see that all
of the sk(t) have a common period of Tp F0 is allowed to take any value and all
members of the set are distinct, in the sense that if k1 �= k2 , then sk1(t) �= sk2(t).

monically related complex exponentials of the form

xa(t) =
∞∑

k=−∞
cksk(t) =

∞∑
k=−∞

cke
jk�0t

where ck , k = 0, ±1, ±2, . . . are arbitrary complex constants. The signal xa(t)
is periodic with fundamental period Tp = 1/F0 , and its representation in terms of

Introduction

(3.15)

3.3

(3.16)

(3.17)

.

From the basic signals in (3.16) we can construct a linear combination of har-

17



a

are the Fourier series coefficients and the signal sk(t) is called the kth harmonic
of xa(t).

Discrete-time exponentials. Since a discrete-time complex exponential is periodic
if its relative frequency is a rational number, we choose f0 = 1/N and we define the
sets of harmonically related complex exponentials by

sk(n) = ej2πkf0n, k = 0,±1,±2, . . .

sk+N(n) = ej2πn(k+N)/N = ej2πnsk(n) = sk(n)

Furthermore, all members of the set
have a common period of N samples. Clearly, we can choose any consecutive N
complex exponentials, say from k = n0 to k = n0 + N − 1, to form a harmonically
related set with fundamental frequency f0 = 1/N . Most often, for convenience, we
choose the set that corresponds to n0 = 0, that is, the set

sk(n) = ej2πkn/N , k = 0, 1, 2, . . . , N − 1

As in the case of continuous-time signals, it is obvious that the linear combination

x(n) =
N−1∑
k=0

cksk(n) =
N−1∑
k=0

cke
j2πkn/N

results in a periodic signal with fundamental period N . As we shall see later, this is
the Fourier series representation for a periodic discrete-time sequence with Fourier
coefficients {ck}. The sequence sk(n) is called the kth harmonic of x(n).

Stored in the memory of a digital signal processor is one cycle of the sinusoidal signal

x(n) = sin
(

2πn
N
+ θ

)

where θ = 2πq/N , where q and N are integers.
(a) Determine how this table of values can be used to obtain values of harmonically related

sinusoids having the same phase.

(b) Determine how this table can be used to obtain sinusoids of the same frequency but
different phase.

Introduction

(3.17) is called the Fourier series expansion for x (t). The complex-valued constants

In contrast to the continuous-time case, we note that

(3.18)

This means that, consistent with (3.10), there are only N distinct periodic complex
exponentials in the set described by (3.18).

(3.19)

(3.20)

EXAMPLE 3.1

18



Solution.
(a) Let xk(n) denote the sinusoidal signal sequence

xk(n) = sin
(

2πnk
N
+ θ

)

This is a sinusoid with frequency fk = k/N , which is harmonically related to x(n). But
xk(n) may be expressed as

xk(n) = sin
[

2π(kn)
N

+ θ
]

= x(kn)

Thus we observe that xk(0) = x(0), xk(1) = x(k), xk(2) = x(2k), and so on. Hence the
sinusoidal sequence xk(n) can be obtained from the table of values of x(n) by taking
every k th value of x(n), beginning with x(0). In this manner we can generate the values
of all harmonically related sinusoids with frequencies fk = k/N for k = 0, 1, . . . , N − 1.

(b) We can control the phase θ of the sinusoid with frequency fk = k/N by taking the first
value of the sequence from memory location q = θN/2π , where q is an integer. Thus
the initial phase θ controls the starting location in the table and we wrap around the table
each time the index (kn) exceeds N .

radar signals, sonar signals, and various communications signals such as audio and
video signals, are analog. To process analog signals by digital means, it is first nec-
essary to convert them into digital form, that is, to convert them to a sequence of
numbers having finite precision. This procedure is called analog-to-digital (A/D)
conversion, and the corresponding devices are called A/D converters (ADCs).

Conceptually, we view A/D conversion as a three-step process. This process is

1. Sampling. This is the conversion of a continuous-time signal into a discrete-time
signal obtained by taking “samples” of the continuous-time signal at discrete-
time instants. Thus, if xa(t) is the input to the sampler, the output is xa(nT ) ≡
x(n), where T is called the sampling interval.

xa(t) xq(n) 01011…x(n)Sampler Quantizer

A/D converter

Coder

Quantized
signal

Discrete-time
signal

Analog
signal

Digital
signal

Basic parts of an analog-to-digital (A/D) converter.

Introduction

Analog-to-Digital and Digital-to-Analog Conversion

Most signals of practical interest, such as speech, biological signals, seismic signals,

4

illustrated in Fig. 4.1.

Figure 4.1

19



2. Quantization. This is the conversion of a discrete-time continuous-valued signal
into a discrete-time, discrete-valued (digital) signal. The value of each signal
sample is represented by a value selected from a finite set of possible values.
The difference between the unquantized sample x(n) and the quantized output
xq(n) is called the quantization error.

3. Coding. In the coding process, each discrete value xq(n) is represented by a b-bit
binary sequence.

Although we model the A/D converter as a sampler followed by a quantizer and
coder, in practice the A/D conversion is performed by a single device that takes xa(t)
and produces a binary-coded number. The operations of sampling and quantization
can be performed in either order but, in practice, sampling is always performed
before quantization.

In many cases of practical interest (e.g., speech processing) it is desirable to con-
vert the processed digital signals into analog form. (Obviously, we cannot listen to
the sequence of samples representing a speech signal or see the numbers correspond-
ing to a TV signal.) The process of converting a digital signal into an analog signal
is known as digital-to-analog (D/A) conversion. All D/A converters “connect the
dots” in a digital signal by performing some kind of interpolation, whose accuracy

Sampling and quantization are treated in this section. In particular, we demon-
strate that sampling does not result in a loss of information, nor does it introduce
distortion in the signal if the signal bandwidth is finite. In principle, the analog signal
can be reconstructed from the samples, provided that the sampling rate is sufficiently
high to avoid the problem commonly called aliasing. On the other hand, quantization

Zero-order hold
digital-to-analog
(D/A) conversion.

Introduction

Figure 4.2

depends on the quality of the D/A conversion process. Figure 4.2 illustrates a sim-

Other approximations are possible, such as linearly connecting a pair of successive 
samples (linear interpolation), fitting a quadratic through three successive samples 
(quadratic interpolation), and so on. Is there an optimum (ideal) interpolator? For 

introduced in the following section specifies the optimum form of interpolation.

ple form of D/A conversion, called a zero-order hold or a staircase approximation. 

signals having a limited frequency content (finite bandwidth), the sampling theorem 

20



is a noninvertible or irreversible process that results in signal distortion. We shall
show that the amount of distortion is dependent on the accuracy, as measured by the
number of bits, in the A/D conversion process. The factors affecting the choice of
the desired accuracy of the A/D converter are cost and sampling rate. In general,
the cost increases with an increase in accuracy and/or sampling rate.

Sampling of Analog Signals

There are many ways to sample an analog signal. We limit our discussion to periodic
or uniform sampling, which is the type of sampling used most often in practice. This
is described by the relation

x(n) = xa(nT ), −∞ < n <∞

where x(n) is the discrete-time signal obtained by “taking samples” of the analog
signal xa(t) every T seconds. The time
interval T between successive samples is called the sampling period or sample interval
and its reciprocal 1/T = Fs is called the sampling rate (samples per second) or the
sampling frequency (hertz).

Periodic sampling establishes a relationship between the time variables t and n
of continuous-time and discrete-time signals, respectively. Indeed, these variables
are linearly related through the sampling period T or, equivalently, through the
sampling rate Fs = 1/T , as

t = nT = n
Fs

variable F (or �) for analog signals and the frequency variable f (or ω) for discrete-
time signals. To establish this relationship, consider an analog sinusoidal signal of
the form

xa(t) = A cos(2πF t + θ)

Analog
signal

Discrete-time
signal

xa(t) x(n) = xa(nT)

Fs = 1/T

xa(t)

Sampler

xa(t)

0 t

x(n)

0

T 2T

1 2 3 4 5 6 7 8 9

… 5T … … t = nT9T

n

x(n) = xa(nT)

Periodic sampling of an analog signal.

Introduction

4.1

(4.1)

(4.2)

This procedure is illustrated in Fig. 4.3.

As a consequence of (4.2), there exists a relationship between the frequency

(4.3)

Figure 4.3

21



which, when sampled periodically at a rate Fs

xa(nT ) ≡ x(n) = A cos(2πFnT + θ)

= A cos
(

2πnF
Fs
+ θ

)

f

are linearly related as

f = F
Fs

or, equivalently, as
ω = �T

s is known.

for continuous-

−∞ < F <∞
−∞ < � <∞

However, the situation is different for discrete-time sinusoids.

−1
2
< f <

1
2

−π < ω < π

continuous-time sinusoid when sampled at a rate Fs = 1/T must fall in the range

− 1
2T
= −Fs

2
≤ F ≤ Fs

2
= 1

2T

−π
T
= −πFs ≤ � ≤ πFs = π

T

From these relations we observe that the fundamental difference between con-
tinuous-time and discrete-time signals is in their range of values of the frequency
variables F and f , or � and ω . Periodic sampling of a continuous-time signal
implies a mapping of the infinite frequency range for the variable F (or �) into a
finite frequency range for the variable f (or ω). Since the highest frequency in a

Introduction

= 1/T samples per second, yields

(4.4)

(4.5)

(4.6)

The relation in (4.5) justifies the name relative or normalized frequency, which is

f to determine the frequency F in hertz only if the sampling frequency F

If we compare (4.4) with (3.9), we note that the frequency variables F and

sometimes used to describe the frequency variable f . As (4.5) implies, we can use

or �

(4.7)

(4.8)

By substituting from (4.5) and (4.6) into (4.8), we find that the frequency of the

or, equivalently,

(4.9)

(4.10)

These relations are summarized in Table 1.

We recall that the ranges of the frequency variables F
time sinusoids
from   Section  3.1

are

we recall that
 From  Section  3.2

22



Relations Among Frequency Variables

Continuous-time signals Discrete-time signals

� = 2πF ω = 2πf
radians

sec Hz
radians
sample

cycles
sample

ω = ΩT, f = F/Fs

Ω = ω/T,F = f � Fs

−π ≤ ω ≤ π
− 12 ≤ f ≤ 12

−∞ < � <∞ −π/T ≤ � ≤ π/T
−∞ < F <∞ −F2/2 ≤ F ≤ Fs/2

discrete-time signal is ω = π or f = 12 , it follows that, with a sampling rate Fs , the
corresponding highest values of F and � are

Fmax = Fs2 =
1

2T

�max = πFs = π
T

Therefore, sampling introduces an ambiguity, since the highest frequency in a contin-
uous-time signal that can be uniquely distinguished when such a signal is sampled at
a rate Fs = 1/T is Fmax = Fs/2, or �max = πFs . To see what happens to frequencies
above Fs/2, let us consider the following example.

The implications of these frequency relations can be fully appreciated by considering the two
analog sinusoidal signals

x1(t) = cos 2π(10)t

x2(t) = cos 2π(50)t
which are sampled at a rate Fs = 40 Hz. The corresponding discrete-time signals or sequences
are

x1(n) = cos 2π
(

10
40

)
n = cos π

2
n

x2(n) = cos 2π
(

50
40

)
n = cos 5π

2
n

However, cos 5πn/2 = cos(2πn + πn/2) = cosπn/2. Hence x2(n) = x1(n). Thus the sinu-
soidal signals are identical and, consequently, indistinguishable. If we are given the sampled
values generated by cos(π/2)n, there is some ambiguity as to whether these sampled values
correspond to x1(t) or x2(t). Since x2(t) yields exactly the same values as x1(t) when the two
are sampled at Fs = 40 samples per second, we say that the frequency F2 = 50 Hz is an alias
of the frequency F1 = 10 Hz at the sampling rate of 40 samples per second.

Introduction

TABLE 1

(4.11)

(4.12)

EXAMPLE 4.1

(4.13)

23



It is important to note that F2 is not the only alias of F1 . In fact at the sampling rate of
40 samples per second, the frequency F3 = 90 Hz is also an alias of F1 , as is the frequency
F4 = 130 Hz, and so on. All of the sinusoids cos 2π(F1 + 40k)t , k = 1, 2, 3, 4, . . ., sampled at
40 samples per second, yield identical values. Consequently, they are all aliases of F1 = 10 Hz.

In general, the sampling of a continuous-time sinusoidal signal

xa(t) = A cos(2πF0t + θ)

with a sampling rate Fs = 1/T results in a discrete-time signal

x(n) = A cos(2πf0n+ θ)

where f0 = F0/Fs is the relative frequency of the sinusoid. If we assume that
−Fs/2 ≤ F0 ≤ Fs/2, the frequency f0 of x(n) is in the range − 12 ≤ f0 ≤ 12 , which is
the frequency range for discrete-time signals. In this case, the relationship between
F0 and f0 is one-to-one, and hence it is possible to identify (or reconstruct) the analog
signal xa(t) from the samples x(n).

On the other hand, if the sinusoids

xa(t) = A cos(2πFkt + θ)

where

Fk = F0 + kFs, k = ±1,±2, . . .

s k

frequency range −Fs/2 ≤ F ≤ Fs/2. Consequently, the sampled signal is

x(n) ≡ xa(nT ) = A cos
(

2π
F0 + kFs
Fs

n+ θ
)

= A cos(2πnF0/Fs + θ + 2πkn)
= A cos(2πf0n+ θ)

Thus an infinite number of continuous-time sinusoids is represented by sampling the
same discrete-time signal (i.e., by the same set of samples). Consequently, if we are
given the sequence x(n), an ambiguity exists as to which continuous-time signal xa(t)
these values represent. Equivalently, we can say that the frequencies Fk = F0+ kFs ,
−∞ < k <∞ (k integer) are indistinguishable from the frequency F0 after sampling
and hence they are aliases of F0 . The relationship between the frequency variables

Introduction

(4.14)

(4.15)

(4.16)

are sampled at a rate F , it is clear that the frequency F is outside the fundamental

(4.17)

which is identical to the discrete-time signal in (4.15) obtained by sampling (4.14).

of the continuous-time and discrete-time signals is illustrated in Fig. 4.4.

24



Relationship between
the continuous-time and
discrete-time frequency
variables in the case of
periodic sampling.

f ω

− 

−

− π1
2

2

π
1
2

0 Fs Fs
F

2

Fs−Fs

frequencies F0 = 18 Hz and F1 = − 78 Hz yield identical samples when a sam-
pling rate of Fs = 1 Hz is used.
F0 = F1 + Fs = (− 78 + 1) Hz = 18 Hz.

Since Fs/2, which corresponds to ω = π , is the highest frequency that can be
represented uniquely with a sampling rate Fs , it is a simple matter to determine the
mapping of any (alias) frequency above Fs/2 (ω = π) into the equivalent frequency
below Fs/2. We can use Fs/2 or ω = π as the pivotal point and reflect or “fold” the
alias frequency to the range 0 ≤ ω ≤ π . Since the point of reflection is Fs/2 (ω = π),
the frequency Fs/2 (ω = π) is called the folding frequency.

1
0

2 3 5 6 7 8 Time, sec

A
m

pl
itu

de

Fs = 1 Hz

4

F1 = − 
7
 Hz

8
F2 =

1
 Hz

8

Illustration of aliasing.

Consider the analog signal
xa(t) = 3 cos 100πt

(a) Determine the minimum sampling rate required to avoid aliasing.
(b) Suppose that the signal is sampled at the rate Fs = 200 Hz. What is the discrete-time

signal obtained after sampling?

Introduction

Figure 4.4

An example of aliasing is illustrated in Fig. 4.5, where two sinusoids with

From (4.17) it easily follows that for k = −1,

Figure 4.5

EXAMPLE 4.2

25



(c) Suppose that the signal is sampled at the rate Fs = 75 Hz. What is the discrete-time
signal obtained after sampling?

(d) What is the frequency 0 < F < Fs/2 of a sinusoid that yields samples identical to those
obtained in part (c)?

Solution.
(a) The frequency of the analog signal is F = 50 Hz. Hence the minimum sampling rate

required to avoid aliasing is Fs = 100 Hz.
(b) If the signal is sampled at Fs = 200 Hz, the discrete-time signal is

x(n) = 3 cos 100π
200

n = 3 cos π
2
n

(c) If the signal is sampled at Fs = 75 Hz, the discrete-time signal is

x(n) = 3 cos 100π
75

n = 3 cos 4π
3
n

= 3 cos
(

2π − 2π
3

)
n

= 3 cos 2π
3
n

(d) For the sampling rate of Fs = 75 Hz, we have

F = fFs = 75f

The frequency of the sinusoid in part (c) is f = 13 . Hence

F = 25 Hz

Clearly, the sinusoidal signal

ya(t) = 3 cos 2πF t

= 3 cos 50πt

sampled at Fs = 75 samples/s yields identical samples. Hence F = 50 Hz is an alias of
F = 25 Hz for the sampling rate Fs = 75 Hz.

The Sampling Theorem

Given any analog signal, how should we select the sampling period T or, equivalently,
the sampling rate Fs ? To answer this question, we must have some information about
the characteristics of the signal to be sampled. In particular, we must have some gen-
eral information concerning the frequency content of the signal. Such information is
generally available to us. For example, we know generally that the major frequency
components of a speech signal fall below 3000 Hz. On the other hand, television

Introduction

4.2

26



signals, in general, contain important frequency components up to 5 MHz. The in-
formation content of such signals is contained in the amplitudes, frequencies, and
phases of the various frequency components, but detailed knowledge of the charac-
teristics of such signals is not available to us prior to obtaining the signals. In fact,
the purpose of processing the signals is usually to extract this detailed information.
However, if we know the maximum frequency content of the general class of signals
(e.g., the class of speech signals, the class of video signals, etc.), we can specify the
sampling rate necessary to convert the analog signals to digital signals.

Let us suppose that any analog signal can be represented as a sum of sinusoids
of different amplitudes, frequencies, and phases, that is,

xa(t) =
N∑
i=1

Ai cos(2πFit + θi)

where N denotes the number of frequency components. All signals, such as speech
and video, lend themselves to such a representation over any short time segment.
The amplitudes, frequencies, and phases usually change slowly with time from one
time segment to another. However, suppose that the frequencies do not exceed some
known frequency, say Fmax . For example, Fmax = 3000 Hz for the class of speech
signals and Fmax = 5 MHz for television signals. Since the maximum frequency may
vary slightly from different realizations among signals of any given class (e.g., it may
vary slightly from speaker to speaker), we may wish to ensure that Fmax does not
exceed some predetermined value by passing the analog signal through a filter that
severely attenuates frequency components above Fmax . Thus we are certain that no
signal in the class contains frequency components (having significant amplitude or
power) above Fmax . In practice, such filtering is commonly used prior to sampling.

From our knowledge of Fmax , we can select the appropriate sampling rate. We
know that the highest frequency in an analog signal that can be unambiguously
reconstructed when the signal is sampled at a rate Fs = 1/T is Fs/2. Any frequency
above Fs/2 or below −Fs/2 results in samples that are identical with a corresponding
frequency in the range −Fs/2 ≤ F ≤ Fs/2. To avoid the ambiguities resulting from
aliasing, we must select the sampling rate to be sufficiently high. That is, we must
select Fs/2 to be greater than Fmax . Thus to avoid the problem of aliasing, Fs is
selected so that

Fs > 2Fmax

where Fmax is the largest frequency component in the analog signal. With the sam-
pling rate selected in this manner, any frequency component, say |Fi | < Fmax , in the
analog signal is mapped into a discrete-time sinusoid with a frequency

−1
2
≤ fi = Fi

Fs
≤ 1

2

or, equivalently,
−π ≤ ωi = 2πfi ≤ π

Introduction

(4.18)

(4.19)

(4.20)

(4.21)

Since,  is the highest (unique) frequency in a discrete-time sig-
nal, the choice of sampling rate according to (4.19) avoids the problem of aliasing.

|f | = 12 or |ω| = π

27



In other words, the condition Fs > 2Fmax ensures that all the sinusoidal compo-
nents in the analog signal are mapped into corresponding discrete-time frequency
components with frequencies in the fundamental interval. Thus all the frequency
components of the analog signal are represented in sampled form without ambigu-
ity, and hence the analog signal can be reconstructed without distortion from the
sample values using an “appropriate” interpolation (digital-to-analog conversion)
method. The “appropriate” or ideal interpolation formula is specified by the sam-
pling theorem.

Sampling Theorem. If the highest frequency contained in an analog signal xa(t) is
Fmax = B and the signal is sampled at a rate Fs > 2Fmax ≡ 2B , then xa(t) can be

g(t) = sin 2πBt
2πBt

Thus xa(t) may be expressed as

xa(t) =
∞∑

n=−∞
xa

(
n

Fs

)
g

(
t − n

Fs

)

where xa(n/Fs) = xa(nT ) ≡ x(n) are the samples of xa(t).
When the sampling of xa(t) is performed at the minimum sampling rate Fs = 2B ,

xa(t) =
∞∑

n=−∞
xa

( n
2B

) sin 2πB(t − n/2B)
2πB(t − n/2B)

N = 2B = 2Fmax

Ideal D/A conversion
(interpolation).

sample of xa(t)xa(t)

(n − 2)T (n − 1)T (n + 1)T

t

nT

Introduction

exactly recovered from its sample values using the interpolation function

(4.22)

(4.23)

the reconstruction formula in (4.23) becomes

The sampling rate F

(4.24)

is called the Nyquist rate. Figure  4.6 illustrates
the ideal D/A conversion process using the interpolation function in (4.22).

Figure 4.6

As can be observed from either (4.23) or (4.24), the reconstruction of
from the sequence is a complicated process, involving a weighted sum of the 
interpolation function  and its time-shifted versions

 Because of the complexity and the 
infinite number of samples required in (4.23) or (4.24), these reconstruction formu-
las are primarily of theoretical interest.

 

 

a

x(n)

g(t) g(t−nT ) for−∞< n<∞,

x (t)

where the weighting factors are the samples x(n).

28



Consider the analog signal

xa(t) = 3 cos 50πt + 10 sin 300πt − cos 100πt

What is the Nyquist rate for this signal?

Solution. The frequencies present in the signal above are

F1 = 25 Hz, F2 = 150 Hz, F3 = 50 Hz

Thus Fmax

Fs > 2Fmax = 300 Hz

The Nyquist rate is FN = 2Fmax . Hence

FN = 300 Hz

Discussion. It should be observed that the signal component 10 sin 300πt , sampled at the
Nyquist rate FN = 300, results in the samples 10 sin πn, which are identically zero. In other
words, we are sampling the analog sinusoid at its zero-crossing points, and hence we miss this
signal component completely. This situation does not occur if the sinusoid is offset in phase
by some amount θ . In such a case we have 10 sin(300πt + θ) sampled at the Nyquist rate
FN = 300 samples per second, which yields the samples

10 sin(πn+ θ) = 10(sin πn cos θ + cosπn sin θ)

= 10 sin θ cosπn

= (−1)n10 sin θ

Thus if θ �= 0 or π , the samples of the sinusoid taken at the Nyquist rate are not all zero.
However, we still cannot obtain the correct amplitude from the samples when the phase θ is
unknown. A simple remedy that avoids this potentially troublesome situation is to sample the
analog signal at a rate higher than the Nyquist rate.

Consider the analog signal

xa(t) = 3 cos 2000πt + 5 sin 6000πt + 10 cos 12,000πt

(a) What is the Nyquist rate for this signal?

(b) Assume now that we sample this signal using a sampling rate Fs = 5000 samples/s. What
is the discrete-time signal obtained after sampling?

(c) What is the analog signal ya(t) that we can reconstruct from the samples if we use ideal
interpolation?

Introduction

EXAMPLE 4.3

EXAMPLE 4.4

= 150 Hz and according to (4.19),

29



Solution.
(a) The frequencies existing in the analog signal are

F1 = 1 kHz, F2 = 3 kHz, F3 = 6 kHz

Thus Fmax = 6 kHz, and according to the sampling theorem,

Fs > 2Fmax = 12 kHz

The Nyquist rate is
FN = 12 kHz

(b) Since we have chosen Fs = 5 kHz, the folding frequency is
Fs

2
= 2.5 kHz

and this is the maximum frequency that can be represented uniquely by the sampled

x(n) = xa(nT ) = xa
(
n

Fs

)

= 3 cos 2π ( 15 ) n+ 5 sin 2π ( 35 ) n+ 10 cos 2π ( 65 ) n
= 3 cos 2π ( 15 ) n+ 5 sin 2π (1− 25 ) n+ 10 cos 2π (1+ 15 ) n
= 3 cos 2π ( 15 ) n+ 5 sin 2π (− 25 ) n+ 10 cos 2π ( 15 ) n

Finally, we obtain
x(n) = 13 cos 2π ( 15 ) n− 5 sin 2π ( 25 ) n

s = 5 kHz, the folding
frequency is Fs/2 = 2.5 kHz. This is the maximum frequency that can be represented

0 = Fk − kFs . Thus F0 can be
obtained by subtracting from Fk an integer multiple of Fs such that −Fs/2 ≤ F0 ≤ Fs/2.
The frequency F1 is less than Fs/2 and thus it is not affected by aliasing. However, the
other two frequencies are above the folding frequency and they will be changed by the
aliasing effect. Indeed,

F ′2 = F2 − Fs = −2 kHz

F ′3 = F3 − Fs = 1 kHz

1 = 15 , f2 = − 25 , and f3 = 15 , which are in agreement with
the result above.

(c) Since the frequency components at only 1 kHz and 2 kHz are present in the sampled
signal, the analog signal we can recover is

ya(t) = 13 cos 2000πt − 5 sin 4000πt

which is obviously different from the original signal xa(t). This distortion of the original
analog signal was caused by the aliasing effect, due to the low sampling rate used.

Introduction

signal. By making use of (4.2) we obtain

The same result can be obtained using Fig. 4.4. Indeed, since F

uniquely by the sampled signal. From (4.17) we have F

From (4.5) it follows that f

30



Although aliasing is a pitfall to be avoided, there are two useful practical ap-
plications based on the exploitation of the aliasing effect. These applications are
the stroboscope and the sampling oscilloscope. Both instruments are designed to
operate as aliasing devices in order to represent high frequencies as low frequencies.

To elaborate, consider a signal with high-frequency components confined to a
given frequency band B1 < F < B2 , where B2−B1 ≡ B is defined as the bandwidth
of the signal. We assume that B << B1 < B2 . This condition means that the
frequency components in the signal are much larger than the bandwidth B of the
signal. Such signals are usually called bandpass or narrowband signals. Now, if this
signal is sampled at a rate Fs ≥ 2B , but Fs << B1 , then all the frequency components
contained in the signal will be aliases of frequencies in the range 0 < F < Fs/2.
Consequently, if we observe the frequency content of the signal in the fundamental
range 0 < F < Fs/2, we know precisely the frequency content of the analog signal
since we know the frequency band B1 < F < B2 under consideration. Consequently,
if the signal is a narrowband (bandpass) signal, we can reconstruct the original signal
from the samples, provided that the signal is sampled at a rate Fs > 2B , where B
is the bandwidth. This statement constitutes another form of the sampling theorem,
which we call the bandpass form in order to distinguish it from the previous form of
the sampling theorem, which applies in general to all types of signals. The latter is
sometimes called the baseband form.

Quantization of Continuous-Amplitude Signals

As we have seen, a digital signal is a sequence of numbers (samples) in which each
number is represented by a finite number of digits (finite precision).

The process of converting a discrete-time continuous-amplitude signal into a
digital signal by expressing each sample value as a finite (instead of an infinite)
number of digits is called quantization. The error introduced in representing the
continuous-valued signal by a finite set of discrete value levels is called quantization
error or quantization noise.

We denote the quantizer operation on the samples x(n) as Q[x(n)] and let xq(n)
denote the sequence of quantized samples at the output of the quantizer. Hence

xq(n) = Q[x(n)]

Then the quantization error is a sequence eq(n) defined as the difference between
the quantized value and the actual sample value. Thus

eq(n) = xq(n)− x(n)

We illustrate the quantization process with an example. Let us consider the discrete-
time signal

x(n) =
{

0.9n, n ≥ 0
0, n < 0

Introduction

4.3

(4.25)

31



0 1 2 3 4 5 6 7

T

T = 1 sec

8 n…

…
0.2

0.4

0.6

0.8

1.0
x(n) = 0.9n

xa(t) = 0.9t

(a)

0 1 2 3 4 5 6 7 8 n…

…
0.2

0.4

0.6

0.8

1.0

0.1

0.3

0.5

0.7

0.9

xa(t) = 0.9t xq(n)
Levels of 
quantization

(b)

Range of
the
quantizer

Quantization
step∆

Illustration of quantization.

obtained by sampling the analog exponential signal xa(t) = 0.9t , t ≥ 0 with a sam-
pling frequency Fs
the values of the first 10 samples of x(n), reveals that the description of the sample
value x(n) requires n significant digits. It is obvious that this signal cannot be pro-
cessed by using a calculator or a digital computer since only the first few samples
can be stored and manipulated. For example, most calculators process numbers with
only eight significant digits.

However, let us assume that we want to use only one significant digit. To elim-
inate the excess digits, we can either simply discard them (truncation) or discard
them by rounding the resulting number (rounding). The resulting quantized signals
q We discuss only quantization by rounding, although

it is just as easy to treat truncation. The rounding process is graphically illustrated

levels, whereas the distance � between two successive quantization levels is called
the quantization step size or resolution. The rounding quantizer assigns each sam-
ple of x(n) to the nearest quantization level. In contrast, a quantizer that performs
truncation would have assigned each sample of x(n) to the quantization level below

Introduction

Figure 4.7

= 1 Hz (see Fig. 4.7(a)). Observation of Table 2, which shows

x (n) are shown in Table 2.

in Fig. 4.7(b). The values allowed in the digital signal are called the quantization

32



x(n) xq(n) xq(n) eq(n) = xq(n)− x(n)
n Discrete-time signal (Truncation) (Rounding) (Rounding)

0 1 1.0 1.0 0.0

1 0.9 0.9 0.9 0.0

2 0.81 0.8 0.8 −0.01
3 0.729 0.7 0.7 −0.029
4 0.6561 0.6 0.7 0.0439

5 0.59049 0.5 0.6 0.00951

6 0.531441 0.5 0.5 −0.031441
7 0.4782969 0.4 0.5 0.0217031

8 0.43046721 0.4 0.4 −0.03046721
9 0.387420489 0.3 0.4 0.012579511

it. The quantization error eq(n) in rounding is limited to the range of −�/2 to �/2,
that is,

−�
2
≤ eq(n) ≤ �2

In other words, the instantaneous quantization error cannot exceed half of the quan-

If xmin and xmax represent the minimum and maximum values of x(n) and L is
the number of quantization levels, then

� = xmax − xmin
L− 1

We define the dynamic range of the signal as xmax − xmin . In our example we have
xmax = 1, xmin = 0, and L = 11, which leads to � = 0.1. Note that if the dynamic
range is fixed, increasing the number of quantization levels L results in a decrease of
the quantization step size. Thus the quantization error decreases and the accuracy
of the quantizer increases. In practice we can reduce the quantization error to an
insignificant amount by choosing a sufficient number of quantization levels.

Theoretically, quantization of analog signals always results in a loss of informa-
tion. This is a result of the ambiguity introduced by quantization. Indeed, quantiza-
tion is an irreversible or noninvertible process (i.e., a many-to-one mapping) since
all samples in a distance �/2 about a certain quantization level are assigned the
same value. This ambiguity makes the exact quantitative analysis of quantization
extremely difficult.

Introduction

TABLE 2

(4.26)

tization step (see Table 2).

(4.27)

Numerical Illustration of Quantization with One Significant Digit Using  
Truncation or Rounding

33



Sampling and quantization of a sinusoidal signal.

Quantization of Sinusoidal Signals

xa(t) = A cos�0t using a rectangular grid. Horizontal lines within the range of
the quantizer indicate the allowed levels of quantization. Vertical lines indicate the
sampling times. Thus, from the original analog signal xa(t) we obtain a discrete-time
signal x(n) = xa(nT ) by sampling and a discrete-time, discrete-amplitude signal
xq(nT ) after quantization. In practice, the staircase signal xq(t) can be obtained by
using a zero-order hold. This analysis is useful because sinusoids are used as test
signals in A/D converters.

If the sampling rate Fs satisfies the sampling theorem, quantization is the only
error in the A/D conversion process.

Thus we can evaluate the quantization error by quantizing the analog signal xa(t)
a

a

q a q

Introduction

Figure 4.8

4.4

Figure 4.8 illustrates the sampling and quantization of an analog sinusoidal signal

instead of the discrete-time signal x(n) = x (nT ) . Inspection of Fig. 4.8 indicates

The quantization error e (t) = x (t)− x (t).Figure 4.9

that the signal x (t) is almost linear between quantization levels (see Fig. 4.9). The

34



q a q In
a The

mean-square error power Pq is

Pq = 12τ
∫ τ
−τ
e2q(t) dt =

1
τ

∫ τ
0
e2q(t) dt

Since eq(t) = (�/2τ)t , −τ ≤ t ≤ τ , we have

Pq = 1
τ

∫ τ
0

(
�

2τ

)2
t2 dt = �

2

12

the quantization step is � = 2A/2b . Hence

Pq = A
2/3

22b

The average power of the signal xa(t) is

Px = 1
Tp

∫ Tp
0
(A cos�0t)2 dt = A

2

2

The quality of the output of the A/D converter is usually measured by the signal-to-
quantization noise ratio (SQNR), which provides the ratio of the signal power to the
noise power:

SQNR = Px
Pq
= 3

2
· 22b

Expressed in decibels (dB), the SQNR is

SQNR(dB) = 10 log10 SQNR = 1.76+ 6.02b
This implies that the SQNR increases approximately 6 dB for every bit added to the
word length, that is, for each doubling of the quantization levels.

Coding of Quantized Samples

The coding process in an A/D converter assigns a unique binary number to each
quantization level. If we have L levels we need at least L different binary numbers.
With a word length of b bits we can create 2b different binary numbers. Hence
we have 2b ≥ L, or equivalently, b ≥ log2 L. Thus the number of bits required in
the coder is the smallest integer greater than or equal to log2 L. In our example

available A/D converters may be obtained with finite precision of b = 16 or less.
Generally, the higher the sampling speed and the finer the quantization, the more
expensive the device becomes.

Introduction

corresponding quantization error e (t) = x (t) − x (t) is shown in Fig. 4.9.
denotes the time that x (t) stays within the quantization levels.

(4.28)

If the quantizer has b bits of accuracy and the quantizer covers the entire range 2A,

(4.29)

(4.30)

(4.31)

(4.32)

4.5

(Table 2) it can easily be seen that we need a coder with b = 4 bits. Commercially

Fig. 4.9, τ

Although formula (4.32) was derived for sinusoidal signals, we shall a similar 
result holds for every signal whose dynamic range spans the range of the quantizer. 
This relationship is extremely important because it dictates the number of bits re-
quired by a specific application to assure a given signal-to-noise ratio. For example, 
most compact disc players use a sampling frequency of 44.1 kHz and 16-bit sample 
resolution, which implies a SQNR of more than 96 dB.

35



Linear point connector
(with T -second delay). T0 2T 3T 4T 5T 6T 7T

t

A
m

pl
itu

de

Original signal

Linear interpolation (with T-second delay)

Digital-to-Analog Conversion

To convert a digital signal into an analog signal we can use a digital-to-analog (D/A)
converter. As stated previously, the task of a D/A converter is to interpolate between
samples.

In general, suboptimum interpolation techniques result in passing frequencies
above the folding frequency. Such frequency components are undesirable and are
usually removed by passing the output of the interpolator through a proper analog
filter, which is called a postfilter or smoothing filter.

Thus D/A conversion usually involves a suboptimum interpolator followed by a
postfilter.

Analysis of Digital Signals and Systems Versus Discrete-Time
Signals and Systems

We have seen that a digital signal is defined as a function of an integer independent
variable and its values are taken from a finite set of possible values. The usefulness
of such signals is a consequence of the possibilities offered by digital computers.
Computers operate on numbers, which are represented by a stringof 0’s and 1’s. The
length of this string (word length) is fixed and finite and usually is 8, 12, 16, or 32 bits.
The effects of finite word length in computations cause complications in the analysis
of digital signal processing systems. To avoid these complications, we neglect the
quantized nature of digital signals and systems in much of our analysis and consider
them as discrete-time signals and systems.

Introduction

Figure 4.10

4.6

4.7

point

The sampling theorem specifies the optimum interpolation for a bandlimited 
signal. However, this type of interpolation is too complicated and, hence, impracti-
cal, as indicated previously. From a practical viewpoint, the simplest D/A converter 
is the zero-order hold shown in Fig. 4.2, which simply holds constant the value of 
one sample until the next one is received. Additional improvement can be obtained 
by using linear

interpolation as shown in Fig. 4.10 to connect successive samples with straight- 
line segments. Better interpolation can be achieved by using more sophisticated 
higher-order interpolation techniques.

finite word length is an important topic, since many digital signal processing  prob-
lems are solved with small computers or microprocessors that employ fixed-

Although not covered in this chapter, investigating the consequences of using a  

36



Consequently, one must look carefully at the problem of finite-precision
arithmetic and account for it in the design of software and hardware that performs
the desired signal processing tasks.

Summary and References

Quantization effects that are inherent in the A/D conversion of a signal were also

Finally, the topic of signal reconstruction, or D/A conversion, was described
briefly.

There are numerous practical applications of digital signal processing. The
book edited by Oppenheim (1978) treats applications to speech processing, image
processing, radar signal processing, sonar signal processing, and geophysical signal
processing.

Problems

Classify the following signals according to whether they are (1) one- or multi-dimen-
sional; (2) single or multichannel, (3) continuous time or discrete time, and (4) analog
or digital (in amplitude). Give a brief explanation.

(a) Closing prices of utility stocks on the New York Stock Exchange.

(b) A color movie.

(c) Position of the steering wheel of a car in motion relative to car’s reference frame.

(d) Position of the steering wheel of a car in motion relative to ground reference
frame.

(e) Weight and height measurements of a child taken every month.

2 Determine which of the following sinusoids are periodic and compute their funda-
mental period.

(a) cos 0.01πn (b) cos
(
π 30n105

)
(c) cos 3πn (d) sin 3n (e) sin

(
π 62n10

)

Introduction

5

1

introduced in this chapter. Signal quantization is best treated in statistical terms. 

In this introductory chapter we have attempted to provide the motivation for digital 
signal processing as an alternative to analog signal processing. We presented the ba-
sic elements of a digital signal processing system and defined the operations needed 
to convert an analog signal into a digital signal ready for processing. Of particular 
importance is the sampling theorem, which was introduced by Nyquist (1928) and 
later popularized in the classic paper by Shannon (1949). Sinusoidal signals were 
introduced primarily for the purpose of illustrating the aliasing phenomenon and 
for the subsequent development of the sampling theorem. 

 arithmetic.

37



3 Determine whether or not each of the following signals is periodic. In case a signal
is periodic, specify its fundamental period.

(a) xa(t) = 3 cos(5t + π/6)
(b) x(n) = 3 cos(5n+ π/6)
(c) x(n) = 2 exp[j (n/6− π)]
(d) x(n) = cos(n/8) cos(πn/8)
(e) x(n) = cos(πn/2)− sin(πn/8)+ 3 cos(πn/4+ π/3)

4 (a) Show that the fundamental period Np of the signals

sk(n) = ej2πkn/N , k = 0, 1, 2, . . .

is given by Np = N/GCD(k,N), where GCD is the greatest common divisor of
k and N .

(b) What is the fundamental period of this set for N = 7?
(c) What is it for N = 16?

5 Consider the following analog sinusoidal signal:

xa(t) = 3 sin(100πt)

(a) Sketch the signal xa(t) for 0 ≤ t ≤ 30 ms.
(b) The signal xa(t) is sampled with a sampling rate Fs = 300 samples/s. Determine

the frequency of the discrete-time signal x(n) = xa(nT ), T = 1/Fs , and show
that it is periodic.

(c) Compute the sample values in one period of x(n). Sketch x(n) on the same
diagram with xa(t). What is the period of the discrete-time signal in milliseconds?

(d) Can you find a sampling rate Fs such that the signal x(n) reaches its peak value
of 3? What is the minimum Fs suitable for this task?

6 A continuous-time sinusoid xa(t) with fundamental period Tp = 1/F0 is sampled at
a rate Fs = 1/T to produce a discrete-time sinusoid x(n) = xa(nT ).
(a) Show that x(n) is periodic if T/Tp = k/N (i.e., T/Tp is a rational number).
(b) If x(n) is periodic, what is its fundamental period Tp in seconds?

(c) Explain the statement: x(n) is periodic if its fundamental period Tp , in seconds,
is equal to an integer number of periods of xa(t).

7 An analog signal contains frequencies up to 10 kHz.

(a) What range of sampling frequencies allows exact reconstruction of this signal
from its samples?

(b) Suppose that we sample this signal with a sampling frequency Fs = 8 kHz.
Examine what happens to the frequency F1 = 5 kHz.

(c) Repeat part (b) for a frequency F2 = 9 kHz.

Introduction

38



8 An analog electrocardiogram (ECG) signal contains useful frequencies up to 100 Hz.

(a) What is the Nyquist rate for this signal?

(b) Suppose that we sample this signal at a rate of 250 samples/s. What is the highest
frequency that can be represented uniquely at this sampling rate?

9 An analog signal xa(t) = sin(480πt)+ 3 sin(720πt) is sampled 600 times per second.
(a) Determine the Nyquist sampling rate for xa(t).

(b) Determine the folding frequency.

(c) What are the frequencies, in radians, in the resulting discrete time signal x(n)?

(d) If x(n) is passed through an ideal D/A converter, what is the reconstructed signal
ya(t)?

10 A digital communication link carries binary-coded words representing samples of an
input signal

xa(t) = 3 cos 600πt + 2 cos 1800πt
The link is operated at 10,000 bits/s and each input sample is quantized into 1024
different voltage levels.

(a) What are the sampling frequency and the folding frequency?

(b) What is the Nyquist rate for the signal xa(t)?

(c) What are the frequencies in the resulting discrete-time signal x(n)?

(d) What is the resolution �?

11 The sampling
periods of the A/D and D/A converters are T = 5 ms and T ′ = 1 ms, respectively.
Determine the output ya(t) of the system, if the input is

xa(t) = 3 cos 100πt + 2 sin 250πt (t in seconds)

The postfilter removes any frequency component above Fs/2.

xa(t) ya(t)x(n)A/D
T

D/A
T ′

Postfilter

12
the periodicity properties of sinusoidal functions.

(b) What is the analog signal we can obtain from x(n) if in the reconstruction process
we assume that Fs = 10 kHz?

13 The discrete-time signal x(n) = 6.35 cos(π/10)n is quantized with a resolution (a)� =
0.1 or (b) � = 0.02. How many bits are required in the A/D converter in each case?

Introduction

Figure P11

(a) Derive the expression for the discrete-time signal x(n) in Example 4.2 using

Consider the simple signal processing system shown in Fig. P11.

39



14 Determine the bit rate and the resolution in the sampling of a seismic signal with
dynamic range of 1 volt if the sampling rate is Fs = 20 samples/s and we use an 8-bit
A/D converter. What is the maximum frequency that can be present in the resulting
digital seismic signal?

15 Sampling of sinusoidal signals: aliasing Consider the following continuous-time
sinusoidal signal

xa(t) = sin 2πF0t, −∞ < t <∞
Since xa(t) is described mathematically, its sampled version can be described by
values every T seconds. The sampled signal is described by the formula

x(n) = xa(nT ) = sin 2π F0
Fs
n, −∞ < n <∞

where Fs = 1/T is the sampling frequency.
(a) Plot the signal x(n), 0 ≤ n ≤ 99 for Fs = 5 kHz and F0 = 0.5, 2, 3, and 4.5 kHz.

Explain the similarities and differences among the various plots.

(b) Suppose that F0 = 2 kHz and Fs = 50 kHz.
1. Plot the signal x(n). What is the frequency f0 of the signal x(n)?

2. Plot the signal y(n) created by taking the even-numbered samples of x(n).
Is this a sinusoidal signal? Why? If so, what is its frequency?

16 Quantization error in A/D conversion of a sinusoidal signal Let xq(n) be the signal
obtained by quantizing the signal x(n) = sin 2πf0n. The quantization error power
Pq is defined by

Pq = 1
N

N−1∑
n=0

e2(n) = 1
N

N−1∑
n=0

[xq(n)− x(n)]2

The “quality” of the quantized signal can be measured by the signal-to-quantization
noise ratio (SQNR) defined by

SQNR = 10 log10
Px

Pq

where Px is the power of the unquantized signal x(n).

(a) For f0 = 1/50 and N = 200, write a program to quantize the signal x(n), using
truncation, to 64, 128, and 256 quantization levels. In each case plot the signals
x(n), xq(n), and e(n) and compute the corresponding SQNR.

(b) Repeat part (a) by using rounding instead of truncation.

(c) Comment on the results obtained in parts (a) and (b).

(d) Compare the experimentally measured SQNR with the theoretical SQNR pre-

Introduction

dicted by formula (4.32) and comment on the differences and similarities.

40



Introduction

Answers to Selected Problems

1 (a) One dimensional, multichannel, discrete time, and digital.

(b) Multi dimensional, single channel, continuous-time, analog.

(c) One dimensional, single channel, continuous-time, analog.

(d) One dimensional, single channel, continuous-time, analog.

(e) One dimensional, multichannel, discrete-time, digital.

3 (a) Periodic with period Tp = 2π5 .
(c) f = 112π ⇒ non-periodic.
(e) cos

(
πn

2

)
is periodic with periodNp = 4; sin

(
πn

8

)
is periodic with periodNp = 16; cos

(
πn

4 + π3
)

is periodic with period Np = 8. Therefore, x(n) is periodic with period Np = 16 (16 is the
least common multiple of 4, 8, 16).

4 (b) N = 7; k = 0 1 2 3 4 5 6 7; GCD(k,N) = 7 1 1 1 1 1 1 1 7; Np = 1 7 7 7 7 7 1.
5 (a) Yes. x(1) = 3 = 3 sin

(
100π
Fs

)
⇒ 200 samples/sec.

6 (b) If x(n) is periodic, then f = k/N where N is the period. Then, Td =
(
k

f
T
)
= k

(
Tp

T

)
T =

kTp . Thus, it takes k periods (kTp) of the analog signal to make 1 period (Td) of the discrete
signal.

8 (a) Fmax = 100Hz , Fs ≥ 2Fmax = 200 Hz.
(b) Ffold = Fx2 = 125 Hz.

10 (a) Fs = 1000 samples/sec; Ffold = 500 Hz.
(b) Fmax = 900 Hz; FN = 2Fmax = 1800 Hz.
(c) f1 = 0.3; f2 = 0.9; But f2 = 0.9 > 0.5 ⇒ f2 = 0.1. Hence, x(n) = 3 cos[(2π)(0.3)n] +

2 cos[(2π)(0.1)n].

(d) � = 101023 .

41



This page intentionally left blank 



Discrete-Time Signals
and Systems

The major emphasis in this chapter is the characterization of discrete-time sys-
tems in general and the class of linear time-invariant (LTI) systems in particular. A
number of important time-domain properties of LTI systems are defined and devel-
oped, and an important formula, called the convolution formula, is derived which
allows us to determine the output of an LTI system to any given arbitrary input sig-
nal. In addition to the convolution formula, difference equations are introduced as
an alternative method for describing the input–output relationship of an LTI system,
and in addition, recursive and nonrecursive realizations of LTI systems are treated.

Our motivation for the emphasis on the study of LTI systems is twofold. First,
there is a large collection of mathematical techniques that can be applied to the anal-
ysis of LTI systems. Second, many practical systems are either LTI systems or can be
approximated by LTI systems. Because of its importance in digital signal processing
applications and its close resemblance to the convolution formula, we also introduce
the correlation between two signals. The autocorrelation and crosscorrelation of
signals are defined and their properties are presented.

The sinusoid is an important elementary signal that serves as a basic building block 
in more complex signals. However, there are other elementary signals that are im-
portant in our treatment of signal processing. These discrete-time signals are intro-
duced in this chapter and are used as basis functions or building blocks to describe 
more complex signals.

From Chapter 2 of Digital Signal Processing: Principles, Algorithms, and Applications, Fourth Edition.
John G. Proakis, Dimitris G. Manolakis. Copyright © 2007 by Pearson Education, Inc. All rights reserved.

43



In the sequel we will assume that a discrete-time signal is defined for every integer
value n for −∞ < n < ∞. By tradition, we refer to x(n) as the “nth sample” of
the signal even if the signal x(n) is inherently discrete time (i.e., not obtained by
sampling an analog signal). If, indeed, x(n) was obtained from sampling an analog
signal xa(t), then x(n) ≡ xa(nT ), where T is the sampling period (i.e., the time
between successive samples).

1. Functional representation, such as

x(n) =
{ 1, for n = 1, 3

4, for n = 2
0, elsewhere

2. Tabular representation, such as

n

x(n)

∣∣∣∣ · · · −2 −1 0 1 2 3 4 5 . . .· · · 0 0 0 1 4 1 0 0 . . .
3. Sequence representation

An infinite-duration signal or sequence with the time origin (n = 0) indicated by the
symbol ↑ is represented as

x(n) = {. . . 0, 0
↑
, 1, 4, 1, 0, 0, . . .}

A sequence x(n), which is zero for n < 0, can be represented as

x(n) = {0
↑
, 1, 4, 1, 0, 0, . . .}

−4

−3

−2 −1 0 1 2 3

4

5

……

0.9

−0.8 −0.8

0.7 0.7

1.5
2

n

x(n)

1.7

1.0 1.2

 is a function of an independent variable that is an integer. 

signal is not defined  at instants between two successive samples. Also, it is incorrect 
to think that  is equal to zero if n is not an integer. Simply, the signal  is not 
defined for noninteger values of .

x(n)

x(n) x(n)

n

Discrete-Time Signals

It is graphically represented as in Fig. 1.1. It is important to note that a discrete-time 
A discrete-time signal

1

(1.1)

(1.2)

(1.3)

Graphical representation of a discrete-time signal.Figure 1.1

Besides the graphical representation of  a discrete-time signal or sequence as 
illustrated in Fig. 1.1, there are some alternative representations that are often more 
convenient to use. These are:

Discrete-Time Signals and Systems

44



The time origin for a sequence x(n), which is zero for n < 0, is understood to be the
first (leftmost) point in the sequence.

A finite-duration sequence can be represented as

x(n) = {3,−1,−2
↑
, 5, 0, 4,−1}

whereas a finite-duration sequence that satisfies the condition x(n) = 0 for n < 0
can be represented as

x(n) = {0
↑
, 1, 4, 1}

In our study of discrete-time signals and systems there are a number of basic signals
that appear often and play an important role. These signals are defined below.

1. The unit sample sequence is denoted as δ(n) and is defined as

δ(n) ≡
{

1, for n = 0
0, for n �= 0

In words, the unit sample sequence is a signal that is zero everywhere, except
at n = 0 where its value is unity. This signal is sometimes referred to as a unit
impulse. In contrast to the analog signal δ(t), which is also called a unit impulse
and is defined to be zero everywhere except at t = 0, and has unit area, the
unit sample sequence is much less mathematically complicated. The graphical

2. The unit step signal is denoted as u(n) and is defined as

u(n) ≡
{

1, for n ≥ 0
0, for n < 0

Graphical representation of
the unit sample signal.

Discrete-Time Signals and Systems

(1.4)

(1.5)

Some Elementary Discrete-Time Signals1.1

(1.6)

representation of δ(n) is shown in Fig. 1.2.

(1.7)

Figure 1.3 illustrates the unit step signal.

Figure 1.2

The signal in (1.4) consists of seven samples or points (in time), so it is called or 
identified as a seven-point sequence. Similarly, the sequence given by (1.5) is a four-
point sequence.

45



Graphical representation of
the unit step signal.

…

n

u (n)

1

0 1 2 3 4 5 6 7

3. The unit ramp signal is denoted as ur(n) and is defined as

ur(n) ≡
{
n, for n ≥ 0
0, for n < 0

4. The exponential signal is a sequence of the form

x(n) = an for all n

for various values of the parameter a .

When the parameter a is complex valued, it can be expressed as

a ≡ rejθ

where r and θ are now the parameters. Hence we can express x(n) as

x(n) = rnejθn

= rn(cos θn+ j sin θn)

Since x(n) is now complex valued, it can be represented graphically by plotting the
real part

xR(n) ≡ rn cos θn
as a function of n, and separately plotting the imaginary part

xI (n) ≡ rn sin θn

Graphical representation of
the unit ramp signal.

…

ur(n)

n

Discrete-Time Signals and Systems

Figure 1.3

(1.8)

(1.9)

This signal is illustrated in Fig. 1.4.

If the parameter a is real, then x(n) is a real signal. Figure 1.5 illustrates x(n)

(1.10)

(1.11)

(1.12)

Figure 1.4

46



|x(n)| = A(n) ≡ rn

and the phase function
� x(n) = φ(n) ≡ θn

the phase function is linear with n. However, the phase is defined only over the
interval −π < θ ≤ π or, equivalently, over the interval 0 ≤ θ < 2π . Consequently,
by convention φ(n) is plotted over the finite interval −π < θ ≤ π or 0 ≤ θ < 2π . In
other words, we subtract multiples of 2π from φ(n) before plotting. The subtraction
of multiples of 2π from φ(n) is equivalent to interpreting the function φ(n) as φ(n),
modulo 2π .

Classification of Discrete-Time Signals

time signals according to a number of different characteristics.

The energy E of a signal x(n) is defined as

E ≡
∞∑

n=−∞
|x(n)|2

Graphical representation of exponential signals.

Discrete-Time Signals and Systems

(1.13)

(1.14)

Figure 1.7 illustrates

The mathematical methods employed in the analysis of discrete-time signals and sys-

Energy signals and power signals.

tems depend on the characteristics of the signals. In this section we classify discrete-

1.2

(1.15)

Figure 1.5

as a function of  Figure 1.6 illustrates the graphs of  and 
We observe that the signals  are a damped (decaying expo-

the frequency of the sinusoid, previously denoted by the (normalized) frequency 
variable θ . Clearly, if  the damping disappears and  have 
a fixed amplitude, which is unity.

R(n) and xI (n) for r = 0.9
θ =π/10. xR(n) and xI (n)

θ

ω r=1, xR(n), xI (n), and x(n)

n. x

Alternatively, the signal  given by (1.10) can be represented graphically by 
the amplitude function

x(n)

A(n) and φ(n) for r = 0.9 and θ = π/10. We observe that

nential) cosine function and a damped sine function. The angle variable θ  is simply 

47



We have used the magnitude-squared values of x(n), so that our definition applies
to complex-valued signals as well as real-valued signals. The energy of a signal can
be finite or infinite. If E is finite (i.e., 0 < E < ∞), then x(n) is called an energy
signal. Sometimes we add a subscript x to E and write Ex to emphasize that Ex is
the energy of the signal x(n).

Many signals that possess infinite energy have a finite average power. The aver-
age power of a discrete-time signal x(n) is defined as

P = lim
N→∞

1
2N + 1

N∑
n=−N

|x(n)|2

If we define the signal energy of x(n) over the finite interval −N ≤ n ≤ N as

EN ≡
N∑

n=−N
|x(n)|2

then we can express the signal energy E as

E ≡ lim
N→∞

EN

0 5 10 15 20 25 30 35 40
-1

-0.5

0

0.5

1

n

x R
(n

)

0 5 10 15 20 25 30 35 40
-1

-0.5

0

0.5

1

n

x I
(n

)

(a)

(b)

exponential signal.

Discrete-Time Signals and Systems

(1.16)

(1.17)

(1.18)

Graph of the real and imaginary components of a complex-valuedFigure 1.6

48



and the average power of the signal x(n) as

P ≡ lim
N→∞

1
2N + 1EN

Clearly, if E is finite, P = 0. On the other hand, if E is infinite, the average
power P may be either finite or infinite. If P is finite (and nonzero), the signal is
called a power signal. The following example illustrates such a signal.

Determine the power and energy of the unit step sequence. The average power of the unit
step signal is

P = lim
N→∞

1
2N + 1

N∑
n=0

u2(n)

= lim
N→∞

N + 1
2N + 1 = limN→∞

1+ 1/N
2 + 1/N =

1
2

Consequently, the unit step sequence is a power signal. Its energy is infinite.

Graph of amplitude and phase function of a complex-valued
exponential signal: (a) graph of A(n) = rn , r = 0.9; (b) graph of φ(n) =
(π/10)n, modulo 2π plotted in the range (−π, π].

Discrete-Time Signals and Systems

(1.19)

EXAMPLE 1.1

Figure 1.7

49



Similarly, it can be shown that the complex exponential sequence x(n) = Aejω0n
has average power A2 , so it is a power signal. On the other hand, the unit ramp
sequence is neither a power signal nor an energy signal.

Periodic signals and aperiodic signals.

x(n+N) = x(n) for all n

x(n) = A sin 2πf0n

is periodic when f0 is a rational number, that is, if f0 can be expressed as

f0 = k
N

where k and N are integers.
The energy of a periodic signal x(n) over a single period, say, over the interval

0 ≤ n ≤ N − 1, is finite if x(n) takes on finite values over the period. However, the
energy of the periodic signal for −∞ ≤ n ≤ ∞ is infinite. On the other hand, the
average power of the periodic signal is finite and it is equal to the average power
over a single period. Thus if x(n) is a periodic signal with fundamental period N and
takes on finite values, its power is given by

P = 1
N

N−1∑
n=0
|x(n)|2

Consequently, periodic signals are power signals.

Symmetric (even) and antisymmetric (odd) signals. A real-valued signal x(n) is
called symmetric (even) if

x(−n) = x(n)
On the other hand, a signal x(n) is called antisymmetric (odd) if

x(−n) = −x(n)

We note that if x(n) is odd, then x(0) = 0. Examples of signals with even and odd

Discrete-Time Signals and Systems

N(N>0) A signal is periodic with period   
if and only if

(1.20)

We have already observed that the sinusoidal signal of the form

(1.21)

(1.22)

(1.23)

(1.24)

(1.25)

symmetry are illustrated in Fig. 1.8.

The smallest value of  for which (1.20) holds is called the (fundamental) period. 
If there is no value of  that satisfies (1.20), the signal is called nonperiodic or ape-
riodic.

N

N

ψx(n)

50



−1 0 1 2 3 4−2−3−4

−1 0

1 2 3 4 5

−2−3−4−5

x (n)

x (n)

n

n

(a)

(b)

Example of even (a) and odd (b) signals.

We wish to illustrate that any arbitrary signal can be expressed as the sum of
two signal components, one of which is even and the other odd. The even signal
component is formed by adding x(n) to x(−n) and dividing by 2, that is,

xe(n) = 12[x(n)+ x(−n)]

e Similarly, we form an odd
signal component xo(n) according to the relation

xo(n) = 12[x(n)− x(−n)]

x(n) = xe(n)+ xo(n)

Discrete-Time Signals and Systems

Figure 1.8

(1.26)

Clearly, x (n) satisfies the symmetry condition (1.24).

(1.27)

(1.28)

Thus any arbitrary signal can be expressed as in (1.28).

Again, it is clear that  satisfies (1.25); hence it is indeed odd. Now, if we add 
the two signal components, defined by (1.26) and (1.27), we obtain , that is,

xo(n)

x(n)

51



Simple Manipulations of Discrete-Time Signals

In this section we consider some simple modifications or manipulations involving the
independent variable and the signal amplitude (dependent variable).

Transformation of the independent variable (time). A signal x(n) may be shifted
in time by replacing the independent variable n by n− k , where k is an integer. If k
is a positive integer, the time shift results in a delay of the signal by k units of time.
If k is a negative integer, the time shift results in an advance of the signal by |k| units
in time.

signals x(n− 3) and x(n+ 2).

to shifting a signal to the right, whereas advance implies shifting the signal to the left on the
time axis.

Graphical representation of
a signal, and its delayed and
advanced versions.

Discrete-Time Signals and Systems

1.3

EXAMPLE 1.2

A signal x(n) is graphically illustrated in Fig. 1.9(a). Show a graphical representation of the

x(n) by two units in time. The result is illustrated in Fig. 1.9(c). Note that delay corresponds

Solution. The signal x(n− 3) is obtained by delaying x(n) by three units in time. The result
is illustrated in Fig. 1.9(b). On the other hand, the signal x(n+ 2) is obtained by advancing

Figure 1.9

52



If the signal x(n) is stored on magnetic tape or on a disk or, perhaps, in the
memory of a computer, it is a relatively simple operation to modify the base by
introducing a delay or an advance. On the other hand, if the signal is not stored
but is being generated by some physical phenomenon in real time, it is not possible
to advance the signal in time, since such an operation involves signal samples that
have not yet been generated. Whereas it is always possible to insert a delay into
signal samples that have already been generated, it is physically impossible to view
the future signal samples. Consequently, in real-time signal processing applications,
the operation of advancing the time base of the signal is physically unrealizable.

Another useful modification of the time base is to replace the independent vari-
able n by −n. The result of this operation is a folding or a reflection of the signal
about the time origin n = 0.

Graphical illustration of
the folding and shifting
operations.

Discrete-Time Signals and Systems

Show the graphical representation of the signals x(−n) and x(−n+2), where x(n) is the signal

EXAMPLE 1.3

illustrated in Fig. 1.10(a).

Figure 1.10

53



Solution.
y(1) = x(−1), y(2) = x(−2), and so on. Also, y(−1) = x(1), y(−2) = x(2), and so on.
Therefore, y(n) is simply x(n) reflected or folded about the time origin n = 0. The signal
y(n) = x(−n + 2) is simply x(−n) delayed by two units in time. The resulting signal is

to compute samples, such as y(0) = x(2), y(1) = x(1), y(2) = x(0), y(−1) = x(3), and so on.

It is important to note that the operations of folding and time delaying (or ad-
vancing) a signal are not commutative. If we denote the time-delay operation by TD
and the folding operation by FD, we can write

TDk[x(n)] = x(n− k), k > 0
FD[x(n)] = x(−n)

Now
TDk{FD[x(n)]} = TDk[x(−n)] = x(−n+ k)

whereas
FD{TDk[x(n)]} = FD[x(n− k)] = x(−n− k)

Note that because the signs of n and k in x(n− k) and x(−n+ k) are different, the
result is a shift of the signals x(n) and x(−n) to the right by k samples, corresponding
to a time delay.

A third modification of the independent variable involves replacing n by µn,
where µ is an integer. We refer to this time-base modification as time scaling or
down-sampling.

Show the graphical representation of the signal y(n) = x(2n), where x(n) is the signal illus-

Solution. We note that the signal y(n) is obtained from x(n) by taking every other sample
from x(n), starting with x(0). Thus y(0) = x(0), y(1) = x(2), y(2) = x(4), . . . and y(−1) =
x(−2), y(−2) = x(−4), and so on. In other words, we have skipped the odd-numbered
samples in x(n) and retained the even-numbered samples. The resulting signal is illustrated

If the signal x(n) was originally obtained by sampling an analog signal xa(t), then
x(n) = xa(nT ), where T is the sampling interval. Now, y(n) = x(2n) = xa(2T n).

ing the sampling rate from 1/T to 1/2T , that is, to decreasing the rate by a factor of
2. This is a down-sampling operation.

Addition, multiplication, and scaling of sequences. Amplitude modifications in-
clude addition, multiplication, and scaling of discrete-time signals.

Discrete-Time Signals and Systems

(1.29)

(1.30)

(1.31)

EXAMPLE 1.4

trated in Fig. 1.11(a).

in Fig. 1.11(b).

Hence the time-scaling operation described in Example 1.4 is equivalent to chang-

The new signal y(n) = x(−n) is shown in Fig.                  1.10(b). Note that y(0) = x(0),

illustrated in Fig. 1.10(c). A simple way to verify that the result in Fig. 1.10(c) is correct is

54



←7←6←5←4
←3←2←1 0 1 2 3 4 5 6

(a)

(b)

n

n

x(n)

y(n) = x(2n)

←4

←3

←2←1 0 1 2 3

Graphical illustration of down-sampling operation.

Amplitude scaling of a signal by a constant A is accomplished by multiplying the
value of every signal sample by A. Consequently, we obtain

y(n) = Ax(n), −∞ < n <∞

The sum of two signals x1(n) and x2(n) is a signal y(n), whose value at any instant
is equal to the sum of the values of these two signals at that instant, that is,

y(n) = x1(n)+ x2(n), −∞ < n <∞

The product of two signals is similarly defined on a sample-to-sample basis as

y(n) = x1(n)x2(n), −∞ < n <∞

Discrete-Time Systems

In many applications of digital signal processing we wish to design a device or an
algorithm that performs some prescribed operation on a discrete-time signal. Such a
device or algorithm is called a discrete-time system. More specifically, a discrete-time
system is a device or algorithm that operates on a discrete-time signal, called the input
or excitation, according to some well-defined rule, to produce another discrete-time

Discrete-Time Signals and Systems

Figure 1.11

2

55



… …

Input signal
or excitation

Output signal
or response

x(n) y(n)Discrete-time
System

Block diagram representation of a discrete-time system.

signal called the output or response of the system. In general, we view a system as an
operation or a set of operations performed on the input signal x(n) to produce the
output signal y(n). We say that the input signal x(n) is transformed by the system

y(n) ≡ T [x(n)]
where the symbol T
ing performed by the system on x(n) to produce y(n). The mathematical relationship

There are various ways to describe the characteristics of the system and the oper-
ation it performs on x(n) to produce y(n). In this chapter we shall be concerned with
the time-domain characterization of systems. We shall begin with an input–output
description of the system. The input–output description focuses on the behavior
at the terminals of the system

Input–Output Description of Systems

The input–output description of a discrete-time system consists of a mathematical
expression or a rule, which explicitly defines the relation between the input and
output signals (input–output relationship). The exact internal structure of the system
is either unknown or ignored. Thus the only way to interact with the system is by
using its input and output terminals (i.e., the system is assumed to be a “black box”
to the user). To reflect this philosophy, we use the graphical representation depicted

notation
x(n)

T−→ y(n)
which simply means that y(n) is the response of the system T
The following examples illustrate several different systems.

Determine the response of the following sytems to the input signal

x(n) =
{ |n|, −3 ≤ n ≤ 3

0, otherwise

Discrete-Time Signals and Systems

Figure 2.1

denotes the transformation (also called an operator) or process-

into a signal y(n), and express the general relationship between x(n) and y(n) as

(2.1)

in (2.1) is depicted graphically in Fig. 2.1.

2.1

in Fig. 2.1, and the general input–output relationship in (2.1) or, alternatively, the

to the excitation x(n).

(2.2)

EXAMPLE 2.1

and ignores the detailed internal construction or re-
alization of the system.

56



(a) y(n) = x(n) (identity system)
(b) y(n) = x(n− 1) (unit delay system)
(c) y(n) = x(n+ 1) (unit advance system)

(d) y(n) = 1
3

[x(n+ 1)+ x(n)+ x(n− 1)] (moving average filter)

(e) y(n) = median{x(n+ 1), x(n), x(n− 1)} (median filter)

(f) y(n) =
n∑

k=−∞
x(k) = x(n)+ x(n− 1)+ x(n− 2)+ · · · (accumulator)

Solution. First, we determine explicitly the sample values of the input signal

x(n) = {. . . , 0, 3, 2, 1, 0
↑
, 1, 2, 3, 0, . . .}

Next, we determine the output of each system using its input–output relationship.
(a) In this case the output is exactly the same as the input signal. Such a system is known as

the identity system.
(b) This system simply delays the input by one sample. Thus its output is given by

x(n) = {. . . , 0, 3, 2, 1
↑
, 0, 1, 2, 3, 0, . . .}

(c) In this case the system “advances” the input one sample into the future. For example, the
value of the output at time n = 0 is y(0) = x(1). The response of this system to the given
input is

x(n) = {. . . , 0, 3, 2, 1, 0, 1
↑
, 2, 3, 0, . . .}

(d) The output of this system at any time is the mean value of the present, the immediate
past, and the immediate future samples. For example, the output at time n = 0 is

y(0) = 1
3

[x(−1)+ x(0)+ x(1)] = 1
3

[1+ 0+ 1] = 2
3

Repeating this computation for every value of n, we obtain the output signal

y(n) = {. . . , 0, 1, 5
3
, 2, 1,

2
3
↑
, 1, 2,

5
3
, 1, 0, . . .}

(e) This system selects as its output at time n the median value of the three input samples
x(n− 1), x(n), and x(n+ 1). Thus the response of this system to the input signal x(n) is

y(n) = {0, 2, 2, 1, 1, 1
↑
, 2, 2, 0, 0, 0, . . .}

(f) This system is basically an accumulator that computes the running sum of all the past
input values up to present time. The response of this system to the given input is

y(n) = {. . . , 0, 3, 5, 6, 6
↑
, 7, 9, 12, 0, . . .}

Discrete-Time Signals and Systems

(2.3)

57



at time n = n0 depends not only on the value of the input at n = n0 [i.e., x(n0)],
but also on the values of the input applied to the system before and after n = n0 .
Consider, for instance, the accumulator in the example. We see that the output at
time n = n0 depends not only on the input at time n = n0 , but also on x(n) at times
n = n0 − 1, n0 − 2, and so on. By a simple algebraic manipulation the input–output
relation of the accumulator can be written as

y(n) =
n∑

k=−∞
x(k) =

n−1∑
k=−∞

x(k)+ x(n)

= y(n− 1)+ x(n)

which justifies the term accumulator. Indeed, the system computes the current value
of the output by adding (accumulating) the current value of the input to the previous
output value.

There are some interesting conclusions that can be drawn by taking a close look
into this apparently simple system. Suppose that we are given the input signal x(n)
for n ≥ n0 , and we wish to determine the output y(n) of this system for n ≥ n0 . For
n = n0 , n0

y(n0) = y(n0 − 1)+ x(n0)
y(n0 + 1) = y(n0)+ x(n0 + 1)

and so on. Note that we have a problem in computing y(n0), since it depends on
y(n0 − 1). However,

y(n0 − 1) =
n0−1∑
k=−∞

x(k)

that is, y(n0 − 1) “summarizes” the effect on the system from all the inputs which
had been applied to the system before time n0 . Thus the response of the system for
n ≥ n0 to the input x(n) that is applied at time n0 is the combined result of this input
and all inputs that had been applied previously to the system. Consequently, y(n),
n ≥ n0 is not uniquely determined by the input x(n) for n ≥ n0 .

The additional information required to determine y(n) for n ≥ n0 is the initial
condition y(n0 − 1). This value summarizes the effect of all previous inputs to the
system. Thus the initial condition y(n0 − 1) together with the input sequence x(n)
for n ≥ n0 uniquely determine the output sequence y(n) for n ≥ n0 .

If the accumulator had no excitation prior to n0 , the initial condition is y(n0−1)
= 0. In such a case we say that the system is initially relaxed. Since y(n0 − 1) = 0,
the output sequence y(n) depends only on the input sequence x(n) for n ≥ n0 .

It is customary to assume that every system is relaxed at n = −∞. In this case,
if an input x(n) is applied at n = −∞, the corresponding output y(n) is solely and
uniquely determined by the given input.

Discrete-Time Signals and Systems

We observe that for several of the systems considered in Example 2.1 the output

(2.4)

+ 1, . . . , (2.4) gives

58



its output under the condition that:

(a) It is initially relaxed [i.e., y(−1) = 0].
(b) Initially, y(−1) = 1.

Solution. The output of the system is defined as

y(n) =
n∑

k=−∞
x(k) =

−1∑
k=−∞

x(k)+
n∑
k=0

x(k)

= y(−1)+
n∑
k=0

x(k)

= y(−1)+ n(n+ 1)
2

(a) If the system is initially relaxed, y(−1) = 0 and hence

y(n) = n(n+ 1)
2

, n ≥ 0

(b) On the other hand, if the initial condition is y(−1) = 1, then

y(n) = 1+ n(n+ 1)
2

= n
2 + n+ 2

2
, n ≥ 0

Block Diagram Representation of Discrete-Time Systems

It is useful at this point to introduce a block diagram representation of discrete-time
systems. For this purpose we need to define some basic building blocks that can be
interconnected to form complex systems.

An adder.
two signal sequences to form another (the sum) sequence, which we denote as y(n).
Note that it is not necessary to store either one of the sequences in order to perform
the addition. In other words, the addition operation is memoryless.

Graphical representation of
an adder.

+

x 1(n )

y(n )  = x1(n )  + x2(n )

x2(n )

Discrete-Time Signals and Systems

EXAMPLE 2.2

The accumulator described by (2.30) is excited by the sequence x(n) = nu(n) . Determine

2.2

Figure 2.2 illustrates a system (adder) that performs the addition of

Figure 2.2

59



A constant multiplier.
applying a scale factor on the input x(n). Note that this operation is also memoryless.

Graphical representation of
a constant multiplier.

ax (n ) y (n )  = ax (n )  

A signal multiplier.
to form another (the product) sequence, denoted in the figure as y(n). As in the pre-
ceding two cases, we can view the multiplication operation as memoryless.

Graphical representation of
a signal multiplier.

y(n )  = x1(n )x2(n )x1(n )

x2(n )

�

A unit delay element.

input signal is x(n), the output is x(n − 1). In fact, the sample x(n − 1) is stored in
memory at time n− 1 and it is recalled from memory at time n to form

y(n) = x(n− 1)
Thus this basic building block requires memory. The use of the symbol z−1 to denote

Graphical representation of
the unit delay element.

y(n )  = x(n−1)x(n )
z−1

A unit advance element. In contrast to the unit delay, a unit advance moves the

operation, with the operator z being used to denote the unit advance. We observe
that any such advance is physically impossible in real time, since, in fact, it involves
looking into the future of the signal. On the other hand, if we store the signal in the
memory of the computer, we can recall any sample at any time. In such a non-real-
time application, it is possible to advance the signal x(n) in time.

Graphical representation of
the unit advance element.

y(n )  = x(n�1)x(n )
z

Using basic building blocks introduced above, sketch the block diagram representation of the
discrete-time system described by the input–output relation

y(n) = 1
4
y(n− 1)+ 1

2
x(n)+ 1

2
x(n− 1)

where x(n) is the input and y(n) is the output of the system.

Discrete-Time Signals and Systems

the unit of delay will become apparent when discussing the z-transform.

This operation is depicted by Fig. 2.3, and simply represents

Figure 2.3

Figure 2.4 illustrates the multiplication of two signal sequences

Figure 2.4

The unit delay is a special system that simply delays the
signal passing through it by one sample. Figure 2.5 illustrates such a system. If the

Figure 2.5

Figure 2.6

input x(n) ahead by one sample in time to yield x(n+ 1). Figure 2.6 illustrates this

EXAMPLE 2.3

(2.5)

60



Black box

Black box

x (n )
y (n )

y (n )
x (n )

+

+ +

+

0.25

0.25

0.5

0.5

0.5

(a)

(b)

z−1

z−1

z−1

z−1

Block diagram realizations of the system y(n) = 0.25y(n − 1) +
0.5x(n)+ 0.5x(n− 1).

Solution.
0.5, multiplying the previous input x(n− 1) by 0.5, adding the two products, and then adding
the previous output y(n − 1) multiplied by 14 .

y(n) = 1
4
y(n− 1)+ 1

2
[x(n)+ x(n− 1)]

from the “viewpoint” of an input–output or an external description, we are not concerned
about how the system is realized. On the other hand, if we adopt an internal description of
the system, we know exactly how the system building blocks are configured. In terms of such
a realization, we can see that a system is relaxed at time n = n0 if the outputs of all the delays
existing in the system are zero at n = n0 (i.e., all memory is filled with zeros).

Classification of Discrete-Time Systems

We stress the point that for a system to possess a given property, the property
must hold for every possible input signal to the system. If a property holds for some

Discrete-Time Signals and Systems

Figure 2.7

(2.6)

According to (2.5), the output y(n) is obtained by multiplying the input x(n) by

Figure 2.7(a) illustrates this block diagram
realization of the system. A simple rearrangement of (2.5), namely,

2.3

leads to the block diagram realization shown in Fig. 2.7(b). Note that if we treat “the system”

In the analysis as well as in the design of systems, it is desirable to classify the sys-
tems according to the general properties that they satisfy. In fact, the mathematical 
techniques developed in this chapter and future study for analyzing and designing 
discrete-time systems depend heavily on the general characteristics of the systems 
that are being considered. For this reason it is necessary for us to develop a number 
of properties or categories that can be used to describe the general characteristics 
of systems.

61



input signals but not for others, the system does not possess that property. Thus a
counterexample is sufficient to prove that a system does not possess a property. How-
ever, to prove that the system has some property, we must prove that this property
holds for every possible input signal.

Static versus dynamic systems. A discrete-time system is called static or memo-
ryless if its output at any instant n depends at most on the input sample at the same
time, but not on past or future samples of the input. In any other case, the system
is said to be dynamic or to have memory. If the output of a system at time n is
completely determined by the input samples in the interval from n−N to n(N ≥ 0),
the system is said to have memory of duration N . If N = 0, the system is static. If
0 < N <∞, the system is said to have finite memory, whereas if N = ∞, the system
is said to have infinite memory.

The systems described by the following input–output equations

y(n) = ax(n)
y(n) = nx(n)+ bx3(n)

are both static or memoryless. Note that there is no need to store any of the past
inputs or outputs in order to compute the present output. On the other hand, the
systems described by the following input–output relations

y(n) = x(n)+ 3x(n− 1)

y(n) =
n∑
k=0

x(n− k)

y(n) =
∞∑
k=0

x(n− k)

memory.
We observe that static or memoryless systems are described in general by input–

output equations of the form

y(n) = T [x(n), n]

and they do not include delay elements (memory).

Time-invariant versus time-variant systems. We can subdivide the general class
of systems into the two broad categories, time-invariant systems and time-variant
systems. A system is called time-invariant if its input–output characteristics do not
change with time. To elaborate, suppose that we have a system T in a relaxed state

Discrete-Time Signals and Systems

(2.7)

(2.8)

(2.9)

(2.10)

(2.11)

are dynamic systems or systems with memory. The systems described by (2.9) and
(2.10) have finite memory , whereas the system described by (2.11) has infinite

(2.12)

62



we write
y(n) = T [x(n)]

Now suppose that the same input signal is delayed by k units of time to yield x(n−k),
and again applied to the same system. If the characteristics of the system do not
change with time, the output of the relaxed system will be y(n − k). That is, the
output will be the same as the response to x(n), except that it will be delayed by
the same k units in time that the input was delayed. This leads us to define a time-
invariant or shift-invariant system as follows.

Definition. A relaxed system T is time invariant or shift invariant if and only if

x(n)
T−→ y(n)

implies that

x(n− k) T−→ y(n− k)

for every input signal x(n) and every time shift k .
To determine if any given system is time invariant, we need to perform the test

specified by the preceding definition. Basically, we excite the system with an arbitrary
input sequence x(n), which produces an output denoted as y(n). Next we delay the
input sequence by some amount k and recompute the output. In general, we can
write the output as

y(n, k) = T [x(n− k)]
Now if this output y(n, k) = y(n− k), for all possible values of k , the system is time
invariant. On the other hand, if the output y(n, k) �= y(n− k), even for one value of
k , the system is time variant.

Solution.
(a) This system is described by the input–output equations

y(n) = T [x(n)] = x(n)− x(n− 1)

Now if the input is delayed by k units in time and applied to the system, it is clear from
the block diagram that the output will be

y(n, k) = x(n− k)− x(n− k − 1)

obtain
y(n− k) = x(n− k)− x(n− k − 1)

y(n− k). Therefore, the system is time invariant.

Discrete-Time Signals and Systems

which, when excited by an input signal x(n), produces an output signal y(n). Thus

(2.13)

(2.14)

EXAMPLE 2.4

Determine if the systems shown in Fig. 2.8 are time invariant or time variant.

(2.15)

(2.16)

(2.17)

On the other hand, from (2.14) we note that if we delay y(n) by k units in time, we

Since the right-hand sides of (2.16) and (2.17) are identical, it follows that y(n, k) =

63



Examples of a
time-invariant (a) and
some time-variant systems
(b)–(d).

“Differentiator”

“Time” multiplier

“Folder”

Modulator

x (n )

x (n )

x (n )

x(n)

(a)

(b)

(c)

(d)

�

+

+

y (n )  = x (n )−  x(n  −  1)

y (n )  = nx (n )

y(n )  = x (−n )

y(n )  = x(n )  cos ω0n

cos ω0n

z←1

T

−

n

(b) The input–output equation for this system is

y(n) = T [x(n)] = nx(n)
The response of this system to x(n− k) is

y(n, k) = nx(n− k)

y(n− k) = (n− k)x(n− k)

= nx(n− k)− kx(n− k)
This system is time variant, since y(n, k) �= y(n− k).

(c) This system is described by the input–output relation

y(n) = T [x(n)] = x(−n)
The response of this system to x(n− k) is

y(n, k) = T [x(n− k)] = x(−n− k)

y(n− k) = x(−n+ k)
Since y(n, k) �= y(n− k), the system is time variant.

Discrete-Time Signals and Systems

Figure 2.8

(2.18)

(2.19)

Now if we delay y(n) in (2.18) by k units in time, we obtain

(2.20)

(2.21)

(2.22)

Now, if we delay the output y(n), as given by (2.21), by k units in time, the result will be

(2.23)

64



(d) The input–output equation for this system is

y(n) = x(n) cosω0n

The response of this system to x(n− k) is

y(n, k) = x(n− k) cosω0n

it is evident that the system is time variant.

Linear versus nonlinear systems. The general class of systems can also be subdi-
vided into linear systems and nonlinear systems. A linear system is one that satisfies
the superposition principle. Simply stated, the principle of superposition requires
that the response of the system to a weighted sum of signals be equal to the cor-
responding weighted sum of the responses (outputs) of the system to each of the
individual input signals. Hence we have the following definition of linearity.

Definition. A system is linear if and only if

T [a1x1(n)+ a2x2(n)] = a1T [x1(n)]+ a2T [x2(n)]

1 2 1 and
2

2

+

+

T

T

T
y(n )

y ′(n )

x1(n )

x2(n )

x1(n )

x2(n )

a2

a1

a1

a2

Graphical representation of the superposition prin-
ciple. T is linear if and only if y(n) = y ′(n).

Discrete-Time Signals and Systems

(2.24)

(2.25)

If the expression in (2.24) is delayed by k units and the result is compared to (2.25),

for any arbitrary input sequences x (n) and x (n), and any arbitrary constants a

(2.26)

a . Figure 2.9 gives a pictorial illustration of the superposition principle.
The superposition principle embodied in the relation (2.26) can be separated

into two parts. First, suppose that a = 0. Then (2.26) reduces to

Figure 2.9

65



T [a1x1(n)] = a1T [x1(n)] = a1y1(n)
where

y1(n) = T [x1(n)]

system. That is, if the response of the system to the input x1(n) is y1(n), the response
to a1x1(n) is simply a1y1(n). Thus any scaling of the input results in an identical
scaling of the corresponding output.

Second, suppose that a1 = a2
T [x1(n)+ x2(n)] = T [x1(n)]+ T [x1(n)]

= y1(n)+ y2(n)
This relation demonstrates the additivity property of a linear system. The additivity
and multiplicative properties constitute the superposition principle as it applies to
linear systems.

weighted linear combination of signals by induction. In general, we have

x(n) =
M−1∑
k=1

akxk(n)
T−→ y(n) =

M−1∑
k=1

akyk(n)

where
yk(n) = T [xk(n)], k = 1, 2, . . . ,M − 1

1 = 0, then y(n) = 0. In other words, a relaxed,
linear system with zero input produces a zero output. If a system produces a nonzero
output with a zero input, the system may be either nonrelaxed or nonlinear. If a
relaxed system does not satisfy the superposition principle as given by the definition
above, it is called nonlinear.

Determine if the systems described by the following input–output equations are linear or
nonlinear.
(a) y(n) = nx(n) (b) y(n) = x(n2) (c) y(n) = x2(n)
(d) y(n) = Ax(n)+ B (e) y(n) = ex(n)

Solution.
(a) For two input sequences x1(n) and x2(n), the corresponding outputs are

y1(n) = nx1(n)

y2(n) = nx2(n)
A linear combination of the two input sequences results in the output

y3(n) = T [a1x1(n)+ a2x2(n)] = n[a1x1(n)+ a2x2(n)]

= a1nx1(n)+ a2nx2(n)

Discrete-Time Signals and Systems

(2.27)

The relation (2.27) demonstrates the multiplicative or scaling property of a linear

(2.28)

= 1 in (2.26). Then

(2.29)

(2.30)

We observe from (2.27) that if a

EXAMPLE 2.5

(2.31)

(2.32)

The linearity condition embodied in (            2.26) can be extended arbitrarily to any

66



a1y1(n)+ a2y2(n) = a1nx1(n)+ a2nx2(n)

(b) As in part (a), we find the response of the system to two separate input signals x1(n) and
x2(n). The result is

y1(n) = x1(n2)

y2(n) = x2(n2)

The output of the system to a linear combination of x1(n) and x2(n) is

y3(n) = T [a1x1(n)+ a2x2(n)] = a1x1(n2)+ a2x2(n2)

a1y1(n)+ a2y2(n) = a1x1(n2)+ a2x2(n2)

(c) The output of the system is the square of the input. (Electronic devices that have such an
input–output characteristic are called square-law devices.) From our previous discussion
it is clear that such a system is memoryless. We now illustrate that this system is nonlinear.

y1(n) = x21 (n)

y2(n) = x22 (n)

The response of the system to a linear combination of these two input signals is

y3(n) = T [a1x1(n)+ a2x2(n)]

= [a1x1(n)+ a2x2(n)]2

2
1

2
1 1 2 1 2

2
2

2
2

On the other hand, if the system is linear, it will produce a linear combination of the two

a1y1(n)+ a2y2(n) = a1x21 (n)+ a2x22 (n)

Discrete-Time Signals and Systems

On the other hand, a linear combination of the two outputs in (2.31) results in the output

(2.33)

Since the right-hand sides of (2.32) and (2.33) are identical, the system is linear.

(2.34)

(2.35)

Finally, a linear combination of the two outputs in (2.34) yields

(2.36)

By comparing (2.35) with (2.36), we conclude that the system is linear.

The responses of the system to two separate input signals are

(2.37)

= a x (n)+ 2a a x (n)x (n)+ a x (n)

(2.38)

outputs in (2.37), namely,

(2.39)

Since the actual output of the system, as given by (2.38), is not equal to (2.39),
 the system is nonlinear.

67



(d) Assuming that the system is excited by x1(n) and x2(n) separately, we obtain the corre-
sponding outputs

y1(n) = Ax1(n)+ B

y2(n) = Ax2(n)+ B
A linear combination of x1(n) and x2(n) produces the output

y3(n) = T [a1x1(n)+ a2x2(n)]

= A[a1x1(n)+ a2x2(n)]+ B

1 1 2 2

On the other hand, if the system were linear, its output to the linear combination of x1(n)
and x2(n) would be a linear combination of y1(n) and y2(n), that is,

a1y1(n)+ a2y2(n) = a1Ax1(n)+ a1B + a2Ax2(n)+ a2B

test.
The reason that this system fails to satisfy the linearity test is not that the system is

nonlinear (in fact, the system is described by a linear equation) but the presence of the
constant B . Consequently, the output depends on both the input excitation and on the
parameter B �= 0. Hence, for B �= 0, the system is not relaxed. If we set B = 0, the
system is now relaxed and the linearity test is satisfied.

(e) Note that the system described by the input–output equation

y(n) = ex(n)

is relaxed. If x(n) = 0, we find that y(n) = 1. This is an indication that the system is
nonlinear. This, in fact, is the conclusion reached when the linearity test is applied.

Causal versus noncausal systems. We begin with the definition of causal discrete-
time systems.
Definition. A system is said to be causal if the output of the system at any time n
[i.e., y(n)] depends only on present and past inputs [i.e., x(n), x(n−1), x(n−2), . . .],
but does not depend on future inputs [i.e., x(n+ 1), x(n+ 2), . . .]. In mathematical
terms, the output of a causal system satisfies an equation of the form

y(n) = F [x(n), x(n− 1), x(n− 2), . . .]
where F [·] is some arbitrary function.

If a system does not satisfy this definition, it is called noncausal. Such a system
has an output that depends not only on present and past inputs but also on future
inputs.

It is apparent that in real-time signal processing applications we cannot observe
future values of the signal, and hence a noncausal system is physically unrealizable
(i.e., it cannot be implemented). On the other hand, if the signal is recorded so that
the processing is done off-line (nonreal time), it is possible to implement a noncausal
system, since all values of the signal are available at the time of processing. This is
often the case in the processing of geophysical signals and images.

Discrete-Time Signals and Systems

(2.40)

= Aa x (n)+ a Ax (n)+ B

(2.41)

(2.42)

Clearly, (2.41) and (2.42) are different and hence the system fails to satisfy the linearity

(2.43)

(2.44)

68



Determine if the systems described by the following input–output equations are causal or
noncausal.
(a) y(n) = x(n)−x(n−1) (b) y(n) =∑nk=−∞ x(k) (c) y(n) = ax(n) (d) y(n) = x(n)+3x(n+4)
(e) y(n) = x(n2) (f) y(n) = x(2n) (g) y(n) = x(−n)
Solution. The systems described in parts (a), (b), and (c) are clearly causal, since the output
depends only on the present and past inputs. On the other hand, the systems in parts (d),
(e), and (f) are clearly noncausal, since the output depends on future values of the input. The
system in (g) is also noncausal, as we note by selecting, for example, n = −1, which yields
y(−1) = x(1). Thus the output at n = −1 depends on the input at n = 1, which is two units
of time into the future.

Stable versus unstable systems. Stability is an important property that must be
considered in any practical application of a system. Unstable systems usually exhibit
erratic and extreme behavior and cause overflow in any practical implementation.
Here, we define mathematically what we mean by a stable system, and later, in Sec-

systems.
Definition. An arbitrary relaxed system is said to be bounded input–bounded output
(BIBO) stable if and only if every bounded input produces a bounded output.

The condition that the input sequence x(n) and the output sequence y(n) are
bounded is translated mathematically to mean that there exist some finite numbers,
say Mx and My , such that

|x(n)| ≤ Mx <∞, |y(n)| ≤ My <∞
for all n. If, for some bounded input sequence x(n), the output is unbounded (infi-
nite), the system is classified as unstable.

Consider the nonlinear system described by the input–output equation

y(n) = y2(n− 1)+ x(n)
As an input sequence we select the bounded signal

x(n) = Cδ(n)
where C is a constant. We also assume that y(−1) = 0. Then the output sequence is

y(0) = C, y(1) = C2, y(2) = C4, . . . , y(n) = C2n

Clearly, the output is unbounded when 1 < |C| <∞. Therefore, the system is BIBO unstable,
since a bounded input sequence has resulted in an unbounded output.

Interconnection of Discrete-Time Systems

Discrete-time systems can be interconnected to form larger systems. There are two
basic ways in which systems can be interconnected: in cascade (series) or in parallel.

systems are different.

Discrete-Time Signals and Systems

EXAMPLE 2.6

(2.45)

EXAMPLE 2.7

2.4

These interconnections are illustrated in Fig. 2.10. Note that the two interconnected

tion 3.6, we explore the implications of this definition for linear, time-invariant

69



Cascade (a) and parallel
(b) interconnections of
systems.

+

y(n )

T1

T2

Tp

Tc

T1 T2

y3(n )

y1(n )

y1(n )

y2(n )

x(n )

x(n )

(a)

(b)

In the cascade interconnection the output of the first system is

y1(n) = T1[x(n)]

and the output of the second system is

y(n) = T2[y1(n)]
= T2{T1[x(n)]}

We observe that systems T1 and T2 can be combined or consolidated into a single
overall system

Tc ≡ T2T1
Consequently, we can express the output of the combined system as

y(n) = Tc[x(n)]

In general, the order in which the operations T1 and T2 are performed is impor-
tant. That is,

T2T1 �= T1T2
for arbitrary systems. However, if the systems T1 and T2 are linear and time invariant,
then (a) Tc is time invariant and (b) T2T1 = T1T2 , that is, the order in which the
systems process the signal is not important. T2T1 and T1T2 yield identical output
sequences.

invariance, suppose that T1 and T2 are time invariant; then

x(n− k) T1−→ y1(n− k)

Discrete-Time Signals and Systems

Figure 2.10

(2.46)

(2.47)

(2.48)

The proof of (a) follows. The proof of (b) is given in Section 3.4. To prove time

70



and

y1(n− k) T2−→ y(n− k)

Thus

x(n− k) Tc=T2T1−→ y(n− k)

and therefore, Tc is time invariant.
In the parallel interconnection, the output of the system T1 is y1(n) and the

output of the system T2 is y2(n). Hence the output of the parallel interconnection is

y3(n) = y1(n)+ y2(n)
= T1[x(n)]+ T2[x(n)]
= (T1 + T2)[x(n)]
= Tp[x(n)]

where Tp = T1 + T2 .
In general, we can use parallel and cascade interconnection of systems to con-

struct larger, more complex systems. Conversely, we can take a larger system and
break it down into smaller subsystems for purposes of analysis and implementation.
We shall use these notions later, in the design and implementation of digital filters.

3 Analysis of Discrete-Time Linear Time-Invariant Systems

properties or categories, namely: linearity, causality, stability, and time invariance.
Having done so, we now turn our attention to the analysis of the important class
of linear, time-invariant (LTI) systems. In particular, we shall demonstrate that
such systems are characterized in the time domain simply by their response to a
unit sample sequence. We shall also demonstrate that any arbitrary input signal
can be decomposed and represented as a weighted sum of unit sample sequences.
As a consequence of the linearity and time-invariance properties of the system, the
response of the system to any arbitrary input signal can be expressed in terms of
the unit sample response of the system. The general form of the expression that
relates the unit sample response of the system and the arbitrary input signal to the
output signal, called the convolution sum or the convolution formula, is also derived.
Thus we are able to determine the output of any linear, time-invariant system to any
arbitrary input signal.

Techniques for the Analysis of Linear Systems

There are two basic methods for analyzing the behavior or response of a linear
system to a given input signal. One method is based on the direct solution of the
input–output equation for the system, which, in general, has the form

y(n) = F [y(n− 1), y(n− 2), . . . , y(n−N), x(n), x(n− 1), . . . , x(n−M)]

Discrete-Time Signals and Systems

In Section 2 we classified systems in accordance with a number of characteristic

3.1

71



where F [·] denotes some function of the quantities in brackets. Specifically, for an
LTI system, we shall see later that the general form of the input–output relationship is

y(n) = −
N∑
k=1

aky(n− k)+
M∑
k=0

bkx(n− k)

where {ak} and {bk} are constant parameters that specify the system and are indepen-

equation and represents one way to characterize the behavior of a discrete-time LTI

The second method for analyzing the behavior of a linear system to a given
input signal is first to decompose or resolve the input signal into a sum of elementary
signals. The elementary signals are selected so that the response of the system to
each signal component is easily determined. Then, using the linearity property of
the system, the responses of the system to the elementary signals are added to obtain
the total response of the system to the given input signal. This second method is the
one described in this section.

To elaborate, suppose that the input signal x(n) is resolved into a weighted sum
of elementary signal components {xk(n)} so that

x(n) =
∑
k

ckxk(n)

where the {ck} are the set of amplitudes (weighting coefficients) in the decomposition

signal component xk(n) is yk(n). Thus,

yk(n) ≡ T [xk(n)]

k k k k

consequence of the scaling property of the linear system.
Finally, the total response to the input x(n) is

y(n) = T [x(n)] = T
[∑

k

ckxk(n)

]

=
∑
k

ckT [xk(n)]

=
∑
k

ckyk(n)

Although to a large extent, the choice of the elementary signals appears to be
arbitrary, our selection is heavily dependent on the class of input signals that we
wish to consider. If we place no restriction on the characteristics of the input signals,

Discrete-Time Signals and Systems

(3.1)

dent of x(n) and y(n). The input–output relationship in (3.1) is called a difference

system. The solution of (3.1) is the subject of Section 4.

(3.2)

of the signal x(n). Now suppose that the response of the system to the elementary

assuming that the system is relaxed and that the response to c x (n) is c y (n), as a

(3.3)

(3.4)

In (3.4) we used the additivity property of the linear system.

72



xk(n) = ejωkn, k = 0, 1, . . . , N − 1

where the frequencies {ωk} are harmonically related, that is,

ωk =
(

2π
N

)
k, k = 0, 1, . . . , N − 1

The frequency 2π/N is called the fundamental frequency, and all higher-frequency
components are multiples of the fundamental frequency component. This subclass
of input signals is considered in more detail later.

For the resolution of the input signal into a weighted sum of unit sample se-
quences, we must first determine the response of the system to a unit sample se-
quence and then use the scaling and multiplicative properties of the linear system to
determine the formula for the output given any arbitrary input. This development
is described in detail as follows.

Resolution of a Discrete-Time Signal into Impulses

Suppose we have an arbitrary signal x(n) that we wish to resolve into a sum of unit
sample sequences. To utilize the notation established in the preceding section, we
select the elementary signals xk(n) to be

xk(n) = δ(n− k)

where k represents the delay of the unit sample sequence. To handle an arbitrary
signal x(n) that may have nonzero values over an infinite duration, the set of unit
impulses must also be infinite, to encompass the infinite number of delays.

Now suppose that we multiply the two sequences x(n) and δ(n − k). Since
δ(n− k) is zero everywhere except at n = k , where its value is unity, the result of this
multiplication is another sequence that is zero everywhere except at n = k , where

x(n)δ(n− k) = x(k)δ(n− k)

is a sequence that is zero everywhere except at n = k , where its value is x(k). If we
repeat the multiplication of x(n) with δ(n −m), where m is another delay (m �= k),
the result will be a sequence that is zero everywhere except at n = m, where its value
is x(m). Hence

x(n)δ(n−m) = x(m)δ(n−m)

Discrete-Time Signals and Systems

(3.5)

(3.6)

3.2

(3.7)

its value is x(k), as illustrated in Fig. 3.1. Thus

(3.8)

(3.9)

x(n) N

their resolution into a weighted sum of unit sample (impulse) sequences proves to 
be mathematically convenient and completely general. On the other hand, if we 
restrict our attention to a subclass of input signals, there may be another set of ele-
mentary signals that is more convenient mathematically in the determination of the 
output. For example, if the input signal  is periodic with period , a mathemati-
cally convenient set of elementary signals is the set of exponentials

73



Multiplication of a signal x(n) with a shifted unit
sample sequence.

In other words, each multiplication of the signal x(n) by a unit impulse at some
delay k , [i.e., δ(n − k)], in essence picks out the single value x(k) of the signal x(n)
at the delay where the unit impulse is nonzero. Consequently, if we repeat this
multiplication over all possible delays, −∞ < k < ∞, and sum all the product
sequences, the result will be a sequence equal to the sequence x(n), that is,

x(n) =
∞∑

k=−∞
x(k)δ(n− k)

number of scaled unit sample sequences where the unit sample sequence δ(n−k) has

or decomposition of any arbitrary signal x(n) into a weighted (scaled) sum of shifted
unit sample sequences.

Consider the special case of a finite-duration sequence given as

x(n) = {2, 4
↑
, 0, 3}

Resolve the sequence x(n) into a sum of weighted impulse sequences.

Discrete-Time Signals and Systems

Figure 3.1

(3.10)

EXAMPLE 3.1

We emphasize that the right-hand side of (3.10) is the summation of an infinite

an amplitude value of x(k). Thus the right-hand side of (3.10) gives the resolution

74



Solution. Since the sequence x(n) is nonzero for the time instants n = −1, 0, 2, we need

x(n) = 2δ(n+ 1)+ 4δ(n)+ 3δ(n− 2)

Response of LTI Systems to Arbitrary Inputs:
The Convolution Sum

Having resolved an arbitrary input signal x(n) into a weighted sum of impulses, we
are now ready to determine the response of any relaxed linear system to any input
signal. First, we denote the response y(n, k) of the system to the input unit sample
sequence at n = k by the special symbol h(n, k), −∞ < k <∞. That is,

y(n, k) ≡ h(n, k) = T [δ(n− k)]

of the input impulse. If the impulse at the input is scaled by an amount ck ≡ x(k),
the response of the system is the correspondingly scaled output, that is,

ckh(n, k) = x(k)h(n, k)

Finally, if the input is the arbitrary signal x(n) that is expressed as a sum of weighted
impulses, that is,

x(n) =
∞∑

k=−∞
x(k)δ(n− k)

then the response of the system to x(n) is the corresponding sum of weighted outputs,
that is,

y(n) = T [x(n)] = T
[ ∞∑
k=−∞

x(k)δ(n− k)
]

=
∞∑

k=−∞
x(k)T [δ(n− k)]

=
∞∑

k=−∞
x(k)h(n, k)

known as the superposition summation.

arbitrary input sequence x(n). This expression is a function of both x(n) and the
responses h(n, k) of the system to the unit impulses δ(n − k) for −∞ < k < ∞.

invariance property.

Discrete-Time Signals and Systems

3.3

(3.11)

(3.12)

In (3.11) we note that n is the time index and k is a parameter showing the location

(3.13)

(3.14)

Clearly, (3.14) follows from the superposition property of linear systems, and is

We note that (3.14) is an expression for the response of a linear system to any

In deriving (3.14) we used the linearity property of the system but not its time-

(time-variant) system.
Thus the expression in (3.14) applies to any relaxed linear

three impulses at delays k = −1, 0, Following (3.10) we find that

75



considerably. In fact, if the response of the LTI system to the unit sample sequence
δ(n) is denoted as h(n), that is,

h(n) ≡ T [δ(n)]
then by the time-invariance property, the response of the system to the delayed unit
sample sequence δ(n− k) is

h(n− k) = T [δ(n− k)]

y(n) =
∞∑

k=−∞
x(k)h(n− k)

Now we observe that the relaxed LTI system is completely characterized by a single
function h(n), namely, its response to the unit sample sequence δ(n). In contrast,
the general characterization of the output of a time-variant, linear system requires
an infinite number of unit sample response functions, h(n, k), one for each possible
delay.

function of the input signal x(n) and the unit sample (impulse) response h(n) is
called a convolution sum. We say that the input x(n) is convolved with the impulse
response h(n) to yield the output y(n). We shall now explain the procedure for
computing the response y(n), both mathematically and graphically, given the input
x(n) and the impulse response h(n) of the system.

Suppose that we wish to compute the output of the system at some time instant,
0 0 is given as

y(n0) =
∞∑

k=−∞
x(k)h(n0 − k)

Our first observation is that the index in the summation is k , and hence both the
input signal x(k) and the impulse response h(n0 − k) are functions of k . Second,
we observe that the sequences x(k) and h(n0 − k) are multiplied together to form a
product sequence. The output y(n0) is simply the sum over all values of the product
sequence. The sequence h(n0 − k) is obtained from h(k) by, first, folding h(k) about
k = 0 (the time origin), which results in the sequence h(−k). The folded sequence
is then shifted by n0 to yield h(n0 − k). To summarize, the process of computing the
convolution between x(k) and h(k) involves the following four steps.

1. Folding. Fold h(k) about k = 0 to obtain h(−k).
2. Shifting. Shift h(−k) by n0 to the right (left) if n0 is positive (negative), to obtain
h(n0 − k).

3. Multiplication. Multiply x(k) by h(n0 − k) to obtain the product sequence
vn0(k) ≡ x(k)h(n0 − k).

4. Summation. Sum all the values of the product sequence vn0(k) to obtain the
value of the output at time n = n0 .

Discrete-Time Signals and Systems

If, in addition, the system is time invariant, the formula in (3.14) simplifies

(3.15)

(3.16)

Consequently, the formula in (3.14) reduces to

(3.17)

The formula in (3.17) that gives the response y(n) of the LTI system as a

say n = n . According to (3.17), the response at n = n

(3.18)

76



We note that this procedure results in the response of the system at a single time
instant, say n = n0 . In general, we are interested in evaluating the response of the
system over all time instants −∞ < n <∞. Consequently, steps 2 through 4 in the
summary must be repeated, for all possible time shifts −∞ < n <∞.

In order to gain a better understanding of the procedure for evaluating the
convolution sum, we shall demonstrate the process graphically. The graphs will aid
us in explaining the four steps involved in the computation of the convolution sum.

The impulse response of a linear time-invariant system is

h(n) = {1, 2
↑
, 1,−1}

x(n) = {1
↑
, 2, 3, 1}

The first step in the computation of the convolution sum is to fold h(k). The folded
Now we can compute the output at n = 0,

y(0) =
∞∑

k=−∞
x(k)h(−k)

Since the shift n = 0, we use h(−k) directly without shifting it. The product sequence

v0(k) ≡ x(k)h(−k)

y(0) =
∞∑
h = −∞v0(k) = 4

We continue the computation by evaluating the response of the system at n = 1. Accord-

y(1) =
∞∑

h=−∞
x(k)h(1− k)

The sequence h(1− k) is simply the folded sequence h(−k) shifted to the right by one unit in

v1(k) = x(k)h(1− k)

Finally, the sum of all the values in the product sequence
yields

y(1) =
∞∑

k=−∞
v1(k) = 8

Discrete-Time Signals and Systems

EXAMPLE 3.2

(3.19)

Determine the response of the system to the input signal

(3.20)

We shall compute the convolution according to the formula (3.17), but we shall

input signal sequence x(k) and the impulse response h(k) of the system, using k as the time

Solution.
use graphs of the sequences to aid us in the computation. In Fig. 3.2(a) we illustrate the

index in order to be consistent with (3.17).

according to (3.17), which is

(3.21)

(3.22)

is also shown in Fig. 3.2(b). Finally, the sum of all the terms in the product sequence yields

(3.23)

ing to (3.17),

(3.24)

is also illustrated in Fig. 3.2(c).

time. This sequence is illustrated in Fig. 3.2(c). The product sequence

sequence h(−k) is illustrated in Fig. 3.2(b).

77



0−1 1 3

0

0

0 0

0

0

0 1 2 3

3

4(a)

(b)

(c)

(d)

(e)

k

k

k

k

k

k

k

k

2

2

−1
−2

1

2

2

2

2

−1−2

−3

1

−1

1 2

h(k)

h(−k)

h(1 − k)

h(−1 − k)

x(k)

Fold

Shift

Multiply
with x(k)

Mu
ltip

ly

Multiply
with x(k)

Product
sequence

Product
sequence

Product
sequence

−1

1 2

1

1

2

|1| 2

4

2

−1

ν0(k)

ν1(k)

ν−1(k)

0

y(n) = 

1 2−1−2
−1−2

−3 3
4 5

6 7
n

Σ

8 8

4

1

3

∞

k = −∞
vn (k)

Graphical computation of convolution.

Discrete-Time Signals and Systems

Figure 3.2

78



In a similar manner, we obtain y(2) by shifting h(−k) two units to the right, forming the
product sequence v2(k) = x(k)h(2−k) and then summing all the terms in the product sequence
obtaining y(2) = 8. By shifting h(−k) farther to the right, multiplying the corresponding
sequence, and summing over all the values of the resulting product sequences, we obtain
y(3) = 3, y(4) = −2, y(5) = −1. For n > 5, we find that y(n) = 0 because the product
sequences contain all zeros. Thus we have obtained the response y(n) for n > 0.

y(−1) =
∞∑

k=−∞
x(k)h(−1− k)

left. The resulting sequence is illustrated in Fig. 3.2(d). The corresponding product sequence

obtain
y(−1) = 1

to the left always result in an all-zero product sequence, and hence

y(n) = 0 for n ≤ −2

Now we have the entire response of the system for −∞ < n <∞, which we summarize
below as

y(n) = {. . . , 0, 0, 1, 4
↑
, 8, 8, 3,−2,−1, 0, 0, . . .}

graphs of the sequences to aid us in visualizing the steps involved in the computation
procedure.

Before working out another example, we wish to show that the convolution
operation is commutative in the sense that it is irrelevant which of the two sequences
is folded and shifted.

y(n) =
∞∑

m=−∞
x(n−m)h(m)

y(n) =
∞∑

k=−∞
x(n− k)h(k)

Discrete-Time Signals and Systems

(3.25)

Next we wish to evaluate y(n) for n < 0. We begin with n = −1. Then

is also shown in Fig. 3.2(d). Finally, summing over the values of the product sequence, we

Now the sequence h(−1− k) is simply the folded sequence h(−k) shifted one time unit to the

From observation of the graphs of Fig. 3.2, it is clear that any further shifts of h(−1− k)

(3.26)

In Example 3.2 we illustrated the computation of the convolution sum, using

Indeed, if we begin with (3.17) and make a change in the
variable of the summation, from k to m, by defining a new index m = n − k , then

Since m is a dummy index, we may simply replace m by k so that

k = n−m and (3.17) becomes

(3.27)

(3.28)

while the input sequence is folded and shifted. Although the output y(n) in (3.28)
The expression in (3.28) involves leaving the impulse response h(k) unaltered,

79



formula are not identical. In fact, if we define the two product sequences as

vn(k) = x(k)h(n− k)
wn(k) = x(n− k)h(k)

it can be easily shown that
vn(k) = wn(n− k)

and therefore,

y(n) =
∞∑

k=−∞
vn(k) =

∞∑
k=−∞

wn(n− k)

since both sequences contain the same sample values in a different arrangement. The

Determine the output y(n) of a relaxed linear time-invariant system with impulse response

h(n) = anu(n), |a|1
when the input is a unit step sequence, that is,

x(n) = u(n)

Solution. In this case both h(n) and x(n) are infinite-duration sequences. We use the
The sequences

0(k), v1(k), and v2(k)

and (e), respectively. Thus we obtain the outputs

y(0) = 1

y(1) = 1+ a

y(2) = 1+ a + a2

Clearly, for n > 0, the output is

y(n) = 1+ a + a2 + · · · + an

= 1− a
n+1

1− a
On the other hand, for n < 0, the product sequences consist of all zeros. Hence

y(n) = 0, n < 0

as n approaches infinity is

y(∞) = lim
n→∞ y(n) =

1
1− a

Discrete-Time Signals and Systems

is identical to (3.17), the product sequences in the two forms of the convolution

EXAMPLE 3.3

form of the convolution formula given by (3.28) in which x(k) is folded.

reader is encouraged to rework Example 3.2 using the convolution sum in (3.28).

h(k), x(k), and x(−k) are shown in Fig. 3.3. The product sequences v
corresponding to x(−k) h(k), x(1− k)h(k), and x(2− k)h(k) are illustrated in Fig. 3.3(c), (d),

(3.29)

exponential rise in the output as a function of n. Since |a| < 1, the final value of the output

(3.30)

A graph of the output y(n) is illustrated in Fig. 3.3(f) for the case 0 < a < 1. Note the

80



(a) (b)

(c)

(d)

(e)

(f)

k k

k k

k k

k k

k

h(k)

y(n)

x(−k)

x(1 − k)

x(2 − k)

x(k)

ν0(k)

ν1(k)

ν2(k)

1 1

1

1

11

1

1

0−1 −1

−1

−1

−2−3 1

0 1 2 3 4 0 1 2

0 1 2

0 1

0 1 2

3 4

…

…

0−1−2 1 2

0−1 1 2

…

…

…

a

a

a2

a
a2

a3 a4 …

asymptote

0−1−2 1

1

2 3 4 5

1 + a
1 + a + a2

1
1 − a

Discrete-Time Signals and Systems

Figure 3.3 Graphical computation of convolution in Example 3.3.

To summarize, the convolution formula provides us with a means for comput-
ing the response of a relaxed, linear time-invariant system to any arbitrary input 
signal the input signal to the system, h(n) is the impulse response of the system, and 
y(n) is the output of the system in response to the input signal x(n). The evalua-
tion of the convolution formula involves four operations, namely: folding either the 
impulse either h(−k) or x(−k), respectively, shifting the folded sequence by n units 
in time to yield either h(n−k) or x(n−k), multiplying the two sequences to yield the 
product Discrete-Time Signals and Systems Figure 3.3 Graphical computation of 
convolution in Example 3.3. x(n). It takes one of two equivalent forms, either (3.17) 
or (3.28), where x(n) is response as specified by (3.17) or the input sequence as 
specified by (3.28) to yield

81



sequence, either x(k)h(n− k) or x(n− k)h(k), and finally summing all the values in
the product sequence to yield the output y(n) of the system at time n. The folding
operation is done only once. However, the other three operations are repeated for
all possible shifts −∞ < n <∞ in order to obtain y(n) for −∞ < n <∞.

Properties of Convolution and the Interconnection of LTI
Systems

In this section we investigate some important properties of convolution and interpret
these properties in terms of interconnecting linear time-invariant systems. We should
stress that these properties hold for every input signal.

It is convenient to simplify the notation by using an asterisk to denote the con-
volution operation. Thus

y(n) = x(n) ∗ h(n) ≡
∞∑

k=−∞
x(k)h(n− k)

In this notation the sequence following the asterisk [i.e., the impulse response h(n)]
is folded and shifted. The input to the system is x(n). On the other hand, we also
showed that

y(n) = h(n) ∗ x(n) ≡
∞∑

k=−∞
h(k)x(n− k)

In this form of the convolution formula, it is the input signal that is folded. Alter-
natively, we may interpret this form of the convolution formula as resulting from an
interchange of the roles of x(n) and h(n). In other words, we may regard x(n) as the

illustrates this interpretation.

Identity and Shifting Properties. We also note that the unit sample sequence δ(n)
is the identity element for convolution, that is

y(n) = x(n) ∗ δ(n) = x(n)

If we shift δ(n) by k , the convolution sequence is shifted also by k , that is

x(n) ∗ δ(n− k) = y(n− k) = x(n− k)

We can view convolution more abstractly as a mathematical operation between
two signal sequences, say x(n) and h(n), that satisfies a number of properties. The

x(n) y(n) h(n)
h(n) x(n)

y(n)

Interpretation of the commutative property of convolution.

Discrete-Time Signals and Systems

3.4

(3.31)

(3.32)

impulse response of the system and h(n) as the excitation or input signal. Figure 3.4

property embodied in (3.31) and (3.32) is called the commutative law.

Figure 3.4

82



Commutative law

x(n) ∗ h(n) = h(n) ∗ x(n)

Viewed mathematically, the convolution operation also satisfies the associative
law, which can be stated as follows.

Associative law

[x(n) ∗ h1(n)] ∗ h2(n) = x(n) ∗ [h1(n) ∗ h2(n)]

From a physical point of view, we can interpret x(n) as the input signal to a
linear time-invariant system with impulse response h1(n). The output of this system,
denoted as y1(n), becomes the input to a second linear time-invariant system with
impulse response h2(n). Then the output is

y(n) = y1(n) ∗ h2(n)
= [x(n) ∗ h1(n)] ∗ h2(n)

h(n) = h1(n) ∗ h2(n)

and

y(n) = x(n) ∗ h(n)

Furthermore, since the convolution operation satisfies the commutative property,
one can interchange the order of the two systems with responses h1(n) and h2(n)

trates the associative property.

(b)

(a)

x(n) y(n) x(n) y(n)h(n) =
h1(n) h1(n) * h2(n)

h2(n)

x(n) y(n) x(n) y(n)
h1(n) h1(n)h2(n) h2(n)

Implications of the associative (a) and the associative and commutative
(b) properties of convolution.

Discrete-Time Signals and Systems

(3.33)

(3.34)

without altering the overall input–output relationship. Figure 3.5 graphically illus-

which is precisely the left-hand side of (3.34). Thus the left-hand side of (3.34) corre-
sponds to having two linear time-invariant systems in cascade. Now the righthhand 
side of (3.34) indicates that the input x(n) is applied to an equivalent system having 
an impulse response, say h(n), which is equal to the convolution of the two impulse 
responses. That is,

Figure 3.5

83



Determine the impulse response for the cascade of two linear time-invariant systems having
impulse responses

h1(n) = (12 )
nu(n)

and

h2(n) = (14 )
nu(n)

Solution. To determine the overall impulse response of the two systems in cascade, we
simply convolve h1(n) with h2(n). Hence

h(n) =
∞∑

k=−∞
h1(k)h2(n− k)

where h2(n) is folded and shifted. We define the product sequence

vn(k) = h1(k)h2(n− k)

= (1
2
)k(

1
4
)n−k

which is nonzero for k ≥ 0 and n − k ≥ 0 or n ≥ k ≥ 0. On the other hand, for n < 0, we
have vn(k) = 0 for all k , and hence

h(n) = 0, n < 0

For n ≥ k ≥ 0, the sum of the values of the product sequence vn(k) over all k yields

h(n) =
n∑
k=0
(

1
2
)k(

1
4
)n−k

= (1
4
)n

n∑
k=0

2k

= (1
4
)n(2n+1 − 1)

= (1
2
)n[2− (1

2
)n], n ≥ 0

The generalization of the associative law to more than two systems in cascade fol-
lows easily from the discussion given above. Thus if we have L linear time-invariant
systems in cascade with impulse responses h1(n), h2(n), . . . , hL(n), there is an equiv-
alent linear time-invariant system having an impulse response that is equal to the
(L− 1)-fold convolution of the impulse responses. That is,

h(n) = h1(n) ∗ h2(n) ∗ · · · ∗ hL(n)

Discrete-Time Signals and Systems

EXAMPLE 3.4

(3.35)

84



x(n) x(n)y(n) y(n)
+

h(n) =
h1(n) + h2(n)

h1(n)

h2(n)

Interpretation of the distributive property of convolution: two LTI
systems connected in parallel can be replaced by a single system with h(n) =
h1(n)+ h2(n).

The commutative law implies that the order in which the convolutions are performed
is immaterial. Conversely, any linear time-invariant system can be decomposed into
a cascade interconnection of subsystems. A method for accomplishing the decom-
position will be described later.

law, which may be stated as follows.

x(n) ∗ [h1(n)+ h2(n)] = x(n) ∗ h1(n)+ x(n) ∗ h2(n)

systems with impulse responses h1(n) and h2(n) excited by the same input signal
x(n), the sum of the two responses is identical to the response of an overall system
with impulse response

h(n) = h1(n)+ h2(n)
Thus the overall system is viewed as a parallel combination of the two linear time-

ear time-invariant systems in parallel with impulse responses h1(n), h2(n), . . . , hL(n)

response

h(n) =
L∑
j=1

hj (n)

Conversely, any linear time-invariant system can be decomposed into a parallel in-
terconnection of subsystems.

Causal Linear Time-Invariant Systems

only on present and past inputs but does not depend on future inputs. In other words,
the output of the system at some time instant n, say n = n0 , depends only on values
of x(n) for n ≤ n0 .

In the case of a linear time-invariant system, causality can be translated to a
condition on the impulse response. To determine this relationship, let us consider a

Discrete-Time Signals and Systems

Figure 3.6

Interpreted physically, this law implies that if we have two linear time-invariant

Distributive law

Another property that is satisfied by the convolution operation is the distributive

(3.36)

invariant systems as illustrated in Fig. 3.6.

parallel follows easily by mathematical induction. Thus the interconnection of L lin-

and excited by the same input x(n) is equivalent to one overall system with impulse

The generalization of (3.36) to more than two linear time-invariant systems in

3.5

(3.37)

In Section 2.3 we defined a causal system as one whose output at time n depends

85



linear time-invariant system having an output at time n = n0 given by the convolution
formula

y(n0) =
∞∑

k=−∞
h(k)x(n0 − k)

Suppose that we subdivide the sum into two sets of terms, one set involving present
and past values of the input [i.e., x(n) for n ≤ n0 ] and one set involving future values
of the input [i.e., x(n), n > n0 ]. Thus we obtain

y(n0) =
∞∑
k=0

h(k)x(n0 − k)+
−1∑

k=−∞
h(k)x(n0 − k)

= [h(0)x(n0)+ h(1)x(n0 − 1)+ h(2)x(n0 − 2)+ · · ·]
+ [h(−1)x(n0 + 1)+ h(−2)x(n0 + 2)+ · · ·]

We observe that the terms in the first sum involve x(n0), x(n0 − 1), . . . , which are
the present and past values of the input signal. On the other hand, the terms in the
second sum involve the input signal components x(n0+1), x(n0+2), . . . . Now, if the
output at time n = n0 is to depend only on the present and past inputs, then, clearly,
the impulse response of the system must satisfy the condition

h(n) = 0, n < 0

Since h(n) is the response of the relaxed linear time-invariant system to a unit impulse
applied at n = 0, it follows that h(n) = 0 for n < 0 is both a necessary and a
sufficient condition for causality. Hence an LTI system is causal if and only if its
impulse response is zero for negative values of n.

Since for a causal system, h(n) = 0 for n < 0, the limits on the summation of the
convolution formula may be modified to reflect this restriction.
two equivalent forms

y(n) =
∞∑
k=0

h(k)x(n− k)

=
n∑

k=−∞
x(k)h(n− k)

As indicated previously, causality is required in any real-time signal processing
application, since at any given time n we have no access to future values of the input
signal. Only the present and past values of the input signal are available in computing
the present output.

It is sometimes convenient to call a sequence that is zero for n < 0, a causal
sequence, and one that is nonzero for n < 0 and n > 0, a noncausal sequence. This
terminology means that such a sequence could be the unit sample response of a causal
or a noncausal system, respectively.

Discrete-Time Signals and Systems

(3.38)

Thus we have the

(3.39)

(3.40)

86



If the input to a causal linear time-invariant system is a causal sequence [i.e., if
x(n) = 0 for n < 0], the limits on the convolution formula are further restricted. In
this case the two equivalent forms of the convolution formula become

y(n) =
n∑
k=0

h(k)x(n− k)

=
n∑
k=0

x(k)h(n− k)

forms are identical, and the upper limit is growing with time. Clearly, the response
of a causal system to a causal input sequence is causal, since y(n) = 0 for n < 0.

Determine the unit step response of the linear time-invariant system with impulse response

h(n) = anu(n), |a| < 1
Since the input signal is a unit step, which is a causal signal, and the system is

this problem, one can skip the steps involved with sketching the folded and shifted sequences.

y(n) =
n∑
k=0

ak

= 1− a
n+1

1− a

In this simple case, however, we computed the convolution algebraically without resorting to
the detailed procedure outlined previously.

Stability of Linear Time-Invariant Systems

As indicated previously, stability is an important property that must be considered
in any practical implementation of a system. We defined an arbitrary relaxed system
as BIBO stable if and only if its output sequence y(n) is bounded for every bounded
input x(n).

If x(n) is bounded, there exists a constant Mx such that

|x(n)| ≤ Mx <∞
Similarly, if the output is bounded, there exists a constant My such that

|y(n)| < My <∞
for all n.

Discrete-Time Signals and Systems

(3.41)

We observe that in this case, the limits on the summations for the two alternative

(3.42)

EXAMPLE 3.5

also causal, we can use one of the special forms of the convolution formula, either (3.41)
Solution.

or (3.42). Since x(n) = 1 for n ≥ 0, (3.41) is simpler to use. Because of the simplicity of

Instead, we use direct substitution of the signals sequences in (3.41) and obtain

and y(n) = 0 for n < 0. We note that this result is identical to that obtained in Example 3.3.

3.6

87



Now, given such a bounded input sequence x(n) to a linear time-invariant system,
let us investigate the implications of the definition of stability on the characteristics
of the system. Toward this end, we work again with the convolution formula

y(n) =
∞∑

k=−∞
h(k)x(n− k)

If we take the absolute value of both sides of this equation, we obtain

|y(n)| =
∣∣∣∣∣
∞∑

k=−∞
h(k)x(n− k)

∣∣∣∣∣
Now, the absolute value of the sum of terms is always less than or equal to the sum
of the absolute values of the terms. Hence

|y(n)| ≤
∞∑

k=−∞
|h(k)||x(n− k)|

If the input is bounded, there exists a finite number Mx such that |x(n)| ≤ Mx . By
substituting this upper bound for x(n) in the equation above, we obtain

|y(n)| ≤ Mx
∞∑

k=−∞
|h(k)|

From this expression we observe that the output is bounded if the impulse response
of the system satisfies the condition

Sh ≡
∞∑

k=−∞
|h(k)| <∞

That is, a linear time-invariant system is stable if its impulse response is absolutely
summable. This condition is not only sufficient but it is also necessary to ensure the
stability of the system. Indeed, we shall show that if Sh = ∞, there is a bounded
input for which the output is not bounded. We choose the bounded input

x(n) =
{
h∗(−n)
|h(−n)| , h(n) �= 0
0, h(n) = 0

where h∗(n) is the complex conjugate of h(n). It is sufficient to show that there is
one value of n for which y(n) is unbounded. For n = 0 we have

y(0) =
∞∑

k=−∞
x(−k)h(k) =

∞∑
k=−∞

|h(k)|2
|h(k)| = Sh

Thus, if Sh = ∞, a bounded input produces an unbounded output since y(0) = ∞.

Discrete-Time Signals and Systems

(3.43)

88



n approaches infinity. As a consequence, the output of the system goes to zero as n
approaches infinity if the input is set to zero beyond n > n0 . To prove this, suppose
that |x(n)| < Mx for n < n0 and x(n) = 0 for n ≥ n0 . Then, at n = n0 + N , the
system output is

y(n0 +N) =
N−1∑
k=−∞

h(k)x(n0 +N − k)+
∞∑
k=N

h(k)x(n0 +N − k)

But the first sum is zero since x(n) = 0 for n ≥ n0 . For the remaining part, we take
the absolute value of the output, which is

|y(n0 +N)| =
∣∣∣∣∣
∞∑
k=N

h(k)x(n0 +N − k)
∣∣∣∣∣ ≤

∞∑
k=N
|h(k)||x(n0 +N − k)|

≤Mx
∞∑
k=N
|h(k)|

Now, as N approaches infinity,

lim
N→∞

∞∑
k=N
|h(n)| = 0

and hence
lim
N→∞

|y(n0 +N)| = 0
This result implies that any excitation at the input to the system, which is of a finite
duration, produces an output that is “transient” in nature; that is, its amplitude decays
with time and dies out eventually, when the system is stable.

Determine the range of values of the parameter a for which the linear time-invariant system
with impulse response

h(n) = anu(n)
is stable.

Solution. First, we note that the system is causal. Consequently, the lower index on the

∞∑
k=0
|ak| =

∞∑
k=0
|a|k = 1+ |a| + |a|2 + · · ·

Clearly, this geometric series converges to
∞∑
k=0
|a|k = 1

1− |a|
provided that |a| < 1. Otherwise, it diverges. Therefore, the system is stable if |a| < 1.
Otherwise, it is unstable. In effect, h(n)must decay exponentially toward zero as n approaches
infinity for the system to be stable.

Discrete-Time Signals and Systems

The condition in (3.43) implies that the impulse response h(n) goes to zero as

EXAMPLE 3.6

summation in (3.43) begins with k = 0. Hence

89



Determine the range of values of a and b for which the linear time-invariant system with
impulse response

h(n) =
{
an, n ≥ 0
bn, n < 0

is stable.

Solution.

∞∑
n=−∞

|h(n)| =
∞∑
n=0
|a|n +

−1∑
n=−∞

|b|n

second sum can be manipulated as follows:

−1∑
n=−∞

|b|n =
∞∑
n=1

1
|b|n =

1
|b|

(
1+ 1|b| +

1
|b|2 + · · ·

)

= β(1+ β + β2 + · · ·) = β
1− β

where β = 1/|b| must be less than unity for the geometric series to converge. Consequently,
the system is stable if both |a| < 1 and |b| > 1 are satisfied.

Systems with Finite-Duration and Infinite-Duration Impulse
Response

Up to this point we have characterized a linear time-invariant system in terms of
its impulse response h(n). It is also convenient, however, to subdivide the class
of linear time-invariant systems into two types, those that have a finite-duration
impulse response (FIR) and those that have an infinite-duration impulse response
(IIR). Thus an FIR system has an impulse response that is zero outside of some
finite time interval. Without loss of generality, we focus our attention on causal FIR
systems, so that

h(n) = 0, n < 0 and n ≥M
The convolution formula for such a system reduces to

y(n) =
M−1∑
k=0

h(k)x(n− k)

A useful interpretation of this expression is obtained by observing that the output at
any time n is simply a weighted linear combination of the input signal samples x(n),
x(n− 1), . . . , x(n−M + 1). In other words, the system simply weights, by the values
of the impulse response h(k), k = 0, 1, . . . ,M−1, the most recent M signal samples

Discrete-Time Signals and Systems

EXAMPLE 3.7

This system is noncasual. The condition on stability given by (3.43) yields

From Example 3.6 we have already determined that the first sum converges for |a| < 1. The

3.7

90



and sums the resulting M products. In effect, the system acts as a window that views
only the most recent M input signal samples in forming the output. It neglects or
simply “forgets” all prior input samples [i.e., x(n−M), x(n−M − 1), . . .]. Thus we
say that an FIR system has a finite memory of length-M samples.

In contrast, an IIR linear time-invariant system has an infinite-duration impulse
response. Its output, based on the convolution formula, is

y(n) =
∞∑
k=0

h(k)x(n− k)

where causality has been assumed, although this assumption is not necessary. Now,
the system output is a weighted [by the impulse response h(k)] linear combination
of the input signal samples x(n), x(n − 1), x(n − 2), . . . . Since this weighted sum
involves the present and all the past input samples, we say that the system has an
infinite memory.

Up to this point we have treated linear and time-invariant systems that are char-
acterized by their unit sample response h(n). In turn, h(n) allows us to determine
the output y(n) of the system for any given input sequence x(n) by means of the
convolution summation,

y(n) =
∞∑

k=−∞
h(k)x(n− k)

If the system is IIR, however, its practical implementation as implied by convo-
lution is clearly impossible, since it requires an infinite number of memory locations,
multiplications, and additions. A question that naturally arises, then, is whether
or not it is possible to realize IIR systems other than in the form suggested by the
convolution summation. Fortunately, the answer is yes, there is a practical and com-
putationally efficient means for implementing a family of IIR systems, as will be
demonstrated in this section. Within the general class of IIR systems, this family of
discrete-time systems is more conveniently described by difference equations. This
family or subclass of IIR systems is very useful in a variety of practical applications,
including the implementation of digital filters, and the modeling of physical phenom-
ena and physical systems.

Discrete-Time Signals and Systems

(4.1)

Discrete-Time Systems Described by Difference Equations4

In general, then, we have shown that any linear time-invariant system is character-
ized by the input–output relationship in (4.1). Moreover, the convolution summa-
tion formula in (4.1) suggests a means for the realization of the system. In the case 
of FIR systems, such a realization involves additions, multiplications, and a finite 
number of memory locations. Consequently, an FIR system is readily implemented 
directly, as implied by the convolution summation.

We investigate the characteristics of FIR and IIR systems in more detail.

91



Recursive and Nonrecursive Discrete-Time Systems

As indicated above, the convolution summation formula expresses the output of the
linear time-invariant system explicitly and only in terms of the input signal. However,
this need not be the case, as is shown here. There are many systems where it is either
necessary or desirable to express the output of the system not only in terms of the
present and past values of the input, but also in terms of the already available past
output values. The following problem illustrates this point.

interval 0 ≤ k ≤ n, defined as

y(n) = 1
n+ 1

n∑
k=0

x(k), n = 0, 1, . . .

samples x(k) for 0 ≤ k ≤ n. Since n is increasing, our memory requirements grow
linearly with time.

Our intuition suggests, however, that y(n) can be computed more efficiently by
utilizing the previous output value y(n−1). Indeed, by a simple algebraic rearrange-

(n+ 1)y(n) =
n−1∑
k=0

x(k)+ x(n)

= ny(n− 1)+ x(n)

and hence

y(n) = n
n+ 1y(n− 1)+

1
n+ 1x(n)

previous output value y(n − 1) by n/(n + 1), multiplying the present input x(n) by
1/(n + 1), and adding the two products. Thus the computation of y(n) by means

illustrated in Fig. 4.1. This is an example of a recursive system. In general, a system

y(n− 2), . . . is called a recursive system.

Realization of a recursive
cumulative averaging
system.

+ �

�

x(n) y(n)

1
n + 1

n

z−1

Discrete-Time Signals and Systems

4.1

Suppose that we wish to compute the cumulative average of a signal x(n) in the

(4.2)

As implied by (4.2), the computation of y(n) requires the storage of all the input

ment of (4.2), we obtain

Now, the cumulative average y(n) can be computed recursively by multiplying the

(4.3)

of (4.3) requires two multiplications, one addition, and one memory location, as

whose output y(n) at time n depends on any number of past output values y(n− 1),

Figure 4.1

92



suppose that we begin the process with n = 0 and proceed forward in time. Thus,

y(0) = x(0)

y(1) = 1
2
y(0)+ 1

2
x(1)

y(2) = 2
3
y(1)+ 1

3
x(2)

and so on. If one grows fatigued with this computation and wishes to pass the problem
to someone else at some time, say n = n0 , the only information that one needs to
provide his or her successor is the past value y(n0 − 1) and the new input samples
x(n), x(n+ 1), . . . . Thus the successor begins with

y(n0) = n0
n0 + 1y(n0 − 1)+

1
n0 + 1x(n0)

and proceeds forward in time until some time, say n = n1 , when he or she becomes
fatigued and passes the computational burden to someone else with the information
on the value y(n1 − 1), and so on.

The point we wish to make in this discussion is that if one wishes to compute the

x(n) applied at n = n0 , we need the value y(n0 − 1) and the input samples x(n) for
n ≥ n0 . The term y(n0
and contains all the information needed to determine the response of the system for
n ≥ n0 to the input signal x(n), independent of what has occurred in the past.

The following example illustrates the use of a (nonlinear) recursive system to
compute the square root of a number.

Square-Root Algorithm
Many computers and calculators compute the square root of a positive number A, using the
iterative algorithm

sn = 12
(
sn−1 + A

sn−1

)
, n = 0, 1, . . .

where sn−1 is an initial guess (estimate) of
√
A. As the iteration converges we have sn ≈ sn−1 .

Then it easily follows that sn ≈
√
A.

Consider now the recursive system

y(n) = 1
2

[
y(n− 1)+ x(n)

y(n− 1)
]

If we excite this system with a step of amplitude A [i.e.,
x(n) = Au(n)] and use as an initial condition y(−1) an estimate of √A, the response y(n) of
the system will tend toward

√
do not need to specify exactly the initial condition. A rough estimate is sufficient for the proper
performance of the system. For example, if we let A = 2 and y(−1) = 1, we obtain y(0) = 32 ,
y(1) = 1.4166667, y(2) = 1.4142157. Similarly, for y(−1) = 1.5, we have y(0) = 1.416667,
y(1) = 1.4142157. Compare these values with the √2, which is approximately 1.4142136.

Discrete-Time Signals and Systems

To determine the computation of the recursive system in (4.3) in more detail,

response (in this case, the cumulative average) of the system (4.3) to an input signal

− 1) is called the initial condition for the system in (4.3)

EXAMPLE 4.1

(4.4)

which is realized as in Fig. 4.2.

A as n increases. Note that in contrast to the system (4.3), we

according to ( 4.3), we obtain

                                

93



Realization of the
square-root system.

+�
x(n) y(n)

z−1

1
y(n − 1)

y(n − 1)

1
2

We have now introduced two simple recursive systems, where the output y(n)
depends on the previous output value y(n− 1) and the current input x(n). Both sys-
tems are causal. In general, we can formulate more complex causal recursive systems,
in which the output y(n) is a function of several past output values and present and
past inputs. The system should have a finite number of delays or, equivalently, should
require a finite number of storage locations to be practically implemented. Thus the
output of a causal and practically realizable recursive system can be expressed in
general as

y(n) = F [y(n− 1), y(n− 2), . . . , y(n−N), x(n), x(n− 1), . . . , x(n−M)]
where F [·] denotes some function of its arguments. This is a recursive equation
specifying a procedure for computing the system output in terms of previous values
of the output and present and past inputs.

In contrast, if y(n) depends only on the present and past inputs, then

y(n) = F [x(n), x(n− 1), . . . , x(n−M)]

(2.4.6). Indeed, the convolution summation for a causal FIR system is

y(n) =
M∑
k=0

h(k)x(n− k)

= h(0)x(n)+ h(1)x(n− 1)+ · · · + h(M)x(n−M)
= F [x(n), x(n− 1), . . . , x(n−M)]

where the function F [·] is simply a linear weighted sum of present and past inputs
and the impulse response values h(n), 0 ≤ n ≤ M , constitute the weighting coeffi-
cients. Consequently, the causal linear time-invariant FIR systems described by the

nonrecursive and recursive systems are illustrated in Fig. 4.3. A simple inspection

the feedback loop in the recursive system, which feeds back the output of the system
into the input. This feedback loop contains a delay element. The presence of this
delay is crucial for the realizability of the system, since the absence of this delay
would force the system to compute y(n) in terms of y(n), which is not possible for
discrete-time systems.

Discrete-Time Signals and Systems

Figure 4.2

(4.5)

Such a system is called nonrecursive. We hasten to add that the causal FIR systems

(4.6)

of this figure reveals that the fundamental difference between these two systems is

convolution formula in Section 3.7 are nonrecursive. The basic differences between

described in Section 3.7 in terms of the convolution sum formula have the form of

94



Basic form for a causal and
realizable (a) nonrecursive
and (b) recursive system.

z−1

x(n) y(n)

x(n) y(n)

(a)

(b)

F [y(n − 1) … , y(n − N),
x(n), … , x(n − M)]

F [x(n), x(n − 1),
 … , x(n − M)]

creates another important difference between recursive and nonrecursive systems.
For example, suppose that we wish to compute the output y(n0) of a system when it is
excited by an input applied at time n = 0. If the system is recursive, to compute y(n0),
we first need to compute all the previous values y(0), y(1), . . . , y(n0−1). In contrast,
if the system is nonrecursive, we can compute the output y(n0) immediately without
having y(n0−1), y(n0−2), . . . . In conclusion, the output of a recursive system should
be computed in order [i.e., y(0), y(1), y(2), . . .], whereas for a nonrecursive system,
the output can be computed in any order [i.e., y(200), y(15), y(3), y(300), etc.]. This
feature is desirable in some practical applications.

Linear Time-Invariant Systems Characterized by
Constant-Coefficient Difference Equations

terms of their impulse responses. In this subsection we focus our attention on a
family of linear time-invariant systems described by an input–output relation called
a difference equation with constant coeffficients. Systems described by constant-
coefficient linear difference equations are a subclass of the recursive and nonrecursive
systems introduced in the preceding subsection. To bring out the important ideas,
we begin by treating a simple recursive system described by a first-order difference
equation.

Suppose that we have a recursive system with an input–output equation

y(n) = ay(n− 1)+ x(n)

Discrete-Time Signals and Systems

Figure 4.3

The presence of the feedback loop or, equivalently, the recursive nature of (4.5)

4.2

In Section 3 we treated linear time-invariant systems and characterized them in

(4.7)

where  is a constant. Figure 4.4 shows a block diagram realization of the system. 
In comparing this system with the cumulative averaging system described by the 
input–output equation (4.3), we observe that the system in (4.7) has a constant coef-
ficient (independent of time), whereas the system described in (4.3) has timevariant 
coefficients. As we will show, (4.7) is an input–output equation for a linear time-
invariant system, whereas (4.3) describes a linear time-variant system.

a

95



Block diagram realization
of a simple recursive
system.

+
x(n) y(n)

z−1

a

Now, suppose that we apply an input signal x(n) to the system for n ≥ 0. We
make no assumptions about the input signal for n < 0, but we do assume the existence
of the initial condition y(−1).
we must solve this equation to obtain an explicit expression for the system output.
Suppose that we compute successive values of y(n) for n ≥ 0, beginning with y(0).
Thus

y(0) = ay(−1)+ x(0)
y(1) = ay(0)+ x(1) = a2y(−1)+ ax(0)+ x(1)
y(2) = ay(1)+ x(2) = a3y(−1)+ a2x(0)+ ax(1)+ x(2)

...
...

y(n) = ay(n− 1)+ x(n)
= an+1y(−1)+ anx(0)+ an−1x(1)+ · · · + ax(n− 1)+ x(n)

or, more compactly,

y(n) = an+1y(−1)+
n∑
k=0

akx(n− k), n ≥ 0

of two parts. The first part, which contains the term y(−1), is a result of the initial
condition y(−1) of the system. The second part is the response of the system to the
input signal x(n).

If the system is initially relaxed at time n = 0, then its memory (i.e., the output
of the delay) should be zero. Hence y(−1) = 0. Thus a recursive system is relaxed if
it starts with zero initial conditions. Because the memory of the system describes, in
some sense, its “state,” we say that the system is at zero state and its corresponding
output is called the zero-state response, and is denoted by yzs(n). Obviously, the

yzs(n) =
n∑
k=0

akx(n− k), n ≥ 0

input signal convolved with the impulse response

h(n) = anu(n)

Discrete-Time Signals and Systems

Figure 4.4

Since (4.7) describes the system output implicitly,

(4.8)

The response y(n) of the system as given by the right-hand side of (4.8) consists

It is interesting to note that (4.9) is a convolution summation involving the

(4.10)

(4.9)

zero-state response of the system (4.7) is given by

96



We also observe that the system described by the first-order difference equation in

is k = 0. Furthermore, the condition y(−1) = 0 implies that the input signal can be

is n, since x(n − k) = 0 for k > n. In effect, we have obtained the result that the

a linear time-invariant IIR system with impulse response given by (4.10).

y(−1) �= 0] and the input x(n) = 0 for all n. Then the output of the system with zero
input is called the zero-input response or natural response and is denoted by yzi(n).

yzi(n) = an+1y(−1), n ≥ 0

sense that it can produce an output without being excited. Note that the zero-input
response is due to the memory of the system.

To summarize, the zero-input response is obtained by setting the input signal
to zero, making it independent of the input. It depends only on the nature of the
system and the initial condition. Thus the zero-input response is a characteristic of
the system itself, and it is also known as the natural or free response of the system.
On the other hand, the zero-state response depends on the nature of the system and
the input signal. Since this output is a response forced upon it by the input signal, it
is usually called the forced response of the system. In general, the total response of
the system can be expressed as y(n) = yzi(n)+ yzs(n).

by linear constant-coefficient difference equations. The general form for such an
equation is

y(n) = −
N∑
k=1

aky(n− k)+
M∑
k=0

bkx(n− k)

or, equivalently,

N∑
k=0

aky(n− k) =
M∑
k=0

bkx(n− k), a0 ≡ 1

The integer N is called the order of the difference equation or the order of the

negative signs.

weighted sum of past outputs y(n − 1), y(n − 2), . . . , y(n − N) as well as past
and present input signals samples. We observe that in order to determine y(n)
for n ≥ 0, we need the input x(n) for all n ≥ 0, and the initial conditions y(−1),

Discrete-Time Signals and Systems

assumed causal and hence the upper limit on the convolution summation in (4.9)

relaxed recursive system described by the first-order difference equation in (4.7) is

Now, suppose that the system described by (4.7) is initially nonrelaxed [i.e.,

(4.7) is causal. As a result, the lower limit on the convolution summation in (4.9)

We observe that a recursive system with nonzero initial condition is nonrelaxed in the

(4.11)

plest possible recursive system in the general class of recursive systems described
The system described by the first-order difference equation in (4.7) is the sim-

(4.12)

(4.13)

system. The negative sign on the right-hand side of (4.12) is introduced as a matter
of convenience to allow us to express the difference equation in (4.13) without any

Equation (4.12) expresses the output of the system at time n directly as a

From (4.7), with x(n) = 0 for −∞ < n <∞, we obtain

97



y(−2), . . . , y(−N). In other words, the initial conditions summarize all that we need
to know about the past history of the response of the system to compute the present
and future outputs. The general solution of the N-order constant-coefficient differ-
ence equation is considered in the following subsection.

At this point we restate the properties of linearity, time invariance, and stability
in the context of recursive systems described by linear constant-coefficient difference
equations. As we have observed, a recursive system may be relaxed or nonrelaxed,
depending on the initial conditions. Hence the definitions of these properties must
take into account the presence of the initial conditions.

We begin with the definition of linearity. A system is linear if it satisfies the
following three requirements:

1. The total response is equal to the sum of the zero-input and zero-state responses
[i.e., y(n) = yzi(n)+ yzs(n)].

2. The principle of superposition applies to the zero-state response (zero-state
linear).

3. The principle of superposition applies to the zero-input response (zero-input
linear).

A system that does not satisfy all three separate requirements is by definition non-
linear. Obviously, for a relaxed system, yzi(n) = 0, and thus requirement 2, which is

We illustrate the application of these requirements by a simple example.

Determine if the recursive system defined by the difference equation

y(n) = ay(n− 1)+ x(n)

is linear.

Solution.

y(n) = yzi(n)+ yzs(n)

Thus the first requirement for linearity is satisfied.
To check for the second requirement, let us assume that x(n) = c1x1(n)+ c2x2(n). Then

yzs(n) =
n∑
k=0

ak[c1x1(n− k)+ c2x2(n− k)]

= c1
n∑
k=0

akx1(n− k)+ c2
n∑
k=0

akx2(n− k)

= c1y(1)zs (n)+ c2y(2)zs (n)

Hence yzs (n) satisfies the principle of superposition, and thus the system is zero-state linear.

Discrete-Time Signals and Systems

EXAMPLE 4.2

the definition of linearity given in Section 2.4, is sufficient.

By combining (4.9) and (4.11), we obtain (4.8), which can be expressed as

(4.9) gives

98



1 1 2 2

yzi(n) = an+1[c1y1(−1)+ c2y2(−1)]
= c1an+1y1(−1)+ c2an+1y2(−1)

= c1y(1)zi (n)+ c2y(2)zi (n)
Hence the system is zero-input linear.

Since the system satisfies all three conditions for linearity, it is linear.

strate linearity for the system described by the first-order difference equation carries
over directly to the general recursive systems described by the constant-coefficient

Determine if the linear time-invariant recursive system described by the difference equation

Solution. Let us assume that the input signal x(n) is bounded in amplitude, that is, |x(n)| ≤
Mx

|y(n)| ≤ |an+1y(−1)| +
∣∣∣∣∣
n∑
k=0

akx(n− k)
∣∣∣∣∣ , n ≥ 0

≤ |a|n+1|y(−1)| +Mx
n∑
k=0
|a|k, n ≥ 0

≤ |a|n+1|y(−1)| +Mx 1− |a|
n+1

1− |a| = My, n ≥ 0

Discrete-Time Signals and Systems

Now let us assume that y(−1) = c y (−1)+ c y (−1). From (4.11) we obtain

Although it is somewhat tedious, the procedure used in Example 4.2 to demon-

difference equation given in (4.13). Hence, a recursive system described by the lin-

of linearity, and therefore it is linear.
ear difference equation in (4.13) also satisfies all three conditions in the definition

The next question that arises is whether or not the causal linear system de-
scribed by the linear constant-coefficient difference equation in (4.13) is time invari-
ant. This is fairly easy, when dealing with systems described by explicit input–output 
mathematical relationships. Clearly, the system described by (4.13) is time invariant 
because the coefficients  and  are constants. On the other hand, if one or more 
of these coefficients depends on time, the system is time variant, since its properties 
change as a function of time. Thus we conclude that the recursive system described 
by a linear constant-coefficient difference equation is linear and time invariant.

The final issue is the stability of the recursive system described by the linear, 
constant-coefficient difference equation in (4.13). In Section 3.6 we introduced the 
concept of bounded input–bounded output (BIBO) stability for relaxed systems. 
For nonrelaxed systems that may be nonlinear, BIBO stability should be viewed 
with some care. However, in the case of a linear time-invariant recursive system 
described by the linear constant-coefficient difference equation in (4.13), it suffices 
to state that such a system is BIBO stable if and only if for every bounded input and 
every bounded initial condition, the total system response is bounded.

ak bk

EXAMPLE 4.3

given in (4.7) is stable.

<∞ for all n ≥ 0. From (4.8) we have

99



If n is finite, the bound My is finite and the output is bounded independently of the value of
a . However, as n → ∞, the bound My remains finite only if |a| < 1 because |a|n → 0 as
n→∞. Then My = Mx/(1− |a|).

Thus the system is stable only if |a| < 1.

condition for BIBO stability in terms of the system parameter a , namely |a| < 1.
We should stress, however, that this task becomes more difficult for higher-order
systems. Fortunately, other simple and more efficient techniques exist for investiga
ting the stability of recursive systems.

4.3 Solution of Linear Constant-Coefficient Difference Equations

Basically, the goal is to determine the output y(n), n ≥ 0, of the system given a
specific input x(n), n ≥ 0, and a set of initial conditions. The direct solution method
assumes that the total solution is the sum of two parts:

y(n) = yh(n)+ yp(n)

The part yh(n) is known as the homogeneous or complementary solution, whereas
yp(n) is called the particular solution.

The homogeneous solution of a difference equation. We begin the problem of

taining first the solution to the homogeneous difference equation

N∑
k=0

aky(n− k) = 0

The procedure for solving a linear constant-coefficient difference equation di-
rectly is very similar to the procedure for solving a linear constant-coefficient differen-
tial equation. Basically, we assume that the solution is in the form of an exponential,
that is,

yh(n) = λn

where the subscript h on y(n) is used to denote the solution to the homogeneous

polynomial equation
N∑
k=0

akλ
n−k = 0

Discrete-Time Signals and Systems

Given a linear constant-coefficient difference equation as the input–output relation-
ship describing a linear time-invariant system, our objective in this subsection is to 
determine an explicit expression for the output  The method that is developed 
is termed the direct method.  An alternative method based on the -transform is 
beyond the scope of this chapter. The -transform approach is called the indirect 
method. 

For the simple first-order system in Example 4.3, we were able to express the

solving the linear constant-coefficient difference equation given by (4.13) by ob-

(4.14)

difference equation. If we substitute this assumed solution in (4.14), we obtain the

(4.15)

z

z

y(n).

-

100



or
λn−N(λN + a1λN−1 + a2λN−2 + · · · + aN−1λ+ aN) = 0

The polynomial in parentheses is called the characteristic polynomial of the sys-
tem. In general, it has N roots, which we denote as λ1 , λ2, . . . , λN . The roots can be
real or complex valued. In practice the coefficients a1 , a2, . . . , aN are usually real.
Complex-valued roots occur as complex-conjugate pairs. Some of the N roots may
be identical, in which case we have multiple-order roots.

For the moment, let us assume that the roots are distinct, that is, there are no
multiple-order roots. Then the most general solution to the homogeneous difference

yh(n) = C1λn1 + C2λn2 + · · · + CNλnN
where C1 , C2, . . . , CN are weighting coefficients.

These coefficients are determined from the initial conditions specified for the
system. Since the input x(n) =
response of the system. The following examples illustrate the procedure.

Determine the homogeneous solution of the system described by the first-order difference
equation

y(n)+ a1y(n− 1) = x(n)
Solution. The assumed solution obtained by setting x(n) = 0 is

yh(n) = λn

λn + a1λn−1 = 0
λn−1(λ+ a1) = 0

λ = −a1
Therefore, the solution to the homogeneous difference equation is

yh(n) = Cλn = C(−a1)n

y(0) = −a1y(−1)

yh(0) = C
and hence the zero-input response of the system is

yzi(n) = (−a1)n+1y(−1), n ≥ 0
With a 1
obtained earlier by iteration of the difference equation.

Discrete-Time Signals and Systems

(4.16)

equation in (4.14) is

(4.17)

0, (4.17) can be used to obtain the zero-input

EXAMPLE 4.4

(4.18)

(4.19)

The zero-input response of the system can be determined from (4.18) and (4.19). With

On the other hand, from (4.19) we have

(4.20)

= −a , this result is consistent with (4.11) for the first-order system, which was

When we substitute this solution in (4.18), we obtain [with x(n) = 0]

x(n) = 0, (4.18) yields

101



Determine the zero-input response of the system described by the homogeneous second-order
difference equation

y(n)− 3y(n− 1)− 4y(n− 2) = 0

Solution. First we determine the solution to the homogeneous equation. We assume the
solution to be the exponential

yh(n) = λn

λn − 3λn−1 − 4λn−2 = 0
λn−2(λ2 − 3λ− 4) = 0

equation is
yh(n) = C1λn1 + C2λn2

= C1(−1)n + C2(4)n

The zero-input response of the system can be obtained from the homogenous solution by

difference equation in (4.21) we have

y(0) = 3y(−1)+ 4y(−2)

y(1) = 3y(0)+ 4y(−1)

= 3[3y(−1)+ 4y(−2)]+ 4y(−1)

= 13y(−1)+ 12y(−2)

y(0) = C1 + C2
y(1) = −C1 + 4C2

By equating these two sets of relations, we have

C1 + C2 = 3y(−1)+ 4y(−2)
−C1 + 4C2 = 13y(−1)+ 12y(−2)

The solution of these two equations is

C1 = −15y(−1)+
4
5
y(−2)

C2 = 165 y(−1)+
16
5
y(−2)

Discrete-Time Signals and Systems

Therefore, the roots are λ = −1, 4, and the general form of the solution to the homogeneous

(4.22)

EXAMPLE 4.5

(21)

Upon substitution of this solution into (4.21), we obtain the characteristic equation

evaluating the constants in (4.22), given the initial conditions y(−1) and y(−2). From the

On the other hand, from (4.22) we obtain

102



Therefore, the zero-input response of the system is

yzi(n) = [−15y(−1)+
4
5
y(−2)](−1)n

+ [ 16
5
y(−1)+ 16

5
y(−2)](4)n, n ≥ 0

For example, if y(−2) = 0 and y(−1) = 5, then C1 = −1, C2 = 16, and hence

yzi(n) = (−1)n+1 + (4)n+2, n ≥ 0

These examples illustrate the method for obtaining the homogeneous solution
and the zero-input response of the system when the characteristic equation contains
distinct roots.

1

yh(n) = C1λn1 + C2nλn1 + C3n2λn1 + · · · + Cmnm−1λn1
+ Cm+1λnm+1 + · · · + CNλn

The particular solution of the difference equation. The particular solution yp(n)

n ≥ 0. In other words, yp(n) is any solution satisfying

N∑
k=0

akyp(n− k) =
M∑
k=0

bkx(n− k), a0 = 1

p

x(n). The following example illustrates the procedure.

Determine the particular solution of the first-order difference equation

y(n)+ a1y(n− 1) = x(n), |a1| < 1

when the input x(n) is a unit step sequence, that is,

x(n) = u(n)

Solution. Since the input sequence x(n) is a constant for n ≥ 0, the form of the solution
that we assume is also a constant. Hence the assumed solution of the difference equation to
the forcing function x(n), called the particular solution of the difference equation, is

yp(n) = Ku(n)

Discrete-Time Signals and Systems

(4.23)

(4.24)

(4.25)

(4.26)

On the other hand, if the characteristic equation contains multiple
roots, the form of the solution given in (4.17) must be modified. For example, if λ
is a root of multiplicity m, then (4.17) becomes

is required to satisfy the difference equation (4.13) for the specific input signal x(n),

To solve (4.25), we assume for y (n), a form that depends on the form of the input

EXAMPLE 4.6

103



Upon substitution of this

Ku(n)+ a1Ku(n− 1) = u(n)

To determine K, we must evaluate this equation for any n ≥ 1, where none of the terms
vanish. Thus

K + a1K = 1

K = 1
1+ a1

Therefore, the particular solution to the difference equation is

yp(n) = 11+ a1 u(n)

In this example, the input x(n), n ≥ 0, is a constant and the form assumed for
the particular solution is also a constant. If x(n) is an exponential, we would assume
that the particular solution is also an exponential. If x(n) were a sinusoid, then yp(n)
would also be a sinusoid. Thus our assumed form for the particular solution takes the

solution for several types of excitation.

Determine the particular solution of the difference equation

y(n) = 5
6
y(n− 1)− 1

6
y(n− 2)+ x(n)

when the forcing function x(n) = 2n , n ≥ 0 and zero elsewhere.

General Form of the Particular Solution for
Several Types of Input Signals

Input Signal, Particular Solution,
x(n) yp(n)

A (constant) K
AMn KMn

AnM K0n
M +K1nM−1 + · · · +KM

AnnM An(K0n
M +K1nM−1 + · · · +KM){

A cosω0n
A sinω0n

}
K1 cosω0n+K2 sinω0n

Discrete-Time Signals and Systems

(4.27)

EXAMPLE 4.7

TABLE 1

basic form of the signal x(n). Table 1 provides the general form of the particular

where K is a scale factor determined so that (4.26) is satisfied.
assumed solution into (4.26), we obtain

104



Solution. The form of the particular solution is

yp(n) = K2n, n ≥ 0

Upon substitution of yp(n) into the difference equation, we obtain

K2nu(n) = 5
6
K2n−1u(n− 1)− 1

6
K2n−2u(n− 2)+ 2nu(n)

To determine the value of K , we can evaluate this equation for any n ≥ 2, where none of the
terms vanish. Thus we obtain

4K = 5
6
(2K)− 1

6
K + 4

and hence K = 85 . Therefore, the particular solution is

yp(n) = 85 2
n, n ≥ 0

We have now demonstrated how to determine the two components of the solu-
tion to a difference equation with constant coefficients. These two components are
the homogeneous solution and the particular solution. From these two components,
we construct the total solution from which we can obtain the zero-state response.

The total solution of the difference equation. The linearity property of the linear
constant-coefficient difference equation allows us to add the homogeneous solution
and the particular solution in order to obtain the total solution. Thus

y(n) = yh(n)+ yp(n)

The resultant sum y(n) contains the constant parameters {Ci} embodied in the ho-
mogeneous solution component yh(n). These constants can be determined to satisfy
the initial conditions. The following example illustrates the procedure.

Determine the total solution y(n), n ≥ 0, to the difference equation

y(n)+ a1y(n− 1) = x(n)

when x(n) is a unit step sequence [i.e., x(n) = u(n)] and y(−1) is the initial condition.

Solution.

yh(n) = C(−a1)n

yp(n) = 11+ a1 u(n)

Discrete-Time Signals and Systems

EXAMPLE 4.8

(4.28)

From (4.19) of Example 4.4, the homogeneous solution is

and from (4.26) of Example 4.6, the particular solution is

105



Consequently, the total solution is

y(n) = C(−a1)n + 11+ a1 , n ≥ 0

where the constant C is determined to satisfy the initial condition y(−1).
In particular, suppose that we wish to obtain the zero-state response of the system de-

y(0)+ a1y(−1) = 1
Hence,

y(0) = 1− a1y(−1)

y(0) = C + 1
1+ a1

By equating these two relations, we obtain

C + 1
1+ a1 = −a1y(−1)+ 1

C = −a1y(−1)+ a11+ a1

y(n) = (−a1)n+1y(−1)+ 1− (−a1)
n+1

1+ a1 , n ≥ 0

= yzi(n)+ yzs(n)

1

the value of the constant C depends both on the initial condition y(−1) and on the
excitation function. Consequently, the value of C influences both the zero-input
response and the zero-state response.

We further observe that the particular solution to the difference equation can be
obtained from the zero-state response of the system. Indeed, if |a1| < 1, which is the

value of yzs(n) as n approaches infinity is the particular solution, that is,

yp(n) = lim
n→∞ yzs(n) =

1
1+ a1

Since this component of the system response does not go to zero as n approaches
infinity, it is usually called the steady-state response of the system. This response
persists as long as the input persists. The component that dies out as n approaches
infinity is called the transient response of the system.

The following example illustrates the evaluation of the total solution for a second-
order recursive system.

Discrete-Time Signals and Systems

(4.29)

scribed by the first-order difference equation in (4.28). Then we set y(−1) = 0. To evaluate
C , we evaluate (4.28) at n = 0, obtaining

Finally, if we substitute this value of C into (4.29), we obtain

On the other hand, (4.29) evaluated at n = 0 yields

(4.30)

We observe that the system response as given by (4.30) is consistent with the

obtained by solving the difference equation iteratively. Furthermore, we note that
response y(n) given in (4.8) for the first-order system (with a = −a) , which was

condition for stability of the system, as will be shown in Section 4.4, the limiting

106



Determine the response y(n), n ≥ 0, of the system described by the second-order difference
equation

y(n)− 3y(n− 1)− 4y(n− 2) = x(n)+ 2x(n− 1)
when the input sequence is

x(n) = 4nu(n)
Solution.

yh(n) = C1(−1)n + C2(4)n

as x(n). Normally, we could assume a solution of the form

yp(n) = K(4)nu(n)

However, we observe that yp(n) is already contained in the homogeneous solution, so that this
particular solution is redundant. Instead, we select the particular solution to be linearly inde-
pendent of the terms contained in the homogeneous solution. In fact, we treat this situation
in the same manner as we have already treated multiple roots in the characteristic equation.
Thus we assume that

yp(n) = Kn(4)nu(n)

Kn(4)nu(n)− 3K(n−1)(4)n−1u(n−1)−4K(n−2)(4)n−2u(n−2) = (4)nu(n)+2(4)n−1u(n−1)

To determine K , we evaluate this equation for any n ≥ 2, where none of the unit step
terms vanish. To simplify the arithmetic, we select n = 2, from which we obtain K = 65 .
Therefore,

yp(n) = 65n(4)
nu(n)

Thus

y(n) = C1(−1)n + C2(4)n + 65n(4)
n, n ≥ 0

where the constants C1 and C2

y(0) = 3y(−1)+ 4y(−2)+ 1
y(1) = 3y(0)+ 4y(−1)+ 6
= 13y(−1)+ 12y(−2)+ 9

y(0) = C1 + C2

y(1) = −C1 + 4C2 + 245

Discrete-Time Signals and Systems

EXAMPLE 4.9

(4.31)

We have already determined the solution to the homogeneous difference equation

(4.32)

for this system in Example 4.5. From (4.22) we have

The particular solution to (4.31) is assumed to be an exponential sequence of the same form

(4.33)

Upon substitution of (4.33) into (4.31), we obtain

(4.34)

The total solution to the difference equation is obtained by adding (4.32) to (4.34).

are determined such that the initial conditions are satisfied.

(4.35)

To accomplish this, we return to (4.31), from which we obtain

On the other hand, (4.35) evaluated at n = 0 and n = 1 yields

107



We can now equate these two sets of relations to obtain C1 and C2 . In so doing, we have
the response due to initial conditions y(−1) and y(−2) (the zero-input response), and the
zero-state response.

the computations above by setting y(−1) = y(−2) = 0. Then we have
C1 + C2 = 1

−C1 + 4C2 + 245 = 9

Hence C1 = − 125 and C2 = 2625 . Finally, we have the zero-state response to the forcing function
x(n) = (4)nu(n) in the form

yzs(n) = − 125 (−1)
n + 26

25
(4)n + 6

5
n(4)n, n ≥ 0

The total response of the system, which includes the response to arbitrary initial conditions,

The Impulse Response of a Linear Time-Invariant Recursive
System

The impulse response of a linear time-invariant system was previously defined as the
response of the system to a unit sample excitation [i.e., x(n) = δ(n)]. In the case of a
recursive system, h(n) is simply equal to the zero-state response of the system when
the input x(n) = δ(n) and the system is initially relaxed.

yzs(n) =
n∑
k=0

akx(n− k)

yzs(n) =
n∑
k=0

akδ(n− k)

= an, n ≥ 0

h(n) = anu(n)

In the general case of an arbitrary, linear time-invariant recursive system, the

yzs(n) =
n∑
k=0

h(k)x(n− k), n ≥ 0

Discrete-Time Signals and Systems

Since we have already solved for the zero-input response in Example 4.5, we can simplify

(4.36)

is the sum of (4.23) and (4.36).

4.4

For example, in the simple first-order recursive system given in (4.7), the zero-
state response given in (4.8), is

(4.37)

When x(n) = δ(n) is substituted into (4.37), we obtain

Hence the impulse response of the first-order recursive system described by (4.7) is

(4.38)

as indicated in Section 4.2.

zero-state response expressed in terms of the convolution summation is

(4.39)

108



yzs(n) = h(n)

linear constant-coefficient difference equation description of the system. In terms
of our discussion in the preceding subsection, we have established the fact that the
total response of the system to any excitation function consists of the sum of two
solutions of the difference equation: the solution to the homogeneous equation plus
the particular solution to the excitation function. In the case where the excitation is
an impulse, the particular solution is zero, since x(n) = 0 for n > 0, that is,

yp(n) = 0
Consequently, the response of the system to an impulse consists only of the solution to
the homogeneous equation, with the {Ck} parameters evaluated to satisfy the initial
conditions dictated by the impulse. The following example illustrates the procedure
for obtaining h(n) given the difference equation for the system.

equation
y(n)− 3y(n− 1)− 4y(n− 2) = x(n)+ 2x(n− 1)

Solution.
neous difference equation for this system is

yh(n) = C1(−1)n + C2(4)n, n ≥ 0
Since the particular solution is zero when x(n) = δ(n), the impulse response of the system is

1 and C2

y(0) = 1
y(1) = 3y(0)+ 2 = 5

where we have imposed the conditions y(−1) = y(−2) = 0, since the system must be relaxed.

y(0) = C1 + C2
y(1) = −C1 + 4C2

By solving these two sets of equations for C1 and C2 , we obtain

C1 = −15 , C2 =
6
5

Therefore, the impulse response of the system is

h(n) =
[
−1

5
(−1)n + 6

5
(4)n

]
u(n)

Discrete-Time Signals and Systems

Now, let us consider the problem of determining the impulse response h(n) given a

(4.40)

When the input is an impulse [i.e., x(n) = δ(n)], (4.39) reduces to

Determine the impulse response h(n) for the system described by the second-order difference

EXAMPLE 4.10

(4.41)

(4.42)

simply given by (4.42), where C must be evaluated to satisfy (4.41).

We have already determined in Example 4.5 that the solution to the homoge-

For n = 0 and n = 1, (4.41) yields

On the other hand, (4.42) evaluated at n = 0 and n = 1 yields

109



When the system is described by an Nth-order linear difference equation of the

yh(n) =
N∑
k=1

Ckλ
nk

when the roots {λk} of the characteristic polynomial are distinct. Hence the impulse
response of the system is identical in form, that is,

h(n) =
N∑
k=1

Ckλ
n
k

where the parameters {Ck} are determined by setting the initial conditions y(−1) =
· · · = y(−N) = 0.

This form of h(n) allows us to easily relate the stability of a system, described
by an Nth-order difference equation, to the values of the roots of the characteristic
polynomial. Indeed, since BIBO stability requires that the impulse response be
absolutely summable, then, for a causal system, we have

∞∑
n=0
|h(n)| =

∞∑
n=0

∣∣∣∣∣
N∑
k=1

Ckλ
nk

∣∣∣∣∣ ≤
N∑
k=1
|Ck|

∞∑
n=0
|λk|n

Now if |λk| < 1 for all k , then
∞∑
n=0
|λk|n <∞

and hence
∞∑
n=0
|h(n)| <∞

On the other hand, if one or more of the |λk| ≥ 1, h(n) is no longer absolutely
summable, and consequently, the system is unstable. Therefore, a necessary and
sufficient condition for the stability of a causal IIR system described by a linear
constant-coefficient difference equation is that all roots of the characteristic poly-
nomial be less than unity in magnitude. The reader may verify that this condition
carries over to the case where the system has roots of multiplicity m.

Finally we note that any recursive system described by a linear constant-coefficient
difference equation is an IIR system. The converse is not true, however. That is,
not every linear time-invariant IIR system can be described by a linear constant-
coefficient difference equation. In other words, recursive systems described by linear
constant-coefficient difference equations are a subclass of linear time-invariant IIR
systems.

Discrete-Time Signals and Systems

type given in (4.13), the solution of the homogeneous equation is

(4.43)

110



Implementation of Discrete-Time Systems

Our treatment of discrete-time systems has been focused on the time-domain char-
acterization and analysis of linear time-invariant systems described by constant-

where we characterize and analyze LTI systems in the frequency domain. Two oth
er important topics that will be treated later are the design and implementation of
these systems.

In practice, system design and implementation are usually treated jointly rather
than separately. Often, the system design is driven by the method of implemen-
tation and by implementation constraints, such as cost, hardware limitations, size
limitations, and power requirements. At this point, we have not as yet developed the
necessary analysis and design tools to treat such complex issues. However, we have
developed sufficient background to consider some basic implementation methods
for realizations of LTI systems described by linear constant-coefficient difference
equations.

5.1 Structures for the Realization of Linear
Time-Invariant Systems

In this subsection we describe structures for the realization of systems described
by linear constant-coefficient difference equations.

As a beginning, let us consider the first-order system

y(n) = −a1y(n− 1)+ b0x(n)+ b1x(n− 1)

which is realized as in Fig. 5.1(a). This realization uses separate delays (memory)
for both the input and output signal samples and it is called a direct form I structure.
Note that this system can be viewed as two linear time-invariant systems in cascade.
The first is a nonrecursive system described by the equation

v(n) = b0x(n)+ b1x(n− 1)

whereas the second is a recursive system described by the equation

y(n) = −a1y(n− 1)+ v(n)

cascaded linear time-invariant systems, the overall system response remains the same.
Thus if we interchange the order of the recursive and nonrecursive systems, we

w(n) = −a1w(n− 1)+ x(n)
y(n) = b0w(n)+ b1w(n− 1)

Discrete-Time Signals and Systems

5

(5.1)

(5.2)

(5.3)

However, as we have seen in Section 3.4, if we interchange the order of the

obtain an alternative structure for the realization of the system described by (5.1).

difference equations
The resulting system is shown in Fig. 5.1(b).

(5.4)

From this figure we obtain the two

(5.5)

-
coefficient linear difference equations. Additional analytical methods are developed,

111



Steps in converting from
the direct form I realization
in (a) to the direct form II
realization in (c).

x(n) b0

b0

b1−a1

b1−a1

b1 −a1

y(n)ν(n)

x(n) y(n)ω(n) b0

x(n) y(n)ω(n)

z−1z−1

z−1 z−1

z−1

(a)

(b)

(c)

+

+

+ +

+

+

ω(n − 1)

ω(n − 1) ω(n − 1)

which provide an alternative algorithm for computing the output of the system de-
In other words, the two

same input w(n) and hence the same output w(n − 1). Consequently, these two

direct form I structure, this new realization requires only one delay for the auxiliary
quantity w(n), and hence it is more efficient in terms of memory requirements. It is
called the direct form II structure and it is used extensively in practical applications.

These structures can readily be generalized for the general linear time-invariant
recursive system described by the difference equation

y(n) = −
N∑
k=1

aky(n− k)+
M∑
k=0

bkx(n− k)

This structure
requires M + N delays and N + M + 1 multiplications. It can be viewed as the

Discrete-Time Signals and Systems

Figure 5.1

scribed by the single difference equation given in (5.1).

tion (5.1).
difference equations (5.4) and (5.5) are equivalent to the single difference equa-

A close observation of Fig. 5.1 reveals that the two delay elements contain the

elements can be merged into one delay, as shown in Fig. 5.1(c). In contrast to the

(5.6)

Figure 5.2 illustrates the direct form I structure for this system.

112



cascade of a nonrecursive system

v(n) =
M∑
k=0

bkx(n− k)

and a recursive system

y(n) = −
N∑
k=1

aky(n− k)+ v(n)

By reversing the order of these two systems, as was previously done for the first-

This structure is the cascade of a recursive system

w(n) = −
N∑
k=1

akw(n− k)+ x(n)

followed by a nonrecursive system

y(n) =
M∑
k=0

bkw(n− k)

+

+

+

+ +

+

+

+

x(n) ν(n) y(n)

z−1

z−1

z−1

z−1

z−1

z−1

−a1

−a2

−aN − 1

−aN

b1

b0

b2

bM − 1

bM

…………

Discrete-Time Signals and Systems

(5.7)

(5.8)

(5.9)

order system, we obtain the direct form II structure shown in Fig. 5.3 for N > M .

(5.10)

(5.6).Direct form I structure of the system described byFigure 5.2

113



We observe that if N ≥ M , this structure requires a number of delays equal to
the order N of the system. However, if M > N , the required memory is specified

II structure requires M + N + 1 multiplications and max{M,N} delays. Because it
requires the minimum number of delays for the realization of the system described

k

y(n) =
M∑
k=0

bkx(n− k)

which is a nonrecursive linear time-invariant system. This system views only the most
recent M + 1 input signal samples and, prior to addition, weights each sample by
the appropriate coefficient bk from the set {bk}. In other words, the system output
is basically a weighted moving average of the input signal. For this reason it is
sometimes called a moving average (MA) system. Such a system is an FIR system

+

+

+

+

+

+

+

+

+

+

x(n) y(n)
ω(n)

ω(n − 1)

ω(n − 2)

ω(n − 3)

ω(n − M)

ω(n − N)

z−1

z−1

z−1

z−1

z−1

b0

b1

b2

b3

bM

−a1

−a2

−a3

−aN−2

−aN−1

−aN 

… … …

(M = N − 2)

Discrete-Time Signals and Systems

by M . Figure 5.3 can easily be modified to handle this case. Thus the direct form

by (5.6), it is sometimes called a canonic form.

1, . . . , N . Then the input–output relationship for the system reduces to
= 0, k =A special case of (5.6) occurs if we set the system parameters a

(5.11)

Figure 5.3 Direct form II structure for the system described by (5.6).

114



with an impulse response h(k) equal to the coefficients bk , that is,

h(k) =
{
bk, 0 ≤ k ≤ M
0, otherwise

reduces to a “purely recursive” system described by the difference equation

y(n) = −
N∑
k=1

aky(n− k)+ b0x(n)

In this case the system output is a weighted linear combination of N past outputs
and the present input.

The most general second-order system is described by the difference equation

y(n) =− a1y(n− 1)− a2y(n− 2)+ b0x(n)
+ b1x(n− 1)+ b2x(n− 2)

= 2 and M = 2. The direct form II
If we set a1 = a2 = 0,

y(n) = b0x(n)+ b1x(n− 1)+ b2x(n− 2)

1 = b2
obtain the purely recursive second-order system described by the difference equation

y(n) = −a1y(n− 1)− a2y(n− 2)+ b0x(n)

Recursive and Nonrecursive Realizations of FIR Systems

We have already made the distinction between FIR and IIR systems, based on
whether the impulse response h(n) of the system has a finite duration, or an infinite
duration. We have also made the distinction between recursive and nonrecursive sys-
tems. Basically, a causal recursive system is described by an input–output equation
of the form

y(n) = F [y(n− 1), . . . , y(n−N), x(n), . . . , x(n−M)]

Discrete-Time Signals and Systems

(5.12)

(5.13)

If we return to (5.6) and set M = 0, the general linear time-invariant system

Linear  time-invariant systems described by a second-order difference equation 
are an  important subclass of the more general systems described by (5.6) or (5.10) 
or (5.13). The reason for their importance will be explained later when we discuss 
quantization effects. Suffice to say at this point that second-order systems are usu-
ally used as basic building blocks for realizing higher-order systems.

(5.14)

which is obtained from (5.6) by setting N
structure for realizing this system is shown in Fig. 5.4(a).
then (5.14) reduces to

(5.15)

(5.16)

which is a special case of (5.13). The structure for realizing this system is shown in
Fig. 5.4(c).

5.2

(5.17)

which is a special case of the FIR system described by (5.11). The structure for real-
= 0 in (5.14), weizing this system is shown in Fig. 5.4(b). Finally, if we set b

115



+ +

+

++

++

+

x(n)
y(n)

y(n)

y(n)

x(n)

x(n)

z−1z−1

z−1

z−1

z −1

z−1

b0

b0

b1

b2

b0 b1 b2

−a1

−a2

−a1 −a2

(a)

(b)

(c)

Structures for the realization of second-order sys-
tems: (a) general second-order system; (b) FIR system; (c) “purely
recursive system.”

and for a linear time-invariant system specifically, by the difference equation

y(n) = −
N∑
k=1

aky(n− k)+
M∑
k=0

bkx(n− k)

On the other hand, causal nonrecursive systems do not depend on past values of the
output and hence are described by an input–output equation of the form

y(n) = F [x(n), x(n− 1), . . . , x(n−M)]

with ak = 0 for k = 1, 2, . . . , N .
In the case of FIR systems, we have already observed that it is always possible to

realize such systems nonrecursively. In fact, with ak

Discrete-Time Signals and Systems

Figure 5.4

(5.18)

(5.19)

and for linear time-invariant systems specifically, by the difference equation in (5.18)

= 0, k = 1, 2, . . . , N , in (5.18),

116



+ + +

x(n)

y(n)

z−1 z−1 z−1 z−1

…

…

M + 1
1

Nonrecursive realization of an FIR moving average system.

we have a system with an input–output equation

y(n) =
M∑
k=0

bkx(n− k)

of the system is simply equal to the coefficients {bk}. Hence every FIR system can
be realized nonrecursively. On the other hand, any FIR system can also be realized
recursively. Although the general proof of this statement is given later, we shall give
a simple example to illustrate the point.

Suppose that we have an FIR system of the form

y(n) = 1
M + 1

M∑
k=0

x(n− k)

for computing the moving average of a signal x(n). Clearly, this system is FIR with
impulse response

h(n) = 1
M + 1 , 0 ≤ n ≤ M

1
M + 1

M∑
k=0

x(n− 1− k)

+ 1
M + 1[x(n)− x(n− 1−M)]

= y(n− 1)+ 1
M + 1[x(n)− x(n− 1−M)]

Discrete-Time Signals and Systems

Figure 5.5

(5.20)

This is a nonrecursive and FIR system. As indicated in (5.12), the impulse response

(5.21)

Figure 5.5 illustrates the structure of the nonrecursive realization of the system.

y(n) =

Now, suppose that we express (5.21) as

(5.22)

Now, (5.22) represents a recursive realization of the FIR system. The structure of
this recursive realization of the moving average system is illustrated in Fig. 5.6.

117



+ +

x(n)
x(n − M − 1)

y(n)

y(n − 1)

z−1 z−1 z−1

z−1
M + 1

1

−

+

…

Recursive realization of an FIR moving average system.

In summary, we can think of the terms FIR and IIR as general characteristics
that distinguish a type of linear time-invariant system, and of the terms recursive
and nonrecursive as descriptions of the structures for realizing or implementing the
system.

Correlation of Discrete-Time Signals

A mathematical operation that closely resembles convolution is correlation. Just as in
the case of convolution, two signal sequences are involved in correlation. In contrast
to convolution, however, our objective in computing the correlation between the two
signals is to measure the degree to which the two signals are similar and thus to extract
some information that depends to a large extent on the application. Correlation of
signals is often encountered in radar, sonar, digital communications, geology, and
other areas in science and engineering.

To be specific, let us suppose that we have two signal sequences x(n) and y(n)
that we wish to compare. In radar and active sonar applications, x(n) can represent
the sampled version of the transmitted signal and y(n) can represent the sampled
version of the received signal at the output of the analog-to-digital (A/D) converter.
If a target is present in the space being searched by the radar or sonar, the received
signal y(n) consists of a delayed version of the transmitted signal, reflected from

We can represent the received signal sequence as

y(n) = αx(n−D)+ w(n)

where α is some attenuation factor representing the signal loss involved in the round-
trip transmission of the signal x(n), D is the round-trip delay, which is assumed to
be an integer multiple of the sampling interval, and w(n) represents the additive
noise that is picked up by the antenna and any noise generated by the electronic
components and amplifiers contained in the front end of the receiver. On the other
hand, if there is no target in the space searched by the radar and sonar, the received
signal y(n) consists of noise alone.

Discrete-Time Signals and Systems

Figure 5.6

6

(6.1)

the target, and corrupted by additive noise.  Figure 6.1 depicts the radar signal recep-
tion problem.

118



Tra
nsm

itted
 sig

nal

Re
flec

ted
 sig

nal

Radar target detection.

Having the two signal sequences, x(n), which is called the reference signal or
transmitted signal, and y(n), the received signal, the problem in radar and sonar
detection is to compare y(n) and x(n) to determine if a target is present and, if so,
to determine the time delay D and compute the distance to the target. In practice,
the signal x(n − D) is heavily corrupted by the additive noise to the point where
a visual inspection of y(n) does not reveal the presence or absence of the desired
signal reflected from the target. Correlation provides us with a means for extracting
this important information from y(n).

Digital communications is another area where correlation is often used. In digital
communications the information to be transmitted from one point to another is
usually converted to binary form, that is, a sequence of zeros and ones, which are
then transmitted to the intended receiver. To transmit a 0 we can transmit the signal
sequence x0(n) for 0 ≤ n ≤ L − 1, and to transmit a 1 we can transmit the signal
sequence x1(n) for 0 ≤ n ≤ L−1, where L is some integer that denotes the number of
samples in each of the two sequences. Very often, x1(n) is selected to be the negative
of x0(n). The signal received by the intended receiver may be represented as

y(n) = xi(n)+w(n), i = 0, 1, 0 ≤ n ≤ L− 1

0 1
and w(n) represents the additive noise and other interference inherent in any com-
munication system. Again, such noise has its origin in the electronic components
contained in the front end of the receiver. In any case, the receiver knows the pos-
sible transmitted sequences x0(n) and x1(n) and is faced with the task of comparing
the received signal y(n) with both x0(n) and x1(n) to determine which of the two
signals better matches y(n). This comparison process is performed by means of the
correlation operation described in the following subsection.

Discrete-Time Signals and Systems

Figure 6.1

where now the uncertainty is whether x (n) or x (n) is the signal component in y(n),

(6.2)

119



6.1 Crosscorrelation and Autocorrelation Sequences

Suppose that we have two real signal sequences x(n) and y(n) each of which has
finite energy. The crosscorrelation of x(n) and y(n) is a sequence rxy(l), which is
defined as

rxy(l) =
∞∑

n=−∞
x(n)y(n− l), l = 0,±1,±2, . . .

or, equivalently, as

rxy(l) =
∞∑

n=−∞
x(n+ l)y(n), l = 0,±1,±2, . . .

The index l is the (time) shift (or lag) parameter and the subscripts xy on the cross-
correlation sequence rxy(l) indicate the sequences being correlated. The order of
the subscripts, with x preceding y , indicates the direction in which one sequence
is shifted, relative to the other.
unshifted and y(n) is shifted by l units in time, to the right for l positive and to the
left for l negative.
x(n) is shifted by l units in time, to the left for l positive and to the right for l negative.
But shifting x(n) to the left by l units relative to y(n) is equivalent to shifting y(n)
to the right by l units relative to x(n).
yield identical crosscorrelation sequences.

the order of the indices xy , we obtain the crosscorrelation sequence

ryx(l) =
∞∑

n=−∞
y(n)x(n− l)

or, equivalently,

ryx(l) =
∞∑

n=−∞
y(n+ l)x(n)

rxy(l) = ryx(−l)
Therefore, ryx(l) is simply the folded version of rxy(l), where the folding is done with
respect to l = 0. Hence, ryx(l) provides exactly the same information as rxy(l), with
respect to the similarity of x(n) to y(n).

Determine the crosscorrelation sequence rxy(l) of the sequences

x(n) = {. . . , 0, 0, 2,−1, 3, 7, 1
↑
, 2,−3, 0, 0, . . .}

y(n) = {. . . , 0, 0, 1,−1, 2,−2, 4
↑
, 1,−2, 5, 0, 0, . . .}

Discrete-Time Signals and Systems

(6.3)

(6.4)

To elaborate, in (6.3), the sequence x(n) is left

Equivalently, in (6.4), the sequence y(n) is left unshifted and

Hence the computations (6.3) and (6.4)

If we reverse the roles of x(n) and y(n) in (6.3) and (6.4) and therefore reverse

(6.5)

(6.6)

(6.7)

By comparing (6.3) with (6.6) or (6.4) with (6.5), we conclude that

EXAMPLE 6.1

120



Solution. xy(l). For l = 0 we have

rxy(0) =
∞∑

n=−∞
x(n)y(n)

The product sequence v0(n) = x(n)y(n) is
v0(n) = {. . . , 0, 0, 2, 1, 6,−14, 4↑, 2, 6, 0, 0, . . .}

and hence the sum over all values of n is

rxy(0) = 7
For l > 0, we simply shift y(n) to the right relative to x(n) by l units, compute the product

sequence vl(n) = x(n)y(n− l), and finally, sum over all values of the product sequence. Thus
we obtain

rxy(1) = 13, rxy(2) = −18, rxy(3) = 16, rxy(4) = −7
rxy(5) = 5, rxy(6) = −3, rxy(l) = 0, l ≥ 7

For l < 0, we shift y(n) to the left relative to x(n) by l units, compute the product
sequence vl(n) = x(n)y(n − l), and sum over all values of the product sequence. Thus we
obtain the values of the crosscorrelation sequence

rxy(−1) = 0, rxy(−2) = 33, rxy(−3) = −14, rxy(−4) = 36
rxy(−5) = 19, rxy(−6) = −9, rxy(−7) = 10, rxy(l) = 0, l ≤ −8

Therefore, the crosscorrelation sequence of x(n) and y(n) is

rxy(l) = {10,−9, 19, 36,−14, 33, 0, 7↑, 13,−18, 16,−7, 5,−3}

The similarities between the computation of the crosscorrelation of two se-
quences and the convolution of two sequences is apparent. In the computation
of convolution, one of the sequences is folded, then shifted, then multiplied by the
other sequence to form the product sequence for that shift, and finally, the values
of the product sequence are summed. Except for the folding operation, the compu-
tation of the crosscorrelation sequence involves the same operations: shifting one
of the sequences, multiplying the two sequences, and summing over all values of
the product sequence. Consequently, if we have a computer program that performs
convolution, we can use it to perform crosscorrelation by providing as inputs to the
program the sequence x(n) and the folded sequence y(−n). Then the convolution
of x(n) with y(−n) yields the crosscorrelation rxy(l), that is,

rxy(l) = x(l) ∗ y(−l)
We note that the absence of folding makes crosscorrelation a noncommutative

operation. In the special case where y(n) = x(n), we have the autocorrelation of
x(n), which is defined as the sequence

rxx(l) =
∞∑

n=−∞
x(n)x(n− l)

Discrete-Time Signals and Systems

Let us use the definition in (6.3) to compute r

(6.8)

(6.9)

121



or, equivalently, as

rxx(l) =
∞∑

n=−∞
x(n+ l)x(n)

In dealing with finite-duration sequences, it is customary to express the auto-
correlation and crosscorrelation in terms of the finite limits on the summation. In
particular, if x(n) and y(n) are causal sequences of length N [i.e., x(n) = y(n) = 0

expressed as

rxy(l) =
N−|k|−1∑
n=l

x(n)y(n− l)

and

rxx(l) =
N−|k|−1∑
n=i

x(n)x(n− l)

where i = l , k = 0 for l ≥ 0, and i = 0, k = l for l < 0.

6.2
Sequences

The autocorrelation and crosscorrelation sequences have a number of important
properties that we now present. To develop these properties, let us assume that we
have two sequences x(n) and y(n) with finite energy from which we form the linear
combination,

ax(n)+ by(n− l)
where a and b are arbitrary constants and l is some time shift. The energy in this
signal is

∞∑
n=−∞

[ax(n)+ by(n− l)]2 = a2
∞∑

n=−∞
x2(n)+ b2

∞∑
n=−∞

y2(n− l)

+ 2ab
∞∑

n=−∞
x(n)y(n− l)

= a2rxx(0)+ b2ryy(0)+ 2abrxy(l)
First, we note that rxx(0) = Ex and ryy(0) = Ey , which are the energies of x(n)

and y(n), respectively. It is obvious that

a2rxx(0)+ b2ryy(0)+ 2abrxy(l) ≥ 0
2 to obtain

rxx(0)
(a
b

)
2+ 2rxy(l)

(a
b

)
+ ryy(0) ≥ 0

Discrete-Time Signals and Systems

(6.10)

for n < 0 and n ≥ N ], the crosscorrelation and autocorrelation sequences may be

(6.11)

Properties of the Autocorrelation and Crosscorrelation

(6.12)

(6.13)

(6.14)

Now, assuming that b �= 0, we can divide (6.14) by b

122



We view this equation as a quadratic with coefficients rxx(0), 2rxy(l), and ryy(0).
Since the quadratic is nonnegative, it follows that the discriminant of this quadratic
must be nonpositive, that is,

4[r2xy(l)− rxx(0)ryy(0)] ≤ 0
Therefore, the crosscorrelation sequence satisfies the condition that

|rxy(l)| ≤
√
rxx(0)ryy(0) =

√
ExEy

|rxx(l)| ≤ rxx(0) = Ex
This means that the autocorrelation sequence of a signal attains its maximum value
at zero lag. This result is consistent with the notion that a signal matches perfectly
with itself at zero shift. In the case of the crosscorrelation sequence, the upper bound

Note that if any one or both of the signals involved in the crosscorrelation are
scaled, the shape of the crosscorrelation sequence does not change; only the am-
plitudes of the crosscorrelation sequence are scaled accordingly. Since scaling is
unimportant, it is often desirable, in practice, to normalize the autocorrelation and
crosscorrelation sequences to the range from −1 to 1. In the case of the autocorrela-
tion sequence, we can simply divide by rxx(0). Thus the normalized autocorrelation
sequence is defined as

ρxx(l) = rxx(l)
rxx(0)

Similarly, we define the normalized crosscorrelation sequence

ρxy(l) = rxy(l)√
rxx(0)ryy(0)

Now |ρxx(l)| ≤ 1 and |ρxy(l)| ≤ 1, and hence these sequences are independent of
signal scaling.

Finally, as we have already demonstrated, the crosscorrelation sequence satisfies
the property

rxy(l) = ryx(−l)
With y(n) = x(n), this relation results in the following important property for the
autocorrelation sequence

rxx(l) = rxx(−l)
Hence the autocorrelation function is an even function. Consequently, it suffices to
compute rxx(l) for l ≥ 0.

Compute the autocorrelation of the signal

x(n) = anu(n), 0 < a < 1
Solution. Since x(n) is an infinite-duration signal, its autocorrelation also has infinite dura-
tion. We distinguish two cases.

Discrete-Time Signals and Systems

(6.15)

(6.16)

on its values is given in (6.15).

(6.17)

(6.18)

(6.19)

EXAMPLE 6.2

In the special case where y(n) = x(n), (6.15) reduces to

123



rxx(l) =
∞∑
n=1

x(n)x(n− l) =
∞∑
n=1

anan−l = a−l
∞∑
n=1
(a2)n

Since a < 1, the infinite series converges and we obtain

rxx(l) = 11− a2 a
|l|, l ≥ 0

Computation of
the autocorrelation
of the signal
x(n) = an , 0 < a < 1.

−1−2 0 1 2 3 n

n

n

1

(b)

(a)

(c)

(d)

x(n)

0 l

l

l

1

x(n − l )

x(n − l )

0

1

−1−2 0 1 2 ……

1

…

……

……

…

…

l < 0

l > 0

1
 rxx(l) =  

1 − a2
  a l

Discrete-Time Signals and Systems

If l ≥ 0, from Fig. 6.2 we observe that

Figure 6.2

124



For l < 0 we have

rxx(l) =
∞∑
n=0

x(n)x(n− l) = a−l
∞∑
n=0
(a2)n = 1

1− a2 a
−l , l < 0

But when l is negative, a−l = a|l| . Thus the two relations for rxx
following expression:

rxx(l) = 11− a2 a
|l|,

The sequence rxx

rxx(−l) = rxx(l)

and

rxx(0) = 11− a2

Therefore, the normalized autocorrelation sequence is

ρxx(l) = rxx(l)
rxx(0)

= a|l|, −∞ < l <∞

6.3 Correlation of Periodic Sequences

energy signals. In this section we consider the correlation sequences of power signals
and, in particular, periodic signals.

Let x(n) and y(n) be two power signals. Their crosscorrelation sequence is
defined as

rxy(l) = lim
M→∞

1
2M + 1

M∑
n=−M

x(n)y(n− l)

If x(n) = y(n), we have the definition of the autocorrelation sequence of a power
signal as

rxx(l) = lim
M→∞

1
2M + 1

M∑
n=−M

x(n)x(n− l)

In particular, if x(n) and y(n) are two periodic sequences, each with period N,

rxy
1
N

N−1∑
n=0

x(n)y(n− l)

Discrete-Time Signals and Systems

(l) can be combined into the

−∞ < l <∞ (6.20)

(6.21)

In Section 6.1 we defined the crosscorrelation and autocorrelation sequences of

(6.22)

(6.23)

the averages indicated in (6.22) and (6.23) over the infinite interval are identical

(l) =

to the averages over a single period, so that (6.22) and (6.23) reduce to

(6.24)

(l) is shown in Fig. 6.2(d). We observe that

125



and

rxx(l) = 1
N

N−1∑
n=0

x(n)x(n− l)

It is clear that rxy(l) and rxx
The factor 1/N can be viewed as a normalization scale factor.

In some practical applications, correlation is used to identify periodicities in
an observed physical signal which may be corrupted by random interference. For
example, consider a signal sequence y(n) of the form

y(n) = x(n)+w(n)

where x(n) is a periodic sequence of some unknown period N and w(n) represents
an additive random interference. Suppose that we observe M samples of y(n), say
0 ≤ n ≤ M − 1, where M >> N . For all practical purposes, we can assume that
y(n) = 0 for n < 0 and n ≥ M .
the normalization factor of 1/M , is

ryy(l) = 1
M

M−1∑
n=0

y(n)y(n− l)

ryy(l) = 1
M

M−1∑
n=0

[x(n)+ w(n)][x(n− l)+w(n− l)]

= 1
M

M−1∑
n=0

x(n)x(n− l)

+ 1
M

M−1∑
n=0

[x(n)w(n− l)+ w(n)x(n− l)]

+ 1
M

M−1∑
n=0

w(n)w(n− l)

= rxx(l)+ rxw(l)+ rwx(l)+ rww(l)

of x(n). Since x(n) is periodic, its autocorrelation sequence exhibits the same period-
icity, thus containing relatively large peaks at l = 0, N , 2N , and so on. However, as
the shift l approaches M , the peaks are reduced in amplitude due to the fact that we
have a finite data record of M samples so that many of the products x(n)x(n− l) are
zero. Consequently, we should avoid computing ryy(l) for large lags, say, l > M/2.

Discrete-Time Signals and Systems

(l) are periodic correlation sequences with period N .

(6.25)

(6.26)

Now the autocorrelation sequence of y(n), using

(6.27)

If we substitute for y(n) from (6.26) into (6.27) we obtain

(6.28)

The first factor on the right-hand side of (6.28) is the autocorrelation sequence

126



The crosscorrelations rxw(l) and rwx(l) between the signal x(n) and the additive
random interference are expected to be relatively small as a result of the expectation
that x(n) and w(n) will be totally unrelated. Finally, the last term on the right-hand

correlation sequence will certainly contain a peak at l = 0, but because of its random
characteristics, rww(l) is expected to decay rapidly toward zero. Consequently, only
rxx(l) is expected to have large peaks for l > 0. This behavior allows us to detect the
presence of the periodic signal x(n) buried in the interference w(n) and to identify
its period.

An example that illustrates the use of autocorrelation to identify a hidden pe-

the autocorrelation (normalized) sequence for the Wölfer sunspot numbers in the
100-year period 1770–1869 for 0 ≤ l ≤ 20, where any value of l corresponds to one
year. There is clear evidence in this figure that a periodic trend exists, with a period
of 10 to 11 years.

Suppose that a signal sequence x(n) = sin(π/5)n, for 0 ≤ n ≤ 99 is corrupted by an additive
noise sequence w(n), where the values of the additive noise are selected independently from
sample to sample, from a uniform distribution over the range (−
/2,
/2), where 
 is a
parameter of the distribution. The observed sequence is y(n) = x(n)+ w(n). Determine the
autocorrelation sequence ryy(l) and thus determine the period of the signal x(n).

Solution. The assumption is that the signal sequence x(n) has some unknown period that
we are attempting to determine from the noise-corrupted observations {y(n)}. Although x(n)
is periodic with period 10, we have only a finite-duration sequence of length M = 100 [i.e.,
10 periods of x(n)]. The noise power level Pw in the sequence w(n) is determined by the
parameter 
. We simply state that Pw = 
2/12. The signal power level is Px = 12 . Therefore,
the signal-to-noise ratio (SNR) is defined as

Px

Pw
=

1
2


2/12
= 6

2

Usually, the SNR is expressed on a logarithmic scale in decibels (dB) as 10 log10 (Px/Pw).

y(n) = x(n)+ w(n) when the SNR = 1 dB. The autocorrelation sequence ryy(l) is illustrated
We observe that the periodic signal x(n), embedded in y(n), results in a

periodic autocorrelation function rxx(l) with period N = 10. The effect of the additive noise
is to add to the peak value at l = 0, but for l �= 0, the correlation sequence rww(l) ≈ 0 as a
result of the fact that values of w(n) were generated independently. Such noise is usually called
white noise. The presence of this noise explains the reason for the large peak at l = 0. The
smaller, nearly equal peaks at l = ±10, ±20, . . . are due to the periodic characteristics of x(n).

6.4 Input–Output Correlation Sequences

In this section we derive two input–output relationships for LTI systems in the “cor-
relation domain.” Let us assume that a signal x(n) with known autocorrelation rxx(l)

Discrete-Time Signals and Systems

side of (6.28) is the autocorrelation sequence of the random sequence w(n). This

riodicity in an observed physical signal is shown in Fig. 6.3. This figure illustrates

 EXAMPLE 6.3

Figure 6.4 illustrates a sample of a noise sequence w(n), and the observed sequence

in Fig. 6.4(c).

127



1770 1790 1810 1830 1850 1870
0

20

0.2

0.4

0.6

0.8

1

0 10 l

20155

40

60

80

100

120

140

160

Year

rxx(l)

Lag   Years

(a)

(b)

N
um

be
r 

of
 S

un
sp

ot
s

Identification of periodicity in the Wölfer sunspot numbers: (a) annual
Wölfer sunspot numbers; (b) normalized autocorrelation sequence.

is applied to an LTI system with impulse response h(n), producing the output signal

y(n) = h(n) ∗ x(n) =
∞∑

k=−∞
h(k)x(n− k)

The crosscorrelation between the output and the input signal is

ryx(l) = y(l) ∗ x(−l) = h(l) ∗ [x(l) ∗ x(−l)]

Discrete-Time Signals and Systems

Figure 6.3

128



(a)

(b)

(c)

0

y(n)

ω(n)

ryy(l)

SNR = 1 dB

n

l

n

Use of autocorrelation to detect the presence of a periodic signal
corrupted by noise.

or
ryx(l) = h(l) ∗ rxx(l)

Hence the cross-
correlation between the input and the output of the system is the convolution of
the impulse response with the autocorrelation of the input sequence. Alternatively,
ryx(l) may be viewed as the output of the LTI system when the input sequence is
rxx

rxy(l) = h(−l) ∗ rxx(l)

x(n) = y(n) and the properties of convolution. Thus we have
ryy(l) = y(l) ∗ y(−l)

= [h(l) ∗ x(l)] ∗ [h(−l) ∗ x(−l)]
= [h(l) ∗ h(−l)] ∗ [x(l) ∗ x(−l)]
= rhh(l) ∗ rxx(l)

Discrete-Time Signals and Systems

Figure 6.4

(6.29)

where we have used (6.8) and the properties of convolution.

(l). This is illustrated in Fig. 6.5. If we replace l by −l in (6.29), we obtain

The autocorrelation of the output signal can be obtained by using (6.8) with

(6.30)

129



The autocorrelation rhh(l) of the impulse response h(n) exists if the system is stable.
Furthermore, the stability insures that the system does not change the type (energy

ryy(0) =
∞∑

k=−∞
rhh(k)rxx(k)

which provides the energy (or power) of the output signal in terms of autocorre-
lations. These relationships hold for both energy and power signals. The direct
derivation of these relationships for energy and power signals, and their extensions
to complex signals, are left as exercises for the student.

Input–output relation for
crosscorrelation ryx(n).

Input Output

rxx(n) ryx(n)

LTI
SYSTEM

h(n)

Summary and References

The major theme of this chapter is the characterization of discrete-time signals and
systems in the time domain. Of particular importance is the class of linear time-
invariant (LTI) systems which are widely used in the design and implementation
of digital signal processing systems. We characterized LTI systems by their unit
sample response h(n) and derived the convolution summation, which is a formula
for determining the response y(n) of the system characterized by h(n) to any given
input sequence x(n).

The class of LTI systems characterized by linear difference equations with con-
stant coefficients is by far the most important of the LTI systems in the theory and
application of digital signal processing. The general solution of a linear difference
equation with constant coefficients was derived in this chapter and shown to consist
of two components: the solution of the homogeneous equation, which represents the
natural response of the system when the input is zero, and the particular solution,
which represents the response of the system to the input signal. From the difference
equation, we also demonstrated how to derive the unit sample response of the LTI
system.

Linear time-invariant systems were generally subdivided into FIR (finite-duration
impulse response) and IIR (infinite-duration impulse response) depending on whether
h(n) has finite duration or infinite duration, respectively. The realizations of such
systems were briefly described. Furthermore, in the realization of FIR systems, we
made the distinction between recursive and nonrecursive realizations. On the other
hand, we observed that IIR systems can be implemented recursively, only.

Discrete-Time Signals and Systems

(6.31)

or power) of the input signal. By evaluating (6.30) for l = 0 we obtain

Figure 6.5

7

130



There are a number of texts on discrete-time signals and systems. We men-
tion as examples the books by McGillem and Cooper (1984), Oppenheim and Will-
sky (1983), and Siebert (1986). Linear constant-coefficient difference equations are
treated in depth in the books by Hildebrand (1952) and Levy and Lessman (1961).

The last topic in this chapter, on correlation of discrete-time signals, plays an
important role in digital signal processing, especially in applications dealing with
digital communications, radar detection and estimation, sonar, and geophysics. In
our treatment of correlation sequences, we avoided the use of statistical concepts.
Correlation is simply defined as a mathematical operation between two sequences,
which produces another sequence, called either the crosscorrelation sequence when
the two sequences are different, or the autocorrelation sequence when the two se-
quences are identical.

In practical applications in which correlation is used, one (or both) of the se-
quences is (are) contaminated by noise and, perhaps, by other forms of interference.
In such a case, the noisy sequence is called a random sequence and is characterized
in statistical terms. The corresponding correlation sequence becomes a function of
the statistical characteristics of the noise and any other interference.

Problems

1 A discrete-time signal x(n) is defined as

x(n) =
{ 1+ n3 , −3 ≤ n ≤ −1

1, 0 ≤ n ≤ 3
0, elsewhere

(a) Determine its values and sketch the signal x(n).

(b) Sketch the signals that result if we:

1. First fold x(n) and then delay the resulting signal by four samples.

2. First delay x(n) by four samples and then fold the resulting signal.

(c) Sketch the signal x(−n+ 4).
(d) Compare the results in parts (b) and (c) and derive a rule for obtaining the signal

x(−n+ k) from x(n).
(e) Can you express the signal x(n) in terms of signals δ(n) and u(n)?

2
the following signals.

Discrete-Time Signals and Systems

Supplementary reading on probabilistic and statistical concepts dealing with 
correlation can be found in the books by Davenport (1970), Helstrom (1990), Pee-
bles (1987), and Stark and Woods (1994).

A discrete-time signal x(n) is shown in Fig. P2. Sketch and label carefully each of

131



−1 0

1 1 1 1

21 3 4−2

x(n)

n

1
2

1
2

(a) x(n− 2) (b) x(4− n) (c) x(n+ 2) (d) x(n)u(2− n) (e) x(n− 1)δ(n− 3)
(f) x(n2) (g) even part of x(n) (h) odd part of x(n)

3 Show that
(a) δ(n) = u(n)− u(n− 1)
(b) u(n) =∑nk=−∞ δ(k) =∑∞k=0 δ(n− k)

4 Show that any signal can be decomposed into an even and an odd component. Is the
decomposition unique? Illustrate your arguments using the signal

x(n) = {2, 3, 4
↑
, 5, 6}

5 Show that the energy (power) of a real-valued energy (power) signal is equal to the
sum of the energies (powers) of its even and odd components.

6 Consider the system
y(n) = T [x(n)] = x(n2)

(a) Determine if the system is time invariant.
(b) To clarify the result in part (a) assume that the signal

x(n) =
{

1, 0 ≤ n ≤ 3
0, elsewhere

is applied into the system.

(1) Sketch the signal x(n).
(2) Determine and sketch the signal y(n) = T [x(n)].
(3) Sketch the signal y′2(n) = y(n− 2).
(4) Determine and sketch the signal x2(n) = x(n− 2).
(5) Determine and sketch the signal y2(n) = T [x2(n)].
(6) Compare the signals y2(n) and y(n− 2). What is your conclusion?

(c) Repeat part (b) for the system

y(n) = x(n)− x(n− 1)

Can you use this result to make any statement about the time invariance of this
system? Why?

(d) Repeat parts (b) and (c) for the system

y(n) = T [x(n)] = nx(n)
7 A discrete-time system can be

Discrete-Time Signals and Systems

Figure P2

132



(1) Static or dynamic
(2) Linear or nonlinear
(3) Time invariant or time varying
(4) Causal or noncausal
(5) Stable or unstable

Examine the following systems with respect to the properties above.

(a) y(n) = cos[x(n)]
(b) y(n) =∑n+1k=−∞ x(k)
(c) y(n) = x(n) cos(ω0n)
(d) y(n) = x(−n+ 2)
(e) y(n) = Trun[x(n)], where Trun[x(n)] denotes the integer part of x(n), obtained

by truncation

(f) y(n) = Round[x(n)], where Round[x(n)] denotes the integer part of x(n) ob-
tained by rounding

Remark: The systems in parts (e) and (f) are quantizers that perform truncation and
rounding, respectively.

(g) y(n) = |x(n)|
(h) y(n) = x(n)u(n)
(i) y(n) = x(n)+ nx(n+ 1)
(j) y(n) = x(2n)
(k) y(n) =

{
x(n), if x(n) ≥ 0
0, if x(n) < 0

(l) y(n) = x(−n)
(m) y(n) = sign[x(n)]
(n) The ideal sampling system with input xa(t) and output x(n) = xa(nT ),
−∞ < n <∞

8 Two discrete-time systems T1 and T2 are connected in cascade to form a new system
T

(a) If T1 and T2 are linear, then T is linear (i.e., the cascade connection of two linear
systems is linear).

(b) If T1 and T2 are time invariant, then T is time invariant.
(c) If T1 and T2 are causal, then T is causal.
(d) If T1 and T2 are linear and time invariant, the same holds for T .
(e) If T1 and T2 are linear and time invariant, then interchanging their order does

not change the system T .

(f) As in part (e) except that T1 , T2 are now time varying. (Hint: Use an example.)
(g) If T1 and T2 are nonlinear, then T is nonlinear.
(h) If T1 and T2 are stable, then T is stable.
(i) Show by an example that the inverses of parts (c) and (h) do not hold in general.

Discrete-Time Signals and Systems

as shown in Fig. P8. Prove or disprove the following statements.

133



T1 T2
x(n) y(n)

T = T1T2

9 Let T be an LTI, relaxed, and BIBO stable system with input x(n) and output y(n).
Show that:
(a) If x(n) is periodic with period N [i.e., x(n) = x(n+N) for all n ≥ 0], the output

y(n) tends to a periodic signal with the same period.
(b) If x(n) is bounded and tends to a constant, the output will also tend to a constant.
(c) If x(n) is an energy signal, the output y(n) will also be an energy signal.

10 The following input–output pairs have been observed during the operation of a time-
invariant system:

x1(n) = {1↑, 0, 2}
T←→ y1(n) = {0↑, 1, 2}

x2(n) = {0↑, 0, 3}
T←→ y2(n) = {0↑, 1, 0, 2}

x3(n) = {0↑, 0, 0, 1}
T←→ y3(n) = {1, 2↑, 1}

Can you draw any conclusions regarding the linearity of the system. What is the
impulse response of the system?

11 The following input–output pairs have been observed during the operation of a linear
system:

x1(n) = {−1, 2↑, 1}
T←→ y1(n) = {1, 2↑,−1, 0, 1}

x2(n) = {1,−1↑,−1}
T←→ y2(n) = {−1, 1↑, 0, 2}

x3(n) = {0, 1↑, 1}
T←→ y3(n) = {1↑, 2, 1}

Can you draw any conclusions about the time invariance of this system?
12 The only available information about a system consists of N input–output pairs, of

signals yi(n) = T [xi(n)], i = 1, 2, . . . , N .
(a) What is the class of input signals for which we can determine the output, using

the information above, if the system is known to be linear?
(b) The same as above, if the system is known to be time invariant.

13 Show that the necessary and sufficient condition for a relaxed LTI system to be BIBO
stable is

∞∑
n=−∞

|h(n)| ≤ Mh <∞

for some constant Mn .

Discrete-Time Signals and Systems

Figure P8

134



14 Show that:

(a) A relaxed linear system is causal if and only if for any input x(n) such that

x(n) = 0 for n < n0 ⇒ y(n) = 0 for n < n0

(b) A relaxed LTI system is causal if and only if

h(n) = 0 for n < 0

15

(a) Show that for any real or complex constant a , and any finite integer numbers M
and N , we have

N∑
n = Man =

{
aM − aN+1

1− a , if a �= 1
N −M + 1, if a = 1

(b) Show that if |a| < 1, then
∞∑
n=0

an = 1
1− a

16 (a) If y(n) = x(n) ∗ h(n), show that ∑y =∑x∑h , where ∑x =∑∞n=−∞ x(n).
(b) Compute the convolution y(n) = x(n) ∗ h(n) of the following signals and check

the correctness of the results by using the test in (a).

(1) x(n) = {1, 2, 4}, h(n) = {1, 1, 1, 1, 1}
(2) x(n) = {1, 2,−1}, h(n) = x(n)
(3) x(n) = {0, 1,−2, 3,−4}, h(n) = { 12 , 12 , 1, 12 }
(4) x(n) = {1, 2, 3, 4, 5}, h(n) = {1}
(5) x(n) = {1

↑
,−2, 3}, h(n) = {0

↑
, 0, 1, 1, 1, 1}

(6) x(n) = {0
↑
, 0, 1, 1, 1, 1}, h(n) = {1,−2

↑
, 3}

(7) x(n) = {0
↑
, 1, 4,−3}, h(n) = {1

↑
, 0,−1,−1}

(8) x(n) = {1
↑
, 1, 2}, h(n) = u(n)

(9) x(n) = {1, 1, 0
↑
, 1, 1}, h(n) = {1,−2,−3, 4

↑
}

(10) x(n) = {1, 2, 0
↑
, 2, 1}h(n) = x(n)

(11) x(n) = ( 12 )nu(n), h(n) = ( 14 )nu(n)

Discrete-Time Signals and Systems

135



17 Compute and plot the convolutions x(n)∗h(n) and h(n)∗x(n) for the pairs of signals

(a)

(b)

(c)

(d)

h(n)x(n)

n n0

1

1 2 3 0 1 2 3 4 5 6

6

6

1

1

h(n)

n0−1−2

−1−2

−3

−3−4

1 2 3

h(n)

n

h(n)

n

x(n)

n0

1

1 2 3

x(n)

n

1

3 4 5 6

2 3 4 5

x(n)

n

1

18 Determine and sketch the convolution y(n) of the signals

x(n) =
{

1
3n, 0 ≤ n ≤ 6
0, elsewhere

h(n) =
{

1, −2 ≤ n ≤ 2
0, elsewhere

(a) Graphically
(b) Analytically

19 Compute the convolution y(n) of the signals

x(n) =
{
αn, −3 ≤ n ≤ 5
0, elsewhere

h(n) =
{

1, 0 ≤ n ≤ 4
0, elsewhere

20 Consider the following three operations.
(a) Multiply the integer numbers: 131 and 122.
(b) Compute the convolution of signals: {1, 3, 1} ∗ {1, 2, 2}.
(c) Multiply the polynomials: 1+ 3z+ z2 and 1+ 2z+ 2z2 .
(d) Repeat part (a) for the numbers 1.31 and 12.2.
(e) Comment on your results.

Discrete-Time Signals and Systems

shown in Fig. P17.

Figure P17

136



21 Compute the convolution y(n) = x(n) ∗ h(n) of the following pairs of signals.
(a) x(n) = anu(n), h(n) = bnu(n) when a �= b and when a = b

(b) x(n) =
{ 1, n = −2, 0, 1

2, n = −1
0, elsewhere

h(n) = δ(n)− δ(n− 1)+ δ(n− 4)+ δ(n− 5)

(c) x(n) = u(n+ 1)− u(n− 4)− δ(n− 5);h(n) = [u(n+ 2)− u(n− 3)] · (3− |n|)
(d) x(n) = u(n)− u(n− 5);h(n) = u(n− 2)− u(n− 8)+ u(n− 11)− u(n− 17)

22 Let x(n) be the input signal to a discrete-time filter with impulse response hi(n) and
let yi(n) be the corresponding output.

(a) Compute and sketch x(n) and yi(n) in the following cases, using the same scale
in all figures.

x(n) = {1, 4, 2, 3, 5, 3, 3, 4, 5, 7, 6, 9}
h1(n) = {1, 1}
h2(n) = {1, 2, 1}

h3(n) = {12 ,
1
2
}

h4(n) = {14 ,
1
2
,

1
4
}

h5(n) = {14 ,−
1
2
,

1
4
}

Sketch x(n), y1(n), y2(n) on one graph and x(n), y3(n), y4(n), y5(n) on another
graph

(b) What is the difference between y1(n) and y2(n), and between y3(n) and y4(n)?

(c) Comment on the smoothness of y2(n) and y4(n). Which factors affect the
smoothness?

(d) Compare y4(n) with y5(n). What is the difference? Can you explain it?

(e) Let h6(n) = { 12 ,− 12 }. Compute y6(n). Sketch x(n), y2(n), and y6(n) on the same
figure and comment on the results.

23 Express the output y(n) of a linear time-invariant system with impulse response h(n)
in terms of its step response s(n) = h(n)∗u(n) and the input x(n).

24 The discrete-time system

y(n) = ny(n− 1)+ x(n), n ≥ 0

is at rest [i.e., y(−1) = 0]. Check if the system is linear time invariant and BIBO
stable.

25 Consider the signal γ (n) = anu(n), 0 < a < 1.

Discrete-Time Signals and Systems

137



(a) Show that any sequence x(n) can be decomposed as

x(n) =
∞∑

n=−∞
ckγ (n− k)

and express ck in terms of x(n).
(b) Use the properties of linearity and time invariance to express the output y(n) =

T [x(n)] in terms of the input x(n) and the signal g(n) = T [γ (n)], where T [·] is
an LTI system.

(c) Express the impulse response h(n) = T [δ(n)] in terms of g(n).
26 Determine the zero-input response of the system described by the second-order

difference equation
x(n)− 3y(n− 1)− 4y(n− 2) = 0

27 Determine the particular solution of the difference equation

y(n) = 5
6
y(n− 1)− 1

6
y(n− 2)+ x(n)

when the forcing function is x(n) = 2nu(n).
28

transient response and the steady-state response. Plot these two responses for a1 =
−0.9.

29 Determine the impulse response for the cascade of two linear time-invariant systems
having impulse responses.

h1(n) = an[u(n)− u(n−N)] and h2(n) = [u(n)− u(n−M)]
30 Determine the response y(n), n ≥ 0, of the system described by the second-order

difference equation

y(n)− 3y(n− 1)− 4y(n− 2) = x(n)+ 2x(n− 1)
to the input x(n) = 4nu(n).

31 Determine the impulse response of the following causal system:

y(n)− 3y(n− 1)− 4y(n− 2) = x(n)+ 2x(n− 1)
32 Let x(n), N1 ≤ n ≤ N2 and h(n), M1 ≤ n ≤ M2 be two finite-duration signals.

(a) Determine the range L1 ≤ n ≤ L2 of their convolution, in terms of N1 , N2 , M1
and M2 .

(b) Determine the limits of the cases of partial overlap from the left, full overlap, and
partial overlap from the right. For convenience, assume that h(n) has shorter
duration than x(n).

(c) Illustrate the validity of your results by computing the convolution of the signals

x(n) =
{

1, −2 ≤ n ≤ 4
0, elsewhere

h(n) =
{

2, −1 ≤ n ≤ 2
0, elsewhere

Discrete-Time Signals and Systems

In Example 4.8, equation (4.30), separate the output s equence y(n) into the

138



33 Determine the impulse response and the unit step response of the systems described
by the difference equation

(a) y(n) = 0.6y(n− 1)− 0.08y(n− 2)+ x(n)
(b) y(n) = 0.7y(n− 1)− 0.1y(n− 2)+ 2x(n)− x(n− 2)

34 Consider a system with impulse response

h(n) =
{
( 12 )

n, 0 ≤ n ≤ 4
0, elsewhere

Determine the input x(n) for 0 ≤ n ≤ 8 that will generate the output sequence

y(n) = {1
↑
, 2, 2.5, 3, 3, 3, 2, 1, 0, . . .}

35

←
+

y(n)
h1(n)

h2(n)

h3(n) h4(n)

x(n)

(a) Express the overall impulse response in terms of h1(n), h2(n), h3(n), and h4(n).

(b) Determine h(n) when

h1(n) = {12 ,
1
4
,

1
2
}

h2(n) = h3(n) = (n+ 1)u(n)
h4(n) = δ(n− 2)

(c) Determine the response of the system in part (b) if

x(n) = δ(n+ 2)+ 3δ(n− 1)− 4δ(n− 3)
n

response y(n) of the system to the excitation

x(n) = u(n+ 5)− u(n− 10)

Discrete-Time Signals and Systems

Figure P35

36 Consider the system in Fig. P36 with h(n) = a u(n), −1 < a < 1. Determine the

Consider the interconnection of LTI systems as shown in Fig. P35.

139



−
+

y(n)

h(n)

h(n)

x(n)

z−2

37 Compute and sketch the step response of the system

y(n) = 1
M

M−1∑
k=0

x(n− k)

38 Determine the range of values of the parameter a for which the linear time-invariant
system with impulse response

h(n) =
{
an, n ≥ 0, n even
0, otherwise

is stable.
39 Determine the response of the system with impulse response

h(n) = anu(n)

to the input signal
x(n) = u(n)− u(n− 10)

(Hint: The solution can be obtained easily and quickly by applying the linearity and

40 Determine the response of the (relaxed) system characterized by the impulse re-
sponse

h(n) = (1
2
)nu(n)

to the input signal

x(n) =
{

1, 0 ≤ n < 10
0, otherwise

41 Determine the response of the (relaxed) system characterized by the impulse re-
sponse

h(n) = (1
2
)nu(n)

to the input signals

(a) x(n) = 2nu(n)
(b) x(n) = u(−n)

Discrete-Time Signals and Systems

Figure P36

time-invariance properties to the result in Example 3.5.)

140



42 Three systems with impulse responses h1(n) = δ(n) − δ(n − 1), h2(n) = h(n), and
h3(n) = u(n), are connected in cascade.
(a) What is the impulse response, hc(n), of the overall system?

(b) Does the order of the interconnection affect the overall system?

43 (a) Prove and explain graphically the difference between the relations

x(n)δ(n− n0) = x(n0)δ(n− n0) and x(n) ∗ δ(n− n0) = x(n− n0)

(b) Show that a discrete-time system, which is described by a convolution summa-
tion, is LTI and relaxed,

(c) What is the impulse response of the system described by y(n) = x(n− n0)?
44 Two signals s(n) and v(n) are related through the following difference equations.

s(n)+ a1s(n− 1)+ · · · + aNs(n−N) = b0v(n)

Design the block diagram realization of:

(a) The system that generates s(n) when excited by v(n).

(b) The system that generates v(n) when excited by s(n).

(c) What is the impulse response of the cascade interconnection of systems in parts
(a) and (b)?

45 Compute the zero-state response of the system described by the difference equation

y(n)+ 1
2
y(n− 1) = x(n)+ 2x(n− 2)

to the input
x(n) = {1, 2, 3

↑
, 4, 2, 1}

by solving the difference equation recursively.
46 Determine the direct form II realization for each of the following LTI systems:

(a) 2y(n)+ y(n− 1)− 4y(n− 3) = x(n)+ 3x(n− 5)
(b) y(n) = x(n)− x(n− 1)+ 2x(n− 2)− 3x(n− 4)

47

++
x(n) y(n)

z−1

1
2

Discrete-Time Signals and Systems

Figure P47

Consider the discrete-time system shown in Fig. P47.

141



(a) Compute the 10 first samples of its impulse response.
(b) Find the input–output relation.
(c) Apply the input x(n) = {1

↑
, 1, 1, . . .} and compute the first 10 samples of the

output.

(d) Compute the first 10 samples of the output for the input given in part (c) by using
convolution.

(e) Is the system causal? Is it stable?
48 Consider the system described by the difference equation

y(n) = ay(n− 1)+ bx(n)
(a) Determine b in terms of a so that

∞∑
n=−∞

h(n) = 1

(b) Compute the zero-state step response s(n) of the system and choose b so that
s(∞) = 1.

(c) Compare the values of b obtained in parts (a) and (b). What did you notice?
49

(a) Determine the impulse response.
(b) Determine a realization for its inverse system, that is, the system which produces

x(n) as an output when y(n) is used as an input.

++
x(n)

y(n)

z−1

2

3

0.8

50

++
x(n)

y(n)

z−1

2
+

z−1

3

0.9

Discrete-Time Signals and Systems

A discrete-time system is realized by the structure shown in Fig. P49.

Figure P49

Consider the discrete-time system shown in Fig. P50.

Figure P50

142



(a) Compute the first six values of the impulse response of the system.

(b) Compute the first six values of the zero-state step response of the system.

(c) Determine an analytical expression for the impulse response of the system.

51 Determine and sketch the impulse response of the following systems for n = 0,
1, . . . , 9.

++

++

+

+

x(n)

x(n)

x(n)

y(n)

y(n)

y(n)

z−1z−1

z−1 z−1

z−1

z−1

z−1

z−1 z−1 z−1

1
2

1
8

1
2

1
3

0.8 0.6

(a)

(b)

(c)

(b)

(d) Classify the systems above as FIR or IIR.

(e) Find an explicit expression for the impulse response of the system in part (c).

Discrete-Time Signals and Systems

Figure P51

(a) Fig. P51(a).

Fig. P51(b).

(c) Fig. P51(c).

143



52

++

x(n)

y(n)

y(n)

z−1 z−1

c1c0 c2

+

+

x(n)
z−1 z−1

a1

a0

a2

++

x(n)

y(n)z−1 z−1

b1b0 b2

(a) Determine and sketch their impulse responses h1(n), h2(n), and h3(n).
(b) Is it possible to choose the coefficients of these systems in such a way that

h1(n) = h2(n) = h3(n)
53

++
x(n)

y(n)

z−1

z−1

1
2

(a) Determine its impulse response h(n).
(b) Show that h(n) is equal to the convolution of the following signals:

h1(n) = δ(n)+ δ(n− 1)

h2(n) = (12 )
nu(n)

54 Compute and sketch the convolution yi(n) and correlation ri(n) sequences for the
following pair of signals and comment on the results obtained.
(a) x1(n) = {1↑, 2, 4} h1(n) = {1↑, 1, 1, 1, 1}
(b) x2(n) = {0↑, 1,−2, 3,−4} h2(n) = {

1
2 , 1, 2↑

, 1, 12 }

Discrete-Time Signals and Systems

Consider the systems shown in Fig. P52.

Figure P52

Consider the system shown in Fig. P53.

Figure P53

144



(c) x3(n) = {1↑, 2, 3, 4} h3(n) = {4↑, 3, 2, 1}
(d) x4(n) = {1↑, 2, 3, 4} h4(n) = {1↑, 2, 3, 4}

55 The zero-state response of a causal LTI system to the input x(n) = {1
↑
, 3, 3, 1} is

y(n) = {1
↑
, 4, 6, 4, 1}. Determine its impulse response.

56

form I structure.
57 Determine the response y(n), n ≥ 0 of the system described by the second-order

difference equation

y(n)− 4y(n− 1)+ 4y(n− 2) = x(n)− x(n− 1)
when the input is

x(n) = (−1)nu(n)
and the initial conditions are y(−1) = y(−2) = 0.

58 Determine the impulse response h(n) for the system described by the second-order
difference equation

y(n)− 4y(n− 1)+ 4y(n− 2) = x(n)− x(n− 1)
59 Show that any discrete-time signal x(n) can be expressed as

x(n) =
∞∑

k=−∞
[x(k)− x(k − 1)]u(n− k)

where u(n− k) is a unit step delayed by k units in time, that is,

u(n− k) =
{

1, n ≥ k
0, otherwise

60 Show that the output of an LTI system can be expressed in terms of its unit step
response s(n) as follows.

y(n) =
∞∑

k=−∞
[s(k)− s(k − 1)]x(n− k)

=
∞∑

k=−∞
[x(k)− x(k − 1)]s(n− k)

61 Compute the correlation sequences rxx(l) and rxy(l) for the following signal se-
quences.

x(n) =
{

1, n0 −N ≤ n ≤ n0 +N
0, otherwise

y(n) =
{

1, −N ≤ n ≤ N
0, otherwise

Discrete-Time Signals and Systems

Prove by direct substitution the equivalence of equations (5.9) and (5.10), which
describe the direct form II structure, to the relation (5.6), which describes the direct

145



62 Determine the autocorrelation sequences of the following signals.

(a) x(n) = {1
↑
, 2, 1, 1}

(b) y(n) = {1
↑
, 1, 2, 1}

What is your conclusion?
63 What is the normalized autocorrelation sequence of the signal x(n) given by

x(n) =
{

1, −N ≤ n ≤ N
0, otherwise

64 An audio signal s(t) generated by a loudspeaker is reflected at two different walls
with reflection coefficients r1 and r2 . The signal x(t) recorded by a microphone close
to the loudspeaker, after sampling, is

x(n) = s(n)+ r1s(n− k1)+ r2s(n− k2)

where k1 and k2 are the delays of the two echoes.

(a) Determine the autocorrelation rxx(l) of the signal x(n).

(b) Can we obtain r1 , r2 , k1 , and k2 by observing rxx(l)?

(c) What happens if r2 = 0?
65 Time-delay estimation in radar Let xa(t) be the transmitted signal and ya(t) be the

received signal in a radar system, where

ya(t) = axa(t − td)+ va(t)

and va(t) is additive random noise. The signals xa(t) and ya(t) are sampled in the
receiver, according to the sampling theorem, and are processed digitally to deter-
mine the time delay and hence the distance of the object. The resulting discrete-time
signals are

x(n) = xa(nT )
y(n) = ya(nT ) = axa(nT −DT )+ va(nT )


= ax(n−D)+ v(n)

Linear feedback shift
register.

+
Modulo-2 adder

Output

0 → �1

1 → �1

1 0 0 0

Discrete-Time Signals and Systems

Figure P65

146



(a) Explain how we can measure the delay D by computing the crosscorrelation
rxy(l).

(b) Let x(n) be the 13-point Barker sequence

x(n) = {+1,+1,+1,+1,+1,−1,−1,+1,+1,−1,+1,−1,+1}
and v(n) be a Gaussian random sequence with zero mean and variance σ 2 = 0.01.
Write a program that generates the sequence y(n), 0 ≤ n ≤ 199 for a = 0.9 and
D = 20. Plot the signals x(n), y(n), 0 ≤ n ≤ 199.

(c) Compute and plot the crosscorrelation rxy(l), 0 ≤ l ≤ 59. Use the plot to
estimate the value of the delay D .

(d) Repeat parts (b) and (c) for σ 2 = 0.1 and σ 2 = 1.
(e) Repeat parts (b) and (c) for the signal sequence

x(n) = {−1,−1,−1,+1,+1,+1,+1,−1,+1,−1,+1,+1,−1,−1,+1}

Note that x(n) is just one period of the periodic sequence obtained from the
feedback shift register.

(f) Repeat parts (b) and (c) for a sequence of period N = 27 − 1, which is obtained

to the modulo-2 adder for (maximal-length) shift-register sequences of length
N = 2m − 1.

Shift-Register Connections for Gener-
ating Maximal-Length Sequences
m Stages Connected to Modulo-2 Adder
1 1
2 1, 2
3 1, 3
4 1, 4
5 1, 4
6 1, 6
7 1, 7
8 1, 5, 6, 7
9 1, 6
10 1, 8
11 1, 10
12 1, 7, 9, 12
13 1, 10, 11, 13
14 1, 5, 9, 14
15 1, 15
16 1, 5, 14, 16
17 1, 15

Discrete-Time Signals and Systems

TABLE 2

from a seven-stage feedback shift register. Table 2 gives the stages connected

which is obtained from the four-stage feedback shift register shown in Fig. P6.5.

147



66 Implementation of LTI systems Consider the recursive discrete-time system de-
scribed by the difference equation

y(n) = −a1y(n− 1)− a2y(n− 2)+ b0x(n)

where a1 = −0.8, a2 = 0.64, and b0 = 0.866.
(a) Write a program to compute and plot the impulse response h(n) of the system

for 0 ≤ n ≤ 49.
(b) Write a program to compute and plot the zero-state step response s(n) of the

system for 0 ≤ n ≤ 100.
(c) Define an FIR system with impulse response hFIR(n) given by

hFIR(n) =
{
h(n), 0 ≤ n ≤ 19
0, elsewhere

where h(n) is the impulse response computed in part (a). Write a program to
compute and plot its step response.

(d) Compare the results obtained in parts (b) and (c) and explain their similarities
and differences.

67 Write a computer program that computes the overall impulse response h(n) of the
The systems T1 , T2 , T3 , and T4 are

specified by

T1 : h1(n) = {1↑,
1
2
,

1
4
,

1
8
,

1
16
,

1
32
}

T2 : h2(n) = {1↑, 1, 1, 1, 1}

T3 : y3(n) = 14x(n)+
1
2
x(n− 1)+ 1

4
x(n− 2)

T4 : y(n) = 0.9y(n− 1)− 0.81y(n− 2)+ v(n)+ v(n− 1)

Plot h(n) for 0 ≤ n ≤ 99.

+
x(n) ν(n)

y(n)

y3(n)

T1 T2

T4

T3

Discrete-Time Signals and Systems

system shown in Fig. P67 for 0 ≤ n ≤ 99.

Figure P67

148



Discrete-Time Signals and Systems

Answers to Selected Problems

7 (a) Static, nonlinear, time invariant, causal, stable.

(c) Static, linear, time variant, causal, stable.

(e) Static, nonlinear, time invariant, causal, stable.

(h) Static, linear, time invariant, causal, stable.

(k) Static, nonlinear, time invariant, causal, stable.

11 Since the system is linear and x1(n) + x2(n) = δ(n), it follows that the impulse response of the
system is y1(n)+ y2(n) =

{
0, 3
↑
,−1, 2, 1

}
.

If the system were time invariant, the response to x3 (n) would be
{

3
↑
, 2, 1, 3, 1

}
. But this is not

the case.

16 (b) (1) y(n) = h(n) ∗ x(n) = {1, 3, 7, 7, 7, 6, 4} ; ∑n y(n) = 35, ∑k h(k) = 5,∑
k x(k) = 7.

(4) y(n) = {1, 2, 3, 4, 5} ; ∑n y(n) = 15, ∑n h(n) = 1, ∑n x(n) = 15
(7) y(n) = {0, 1, 4,−4,−5,−1, 3} ∑n y(n) = −2, ∑n h(n) = −1, ∑n x(n) = 2
(10) y(n) = {1, 4, 4, 4, 10, 4, 4, 4, 1} ; ∑n y(n) = 36, ∑n h(n) = 6, ∑n x(n) = 6

19 y(n) = ∑4k=0 h(k)x(n − k) , x(n) =
{
a−3, a−2, a−1, 1

↑
, a, . . . , a5

}
, h(n) =

{
1
↑
, 1, 1, 1, 1,

}
; y(n) =∑4

k=0 x(n− k) , −3 ≤ n ≤ 9; y(n) = 0, otherwise
22 (a) y1(n) = x(n)+ x(n− 1) = {1, 5, 6, 5, 8, 8, 6, 7, 9, 12, 15, 9}

y4(n) = {0.25, 1.5, 2.75, 2.75, 3.25, 4, 3.5, 3.25, 3.75, 5.25, 6.25, 7, 6, 2.25}
26 With x(n) = 0, we have y(n − 1) + 43y(n − 1) = 0 y(−1) = − 43y(−2); y(0) =

(− 43 )2 y(−2) ;
y(1) = (− 43 )3 y(−2)
Therefore, y(k) = (− 43 )k+2 y(−2)← zero-input response.

32 (a) L1 = N1 +M1 and L2 = N2 +M2
(c) N1 = −2, N2 = 4, M1 = −1, M2 = 2
Partial overlap from left: n = −3 n = −1 L1 = −3; Full overlap: n = 0 n = 3; Partial overlap
from right: n = 4 n = 6 L2 = 6

34 h(n) =
{

1
↑
, 12 ,

1
4 ,

1
8 ,

1
16

}
; y(n) =

{
1
↑
, 2, 2, 5, 3, 3, 3, 2, 1, 0

}

Then, x(n) = {1. 32 , 32 , 74 , 32 }
38

∑∞
n=−∞ |h(n)| =

∑∞
n=0,neven |a|n ;

∑∞
n=− |a|2n ; = 11−|a|2

Stable if |a| < 1
40 y(n) = 2 [1− ( 12 )n+1] u(n)− 2 [1− ( 12 )a−9]u(n− 10)
41 (a) y(n) = 23

[
2n+1 − ( 12 )n+1

]
u(n)

42 (a) hc(n) = h1(n) ∗ h2(n) ∗ h3(n) = [δ(n)− δ(n− 1)] ∗ u(n) ∗ h(n) = h(n)
(b) No.

45 y(n) = − 12 y(n− 1)+ z(n)+ 2x(n− 2); y(−2) = 1, y(−1) = 32 , y(9) = 174 , y(1) = 478 , · · ·
48 (a) y(n) = ay(n− 1)+ bx(n)⇒ h(n) = banu(n) ∑∞n=0 h(n) = b1−a = 1⇒ b = 1− a .

(b) s(n) =∑nk=0 h(n− k) = b [ 1−an+11−a ]u(n)
s(∞) = b1−a = 1⇒ b = 1− a

51 (a) y(n) = 13x(n)+ 13x(n− 3)+ y(n− 1) for x(n) = δ(n), we have
h(n) = { 13 , 13 , 13 , 23 , 23 , 23 , 23 . . . }

(b) y(n) = 12 y(n− 1)+ 18y(n− 2)+ 12x(n− 2), y(−1) = y(−2) = 0
with x(n) = δ(n), h(n) = {0, 0, 12 , 14 , 316 , 19 , 11128 , 15256 , 411024 . . . }

(c) y(n) = 1.4y(n− 1)− 0.48y(n− 2)+ x(n) , y(−1)− y(−2) = 0
with x(n)δ(n), h(n) = {1, 1, 4, 1.48, 1.4, 1.2496, 1.0774, 0.9086, . . . }

(d) All three systems are IIR.

149



Discrete-Time Signals and Systems

54 (a) convolution: y1(n) =
{

1
→

3, 7, 7, 7, 7, 4
}

correlation: γ1(n) =
{

1, 3, 7, 7, 7
→
, 6, 4

}
(c) convolution: y4(n) =

{
1
→
, 4, 10, 20, 25, 24, 16

}
correlation: γ4(n) =

{
4, 11, 20, 30

→
, 20, 11, 4

}
58 h(n) = [c12n + c2n2n]u(n)

With y(0) = 1, y(1) = 3, we have, c1 = 1, and c2 = 12 .

61 x(n) =
{

1, n0 −N ≤ n ≤ n0 +N
0 otherwise

.

rxx(l) =
{

2N + 1− |l|, −2N ≤ l ≤ 2N
0, otherwise

.

63 rxx(l) =
∑∞

n=−∞ x(n)x(n− l) =
{

2N + 1− |l|, −2N ≤ l ≤ 2N
0, otherwise

.

Since rxx(0) = 2N + 1, the normalized autocorrelation is
ρxx(l) =

{ 1
2N+1 (2N + 1− |l|), −2N ≤ l ≤ 2N
0, otherwise

150



The z-Transform and Its
Application to the
Analysis of LTI Systems

Transform techniques are an important tool in the analysis of signals and linear time-
invariant (LTI) systems. In this chapter we introduce the z-transform, develop its
properties, and demonstrate its importance in the analysis and characterization of
linear time-invariant systems.

The z-transform plays the same role in the analysis of discrete-time signals and
LTI systems as the Laplace transform does in the analysis of continuous-time signals
and LTI systems. For example, we shall see that in the z-domain (complex z-plane)
the convolution of two time-domain signals is equivalent to multiplication of their
corresponding z-transforms. This property greatly simplifies the analysis of the re-
sponse of an LTI system to various signals. In addition, the z-transform provides us
with a means of characterizing an LTI system, and its response to various signals, by
its pole–zero locations.

1 The z-Transform

In this section we introduce the z-transform of a discrete-time signal, investigate its
convergence properties, and briefly discuss the inverse z-transform.

We begin this chapter by defining the z-transform. Its important properties are 
presented in Section 2. In Section 3 the transform is used to characterize signals 
in terms of their pole–zero patterns. Section 4 describes methods for inverting 
the z-transform of a signal so as to obtain the time-domain representation of the 
signal. Finally, in Section 6, we treat one-sided z-transform and use it to solve 
linear difference equations with nonzero initial conditions. Section 5 is focused on 
the use of the z-transform in the analysis of LTI systems.

John G. Proakis, Dimitris G. Manolakis. Copyright © 2007 by Pearson Education, Inc. All rights reserved.
From Chapter 3   of Digital Signal Processing: Principles, Algorithms, and Applications, Fourth Edition.

151



X(z) ≡
∞∑

n=−∞
x(n)z−n

transform because it transforms the time-domain signal x(n) into its complex-plane
representation X(z). The inverse procedure [i.e., obtaining x(n) from X(z)] is called

For convenience, the z-transform of a signal x(n) is denoted by

X(z) ≡ Z{x(n)}
whereas the relationship between x(n) and X(z) is indicated by

x(n)
z←→ X(z)

Since the z-transform is an infinite power series, it exists only for those values of
z for which this series converges. The region of convergence (ROC) of X(z) is the
set of all values of z for which X(z) attains a finite value. Thus any time we cite a
z-transform we should also indicate its ROC.

We illustrate these concepts by some simple examples.

Determine the z-transforms of the following finite-duration signals.
(a) x1(n) = {1↑, 2, 5, 7, 0, 1}
(b) x2(n) = {1, 2, 5↑, 7, 0, 1}
(c) x3(n) = {0↑, 0, 1, 2, 5, 7, 0, 1}
(d) x4(n) = {2, 4, 5↑, 7, 0, 1}
(e) x5(n) = δ(n)
(f) x6(n) = δ(n− k), k > 0
(g) x7(n) = δ(n+ k), k > 0
Solution.
(a) X1(z) = 1+ 2z−1 + 5z−2 + 7z−3 + z−5 , ROC: entire z-plane except z = 0
(b) X2(z) = z2 + 2z+ 5+ 7z−1 + z−3 , ROC: entire z-plane except z = 0 and z = ∞
(c) X3(z) = z−2 + 2z−3 + 5z−4 + 7z−5 + z−7 , ROC: entire z-plane except z = 0
(d) X4(z) = 2z2 + 4z+ 5+ 7z−1 + z−3 , ROC: entire z-plane except z = 0 and z = ∞
(e) X5(z) = 1 [i.e., δ(n) z←→ 1], ROC: entire z-plane
(f) X6(z) = z−k [i.e., δ(n− k) z←→ z−k], k > 0, ROC: entire z-plane except z = 0
(g) X7(z) = zk [i.e., δ(n+ k) z←→ zk], k > 0, ROC: entire z-plane except z = ∞

The z-Transform and Its Application to the Analysis of LTI Systems

The z-transform of a discrete-time signal x(n) is defined as the power series

The Direct z-Transform1.1

(1.1)

(1.2)

(1.3)

EXAMPLE 1.1

where z is a complex variable. The relation (1.1) is sometimes called the direct z-

the inverse z-transform and is examined briefly in Section 1.2 and in more detail in
Section 4.

From definition (1.1), we have

152



From this example it is easily seen that the ROC of a finite-duration signal is the
entire z-plane, except possibly the points z = 0 and/or z = ∞. These points are
excluded, because zk (k > 0) becomes unbounded for z = ∞ and z−k (k > 0)
becomes unbounded for z = 0.

From a mathematical point of view the z-transform is simply an alternative

that the coefficient of z−n , in a given transform, is the value of the signal at time n.
In other words, the exponent of z contains the time information we need to identify
the samples of the signal.

In many cases we can express the sum of the finite or infinite series for the z-
transform in a closed-form expression. In such cases the z-transform offers a compact
alternative representation of the signal.

Determine the z-transform of the signal

x(n) = (1
2
)nu(n)

Solution. The signal x(n) consists of an infinite number of nonzero values

x(n) = {1, (1
2
), (

1
2
)2, (

1
2
)3, . . . , (

1
2
)n, . . .}

The z-transform of x(n) is the infinite power series

X(z) = 1+ 1
2
z−1 + (1

2
)2z−2 + (1

2
)nz−n + · · ·

=
∞∑
n=0
(

1
2
)nz−n =

∞∑
n=0
(

1
2
z−1)n

This is an infinite geometric series. We recall that

1+ A+ A2 + A3 + · · · = 1
1−A if |A| < 1

Consequently, for | 12 z−1| < 1, or equivalently, for |z| > 12 , X(z) converges to

X(z) = 1
1− 12 z−1

, ROC: |z| > 1
2

We see that in this case, the z-transform provides a compact alternative representation of the
signal x(n).

The z-Transform and Its Application to the Analysis of LTI Systems

representation of a signal. This is nicely illustrated in Example 1.1, where we see

EXAMPLE 1.2

153



Let us express the complex variable z in polar form as

z = rejθ

where r = |z| and θ = �z. Then X(z) can be expressed as

X(z)|z=rejθ =
∞∑

n=−∞
x(n)r−ne−jθn

In the ROC of X(z), |X(z)| <∞. But

|X(z)| =
∣∣∣∣∣
∞∑

n=−∞
x(n)r−ne−jθn

∣∣∣∣∣
≤

∞∑
n=−∞

|x(n)r−ne−jθn| =
∞∑

n=−∞
|x(n)r−n|

Hence |X(z)| is finite if the sequence x(n)r−n is absolutely summable.
The problem of finding the ROC for X(z) is equivalent to determining the range

of values of r for which the sequence x(n)r−n is absolutely summable. To elaborate,

|X(z)| ≤
−1∑

n=−∞
|x(n)r−n| +

∞∑
n=0

∣∣∣∣x(n)rn
∣∣∣∣

≤
∞∑
n=1
|x(−n)rn| +

∞∑
n=0

∣∣∣∣x(n)rn
∣∣∣∣

must be finite in that region.
values of r small enough such that the product sequence x(−n)rn, 1 ≤ n < ∞, is
absolutely summable. Therefore, the ROC for the first sum consists of all points in
a circle of some radius r1 , where r1

such that the product sequence x(n)/rn , 0 ≤ n <∞, is absolutely summable. Hence

2

follows that the ROC of X(z) is generally specified as the annular region in the z-
plane, r2 < r < r1 , which is the common region where both sums are finite. This

2 > r1 , there is no common
region of convergence for the two sums and hence X(z) does not exist.

The following examples illustrate these important concepts.

The z-Transform and Its Application to the Analysis of LTI Systems

(1.4)

(1.5)

(1.6)

let us express (1.5) as

If X(z) converges in some region of the complex plane, both summations in (1.6)
If the first sum in (1.6) converges, there must exist

<∞, as illustrated in Fig. 1.1(a). On the other
hand , if the second sum in (1.6) converges, there must exist values of r large enough

the ROC for the second sum in (1.6) consists of all points outside a circle of radius
r > r , as illustrated in Fig. 1.1(b).

Since the convergence of X(z) requires that both sums in (1.6) be finite, it

region is illustrated in Fig. 1.1(c). On the other hand, if r

154



Region of convergence for
X(z) and its corresponding
causal and anticausal
components.

Im(z)

Re(z)

z-plane

Region of convergence for

n = 1
Σ |x(−n) rn |

(a)

Im(z)

Re(z)

z-plane

Region of convergence for |X(z)|
r2 < r < r1

(c)

Im(z)

Re(z)

Region of convergence for

x(n)
rn

(b)

z-plane

r1

r2

r1

r2

n = 0
Σ

Determine the z-transform of the signal

x(n) = αnu(n) =
{
αn, n ≥ 0
0, n < 0

Solution.

X(z) =
∞∑
n=0

αnz−n =
∞∑
n=0
(αz−1)n

If |αz−1| < 1 or equivalently, |z| > |α|, this power series converges to 1/(1− αz−1). Thus we
have the z-transform pair

The z-Transform and Its Application to the Analysis of LTI Systems

Figure 1.1

EXAMPLE 1.3

From the definition (1.1) we have

155



Im(z)

Re(z)

(b)(a)

ROC

|α|

0

x(n)

1 2 3 4 5 n

…
…

0

The exponential signal x(n) = αnu(n) (a), and the ROC of its z-
transform (b).

x(n) = αnu(n) z←→ X(z) = 1
1− αz−1 , ROC: |z| > |α|

x(n) and its corresponding ROC. Note that, in general, α need not be real.

x(n) = u(n) z←→ X(z) = 1
1− z−1 , ROC: |z| > 1

Determine the z-transform of the signal

x(n) = −αnu(−n− 1) =
{

0, n ≥ 0
−αn, n ≤ −1

Solution.

X(z) =
−1∑

n=−∞
(−αn)z−n = −

∞∑
l=1
(α−1z)l

where l = −n. Using the formula

A+ A2 + A3 + · · · = A(1+ A+ A2 + · · ·) = A
1− A

when |A| < 1 gives
X(z) = − α

−1z
1− α−1z =

1
1− αz−1

provided that |α−1z| < 1 or, equivalently, |z| < |α|. Thus

x(n) = −αnu(−n− 1) z←→ X(z) = − 1
1− αz−1 , ROC: |z| < |α|

The z-Transform and Its Application to the Analysis of LTI Systems

(1.7)

(1.8)

(1.9)

Figure 1.2

The ROC is the exterior of a circle having radius |α|. Figure 1.2 shows a graph of the signal

If we set α = 1 in (1.7), we obtain the z-transform of the unit step signal

EXAMPLE 1.4

From the definition (1.1) we have

The ROC is now the interior of a circle having radius |α|. This is shown in Fig. 1.3.

156



Im(z)

Re(z)

(b)(a)

ROC

|α|0

x(n)

−5 −4 −3 −2 −1

n

…
…

0 < α < 1

Anticausal signal x(n) = −αnu(−n− 1) (a), and the ROC of its z-transform (b).

the uniqueness of the z-transform.
signal αnu(n) and the anticausal signal −αnu(−n − 1) have identical closed-form
expressions for the z-transform, that is,

Z{αnu(n)} = Z{−αnu(−n− 1)} = 1
1− αz−1

This implies that a closed-form expression for the z-transform does not uniquely
specify the signal in the time domain. The ambiguity can be resolved only if in
addition to the closed-form expression, the ROC is specified. In summary, a discrete-
time signal x(n) is uniquely determined by its z-transform X(z) and the region of
convergence of X(z). In this text the term “z-transform” is used to refer to both the

the point that the ROC of a causal signal is the exterior of a circle of some radius r2
while the ROC of an anticausal signal is the interior of a circle of some radius r1 . The
following example considers a sequence that is nonzero for −∞ < n <∞.

Determine the z-transform of the signal

x(n) = αnu(n)+ bnu(−n− 1)
Solution.

X(z) =
∞∑
n=0

αnz−n +
−1∑

n=−∞
bnz−n =

∞∑
n=0
(αz−1)n +

∞∑
l=1
(b−1z)l

The first power series converges if |αz−1| < 1 or |z| > |α|. The second power series converges
if |b−1z| < 1 or |z| < |b|.

In determining the convergence of X(z), we consider two different cases.

The z-Transform and Its Application to the Analysis of LTI Systems

Figure 1.3

Examples 1 .3 and 1.4 illustrate two very important issues. The first concerns
From (1.7) and (1.9) we see that the causal

closed-form expression and the corresponding ROC. Example 1.3 also illustrates

EXAMPLE 1.5

From definition (1.1) we have

157



ROC for z-transform in

Im(z)

Re(z)

z-plane

|b| < |α|
X(z) does not exist

Im(z)

Re(z)

z-plane

ROC for X(z)

|α| < |b|

|b|

|α|

|b|

|α|

(b)

(a)

Case 1 |b| < |α|:
Consequently, we cannot find values of z for which both power series con-
verge simultaneously. Clearly, in this case, X(z) does not exist.

Case 2 |b| > |α|: In this case there is a ring in the z-plane where both power series converge

X(z) = 1
1− αz−1 −

1
1− bz−1

= b − α
α + b − z− αbz−1

The ROC of X(z) is |α| < |z| < |b|.

This example shows that if there is a ROC for an infinite-duration two-sided

or infinite) and on whether it is causal, anticausal, or two-sided. These facts are

One special case of a two-sided signal is a signal that has infinite duration on
the right side but not on the left [i.e., x(n) = 0 for n < n0 < 0]. A second case is

The z-Transform and Its Application to the Analysis of LTI Systems

(1.10)

Figure 1.4

Example 1.5.

In this case the two ROC above do not overlap, as shown in Fig. 1.4(a).

simultaneously, as shown in Fig. 1.4(b). Then we obtain

signal, it is a ring (annular region) in the z-plane. From Examples 1.1, 1.3, 1.4,
and 1.5, we see that the ROC of a signal depends both on its duration (finite

summarized in Table 1.

158



Characteristic Families of Signals with Their Corresponding
ROCs

Signal ROC

Finite-Duration Signals

0 n

Entire z-plane
except z = 0

Causal

0 n

Two-sided

0 n

Anticausal

Infinite-Duration Signals

0 n

|z| > r2

Causal

0 n

Two-sided

0 n

Anticausal

…

…

… …

r2

|z| < r1

r1

r2 < |z| < r1

r2

r1

a signal that has infinite duration on the left side but not on the right [i.e., x(n) = 0
for n > n1 > 0]. A third special case is a signal that has finite duration on both the
left and right sides [i.e., x(n) = 0 for n < n0 < 0 and n > n1 > 0]. These types
of signals are sometimes called right-sided, left-sided, and finite-duration two-sided
signals, respectively. The determination of the ROC for these three types of signals

to as the two-sided or bilateral z-transform, to distinguish it from the one-sided or

The z-Transform and Its Application to the Analysis of LTI Systems

TABLE 1

is left as an exercise for the reader (Problem 5).
Finally, we note that the z-transform defined by (1.1) is sometimes referred

159



unilateral z-transform given by

X+(z) =
∞∑
n=0

x(n)z−n

The term “two-sided” will be used only in cases where we want to resolve any am-
biguities. Clearly, if x(n) is causal [i.e., x(n) = 0 for n < 0], the one-sided and
two-sided z-transforms are identical. In any other case, they are different.

1.2 The Inverse z-Transform

Often, we have the z-transform X(z) of a signal and we must determine the signal
sequence. The procedure for transforming from the z-domain to the time domain is
called the inverse z-transform. An inversion formula for obtaining x(n) from X(z)
can be derived by using the Cauchy integral theorem, which is an important theorem
in the theory of complex variables.

X(z) =
∞∑

k=−∞
x(k)z−k

n−1 and integrate both sides over
a closed contour within the ROC of X(z) which encloses the origin. Such a contour

∮
Ĉ
X(z)zn−1 dz =

∮
Ĉ

∞∑
k=−∞

x(k)zn−1−k dz

where C denotes the closed contour in the ROC of X(z), taken in a counterclockwise
direction. Since the series converges on this contour, we can interchange the order of

Contour C for integral in

Im(z)

Re(z)

C

r1

r2

The z-Transform and Its Application to the Analysis of LTI Systems

(1.11)

(1.12)

(1.13)

The one-sided z-transform is examined in Section 6. In this text we use the expres-
sion z-transform exclusively to mean the two-sided z-transform defined by (1.1).

To begin, we have the z-transform defined by (1.1) as

Suppose that we multiply both sides of (1.12) by z

is illustrated in Fig. 1.5. Thus we have

Figure 1.5

(1.13).

160



∮
Ĉ
X(z)zn−1dz =

∞∑
k=−∞

x(k)

∮
Ĉ
zn−1−kdz

Now we can invoke the Cauchy integral theorem, which states that

1
2πj

∮
Ĉ
zn−1−k dz =

{
1, k = n
0, k �= n

x(n) = 1
2πj

∮
Ĉ
X(z)zn−1 dz

directly in our evaluation of inverse z-transforms. In our treatment we deal with sig-
nals and systems in the z-domain which have rational z-transforms (i.e., z-transforms
that are a ratio of two polynomials). For such z-transforms we develop a simpler

2 Properties of the z-Transform

The z-transform is a very powerful tool for the study of discrete-time signals and
systems. The power of this transform is a consequence of some very important
properties that the transform possesses. In this section we examine some of these
properties.

In the treatment that follows, it should be remembered that when we combine
several z-transforms, the ROC of the overall transform is, at least, the intersection of
the ROC of the individual transforms. This will become more apparent later, when
we discuss specific examples.

Linearity. If

x1(n)
z←→ X1(z)

and
x2(n)

z←→ X2(z)
then

x(n) = a1x1(n)+ a2x2(n) z←→ X(z) = a1X1(z)+ a2X2(z)
for any constants a1 and a2 . The proof of this property follows immediately from
the definition of linearity and is left as an exercise for the reader.

The linearity property can easily be generalized for an arbitrary number of sig-
nals. Basically, it implies that the z-transform of a linear combination of signals is
the same linear combination of their z-transforms. Thus the linearity property helps
us to find the z-transform of a signal by expressing the signal as a sum of elementary
signals, for each of which, the z-transform is already known.

The z-Transform and Its Application to the Analysis of LTI Systems

(1.14)

(1.15)

(1.16)

(2.1)

integration and summation on the right-hand side of (1.13). Thus (1.13) becomes

where C is any contour that encloses the origin. By applying (1.15), the right-hand
side of (1.14) reduces to 2πjx(n) and hence the desired inversion formula

Although the contour integral in (1.16) provides the desired inversion formula
for determining the sequence x(n) from the z-transform, we shall not use (1.16)

method for inversion that stems from (1.16) and employs a table lookup.

161



Determine the z-transform and the ROC of the signal

x(n) = [3(2n)− 4(3n)]u(n)
Solution. If we define the signals

x1(n) = 2nu(n)
and

x2(n) = 3nu(n)
then x(n) can be written as

x(n) = 3x1(n)− 4x2(n)

X(z) = 3X1(z)− 4X2(z)

αnu(n)
z←→ 1

1− αz−1 , ROC: |z| > |α|

x1(n) = 2nu(n) z←→ X1(z) = 11− 2z−1 , ROC: |z| > 2

x2(n) = 3nu(n) z←→ X2(z) = 11− 3z−1 , ROC: |z| > 3

The intersection of the ROC of X1(z) and X2(z) is |z| > 3. Thus the overall transform X(z) is

X(z) = 3
1− 2z−1 −

4
1− 3z−1 , ROC: |z| > 3

Determine the z-transform of the signals
(a) x(n) = (cosω0n)u(n)
(b) x(n) = (sinω0n)u(n)
Solution.
(a) By using Euler’s identity, the signal x(n) can be expressed as

x(n) = (cosω0n)u(n) = 12 e
jω0nu(n)+ 1

2
e−jω0nu(n)

X(z) = 1
2
Z{ejω0nu(n)} + 1

2
Z{e−jω0nu(n)}

The z-Transform and Its Application to the Analysis of LTI Systems

(2.2)

EXAMPLE 2.1

According to (2.1), its z-transform is

From (1.7) we recall that

By setting α = 2 and α = 3 in (2.2), we obtain

EXAMPLE 2.2

Thus (2.1) implies that

162



If we set α = e±jω0 (|α| = |e±jω0

ejω0nu(n)
z←→ 1

1− ejω0z−1 , ROC: |z| > 1

and

e−jω0nu(n)
z←→ 1

1− e−jω0z−1 , ROC: |z| > 1

Thus

X(z) = 1
2

1
1− ejω0z−1 +

1
2

1
1− e−jω0z−1 , ROC: |z| > 1

After some simple algebraic manipulations we obtain the desired result, namely,

(cosω0n)u(n)
z←→ 1− z

−1 cosω0
1− 2z−1 cosω0 + z−2 , ROC: |z| > 1

(b) From Euler’s identity,

x(n) = (sinω0n)u(n) = 1
2j

[ejω0nu(n)− e−jω0nu(n)]

Thus

X(z) = 1
2j

(
1

1− ejω0z−1 −
1

1− e−jω0z−1
)
, ROC: |z| > 1

and finally,

(sinω0n)u(n)
z←→ z

−1 sinω0
1− 2z−1 cosω0 + z−2 , ROC: |z| > 1

Time shifting. If

x(n)
z←→ X(z)

then
x(n− k) z←→ z−kX(z)

The ROC of z−kX(z) is the same as that of X(z) except for z = 0 if k > 0 and z = ∞
if k < 0. The proof of this property follows immediately from the definition of the

The properties of linearity and time shifting are the key features that make the
z-transform extremely useful for the analysis of discrete-time LTI systems.

By applying the time-shifting property, determine the z-transform of the signals x2(n) and
3 1

The z-Transform and Its Application to the Analysis of LTI Systems

(2.3)

(2.4)

(2.5)

| = 1) in (2.2), we obtain

z-transform given in (1.1)

EXAMPLE 2.3

x (n) in Example 1.1 from the z-transform of x (n).

163



Solution. It can easily be seen that

x2(n) = x1(n+ 2)

and
x3(n) = x1(n− 2)

X2(z) = z2X1(z) = z2 + 2z+ 5+ 7z−1 + z−3

and
X3(z) = z−2X1(z) = z−2 + 2z−3 + 5z−4 + 7z−5 + z−7

Note that because of the multiplication by z2 , the ROC of X2(z) does not include the point
z = ∞, even if it is contained in the ROC of X1(z).

shifting property. Indeed, if we recall that the coefficient of z−n is the sample
value at time n, it is immediately seen that delaying a signal by k(k > 0) samples
[i.e., x(n) → x(n − k)] corresponds to multiplying all terms of the z-transform by
z−k . The coefficient of z−n becomes the coefficient of z−(n+k) .

Determine the transform of the signal

x(n) =
{

1, 0 ≤ n ≤ N − 1
0, elsewhere

Solution.
Indeed,

X(z) =
N−1∑
n=0

1 · z−n = 1+ z−1 + · · · + z−(N−1) =
{
N, if z = 1
1− z−N
1− z−1 , if z �= 1

Since x(n) has finite duration, its ROC is the entire z-plane, except z = 0.
Let us also derive this transform by using the linearity and time-shifting properties. Note

that x(n) can be expressed in terms of two unit step signals

x(n) = u(n)− u(n−N)

X(z) = Z{u(n)} − Z{u(n−N)} = (1− z−N)Z{u(n)}

Z{u(n)} = 1
1− z−1 , ROC: |z| > 1

The z-Transform and Its Application to the Analysis of LTI Systems

(2.6)

We can determine the z-transform of this signal by using the definition (1.1).

(2.7)

(2.8)

Thus from (2.5) we obtain

Example 2 .3 provides additional insight in understanding the meaning of the

EXAMPLE 2.4

By using (2.1) and (2.5) we have

However, from (1.8) we have

which, when combined with (2.8), leads to (2.7).

164



combination of several z-transforms. If the linear combination of several signals
has finite duration, the ROC of its z-transform is exclusively dictated by the finite-
duration nature of this signal, not by the ROC of the individual transforms.

Scaling in the z-domain. If

x(n)
z←→ X(z), ROC: r1 < |z| < r2

then
anx(n)

z←→ X(a−1z), ROC: |a|r1 < |z| < |a|r2
for any constant a , real or complex.

Proof

Z{anx(n)} =
∞∑

n=−∞
anx(n)z−n =

∞∑
n=−∞

x(n)(a−1z)−n

= X(a−1z)

Since the ROC of X(z) is r1 < |z| < r2 , the ROC of X(a−1z) is

r1 < |a−1z| < r2
or

|a|r1 < |z| < |a|r2
To better understand the meaning and implications of the scaling property, we

express a and z in polar form as a = r0ejω0 , z = rejω , and we introduce a new
complex variable w = a−1z. Thus Z{x(n)} = X(z) and Z{anx(n)} = X(w). It can
easily be seen that

w = a−1z =
(

1
r0
r

)
ej (ω−ω0)

This change of variables results in either shrinking (if r0 > 1) or expanding (if
r0 < 1) the z-plane in combination with a rotation (if ω0 �= 2kπ ) of the z-plane (see

This explains why we have a change in the ROC of the new transform
where |a| < 1. The case |a| = 1, that is, a = ejω0 is of special interest because it
corresponds only to rotation of the z-plane.

Mapping of the z-plane
to the w-plane via the
transformation ω = a−1z,
a = r0ejω0 .

ω

z

r

Re(z)

Im(z)
z-plane

0

ω − ω0

w

Re(w)

Im(w)
w-plane

0

ω = a−1z

The z-Transform and Its Application to the Analysis of LTI Systems

(2.9)

Example 2.4 helps to clarify a very important issue regarding the ROC of the

From the definition (1.1)

Fig. 2.1).

Figure 2.1

165



Determine the z-transforms of the signals

(a) x(n) = an(cosω0n)u(n)
(b) x(n) = an(sinω0n)u(n)

Solution.
(a)

an(cosω0n)u(n)
z←→ 1− az

−1 cosω0
1− 2az−1 cosω0 + a2z−2 , |z| > |a|

(b)

an(sinω0n)u(n)
z←→ az

−1 sinω0
1− 2az−1 cosω0 + a2z−2 , |z| > |a|

Time reversal. If

x(n)
z←→ X(z), ROC: r1 < |z| < r2

then

x(−n) z←→ X(z−1), ROC: 1
r2
< |z| < 1

r1

Proof

Z{x(−n)} =
∞∑

n=−∞
x(−n)z−n =

∞∑
l=−∞

x(l)(z−1)−l = X(z−1)

where the change of variable l = −n is made. The ROC of X(z−1) is

r1 < |z−1| < r2 or equivalently 1
r2
< |z| < 1

r1

Note that the ROC for x(n) is the inverse of that for x(−n). This means that if z0
belongs to the ROC of x(n), then 1/z0 is in the ROC for x(−n).

ficient of z−n becomes the coefficient of zn . Thus, folding a signal is equivalent to
replacing z by z−1 in the z-transform formula. In other words, reflection in the time
domain corresponds to inversion in the z-domain.

Determine the z-transform of the signal

x(n) = u(−n)

The z-Transform and Its Application to the Analysis of LTI Systems

(2.10)

(2.11)

(2.12)

EXAMPLE 2.5

From (2.3) and (2.9) we easily obtain

Similarly, (2.4) and (2.9) yield

From the definition (1.1), we have

An intuitive proof of (2.12) is the following. When we fold a signal, the coef-

EXAMPLE 2.6

166



Solution.

u(n)
z←→ 1

1− z−1 , ROC: |z| > 1

u(−n) z←→ 1
1− z , ROC: |z| < 1

Differentiation in the z-domain. If

x(n)
z←→ X(z)

then

nx(n)
z←→−zdX(z)

dz

Proof

dX(z)

dz
=

∞∑
n=−∞

x(n)(−n)z−n−1 = −z−1
∞∑

n=−∞
[nx(n)]z−n

= −z−1Z{nx(n)}

Note that both transforms have the same ROC.

Determine the z-transform of the signal

x(n) = nanu(n)
Solution. 1 1 n

we have that

x1(n) = anu(n) z←→ X1(z) = 11− az−1 , ROC: |z| > |a|

nanu(n)
z←→ X(z) = −z dX1(z)

dz
= az

−1

(1− az−1)2 , ROC: |z| > |a|

nu(n)
z←→ z

−1

(1− z−1)2 , ROC: |z| > 1

The z-Transform and Its Application to the Analysis of LTI Systems

(2.13)

(2.14)

(2.15)

(2.16)

It is known from (1.8) that

By using (2.12), we easily obtain

By differentiating both sides of (1.1), we have

EXAMPLE 2.7

The signal x(n) can be expressed as nx (n), where x (n) = a u(n). From (2.2)

Thus, by using (2.14), we obtain

If we set a = 1 in (2.15), we find the z-transform of the unit ramp signal

167



Determine the signal x(n) whose z-transform is given by

X(z) = log(1+ az−1), |z| > |a|
Solution. By taking the first derivative of X(z), we obtain

dX(z)

dz
= −az

−2

1+ az−1
Thus

−z dX(z)
dz

= az−1
[

1
1− (−a)z−1

]
, |z| > |a|

The inverse z-transform of the term in brackets is (−a)n . The multiplication by z−1 implies a
time delay by one sample (time-shifting property), which results in (−a)n−1u(n− 1). Finally,
from the differentiation property we have

nx(n) = a(−a)n−1u(n− 1)
or

x(n) = (−1)n+1 a
n

n
u(n− 1)

Convolution of two sequences. If

x1(n)
z←→ X1(z)

x2(n)
z←→ X2(z)

then
x(n) = x1(n) ∗ x2(n) z←→ X(z) = X1(z)X2(z)

The ROC of X(z) is, at least, the intersection of that for X1(z) and X2(z).
Proof The convolution of x1(n) and x2(n) is defined as

x(n) =
∞∑

k=−∞
x1(k)x2(n− k)

The z-transform of x(n) is

X(z) =
∞∑

n=−∞
x(n)z−n =

∞∑
n=−∞

[ ∞∑
k=−∞

x1(k)x2(n− k)
]
z−n

Upon interchanging the order of the summations and applying the time-shifting

X(z) =
∞∑

k=−∞
x1(k)

[ ∞∑
n=−∞

x2(n− k)z−n
]

= X2(z)
∞∑

k=−∞
x1(k)z

−k = X2(z)X1(z)

The z-Transform and Its Application to the Analysis of LTI Systems

(2.17)

EXAMPLE 2.8

property in (2.5), we obtain

168



Compute the convolution x(n) of the signals

x1(n) = {1,−2, 1}

x2(n) =
{

1, 0 ≤ n ≤ 5
0, elsewhere

Solution.

X1(z) = 1− 2z−1 + z−2

X2(z) = 1+ z−1 + z−2 + z−3 + z−4 + z−5

1 2

X(z) = X1(z)X2(z) = 1− z−1 − z−6 + z−7

Hence
x(n) = {1

↑
,−1, 0, 0, 0, 0,−1, 1}

The same result can also be obtained by noting that

X1(z) = (1− z−1)2

X2(z) = 1− z
−6

1− z−1
Then

X(z) = (1− z−1)(1− z−6) = 1− z−1 − z−6 + z−7

The reader is encouraged to obtain the same result explicitly by using the convolution sum-
mation formula (time-domain approach).

The convolution property is one of the most powerful properties of the z-trans-
form because it converts the convolution of two signals (time domain) to multipli-
cation of their transforms. Computation of the convolution of two signals, using the
z-transform, requires the following steps:

1. Compute the z-transforms of the signals to be convolved.

X1(z) = Z{x1(n)}
(time domain −→ z-domain)

X2(z) = Z{x2(n)}
2. Multiply the two z-transforms.

X(z) = X1(z)X2(z), (z-domain)
3. Find the inverse z-transform of X(z).

x(n) = Z−1{X(z)}, (z-domain −→ time domain)

The z-Transform and Its Application to the Analysis of LTI Systems

EXAMPLE 2.9

From (1.1), we have

According to (2.17), we carry out the multiplication of X (z) and X (z). Thus

169



This procedure is, in many cases, computationally easier than the direct evaluation
of the convolution summation.

Correlation of two sequences. If

x1(n)
z←→ X1(z)

x2(n)
z←→ X2(z)

then

rx1x2(l) =
∞∑

n=−∞
x1(n)x2(n− l) z←→ Rx1x2(z) = X1(z)X2(z−1)

Proof We recall that
rx1x2(l) = x1(l) ∗ x2(−l)

Using the convolution and time-reversal properties, we easily obtain

Rx1x2(z) = Z{x1(l)}Z{x2(−l)} = X1(z)X2(z−1)

The ROC of Rx1x2(z) is at least the intersection of that for X1(z) and X2(z
−1).

As in the case of convolution, the crosscorrelation of two signals is more easily

ing the result.

Determine the autocorrelation sequence of the signal

x(n) = anu(n), −1 < a < 1

Solution.
gives

Rxx(z) = Z{rxx(l)} = X(z)X(z−1)

X(z) = 1
1− az−1 , ROC: |z| > |a| (causal signal)

X(z−1) = 1
1− az , ROC: |z| <

1
|a| (anticausal signal)

Thus

Rxx(z) = 11− az−1
1

1− az =
1

1− a(z+ z−1)+ a2 , ROC: |a| < |z| <
1
|a|

The z-Transform and Its Application to the Analysis of LTI Systems

(2.18)

done via polynomial multiplication according to (2.18) and then inverse transform-

EXAMPLE 2.10

Since the autocorrelation sequence of a signal is its correlation with itself, (2.18)

From (2.2) we have

and by using (2.15), we obtain

170



Since the ROC of Rxx(z) is a ring, rxx(l) is a two-sided signal, even if x(n) is causal.
To obtain rxx

b = 1/a is simply (1− a2)Rxx(z). Hence it follows that

rxx(l) = 11− a2 a
|l|, −∞ < l <∞

.

Multiplication of two sequences. If

x1(n)
z←→ X1(z)

x2(n)
z←→ X2(z)

then

x(n) = x1(n)x2(n) z←→ X(z) = 12πj
∮

Ĉ
X1(v)X2

( z
v

)
v−1 dv

where C is a closed contour that encloses the origin and lies within the region of
convergence common to both X1(v) and X2(1/v).

Proof The z-transform of x3(n) is

X(z) =
∞∑

n=−∞
x(n)z−n =

∞∑
n=−∞

x1(n)x2(n)z
−n

Let us substitute the inverse transform

x1(n) = 12πj
∮

Ĉ
X1(v)v

n−1dv

for x1(n) in the z-transform X(z) and interchange the order of summation and inte-
gration. Thus we obtain

X(z) = 1
2πj

∮
Ĉ
X1(v)

[ ∞∑
n=−∞

x2(n)
( z
v

)−n]
v−1dv

The sum in the brackets is simply the transform X2(z) evaluated at z/v . Therefore,

X(z) = 1
2πj

∮
Ĉ
X1(v)X2

( z
v

)
v−1 dv

which is the desired result.

The z-Transform and Its Application to the Analysis of LTI Systems

The reader is encouraged to compare this approach with a time-domain solution

(2.19)

(l) , we observe that the z-transform of the sequence in Example 1.5 with

171



To obtain the ROC of X(z) we note that if X1(v) converges for r1l < |v| < r1u
and X2(z) converges for r2l < |z| < r2u , then the ROC of X2(z/v) is

r2l <

∣∣∣ z
v

∣∣∣ < r2u
Hence the ROC for X(z) is at least

r1l r2l < |z| < r1ur2u

Although this property will not be used immediately, it will prove useful later,
especially in our treatment of filter design based on the window technique, where we
multiply the impulse response of an IIR system by a finite-duration “window” which
serves to truncate the impulse response of the IIR system.

For complex-valued sequences x1(n) and x2(n) we can define the product se-
quence as x(n) = x1(n)x∗2 (n). Then the corresponding complex convolution integral
becomes

x(n) = x1(n)x∗2 (n)
z←→ X(z) = 1

2πj

∮
Ĉ
X1(v)X

∗
2

(
z∗

v∗

)
v−1 dv

Parseval’s relation. If x1(n) and x2(n) are complex-valued sequences, then

∞∑
n=−∞

x1(n)x
∗
2 (n) =

1
2πj

∮
Ĉ
X1(v)X

∗
2

(
1
v∗

)
v−1 dv

provided that r1l r2l < 1 < r1ur2u , where r1l < |z| < r1u and r2l < |z| < r2u are the
1 2

The Initial Value Theorem. If x(n) is causal [i.e., x(n) = 0 for n < 0], then

x(0) = lim
z→∞X(z)

Proof

X(z) =
∞∑
n=0

x(n)z−n = x(0)+ x(1)z−1 + x(2)z−2 + · · ·

Obviously, as z→∞, z−n

The z-Transform and Its Application to the Analysis of LTI Systems

(2.20)

(2.21)

(2.22)

(2.23)

The proof of (2.21) is left as an exercise for the reader.

ROC of X (z) and X (z) . The proof of (2.22) follows immediately by evaluating
X(z) in (2.21) at z = 1.

Since x(n) is causal, (1.1) gives

→ 0 since n > 0, and (2.23) follows.

172



Properties of the z-Transform
Property Time Domain z-Domain ROC
Notation x(n) X(z) ROC: r2 < |z| < r1

x1(n) X1(z) ROC1

x2(n) X2(z) ROC2

Linearity a1x1(n)+
a2x2(n)

a1X1(z)+ a2X2(z) At least the intersection of
ROC1 and ROC2

Time shifting x(n− k) z−kX(z) That of X(z), except z = 0 if
k > 0 and z = ∞ if k < 0

Scaling in the
z-domain

anx(n) X(a−1z) |a|r2 < |z| < |a|r1

Time reversal x(−n) X(z−1) 1
r1
< |z| < 1

r2

Conjugation x∗(n) X∗(z∗) ROC

Real part Re{x(n)} 12 [X(z)+X∗(z∗)] Includes ROC
Imaginary part Im{x(n)} 12 j [X(z)−X∗(z∗)] Includes ROC

Differentiation in
the z-domain

nx(n) −z dX(z)
dz

r2 < |z| < r1

Convolution x1(n) ∗ x2(n) X1(z)X2(z) At least, the intersection of
ROC1 and ROC2

Correlation rx1x2 (l) =
x1(l) ∗ x2(−l)

Rx1x2 (z) = X1(z)X2(z−1) At least, the intersection of
ROC of X1(z) and X2(z−1)

Initial value
theorem

If x(n) causal x(0) = lim
z→∞X(z)

Multiplication x1(n)x2(n) 12πj

∮
Ĉ
X1(v)X2

(
z
v

)
v−1 dv At least, r1l r2l < |z| < r1ur2u

Parseval’s relation
∞∑

n=−∞
x1(n)x

∗
2 (n) = 12πj

∮
Ĉ
X1(v)X

∗
2(1/v

∗)v−1dv

All the properties of the z-transform presented in this section are summarized

introduced in the text. The conjugation properties and Parseval’s relation are left as
exercises for the reader.

We have now derived most of the z-transforms that are encountered in many

reference. A simple inspection of this table shows that these z-transforms are all
rational functions (i.e., ratios of polynomials in z−1 ). As will soon become apparent,
rational z-transforms are encountered not only as the z-transforms of various im-
portant signals but also in the characterization of discrete-time linear time-invariant
systems described by constant-coefficient difference equations.

The z-Transform and Its Application to the Analysis of LTI Systems

TABLE 2

in Table 2 for easy reference. They are listed in the same order as they have been

practical applications. These z-transform pairs are summarized in Table 3 for easy

173



Some Common z-Transform Pairs
Signal, x(n) z-Transform, X(z) ROC

1 δ(n) 1 All z

2 u(n) 1
1− z−1 |z| > 1

3 anu(n) 1
1− az−1 |z| > |a|

4 nanu(n) az
−1

(1− az−1)2 |z| > |a|

5 −anu(−n− 1) 1
1− az−1 |z| < |a|

6 −nanu(−n− 1) az−1
(1− az−1)2 |z| < |a|

7 (cosω0n)u(n)
1− z−1 cosω0

1− 2z−1 cosω0 + z−2
|z| > 1

8 (sinω0n)u(n)
z−1 sinω0

1− 2z−1 cosω0 + z−2
|z| > 1

9 (an cosω0n)u(n)
1− az−1 cosω0

1− 2az−1 cosω0 + a2z−2
|z| > |a|

10 (an sinω0n)u(n)
az−1 sinω0

1− 2az−1 cosω0 + a2z−2
|z| > |a|

3 Rational z-Transforms

X(z) is a rational function, that is, a ratio of two polynomials in z−1 (or z). In
this section we discuss some very important issues regarding the class of rational
z-transforms.

Poles and Zeros

The zeros of a z-transform X(z) are the values of z for which X(z) = 0. The poles of
a z-transform are the values of z for which X(z) = ∞. If X(z) is a rational function,
then

X(z) = B(z)
A(z)

= b0 + b1z
−1 + · · · + bMz−M

a0 + a1z−1 + · · · + aNz−N =
∑M

k=0 bkz
−k∑N

k=0 akz−k

If a0 �= 0 and b0 �= 0, we can avoid the negative powers of z by factoring out the
terms b0z−M and a0z−N as follows:

X(z) = B(z)
A(z)

= b0z
−M

a0z−N
zM + (b1/b0)zM−1 + · · · + bM/b0
zN + (a1/a0)zN−1 + · · · + aN/a0

The z-Transform and Its Application to the Analysis of LTI Systems

(3.1)

TABLE 3

As indicated in Section 2, an important family of z-transforms are those for which

3.1

174



Since B(z) and A(z) are polynomials in z, they can be expressed in factored form as

X(z) = B(z)
A(z)

= b0
a0
z−M+N

(z− z1)(z− z2) · · · (z− zM)
(z− p1)(z− p2) · · · (z− pN)

X(z) = GzN−M

M∏
k=1
(z− zk)

N∏
k=1
(z− pk)

where G ≡ b0/a0 . Thus X(z) has M finite zeros at z = z1 , z2, . . . , zM (the roots of
the numerator polynomial), N finite poles at z = p1 , p2, . . . , pN (the roots of the
denominator polynomial), and |N −M| zeros (if N > M ) or poles (if N < M ) at
the origin z = 0. Poles or zeros may also occur at z = ∞. A zero exists at z = ∞ if
X(∞) = 0 and a pole exists at z = ∞ if X(∞) = ∞. If we count the poles and zeros
at zero and infinity, we find that X(z) has exactly the same number of poles as zeros.

We can representX(z) graphically by a pole–zero plot (or pattern) in the complex
plane, which shows the location of poles by crosses (×) and the location of zeros by
circles (◦). The multiplicity of multiple-order poles or zeros is indicated by a number
close to the corresponding cross or circle. Obviously, by definition, the ROC of a
z-transform should not contain any poles.

Determine the pole–zero plot for the signal

x(n) = anu(n), a > 0
Solution.

X(z) = 1
1− az−1 =

z

z− a , ROC: |z| > a

Thus X(z) has one zero at z1 = 0 and one pole at p1 = a . The pole–zero plot is shown in
1 = a is not included in the ROC since the z-transform does

not converge at a pole.

Pole–zero plot for the
causal exponential signal
x(n) = anu(n).

Im(z)

Re(z)

ROC

0
a

The z-Transform and Its Application to the Analysis of LTI Systems

(3.2)

From Table 3 we find that

Fig. 1. Note that the pole p

EXAMPLE 3.1

Figure 3.1

175



Determine the pole–zero plot for the signal

x(n) =
{
an, 0 ≤ n ≤ M − 1
0, elsewhere

where a > 0.

Solution.

X(z) =
M−1∑
n=0

(az−1)n = 1− (az
−1)M

1− az−1 =
zM − aM
zM−1(z− a)

Since a > 0, the equation zM = aM has M roots at

zk = aej2πk/M k = 0, 1, . . . ,M − 1

The zero z0 = a cancels the pole at z = a . Thus

X(z) = (z− z1)(z− z2) · · · (z− zM−1)
zM−1

ROC is the entire z-plane except z = 0 because of the M − 1 poles located at the origin.

Pole–zero pattern for
the finite-duration
signal x(n) = an ,
0 ≤ n ≤ M − 1(a > 0), for
M = 8.

Im(z)

Re(z)

|z| = a
M −1 poles

The z-Transform and Its Application to the Analysis of LTI Systems

From the definition (1.1) we obtain

which has M − 1 zeros and M − 1 poles, located as shown in Fig3.2 for M = 8. Note that the

Figure 3.2

EXAMPLE 3.2

176



Pole-zero pattern for

Im(z)

Re(z)

p1

ROC

p2

z2z1

r

ω0
ω0

to within a scaling factor G. This is illustrated in the following example.

Solution. There are two zeros (M = 2) at z1 = 0, z2 = r cosω0 and two poles (N = 2) at
p1 = rejω0 , p2 = re−jω0

X(z) = G (z− z1)(z− z2)
(z− p1)(z− p2) = G

z(z− r cosω0)
(z− rejω0 )(z− re−jω0 ), ROC: |z| > r

After some simple algebraic manipulations, we obtain

X(z) = G 1− rz
−1 cosω0

1− 2rz−1 cosω0 + r2z−2 , ROC: |z| > r

x(n) = G(rn cosω0n)u(n)

1 2
nomial with real coefficients, when p1 and p2 are complex conjugates. In general,
if a polynomial has real coefficients, its roots are either real or occur in complex-
conjugate pairs.

As we have seen, the z-transform X(z) is a complex function of the complex
variable z = �(z) + j�(z). Obviously, |X(z)|, the magnitude of X(z), is a real and
positive function of z. Since z represents a point in the complex plane, |X(z)| is a

for the z-transform

X(z) = z
−1 − z−2

1− 1.2732z−1 + 0.81z−2
which has one zero at z1 = 1 and two poles at p1 , p2 = 0.9e±jπ/4 . Note the high
peaks near the singularities (poles) and the deep valley close to the zero.

The z-Transform and Its Application to the Analysis of LTI Systems

(3.3)

Figure 3.3

Example 3.3.

Clearly, if we are given a pole–zero plot, we can determine X(z), by using (3.2),

EXAMPLE 3.3

Determine the z-transform and the signal that corresponds to the pole–zero plot of Fig. 3.3.

. By substitution of these relations into (3.2), we obtain

From Table 3 we find that

From Example 3.3, we see that the product (z− p )(z − p ) results in a poly-

two-dimensional function and describes a “surface.” This is illustrated in Fig. 3.4

177



Pole Location and Time-Domain Behavior for Causal Signals

and the form (shape) of the corresponding signal in the time domain. The discussion

results in the preceding subsection. We deal exclusively with real, causal signals.
In particular, we see that the characteristic behavior of causal signals depends on
whether the poles of the transform are contained in the region |z| < 1, or in the

called the unit circle.
If a real signal has a z-transform with one pole, this pole has to be real. The only

such signal is the real exponential

x(n) = anu(n) z←→ X(z) = 1
1− az−1 , ROC: |z| > |a|

having one zero at z1 = 0 and one pole at p1 = a on the real axis.
illustrates the behavior of the signal with respect to the location of the pole relative
to the unit circle. The signal is decaying if the pole is inside the unit circle, fixed if
the pole is on the unit circle, and growing if the pole is outside the unit circle. In
addition, a negative pole results in a signal that alternates in sign. Obviously, causal
signals with poles outside the unit circle become unbounded, cause overflow in digital
systems, and in general, should be avoided.

A causal real signal with a double real pole has the form

x(n) = nanu(n)

Note that in contrast to
the single-pole signal, a double real pole on the unit circle results in an unbounded
signal.

The z-Transform and Its Application to the Analysis of LTI Systems

Figure 3.4 Graph of |X(z)| for the z-transform in (3.3).

3.2

In this subsection we consider the relation between the z-plane location of a pole pair

is based generally on the collection of z-transform pairs given in Table 3 and the

region |z| > 1, or on the circle |z| = 1. Since the circle |z| = 1 has a radius of 1, it is

Figure 3.5

(see Table 3) and its behavior is illustrated in Fig. 3.6.

178



0 1

z-plane

0 n n

…

x(n)

0 1

z-plane

0 n

…

x(n)

0 1

z-plane

0 n

…

x(n)

0 1

z-plane

0 n

…

x(n)

0 1

z-plane

0 n

…

x(n)

0 1

z-plane

0

…

x(n)

Time-domain behavior of a single-real-pole causal signal as a function of
the location of the pole with respect to the unit circle.

0 1

z-plane

0 n n

…

x(n)

0 1

z-plane

0 n

…

x(n)

0 1

z-plane

n

…

0

x(n)

0 1

z-plane

n

…
x(n)

0 1

z-plane

0

0 n

…

0 1

z-plane

0

…

x(n)

x(n)

m = 2

m = 2

m = 2

m = 2

m = 2

m = 2

Time-domain behavior of causal signals corresponding to a double (m = 2)
real pole, as a function of the pole location.

The z-Transform and Its Application to the Analysis of LTI Systems

Figure 3.5

Figure 3.6

179



z-plane

0 1 0 n

r

ωo

x(n)

rn

rn

z-plane

0 1

1

0 n

ωo

x(n) r = 1

z-plane

0 1 0 n

 ωo

x(n)

r

A pair of complex-conjugate poles corresponds to causal
signals with oscillatory behavior.

signal. The distance r of the poles from the origin determines the envelope of the
sinusoidal signal and their angle with the real positive axis, its relative frequency. Note
that the amplitude of the signal is growing if r > 1, constant if r = 1 (sinusoidal
signals), and decaying if r < 1.

poles on the unit circle.
illustrates that multiple poles on the unit circle should be treated with great care.

To summarize, causal real signals with simple real poles or simple complex-
conjugate pairs of poles, which are inside or on the unit circle, are always bounded in
amplitude. Furthermore, a signal with a pole (or a complex-conjugate pair of poles)

The z-Transform and Its Application to the Analysis of LTI Systems

Figure 3.7

Figure 3.7 illustrates the case of a pair of complex-conjugate poles. According
to Table 3, this configuration of poles results in an exponentially weighted sinusoidal

Finally, Fig. 3.8 shows the behavior of a causal signal with a double pair of
This reinforces the corresponding results in Fig. 3.6 and

180



z-plane

0 1 0 n

ω0

x(n)

m = 2

m = 2

Causal signal corresponding to a double pair of
complex-conjugate poles on the unit circle.

near the origin decays more rapidly than one associated with a pole near (but inside)
the unit circle. Thus the time behavior of a signal depends strongly on the location
of its poles relative to the unit circle. Zeros also affect the behavior of a signal but
not as strongly as poles. For example, in the case of sinusoidal signals, the presence
and location of zeros affects only their phase.

At this point, it should be stressed that everything we have said about causal
signals applies as well to causal LTI systems, since their impulse response is a causal
signal. Hence if a pole of a system is outside the unit circle, the impulse response of
the system becomes unbounded and, consequently, the system is unstable.

3.3 The System Function of a Linear Time-Invariant System

x(n)x(n)

Y (z) = H(z)X(z)

where Y (z) is the z-transform of the output sequence y(n), X(z) is the z-transform
of the input sequence x(n) and H(z) is the z-transform of the unit sample response
h(n).

If we know h(n) and x(n), we can determine their corresponding z-transforms
H(z) and X(z), multiply them to obtain Y (z), and therefore determine y(n) by
evaluating the inverse z-transform of Y (z). Alternatively, if we know x(n) and we
observe the output y(n) of the system, we can determine the unit sample response
by first solving for H(z) from the relation

H(z) = Y (z)
X(z)

and then evaluating the inverse z-transform of H(z).

The z-Transform and Its Application to the Analysis of LTI Systems

(3.4)

(3.5)

Figure 3.8

z

Recall that the output of a (relaxed) linear time-invariant system to an input se-
quence  can be obtained by computing the convolution of with the unit 
sample response of the system. The convolution property, derived in Section 2, 
allows us to express this relationship in the -domain as

181



Since

H(z) =
∞∑

n=−∞
h(n)z−n

it is clear that H(z) represents the z-domain characterization of a system, whereas
h(n) is the corresponding time-domain characterization of the system. In other
words, H(z) and h(n) are equivalent descriptions of a system in the two domains.
The transform H(z) is called the system function.

is described by a linear constant-coefficient difference equation of the form

y(n) = −
N∑
k=1

aky(n− k)+
M∑
k=0

bkx(n− k)

we obtain

Y (z) = −
N∑
k=1

akY (z)z
−k +

M∑
k=0

bkX(z)z
−k

Y (z)

(
1+

N∑
k=1

akz
−k

)
= X(z)

(
M∑
k=0

bkz
−k

)

Y (z)

X(z)
= H(z) =

M∑
k=0

bkz
−k

1+
N∑
k=1

akz
−k

Therefore, a linear time-invariant system described by a constant-coefficient differ-
ence equation has a rational system function.

This is the general form for the system function of a system described by a
linear constant-coefficient difference equation. From this general form we obtain
two important special forms. First, if ak

H(z) =
M∑
k=0

bkz
−k = 1

zM

M∑
k=0

bkz
M−k

In this case, H(z) contains M zeros, whose values are determined by the system
parameters {bk}, and an M th-order pole at the origin z = 0. Since the system
contains only trivial poles (at z = 0) and M nontrivial zeros, it is called an all-zero
system. Clearly, such a system has a finite-duration impulse response (FIR), and it is
called an FIR system or a moving average (MA) system.

The z-Transform and Its Application to the Analysis of LTI Systems

(3.6)

(3.7)

(3.8)

(3.9)

The relation in (3.5) is particularly useful in obtaining H(z) when the system

In this case the system function can be determined directly from (3.7) by computing
the z-transform of both sides of (3.7). Thus, by applying the time-shifting property,

= 0 for 1 ≤ k ≤ N , (3.8) reduces to

182



On the other hand, if bk = 0 for 1 ≤ k ≤ M , the system function reduces to

H(z) = b0
1+∑Nk=1 akz−k =

b0z
N∑N

k=0 akzN−k
, a0 ≡ 1

In this case H(z) consists of N poles, whose values are determined by the system
parameters {ak} and an N th-order zero at the origin z = 0. We usually do not make

only nontrivial poles and the corresponding system is called an all-pole system. Due
to the presence of poles, the impulse response of such a system is infinite in duration,
and hence it is an IIR system.

zeros, and hence the corresponding system is called a pole–zero system, with N poles
and M zeros. Poles and/or zeros at z = 0 and z = ∞ are implied but are not counted
explicitly. Due to the presence of poles, a pole–zero system is an IIR system.

The following example illustrates the procedure for determining the system func-
tion and the unit sample response from the difference equation.

Determine the system function and the unit sample response of the system described by the
difference equation

y(n) = 1
2
y(n− 1)+ 2x(n)

Solution. By computing the z-transform of the difference equation, we obtain

Y (z) = 1
2
z−1Y (z)+ 2X(z)

Hence the system function is

H(z) = Y (z)
X(z)

= 2
1− 12 z−1

This system has a pole at z = 12
transform

h(n) = 2(1
2
)nu(n)

This is the unit sample response of the system.

We have now demonstrated that rational z-transforms are encountered in com-
monly used systems and in the characterization of linear time-invariant systems. In

rational functions.

The z-Transform and Its Application to the Analysis of LTI Systems

(3.10)

reference to these trivial zeros. Consequently, the system function in (3.10) contains

The general form of the system function given by (3.8) contains both poles and

EXAMPLE 3.4

and a zero at the origin. Using Table 3 we obtain the inverse

Section 4 we describe several methods for determining the inverse z-transform of

183



4 Inversion of the z-Transform

x(n) = 1
2πj

∮
Ĉ
X(z)zn−1dz

where the integral is a contour integral over a closed path C that encloses the origin
and lies within the region of convergence of X(z). For simplicity, C can be taken as
a circle in the ROC of X(z) in the z-plane.

There are three methods that are often used for the evaluation of the inverse
z-transform in practice:

1.

2. Expansion into a series of terms, in the variables z, and z−1 .

3. Partial-fraction expansion and table lookup.

The Inverse z-Transform by Contour Integration

In this section we demonstrate the use of the Cauchy’s integral theorem to determine
the inverse z-transform directly from the contour integral.

Cauchy’s integral theorem. Let f (z) be a function of the complex variable z and
C be a closed path in the z-plane. If the derivative df (z)/dz exists on and inside the
contour C and if f (z) has no poles at z = z0 , then

1
2πj

∮
Ĉ

f (z)

z− z0 dz =
{
f (z0), if z0 is inside C
0, if z0 is outside C

More generally, if the (k + 1)-order derivative of f (z) exists and f (z) has no poles
at z = z0 , then

1
2πj

∮
Ĉ

f (z)

(z− z0)k dz =



1
(k − 1)!

dk−1f (z)
dzk−1

∣∣∣∣
z=z0

, if z0 is inside C

0, if z0 is outside C

0
integral theorem.

integrals. To be specific, suppose that the integrand of the contour integral is a

The z-Transform and Its Application to the Analysis of LTI Systems

(4.2)

(4.3)

(4.1)

As we saw in Section 1.2, the inverse z-transform is formally given by

Direct evaluation of (4.1), by contour integration.

4.1

The values on the right-hand side of (4.2) and (4.3) are called the residues of
the pole at z = z . The results in (4.2) and (4.3) are two forms of the Cauchy’s

We can apply (4.2) and (4.3) to obtain the values of more general contour

184



proper fraction f (z)/g(z), where f (z) has no poles inside the contour C and g(z) is
a polynomial with distinct (simple) roots z1, z2, . . . , zn inside C . Then

1
2πj

∮
Ĉ

f (z)

g(z)
dz = 1

2πj

∮
Ĉ

[
n∑
i=1

Ai

z− zi

]
dz

=
n∑
i=1

1
2πj

∮
Ĉ

Ai

z− zi dz

=
n∑
i=1

Ai

where

Ai = (z− zi) f (z)
g(z)

∣∣∣∣
z=zi

The values {Ai} are residues of the corresponding poles at z = zi , i = 1, 2, . . . , n.
Hence the value of the contour integral is equal to the sum of the residues of all the
poles inside the contour C .

When g(z) has multiple-order roots as well
as simple roots inside the contour, the partial-fraction expansion, with appropriate

poles.
In the case of the inverse z-transform, we have

x(n) = 1
2πj

∮
Ĉ
X(z)zn−1 dz

=
∑

all poles {zi } inside C
[residue of X(z)zn−1 at z = zi]

=
∑
i

(z− zi)X(z)zn−1|z=zi

provided that the poles {zi} are simple. If X(z)zn−1 has no poles inside the contour
C for one or more values of n, then x(n) = 0 for these values.

The following example illustrates the evaluation of the inverse z-transform by
use of the Cauchy’s integral theorem.

Evaluate the inverse z-transform of

X(z) = 1
1− az−1 , |z| > |a|

using the complex inversion integral.

The z-Transform and Its Application to the Analysis of LTI Systems

(4.4)

(4.5)

(4.6)

We observe that (4.4) was obtained by performing a partial-fraction expansion
of the integrand and applying (4.2).

modifications, and (4.3) can be used to evaluate the residues at the corresponding

EXAMPLE 4.1

185



Solution. We have

x(n) = 1
2πj

∮
Ĉ

zn−1

1− az−1 dz =
1

2πj

∮
Ĉ

zn dz

z− a

f (z) = zn . We distinguish two cases.
1. If n ≥ 0, f (z) has only zeros and hence no poles inside C . The only pole inside C is

z = a . Hence
x(n) = f (z0) = an, n ≥ 0

2. If n < 0, f (z) = zn has an nth-order pole at z = 0, which is also inside C . Thus there
are contributions from both poles. For n = −1 we have

x(−1) = 1
2πj

∮
Ĉ

1
z(z− a) dz =

1
z− a

∣∣∣∣
z=0
+ 1
z

∣∣∣∣
z=a
= 0

If n = −2, we have

x(−2) = 1
2πj

∮
Ĉ

1
z2(z− a) dz =

d

dz

(
1

z− a
)∣∣∣∣

z=0
+ 1
z2

∣∣∣∣
z=a
= 0

By continuing in the same way we can show that x(n) = 0 for n < 0. Thus
x(n) = anu(n)

4.2 The Inverse z-Transform by Power Series Expansion

The basic idea in this method is the following: Given a z-transform X(z) with its
corresponding ROC, we can expand X(z) into a power series of the form

X(z) =
∞∑

n=−∞
cnz
−n

which converges in the given ROC. Then, by the uniqueness of the z-transform,
x(n) = cn for all n. When X(z) is rational, the expansion can be performed by long
division.

To illustrate this technique, we will invert some z-transforms involving the same
expression for X(z), but different ROC. This will also serve to emphasize again the
importance of the ROC in dealing with z-transforms.

Determine the inverse z-transform of

X(z) = 1
1− 1.5z−1 + 0.5z−2

when
(a) ROC: |z| > 1
(b) ROC: |z| < 0.5

The z-Transform and Its Application to the Analysis of LTI Systems

(4.7)

where C is a circle at radius greater than |a|. We shall evaluate this integral using (4.2) with

EXAMPLE 4.2

186



Solution.
(a) Since the ROC is the exterior of a circle, we expect x(n) to be a causal signal. Thus we

seek a power series expansion in negative powers of z . By dividing the numerator of
X(z) by its denominator, we obtain the power series

X(z) = 1
1− 32 z−1 + 12 z−2

= 1+ 3
2
z−1 + 7

4
z−2 + 15

8
z−3 + 31

16
z−4 + · · ·

x(n) = {1
↑
,

3
2
,

7
4
,

15
8
,

31
16
, . . .}

Note that in each step of the long-division process, we eliminate the lowest-power term
of z−1 .

(b) In this case the ROC is the interior of a circle. Consequently, the signal x(n) is anticausal.
To obtain a power series expansion in positive powers of z , we perform the long division
in the following way:

2z2 + 6z3 + 14z4 + 30z5 + 62z6 + · · ·
1
2z
−2 − 32 z−1 + 1

)
1

1− 3z+ 2z2
3z− 2z2
3z− 9z2 + 6z3

7z2 − 6z3
7z2 − 21z3 + 14z4

15z3 − 14z4
15z3 − 45z4 + 30z5

31z4 − 30z5

Thus

X(z) = 1
1− 32 z−1 + 12 z−2

= 2z2 + 6z3 + 14z4 + 30z5 + 62z6 + · · ·

x(n) = {· · · 62, 30, 14, 6, 2, 0, 0
↑
}

We observe that in each step of the long-division process, the lowest-power term of z is
eliminated. We emphasize that in the case of anticausal signals we simply carry out the
long division by writing down the two polynomials in “reverse” order (i.e., starting with
the most negative term on the left).

From this example we note that, in general, the method of long division will not
provide answers for x(n) when n is large because the long division becomes tedious.
Although the method provides a direct evaluation of x(n), a closed-form solution
is not possible, except if the resulting pattern is simple enough to infer the general
term x(n). Hence this method is used only if one wishes to determine the values of
the first few samples of the signal.

The z-Transform and Its Application to the Analysis of LTI Systems

By comparing this relation with (1.1), we conclude that

In this case x(n) = 0 for n ≥ 0. By comparing this result to (1.1), we conclude that

187



Determine the inverse z-transform of

X(z) = log(1+ az−1), |z| > |a|
Solution. Using the power series expansion for log(1+ x), with |x| < 1, we have

X(z) =
∞∑
n=1

(−1)n+1anz−n
n

Thus

x(n) =
{
(−1)n+1 ann , n ≥ 1
0, n ≤ 0

Expansion of irrational functions into power series can be obtained from tables.

The Inverse z-Transform by Partial-Fraction Expansion

In the table lookup method, we attempt to express the function X(z) as a linear
combination

X(z) = α1X1(z)+ α2X2(z)+ · · · + αKXK(z)
where X1(z), . . . , XK(z) are expressions with inverse transforms x1(n), . . . , xK(n)
available in a table of z-transform pairs. If such a decomposition is possible, then
x(n), the inverse z-transform of X(z), can easily be found using the linearity prop-
erty as

x(n) = α1x1(n)+ α2x2(n)+ · · · + αKxK(n)

loss of generality, we assume that a0

X(z) = B(z)
A(z)

= b0 + b1z
−1 + · · · + bMz−M

1+ a1z−1 + · · · + aNz−N
Note that if a0
and denominator by a0 .

N �= 0 and M < N .

is less than the number of finite poles.
An improper rational function (M ≥ N) can always be written as the sum of

a polynomial and a proper rational function. This procedure is illustrated by the
following example.

Express the improper rational transform

X(z) = 1+ 3z
−1 + 116 z−2 + 13z−3

1+ 56z−1 + 16z−2

in terms of a polynomial and a proper function.

The z-Transform and Its Application to the Analysis of LTI Systems

(4.8)

(4.9)

(4.10)

EXAMPLE 4.3

4.3

This approach is particularly useful ifX(z) is a rational function, as in(3.1). Without
= 1, so that (3.1) can be expressed as

�= 1, we can obtain (4.10) from (3.1) by dividing both numerator

A rational function of the form (4.10) is called proper if a
From (3.2) it follows that this is equivalent to saying that the number of finite zeros

EXAMPLE 4.4

188



Solution. First, we note that we should reduce the numerator so that the terms z−2 and z−3
are eliminated. Thus we should carry out the long division with these two polynomials written
in reverse order. We stop the division when the order of the remainder becomes z−1 . Then
we obtain

X(z) = 1+ 2z−1 +
1
6 z
−1

1+ 56z−1 + 16z−2

In general, any improper rational function (M ≥ N) can be expressed as

X(z) = B(z)
A(z)

= c0 + c1z−1 + · · · + cM−Nz−(M−N) + B1(z)
A(z)

The inverse z-transform of the polynomial can easily be found by inspection. We
focus our attention on the inversion of proper rational transforms, since any improper

the development in two steps. First, we perform a partial fraction expansion of the
proper rational function and then we invert each of the terms.

Let X(z) be a proper rational function, that is,

X(z) = B(z)
A(z)

= b0 + b1z
−1 + · · · + bMz−M

1+ a1z−1 + · · · + aNz−N
where

aN �= 0 and M < N
To simplify our discussion we eliminate negative powers of z by multiplying both the

N . This results in

X(z) = b0z
N + b1zN−1 + · · · + bMzN−M
zN + a1zN−1 + · · · + aN

which contains only positive powers of z. Since N > M , the function

X(z)

z
= b0z

N−1 + b1zN−2 + · · · + bMzN−M−1
zN + a1zN−1 + · · · + aN

is also always proper.

1 2 N
of X(z). We distinguish two cases.

Distinct poles. Suppose that the poles p1, p2, . . . , pN are all different (distinct).
Then we seek an expansion of the form

X(z)

z
= A1
z− p1 +

A2

z − p2 + · · · +
AN

z − pN
The problem is to determine the coefficients A1 , A2, . . . , AN . There are two ways to
solve this problem, as illustrated in the following example.

The z-Transform and Its Application to the Analysis of LTI Systems

(4.11)

(4.12)

(4.13)

(4.14)

(4.15)

function can be transformed into a proper function by using (4.11). We carry out

numerator and denominator of (4.12) by z

Our task in performing a partial-fraction expansion is to express (4.14) or,
equivalently, (4.12) as a sum of simple fractions. For this purpose we first factor the
denominator polynomial in (4.14) into factors that contain the poles p , p , . . . , p

189



Determine the partial-fraction expansion of the proper function

X(z) = 1
1− 1.5z−1 + 0.5z−2

Solution. First we eliminate the negative powers, by multiplying both numerator and de-
nominator by z2 . Thus

X(z) = z
2

z2 − 1.5z+ 0.5
The poles of X(z) are p1 = 1 and p2

X(z)

z
= z
(z− 1)(z− 0.5) =

A1

z− 1 +
A2

z− 0.5

A very simple method to determine A1 and A2 is to multiply the equation by the denominator
term (z− 1)(z − 0.5). Thus we obtain

z = (z− 0.5)A1 + (z− 1)A2

Now if we set z = p1 2

1 = (1− 0.5)A1

Thus we obtain the result A1 = 2. 2 = 0.5, thus
eliminating the term involving A1 , so we have

0.5 = (0.5− 1)A2

and hence A2 = −1. Therefore, the result of the partial-fraction expansion is

X(z)

z
= 2
z− 1 −

1
z− 0.5

The example given above suggests that we can determine the coefficients A1 ,
A2, . . . , AN k
k = 1, 2, . . . , N , and evaluating the resulting expressions at the corresponding pole
positions, p1 , p2, . . . , pN . Thus we have, in general,

(z− pk)X(z)
z

= (z− pk)A1
z − p1 + · · · + Ak + · · · +

(z− pk)AN
z − pN

k

Ak = (z− pk)X(z)
z

∣∣∣∣
z=pk

, k = 1, 2, . . . , N

The z-Transform and Its Application to the Analysis of LTI Systems

(4.16)

(4.17)

(4.18)

(4.19)

(4.20)

(4.21)

EXAMPLE 4.5

= 0.5. Consequently, the expansion of the form (4.15) is

= 1 in (4.18), we eliminate the term involving A . Hence

Next we return to (4.18) and set z = p

, by multiplying both sides of (4.15) by each of the terms (z − p ),

Consequently, with z = p , (4.20) yields the kth coefficient as

190



Determine the partial-fraction expansion of

X(z) = 1+ z
−1

1− z−1 + 0.5z−2

Solution.
denominator by z2. Thus

X(z)

z
= z+ 1
z2 − z+ 0.5

The poles of X(z) are complex conjugates

p1 = 12 + j
1
2

and

p2 = 12 − j
1
2

Since p1 2

X(z)

z
= z+ 1
(z− p1)(z − p2) =

A1

z− p1 +
A2

z− p2

To obtain A1 2

A1 = (z− p1)X(z)
z

∣∣∣∣
z=p1
= z+ 1
z− p2

∣∣∣∣
z=p1
=

1
2 + j 12 + 1

1
2 + j 12 − 12 + j 12

= 1
2
− j 3

2

A2 = (z− p2)X(z)
z

∣∣∣∣
z=p2
= z+ 1
z− p1

∣∣∣∣
z=p2
=

1
2 − j 12 + 1

1
2 − j 12 − 12 − j 12

= 1
2
+ j 3

2

poles. The only constraint is that all poles be distinct. We also note that A2 = A∗1 .
It can be easily seen that this is a consequence of the fact that p2 = p∗1 . In other
words, complex-conjugate poles result in complex-conjugate coefficients in the partial-
fraction expansion. This simple result will prove very useful later in our discussion.

Multiple-order poles. If X(z) has a pole of multiplicity l , that is, it contains in its
k
l

this case a different expansion is needed. First, we investigate the case of a double
pole (i.e., l = 2).

The z-Transform and Its Application to the Analysis of LTI Systems

(4.22)

EXAMPLE 4.6

To eliminate negative powers of z in (4.22), we multiply both numerator and

�= p , we seek an expansion of the form (4.15). Thus

and A , we use the formula (4.21). Thus we obtain

The expansion (4.15) and the formula (4.21) hold for both real and complex

denominator the factor (z − p ) , then the expansion (4.15) is no longer true. In

191



Determine the partial-fraction expansion of

X(z) = 1
(1+ z−1)(1− z−1)2

Solution.

X(z)

z
= z

2

(z + 1)(z − 1)2
X(z) has a simple pole at p1 = −1 and a double pole p2 = p3 = 1. In such a case the
appropriate partial-fraction expansion is

X(z)

z
= z

2

(z+ 1)(z− 1)2 =
A1

z+ 1 +
A2

z− 1 +
A3

(z− 1)2
The problem is to determine the coefficients A1 , A2 , and A3 .

We proceed as in the case of distinct poles. To determine A1 , we multiply both sides of

(z + 1)X(z)
z

= A1 + z+ 1
z− 1A2 +

z+ 1
(z− 1)2A3

which, when evaluated at z = −1, yields

A1 = (z+ 1)X(z)
z

∣∣∣∣
z=−1
= 1

4

2

(z − 1)2X(z)
z

= (z− 1)
2

z+ 1 A1 + (z− 1)A2 + A3

3

A3 = (z− 1)2X(z)
z

∣∣∣∣
z=1
= 1

2

The remaining coefficient A2
with respect to z and evaluating the result at z = 1. Note that it is not necessary formally to

2 vanish
when we set z = 1. Thus

A2 = d
dz

[
(z− 1)2X(z)

z

]
z=1
= 3

4

The generalization of the procedure in the example above to the case of an mth-
order pole (z−pk)m is straightforward. The partial-fraction expansion must contain
the terms

A1k

z− pk +
A2k

(z− pk)2 + · · · +
Amk

(z− pk)m
The coefficients {Aik} can be evaluated through differentiation as illustrated in Ex-

The z-Transform and Its Application to the Analysis of LTI Systems

(4.23)

(4.24)

(4.25)

(4.26)

EXAMPLE 4.7

First, we express (4.23) in terms of positive powers of z , in the form

(4.24) by (z+ 1) and evaluate the result at z = −1. Thus (4.24) becomes

Next, if we multiply both sides of (4.24) by (z− 1) , we obtain

Now, if we evaluate (4.25) at z = 1, we obtain A . Thus

can be obtained by differentiating both sides of (4.25)

carry out the differentiation of the right-hand side of (4.25), since all terms except A

ample 4.7 for m = 2.

192



Now that we have performed the partial-fraction expansion, we are ready to
take the final step in the inversion of X(z). First, let us consider the case in which
X(z) contains distinct poles.
follows that

X(z) = A1 11− p1z−1 + A2
1

1− p2z−1 + · · · + AN
1

1− pNz−1

The inverse z-transform, x(n) = Z−1{X(z)}, can be obtained by inverting each term

that these terms can be inverted using the formula

Z−1
{

1
1− pkz−1

}
=



(pk)

nu(n), if ROC: |z| > |pk|
(causal signals)

−(pk)nu(−n− 1), if ROC: |z| < |pk|
(anticausal signals)

If the signal x(n) is causal, the ROC is |z| > pmax , where pmax = max{|p1|,
|p2|, . . . , |pN |}.
and the signal x(n) is given by

x(n) = (A1pn1 + A2pn2 + · · · + ANpnN)u(n)

Thus a
causal signal, having a z-transform that contains real and distinct poles, is a linear
combination of real exponential signals.

Suppose now that all poles are distinct but some of them are complex. In this case

the signal x(n) is real, we should be able to reduce these terms into real components.
If x(n) is real, the polynomials appearing in X(z) have real coefficients. In this case,

j is a pole, its complex conjugate p∗j is also a

partial-fraction expansion are also complex conjugates. Thus the contribution of two
complex-conjugate poles is of the form

xk(n) = [Ak(pk)n + A∗k(p∗k )n]u(n)

These two terms can be combined to form a real signal component. First, we
express Aj and pj in polar form (i.e., amplitude and phase) as

Ak = |Ak|ejαk

pk = rkejβk

where αk and βk are the phase components of Ak and pk . Substitution of these

xk(n) = |Ak |rnk [ej (βkn+αk) + e−j (βkn+αk)]u(n)

The z-Transform and Its Application to the Analysis of LTI Systems

(4.27)

(4.28)

(4.29)

(4.30)

(4.31)

(4.32)

From the partial-fraction expansion (4.15), it easily

in (4.27) and taking the corresponding linear combination. From Table 3 it follows

In this case all terms in (4.27) result in causal signal components

If all poles are real , (4.29) is the desired expression for the signal x(n).

some of the terms in (4.27) result in complex exponential components. However, if

as we have seen in Section 3, if p
pole. As was demonstrated in Example 4.6, the corresponding coefficients in the

relations into (4.30) gives

193



or, equivalently,
xk(n) = 2|Ak |rnk cos(βkn+ αk)u(n)

Thus we conclude that

Z−1
(

Ak

1− pkz−1 +
A∗k

1− p∗k z−1
)
= 2|Ak|rnk cos(βkn+ αk)u(n)

if the ROC is |z| > |pk| = rk .

domain results in a causal sinusoidal signal component with an exponential envelope.
The distance rk of the pole from the origin determines the exponential weighting
(growing if rk > 1, decaying if rk < 1, constant if rk = 1). The angle of the poles with
respect to the positive real axis provides the frequency of the sinusoidal signal. The
zeros, or equivalently the numerator of the rational transform, affect only indirectly
the amplitude and the phase of xk(n) through Ak .

In the case of multiple poles, either real or complex, the inverse transform of
terms of the form A/(z−pk)n is required. In the case of a double pole the following

Z−1
{

pz−1

(1− pz−1)2
}
= npnu(n)

provided that the ROC is |z| > |p|. The generalization to the case of poles with
higher multiplicity is obtained by using multiple differentiation.

Determine the inverse z-transform of

X(z) = 1
1− 1.5z−1 + 0.5z−2

if
(a) ROC: |z| > 1
(b) ROC: |z| < 0.5
(c) ROC: 0.5 < |z| < 1
Solution.

yields

X(z) = 2
1− z−1 −

1
1− 0.5z−1

1 = 1 and p2 = 0.5. However, this requires the
specification of the corresponding ROC.
(a)

x(n) = 2(1)nu(n)− (0.5)nu(n) = (2 − 0.5n)u(n)

The z-Transform and Its Application to the Analysis of LTI Systems

(4.33)

(4.34)

(4.35)

(4.36)

In the case when the ROC is |z| > 1, the signal x(n) is causal and both terms in (4.36)

(4.37)

From (4.34) we observe that each pair of complex-conjugate poles in the z-

transform pair (see Table 3) is quite useful:

EXAMPLE 4.8

This is the same problem that we treated in Example 4.2. The partial-fraction
expansion for X (z) was determined in Example 4.5. The partial-fraction expansion of X(z)

To invert X(z) we should apply (4.28) for p

are causal terms. According to (4.28), we obtain

which agrees with the result in Example 4.2(a).

194



(b) When the ROC is |z| < 0.5, the signal x(n) is anticausal.

x(n) = [−2+ (0.5)n]u(−n− 1)
(c) In this case the ROC 0.5 < |z| < 1 is a ring, which implies that the signal x(n) is two-sided.

Thus one of the terms corresponds to a causal signal and the other to an anticausal signal.
Obviously, the given ROC is the overlapping of the regions |z| > 0.5 and |z| < 1. Hence
the pole p2 = 0.5 provides the causal part and the pole p1 = 1 the anticausal. Thus

x(n) = −2(1)nu(−n− 1)− (0.5)nu(n)

Determine the causal signal x(n) whose z-transform is given by

X(z) = 1+ z
−1

1− z−1 + 0.5z−2
Solution.

X(z) = A1
1− p1z−1 +

A2

1− p2z−1
where

A1 = A∗2 =
1
2
− j 3

2
and

p1 = p∗2 =
1
2
+ j 1

2

of A1 and p1 are

A1 =
√

10
2

e−j71.565

p1 = 1√
2
ejπ/4

Hence

x(n) =
√

10
(

1√
2

)n
cos

(πn
4
− 71.565◦

)
u(n)

Determine the causal signal x(n) having the z-transform

X(z) = 1
(1+ z−1)(1− z−1)2

Solution.

X(z) = 1
4

1
1+ z−1 +

3
4

1
1− z−1 +

1
2

z−1

(1− z−1)2

The z-Transform and Its Application to the Analysis of LTI Systems

(4.38)

(4.39)

Thus both terms in (4.36)
result in anticausal components. From (4.28) we obtain

EXAMPLE 4.9

In Example 4.6 we have obtained the partial-fraction expansion as

Since we have a pair of complex-conjugate poles, we should use (4.34). The polar forms

EXAMPLE 4.10

From Example 4.7 we have

195



x(n) = 1
4
(−1)nu(n)+ 3

4
u(n)+ 1

2
nu(n) =

[
1
4
(−1)n + 3

4
+ n

2

]
u(n)

Decomposition of Rational z-Transforms

At this point it is appropriate to discuss some additional issues concerning the decom-
position of rational z-transforms, which will prove very useful in the implementation
of discrete-time systems.

Suppose that we have a rational z-transform X(z) expressed as

X(z) =

M∑
k=0

bkz
−k

1+
N∑
k=1

akz
−k
= b0

M∏
k=1
(1− zkz−1)

N∏
k=1
(1− pkz−1)

where, for simplicity, we have assumed that a0 ≡ 1. IfM ≥ N [i.e.,X(z) is improper],
we convert X(z) to a sum of a polynomial and a proper function

X(z) =
M−N∑
k=0

ckz
−k +Xpr(z)

If the poles of Xpr(z) are distinct, it can be expanded in partial fractions as

Xpr(z) = A1 11− p1z−1 + A2
1

1− p2z−1 + · · · + AN
1

1− pNz−1
As we have already observed, there may be some complex-conjugate pairs of

Since we usually deal with real signals, we should avoid complex
coefficients in our decomposition. This can be achieved by grouping and combining
terms containing complex-conjugate poles, in the following way:

A

1− pz−1 +
A∗

1− p∗z−1 =
A− Ap∗z−1 + A∗ − A∗pz−1
1− pz−1 − p∗z−1 + pp∗z−2

= b0 + b1z
−1

1+ a1z−1 + a2z−2
where

b0 = 2 Re(A), a1 = −2 Re(p)
b1 = 2 Re(Ap∗), a2 = |p|2

2
1 − 4a2 < 0, can

be

The z-Transform and Its Application to the Analysis of LTI Systems

(4.40)

(4.41)

(4.42)

(4.43)

(4.44)

By applying the inverse transform relations in (4.28) and (4.35), we obtain

4.4

poles in (4.42).

are the desired coefficients. Obviously, any rational transform of the form (4.43)
with coefficients given by (4.44), which is the case when a

inverted using (4.34). By combining (4.41), (4.42), and (4.43) we obtain a

196



partial-fraction expansion for the z-transform with distinct poles that contains real
coefficients. The general result is

X(z) =
M−N∑
k=0

ckz
−k +

K1∑
k=1

bk

1+ akz−1 +
K2∑
k=1

b0k + b1kz−1
1+ a1kz−1 + a2kz−2

where K1 + 2K2 = N . Obviously, if M = N , the first term is just a constant,
and when M < N , this term vanishes. When there are also multiple poles, some

Analternative form is obtained by expressing X(z) as a product of simple terms

to avoid complex coefficients in the decomposition. Such combinations result in
second-order rational terms of the following form:

(1− zkz−1)(1− z∗kz−1)
(1− pkz−1)(1− p∗k z−1)

= 1+ b1kz
−1 + b2kz−2

1+ a1kz−1 + a2kz−2

where
b1k = −2 Re(zk), a1k = −2 Re(pk)

b2k = |zk|2, a2k = |pk|2

Assuming for simplicity that M = N , we see that X(z) can be decomposed in the
following way:

X(z) = b0
K1∏
k=1

1+ bkz−1
1+ akz−1

K2∏
k=1

1+ b1kz−1 + b2kz−2
1+ a1kz−1 + a2kz−2

where N = K1+ 2K2 .

5 Analysis of Linear Time-Invariant Systems in the z-Domain

and related it to the unit sample response and to the difference equation description
of systems. In this section we describe the use of the system function in the determi-

extend this method of analysis to nonrelaxed systems. Our attention is focused on
the important class of pole–zero systems represented by linear constant-coefficient
difference equations with arbitrary initial conditions.

We also consider the topic of stability of linear time-invariant systems and de-
scribe a test for determining the stability of a system based on the coefficients of the
denominator polynomial in the system function. Finally, we provide a detailed anal-
ysis of second-order systems, which form the basic building blocks in the realization
of higher-order systems.

The z-Transform and Its Application to the Analysis of LTI Systems

(4.45)

(4.46)

(4.47)

(4.48)

additional higher-order terms should be included in (4.45).

as in (4.40) . However, the complex-conjugate poles and zeros should be combined

nation of the response o f the system to some excitation signal. In Section 6.3, we

In Section 3.3 we introduced the system function of a linear time-invariant system

197



5.1 Response of Systems with Rational System Functions

Let us consider a pole–zero system described by the general linear constant-coefficient

represent H(z) as a ratio of two polynomials B(z)/A(z), where B(z) is the numerator
polynomial that contains the zeros of H(z), and A(z) is the denominator polynomial
that determines the poles of H(z). Furthermore, let us assume that the input signal
x(n) has a rational z-transform X(z) of the form

X(z) = N(z)
Q(z)

This assumption is not overly restrictive, since, as indicated previously, most signals
of practical interest have rational z-transforms.

If the system is initially relaxed, that is, the initial conditions for the difference
equation are zero, y(−1) = y(−2) = · · · = y(−N) = 0, the z-transform of the output
of the system has the form

Y (z) = H(z)X(z) = B(z)N(z)
A(z)Q(z)

Now suppose that the system contains simple polesp1 , p2, . . . , pN and the z-transform
of the input signal contains poles q1 , q2, . . . , qL , where pk �= qm for all k = 1, 2, . . . , N
and m = 1, 2, . . . , L. In addition, we assume that the zeros of the numerator poly-
nomials B(z) and N(z) do not coincide with the poles {pk} and {qk}, so that there is
no pole–zero cancellation. Then a partial-fraction expansion of Y (z) yields

Y (z) =
N∑
k=1

Ak

1− pkz−1 +
L∑
k=1

Qk

1− qkz−1

The inverse transform of Y (z) yields the output signal from the system in the form

y(n) =
N∑
k=1

Ak(pk)
nu(n)+

L∑
k=1

Qk(qk)
nu(n)

We observe that the output sequence y(n) can be subdivided into two parts. The first
part is a function of the poles {pk} of the system and is called the natural response of
the system. The influence of the input signal on this part of the response is through
the scale factors {Ak}. The second part of the response is a function of the poles {qk}
of the input signal and is called the forced response of the system. The influence of
the system on this response is exerted through the scale factors {Qk}.

We should emphasize that the scale factors {Ak} and {Qk} are functions of both
sets of poles {pk} and {qk}. For example, if X(z) = 0 so that the input is zero, then
Y (z) = 0, and consequently, the output is zero. Clearly, then, the natural response
of the system is zero. This implies that the natural response of the system is different
from the zero-input response.

The z-Transform and Its Application to the Analysis of LTI Systems

(5.1)

(5.2)

(5.3)

(5.4)

difference equation in (3.7) and the corresponding system function i n (3 .8). We

198



When X(z) and H(z) have one or more poles in common or when X(z) and/or
H(z) contain multiple-order poles, then Y (z) will have multiple-order poles. Con-
sequently, the partial-fraction expansion of Y (z) will contain factors of the form
1/(1 − plz−1)k , k = 1, 2, . . . , m, where m is the pole order. The inversion of these
factors will produce terms of the form nk−1pnl in the output y(n) of the system, as

5.2 Transient and Steady-State Responses

As we have seen from our previous discussion, the zero-state response of a system
to a given input can be separated into two components, the natural response and the
forced response. The natural response of a causal system has the form

ynr(n) =
N∑
k=1

Ak(pk)
nu(n)

where {pk}, k = 1, 2, . . . , N are the poles of the system and {Ak} are scale factors
that depend on the initial conditions and on the characteristics of the input sequence.

If |pk| < 1 for all k , then, ynr(n) decays to zero as n approaches infinity. In such a
case we refer to the natural response of the system as the transient response. The rate
at which ynr(n) decays toward zero depends on the magnitude of the pole positions.
If all the poles have small magnitudes, the decay is very rapid. On the other hand, if
one or more poles are located near the unit circle, the corresponding terms in ynr(n)
will decay slowly toward zero and the transient will persist for a relatively long time.

The forced response of the system has the form

yfr(n) =
L∑
k=1

Qk(qk)
nu(n)

where {qk}, k = 1, 2, . . . , L are the poles in the forcing function and {Qk} are scale
factors that depend on the input sequence and on the characteristics of the system. If
all the poles of the input signal fall inside the unit circle, yfr(n) will decay toward zero
as n approaches infinity, just as in the case of the natural response. This should not
be surprising since the input signal is also a transient signal. On the other hand, when
the causal input signal is a sinusoid, the poles fall on the unit circle and consequently,
the forced response is also a sinusoid that persists for all n ≥ 0. In this case, the forced
response is called the steady-state response of the system. Thus, for the system to
sustain a steady-state output for n ≥ 0, the input signal must persist for all n ≥ 0.

The following example illustrates the presence of the steady-state response.

Determine the transient and steady-state responses of the system characterized by the differ-
ence equation

y(n) = 0.5y(n− 1)+ x(n)
when the input signal is x(n) = 10 cos(πn/4)u(n). The system is initially at rest (i.e., it is
relaxed).

The z-Transform and Its Application to the Analysis of LTI Systems

(5.5)

(5.6)

indicated in Section 4.3.

EXAMPLE 5.1

199



Solution. The system function for this system is

H(z) = 1
1− 0.5z−1

and therefore the system has a pole at z = 0.5. The z-transform of the input signal is (from

X(z) = 10(1− (1/
√

2)z−1)

1−√2z−1 + z−2
Consequently,

Y (z) = H(z)X(z)

= 10(1− (1/
√

2)z−1)
(1− 0.5z−1)(1− ejπ/4z−1)(1− e−jπ/4z−1)

= 6.3
1− 0.5z−1 +

6.78e−j28.7
◦

1− ejπ/4z−1 +
6.78ej28.7

◦

1− e−jπ/4z−1

The natural or transient response is

ynr(n) = 6.3(0.5)nu(n)

and the forced or steady-state response is

yfr(n) = [6.78e−j28.7(ejπn/4)+ 6.78ej28.7e−jπn/4]u(n)(π
4
n− 28.7◦

)
u(n)

Thus we see that the steady-state response persists for all n ≥ 0, just as the input signal persists
for all n ≥ 0.

5.3 Causality and Stability

As defined previously, a causal linear time-invariant system is one whose unit sample
response h(n) satisfies the condition

h(n) = 0, n < 0

We have also shown that the ROC of the z-transform of a causal sequence is the
exterior of a circle. Consequently, a linear time-invariant system is causal if and only
if the ROC of the system function is the exterior of a circle of radius r <∞, including
the point z = ∞.

The stability of a linear time-invariant system can also be expressed in terms of
the characteristics of the system function. As we recall from our previous discussion,
a necessary and sufficient condition for a linear time-invariant system to be BIBO
stable is

∞∑
n=−∞

|h(n)| <∞

The z-Transform and Its Application to the Analysis of LTI Systems

Table 3)

= 13.56 cos

200



In turn, this condition implies that H(z) must contain the unit circle within its ROC.
Indeed, since

H(z) =
∞∑

n=−∞
h(n)z−n

it follows that

|H(z)| ≤
∞∑

n=−∞
|h(n)z−n| =

∞∑
n=−∞

|h(n)||z−n|

When evaluated on the unit circle (i.e., |z| = 1),

|H(z)| ≤
∞∑

n=−∞
|h(n)|

Hence, if the system is BIBO stable, the unit circle is contained in the ROC of H(z).
The converse is also true. Therefore, a linear time-invariant system is BIBO stable if
and only if the ROC of the system function includes the unit circle.

We should stress, however, that the conditions for causality and stability are
different and that one does not imply the other. For example, a causal system may
be stable or unstable, just as a noncausal system may be stable or unstable. Similarly,
an unstable system may be either causal or noncausal, just as a stable system may be
causal or noncausal.

For a causal system, however, the condition on stability can be narrowed to some
extent. Indeed, a causal system is characterized by a system function H(z) having as
a ROC the exterior of some circle of radius r . For a stable system, the ROC must
include the unit circle. Consequently, a causal and stable system must have a system
function that converges for |z| > r < 1. Since the ROC cannot contain any poles of
H(z), it follows that a causal linear time-invariant system is BIBO stable if and only
if all the poles of H(z) are inside the unit circle.

A linear time-invariant system is characterized by the system function

H(z) = 3− 4z
−1

1− 3.5z−1 + 1.5z−2

= 1
1− 12 z−1

+ 2
1− 3z−1

Specify the ROC of H(z) and determine h(n) for the following conditions:

(a) The system is stable.

(b) The system is causal.

(c) The system is anticausal.

The z-Transform and Its Application to the Analysis of LTI Systems

EXAMPLE 5.2

201



Solution. The system has poles at z = 12 and z = 3.
(a) Since the system is stable, its ROC must include the unit circle and hence it is 12 < |z| < 3.

Consequently, h(n) is noncausal and is given as

h(n) = (1
2
)nu(n)− 2(3)nu(−n− 1)

(b) Since the system is causal, its ROC is |z| > 3. In this case

h(n) = (1
2
)nu(n)+ 2(3)nu(n)

This system is unstable.

(c) If the system is anticausal, its ROC is |z| < 0.5. Hence

h(n) = −[(1
2
)n + 2(3)n]u(−n− 1)

In this case the system is unstable.

5.4 Pole–Zero Cancellations

When a z-transform has a pole that is at the same location as a zero, the pole is
canceled by the zero and, consequently, the term containing that pole in the inverse
z-transform vanishes. Such pole–zero cancellations are very important in the analysis
of pole–zero systems.

Pole–zero cancellations can occur either in the system function itself or in the
product of the system function with the z-transform of the input signal. In the first
case we say that the order of the system is reduced by one. In the latter case we say
that the pole of the system is suppressed by the zero in the input signal, or vice versa.
Thus, by properly selecting the position of the zeros of the input signal, it is possible
to suppress one or more system modes (pole factors) in the response of the system.
Similarly, by proper selection of the zeros of the system function, it is possible to
suppress one or more modes of the input signal from the response of the system.

When the zero is located very near the pole but not exactly at the same location,
the term in the response has a very small amplitude. For example, nonexact pole–
zero cancellations can occur in practice as a result of insufficiant numerical precision
used in representing the coefficients of the system. Consequently, one should not
attempt to stabilize an inherently unstable system by placing a zero in the input signal
at the location of the pole.

Determine the unit sample response of the system characterized by the difference equation

y(n) = 2.5y(n− 1)− y(n− 2)+ x(n)− 5x(n− 1)+ 6x(n− 2)

The z-Transform and Its Application to the Analysis of LTI Systems

EXAMPLE 5.3

202



Solution. The system function is

H(z) = 1− 5z
−1 + 6z−2

1− 2.5z−1 + z−2

= 1− 5z
−1 + 6z−2

(1− 12 z−1)(1− 2z−1)

This system has poles at p1 = 2 and p1 = 12 . Consequently, at first glance it appears that the
unit sample response is

Y (z) = H(z)X(z) = 1− 5z
−1 + 6z−2

(1− 12 z−1)(1− 2z−1)

= z
(

A

z− 12
+ B
z− 2

)

By evaluating the constants at z = 12 and z = 2, we find that

A = 5
2
, B = 0

The fact that B = 0 indicates that there exists a zero at z = 2 which cancels the pole at
z = 2. In fact, the zeros occur at z = 2 and z = 3. Consequently, H(z) reduces to

H(z) = 1− 3z
−1

1− 12z−1
= z− 3
z− 12

= 1− 2.5z
−1

1− 12 z−1

and therefore

h(n) = δ(n)− 2.5(1
2
)n−1u(n− 1)

The reduced-order system obtained by canceling the common pole and zero is characterized
by the difference equation

y(n) = 1
2
y(n− 1)+ x(n)− 3x(n− 1)

Although the original system is also BIBO stable due to the pole–zero cancellation, in a
practical implementation of this second-order system, we may encounter an instability due to
imperfect cancellation of the pole and the zero.

Determine the response of the system

y(n) = 5
6
y(n− 1)− 1

6
y(n− 2)+ x(n)

to the input signal x(n) = δ(n)− 13 δ(n− 1).

The z-Transform and Its Application to the Analysis of LTI Systems

EXAMPLE 5.4

203



Solution. The system function is

H(z) = 1
1− 56z−1 + 16z−2

= 1(
1− 12 z−1

) (
1− 13z−1

)
This system has two poles, one at z = 12 and the other at z = 13 . The z-transform of the input
signal is

X(z) = 1− 1
3
z−1

In this case the input signal contains a zero at z = 13 which cancels the pole at z = 13 .
Consequently,

Y (z) = H(z)X(z)

Y (z) = 1
1− 12 z−1

and hence the response of the system is

y(n) = (1
2
)nu(n)

Clearly, the mode ( 13 )
n is suppressed from the output as a result of the pole–zero cancellation.

5.5 Multiple-Order Poles and Stability

As we have observed, a necessary and sufficient condition for a causal linear time-
invariant system to be BIBO stable is that all its poles lie inside the unit circle. The
input signal is bounded if its z-transform contains poles {qk}, k = 1, 2, . . . , L, which
satisfy the condition |qk| ≤ 1 for all k . We note that the forced response of the

more distinct poles on the unit circle.
In view of the fact that a bounded input signal may have poles on the unit circle,

it might appear that a stable system may also have poles on the unit circle. This is
not the case, however, since such a system produces an unbounded response when
excited by an input signal that also has a pole at the same position on the unit circle.
The following example illustrates this point.

Determine the step response of the causal system described by the difference equation

y(n) = y(n− 1)+ x(n)
Solution. The system function for the system is

H(z) = 1
1− z−1

The z-Transform and Its Application to the Analysis of LTI Systems

system, given in (5.6), is also bounded, even when the input signal contains one or

EXAMPLE 5.5

204



We note that the system contains a pole on the unit circle at z = 1. The z-transform of the
input signal x(n) = u(n) is

X(z) = 1
1− z−1

which also contains a pole at z = 1. Hence the output signal has the transform
Y (z) = H(z)X(z)

= 1
(1− z−1)2

which contains a double pole at z = 1.
The inverse z-transform of Y (z) is

y(n) = (n+ 1)u(n)
which is a ramp sequence. Thus y(n) is unbounded, even when the input is bounded. Conse-
quently, the system is unstable.

poles be strictly inside the unit circle. If the system poles are all inside the unit circle
and the excitation sequence x(n) contains one or more poles that coincide with the
poles of the system, the output Y (z) will contain multiple-order poles. As indicated
previously, such multiple-order poles result in an output sequence that contains terms
of the form

Akn
b(pk)

nu(n)

where 0 ≤ b ≤ m− 1 and m is the order of the pole. If |pk| < 1, these terms decay
to zero as n approaches infinity because the exponential factor (pk)n dominates the
term nb . Consequently, no bounded input signal can produce an unbounded output
signal if the system poles are all inside the unit circle.

Finally, we should state that the only useful systems which contain poles on the
We call such systems marginally stable.

5.6 Stability of Second-Order Systems

Let us consider a causal two-pole system described by the second-order differ-
ence equation

y(n) = −a1y(n− 1)− a2y(n− 2)+ b0x(n)
The system function is

H(z) = Y (z)
X(z)

= b0
1+ a1z−1 + a2z−1

= b0z
2

z2 + a1z+ a2

The z-Transform and Its Application to the Analysis of LTI Systems

unit circle are digital oscillators.

In this section we provide a detailed analysis of a system having two poles. Two-pole 
systems form the basic building blocks for the realization of higher-order systems.

(5.7)

(5.8)

Example 5.5 demonstrates clearly that BIBO stability requires that the system

205



This system has two zeros at the origin and poles at

p1, p2 = −a12 ±
√
a21 − 4a2

4

The system is BIBO stable if the poles lie inside the unit circle, that is, if |p1| < 1
and |p2| < 1. These conditions can be related to the values of the coefficients a1 and
a2 . In particular, the roots of a quadratic equation satisfy the relations

a1 = −(p1 + p2)
a2 = p1p2

1 and a2 must satisfy
for stability. First, a2 must satisfy the condition

|a2| = |p1p2| = |p1||p2| < 1

The condition for a1 can be expressed as

|a1| < 1+ a2

Therefore, a two-pole system is stable if and only if the coefficients a1 and a2

1 2
system is stable if and only if the point (a1 , a2 ) lies inside the triangle, which we call
the stability triangle.

−1

Stability
triangle

Complex-
conjugate
poles

Real and equal poles

1

a2

a2 = 
a1

Real and distinct poles
1−1−2 2 a1

a2 = −a1 − 1

a2 = a1 − 1

a2 = 1

2

4

Region of stability (stability triangle) in the
(a1 , a2 ) coefficient plane for a second-order system.

The z-Transform and Its Application to the Analysis of LTI Systems

(5.9)

(5.10)

(5.11)

(5.12)

(5.13)

From (5.10) and (5.11) we easily obtain the conditions that a

satisfy the conditions in (5.12) and (5.13).
The stability conditions given in (5.12) and (5.13) define a region in the co-

efficient plane (a , a ), which is in the form of a triangle, as shown in Fig. 5.1. The

Figure 5.1

206



The characteristics of the two-pole system depend on the location of the poles
or, equivalently, on the location of the point (a1 , a2 ) in the stability triangle. The
poles of the system may be real or complex conjugate, depending on the value of the
discriminant � = a21 − 4a2 . The parabola a2 = a21/4 splits the stability triangle into

The region below the parabola (a21 > 4a2 )
corresponds to real and distinct poles. The points on the parabola (a21 = 4a2 ) result
in real and equal (double) poles. Finally, the points above the parabola correspond
to complex-conjugate poles.

Additional insight into the behavior of the system can be obtained from the unit
sample responses for these three cases.

Real and distinct poles (a21 > 4a2 ). Since p1 , p2 are real and p1 �= p2 , the system
function can be expressed in the form

H(z) = A1
1− p1z−1 +

A2

1− p2z−1

where

A1 = b0p1
p1 − p2 , A2 =

−b0p2
p1 − p2

Consequently, the unit sample response is

h(n) = b0
p1 − p2 (p

n+1
1 − pn+12 )u(n)

Therefore, the unit sample response is the difference of two decaying exponential

Real and equal poles (a21 = 4a2). In this case p1 = p2 = p = −a1/2. The system
function is

H(z) = b0
(1− pz−1)2

0 50
n

0.5

1.0

1.5

2.0
h(n)

1 = 0.5, p2 = 0.75;
h(n) = [1/(p1 − p2)](pn+11 − pn+12 )u(n).

The z-Transform and Its Application to the Analysis of LTI Systems

(5.14)

(5.15)

(5.16)

(5.17)

two regions, as illustrated in Fig. 5.1.

sequences. Figure 5.2 illustrates a typical graph for h(n) when the poles are distinct.

Figure 5.2 Plot of h(n) given by (5.16) with p

207



and hence the unit sample response of the system is

h(n) = b0(n+ 1)pnu(n)

We observe that h(n) is the product of a ramp sequence and a real decaying expo-

Complex-conjugate poles (a21 < 4a2). Since the poles are complex conjugate, the
system function can be factored and expressed as

H(z) = A
1− pz−1 +

A∗

1− p∗z−1

= A
1− rejω0z−1 +

A∗

1− re−jω0z−1

where p = rejω and 0 < ω0 < π . Note that when the poles are complex conjugates,
the parameters a1 and a2 are related to r and ω0 according to

a1 = −2r cosω0
a2 = r2

The constant A in the partial-fraction expansion of H(z) is easily shown to be

A = b0p
p − p∗ =

b0re
jω0

r(ejω0 − e−jω0)

= b0e
jω0

j2 sinω0

0 50
n

0.5

1.0

1.5

2.0
h(n)

= 34 ; h(n) = (n +
1)pnu(n).

The z-Transform and Its Application to the Analysis of LTI Systems

(5.18)

(5.19)

(5.20)

(5.21)

nential sequence. The graph of h(n) is shown in Fig. 5.3.

Figure 5.3 Plot of h(n) given by (5.18) with p

208



50
n0

0.2

0.4

0.6

0.8

1.0

1.2

−1.0

−0.8

−1.2

−0.6

−0.4

−0.2

h(n)

0 = 1, ω0 = π/4,
r = 0.9; h(n) = [b0rn/(sinω0)] sin[(n+ 1)ω0]u(n).

Consequently, the unit sample response of a system with complex-conjugate poles is

h(n) = b0r
n

sinω0

ej (n+1)ω0 − e−j (n+1)ω0
2j

u(n)

= b0r
n

sinω0
sin(n+ 1)ω0u(n)

In this case h(n) has an oscillatory behavior with an exponentially decaying
envelope when r < 1. The angle ω0 of the poles determines the frequency of
oscillation and the distance r of the poles from the origin determines the rate of
decay. When r is close to unity, the decay is slow. When r is close to the origin, the

6 The One-sided z-Transform

The two-sided z-transform requires that the corresponding signals be specified for the
entire time range −∞ < n <∞. This requirement prevents its use for a very useful
family of practical problems, namely the evaluation of the output of nonrelaxed
systems. As we recall, these systems are described by difference equations with
nonzero initial conditions. Since the input is applied at a finite time, say n0 , both
input and output signals are specified for n ≥ n0 , but by no means are zero for
n < n0 . Thus the two-sided z-transform cannot be used. In this section we develop
the one-sided z-transform which can be used to solve difference equations with initial
conditions.

The z-Transform and Its Application to the Analysis of LTI Systems

(5.22)

Figure 5.4 Plot of h(n) given by (5.22) with b

decay is fast. A typical graph of h(n) is illustrated in Fig. 5.4.

209



6.1 Definition and Properties

The one-sided or unilateral z-transform of a signal x(n) is defined by

X+(z) ≡
∞∑
n=0

x(n)z−n

We also use the notations Z+{x(n)} and

x(n)
z+←→ X+(z)

The one-sided z-transform differs from the two-sided transform in the lower
limit of the summation, which is always zero, whether or not the signal x(n) is zero
for n < 0 (i.e., causal). Due to this choice of lower limit, the one-sided z-transform
has the following characteristics:

1. It does not contain information about the signal x(n) for negative values of time
(i.e., for n < 0).

2. It is unique only for causal signals, because only these signals are zero for n < 0.

3. The one-sided z-transformX+(z) of x(n) is identical to the two-sided z-transform
of the signal x(n)u(n). Since x(n)u(n) is causal, the ROC of its transform, and
hence the ROC of X+(z), is always the exterior of a circle. Thus when we deal
with one-sided z-transforms, it is not necessary to refer to their ROC.

Solution.

x1(n) = {1↑, 2, 5, 7, 0, 1}
z+←→ X+1 (z) = 1+ 2z−1 + 5z−2 + 7z−3 + z−5

x2(n) = {1, 2, 5↑, 7, 0, 1}
z+←→ X+2 (z) = 5+ 7z−1 + z−3

x3(n) = {0↑, 0, 1, 2, 5, 7, 0, 1}
z+←→ X+3 (z) = z−2 + 2z−3 + 5z−4 + 7z−5 + z−7

x4(n) = {2, 4, 5↑, 7, 0, 1}
z+←→ X+4 (z) = 5+ 7z−1 + z−3

x5(n) = δ(n) z
+←→ X+5 (z) = 1

x6(n) = δ(n− k), k > 0 z
+
←→ X+6 (z) = z−k

x7(n) = δ(n+ k), k > 0 z
+←→ X+7 (z) = 0

Note that for a noncausal signal, the one-sided z-transform is not unique. Indeed, X+2 (z) =
X+4 (z) but x2(n) �= x4(n). Also for anticausal signals, X+(z) is always zero.

The z-Transform and Its Application to the Analysis of LTI Systems

(6.1)

EXAMPLE 6.1

Determine the one-sided z-transform of the signals in Example 1.1.

From the definition (6.1), we obtain

210



Almost all properties we have studied for the two-sided z-transform carry over to
the one-sided z-transform with the exception of the shifting property.

Shfiting Property

Case 1: Time delay If

x(n)
z+←→ X+(z)

then

x(n− k) z+←→ z−k[X+(z)+
k∑
n=1

x(−n)zn], k > 0

In case x(n) is causal, then

x(n− k) z
+
←→ z−kX+(z)

Proof

Z+{x(n− k)} = z−k
[ −1∑
l=−k

x(l)z−l +
∞∑
l=0

x(l)z−l
]

= z−k

 −k∑
l=−1

x(l)z−l +X+(z)



Determine the one-sided z-transform of the signals
(a) x(n) = anu(n)
(b) x1(n) = x(n− 2) where x(n) = an

Solution.
(a)

X+(z) = 1
1− az−1

(b) We will apply the shifting property for k = 2. Indeed, we have

Z+{x(n− 2)} = z−2[X+(z)+ x(−1)z+ x(−2)z2]
= z−2X+(z)+ x(−1)z−1 + x(−2)

Since x(−1) = a−1 , x(−2) = a−2 , we obtain

X+1 (z) =
z−2

1− az−1 + a
−1z−1 + a−2

The z-Transform and Its Application to the Analysis of LTI Systems

(6.2)

(6.3)

From the definition (6.1) we have

By changing the index from l to n = −l , the result in (6.2) is easily obtained.

EXAMPLE 6.2

From (6.1) we easily obtain

211



as follows:

Z+{x(n− k)} = [x(−k)+ x(−k + 1)z−1 + · · · + x(−1)z−k+1]
+ z−kX+(z), k > 0

To obtain x(n− k)(k > 0) from x(n), we should shift x(n) by k samples to the right.
Then k “new” samples, x(−k), x(−k + 1), . . . , x(−1), enter the positive time axis

of these samples. The “old” samples of x(n− k) are the same as those of x(n) simply
shifted by k samples to the right. Their z-transform is obviously z−kX+(z), which is

Case 2: Time advance If

x(n)
z+←→ X+(z)

then

x(n+ k) z+←→ zk
[
X+(z)−

k−1∑
n=0

x(n)z−n
]
, k > 0

Proof

Z+{x(n+ k)} =
∞∑
n=0

x(n+ k)z−n = zk
∞∑
l=k

x(l)z−l

where we have changed the index of summation from n to l = n + k . Now, from

X+(z) =
∞∑
l=0

x(l)z−l =
k−1∑
l=0

x(l)z−l +
∞∑
l=k

x(l)z−l

x2(n) = x(n+ 2)
Solution.

Z+{x(n+ 2)} = z2X+(z)− x(0)z2 − x(1)z

But x(0) = 1, x(1) = a , and X+(z) = 1/(1− az−1). Thus

Z+{x(n+ 2)} = z
2

1− az−1 − z
2 − az

The z-Transform and Its Application to the Analysis of LTI Systems

(6.4)

(6.5)

The meaning of the shifting property can be intuitively explained if we write (6.2)

with x(−k) located at time zero. The first term in (6.4) stands for the z-transform

the second term in (6.4).

From (6.1) we have

(6.1) we obtain

By combining the last two relations, we easily obtain (6.5).

EXAMPLE 6.3

With x(n), as given in Example 6.2, determine the one-sided z-transform of the signal

We will apply the shifting theorem for k = 2. From (6.5), with k = 2, we obtain

212



The case of a time advance can be intuitively explained as follows. To obtain x(n+k),
k > 0, we should shift x(n) by k samples to the left. As a result, the samples
x(0), x(1), . . . , x(k − 1) “leave” the positive time axis. Thus we first remove their
contribution to the X+(z), and then multiply what remains by zk to compensate for
the shifting of the signal by k samples.

The importance of the shifting property lies in its application to the solution of
difference equations with constant coefficients and nonzero initial conditions. This
makes the one-sided z-transform a very useful tool for the analysis of recursive linear
time-invariant discrete-time systems.

An important theorem useful in the analysis of signals and systems is the final
value theorem.

Final Value Theorem. If

x(n)
z+←→ X+(z)

then
lim
n→∞ x(n) = limz→1(z− 1)X

+(z)

+(z) includes the unit circle.

The proof of this theorem is left as an exercise for the reader.
This theorem is useful when we are interested in the asymptotic behavior of a

signal x(n) and we know its z-transform, but not the signal itself. In such cases,
especially if it is complicated to invert X+(z), we can use the final value theorem to
determine the limit of x(n) as n goes to infinity.

The impulse response of a relaxed linear time-invariant system is h(n) = αnu(n), |α| < 1.
Determine the value of the step response of the system as n→∞.
Solution. The step response of the system is

y(n) = h(n) ∗ x(n)
where

x(n) = u(n)
Obviously, if we excite a causal system with a causal input the output will be causal. Since
h(n), x(n), y(n) are causal signals, the one-sided and two-sided z-transforms are identical.

be multiplied to yield the z-transform of the output. Thus

Y (z) = 1
1− αz−1

1
1− z−1 =

z2

(z− 1)(z − α) , ROC: |z| > |α|

Now

(z− 1)Y (z) = z
2

z− α , ROC: |z| < |α|
Since |α| < 1, the ROC of (z − 1)Y (z) includes the unit circle. Consequently, we can apply

lim
n→∞ y(n) = limz→1

z2

z− α =
1

1− α

The z-Transform and Its Application to the Analysis of LTI Systems

(6.6)

The limit in (6.6) exists if the ROC of (z− 1)X

EXAMPLE 6.4

From the convolution property (2.17) we know that the z-transforms of h(n) and x(n) must

(6.6) and obtain

213



Solution of Difference Equations

The one-sided z-transform is a very efficient tool for the solution of difference equa-
tions with nonzero initial conditions. It achieves that by reducing the difference
equation relating the two time-domain signals to an equivalent algebraic equation
relating their one-sided z-transforms. This equation can be easily solved to obtain the
transform of the desired signal. The signal in the time domain is obtained by inverting
the resulting z-transform. We will illustrate this approach with two examples.

The well-known Fibonacci sequence of integer numbers is obtained by computing each term
as the sum of the two previous ones. The first few terms of the sequence are

1, 1, 2, 3, 5, 8, . . .

Determine a closed-form expression for the nth term of the Fibonacci sequence.

Solution. Let y(n) be the nth term of the Fibonacci sequence. Clearly, y(n) satisfies the
difference equation

y(n) = y(n− 1)+ y(n− 2)
with initial conditions

y(0) = y(−1)+ y(−2) = 1
y(1) = y(0)+ y(−1) = 1

Thus we have to

obtain
Y+(z) = [z−1Y+(z)+ y(−1)]+ [z−2Y+(z)+ y(−2)+ y(−1)z−1]

or

Y+(z) = 1
1− z−1 − z2 =

z2

z2 − z− 1
where we have used the fact that y(−1) = 0 and y(−2) = 1.

We can invert Y+(z) by the partial-fraction expansion method. The poles of Y+(z) are

p1 = 1+
√

5
2

, p2 = 1−
√

5
2

and the corresponding coefficients are A1 = p1/
√

5 and A2 = −p2/
√

5. Therefore,

y(n) =
[

1+√5
2
√

5

(
1+√5

2

)n
− 1−

√
5

2
√

5

(
1−√5

2

)n]
u(n)

or, equivalently,

y(n) = 1√
5

(
1
2

)n+1 [(
1+
√

5
)n+1 − (1−√5)n+1] u(n)

The z-Transform and Its Application to the Analysis of LTI Systems

(6.7)

(6.8a)

(6.8b)

(6.9)

(6.10)

6.2

EXAMPLE 6.5

From (6.8b) we have y(−1) = 0. Then (6.8a) gives y(−2) = 1.

By taking the one-sided z -transform of (6.7) and using the shifting property (6.2), we
determine y(n), n ≥ 0, which satisfies (6.7), with initial conditions y(−1) = 0 and y(−2) = 1.

214



Determine the step response of the system

y(n) = αy(n− 1)+ x(n), −1 < α < 1

when the initial condition is y(−1) = 1.

Solution.

Y+(z) = α[z−1Y+(z)+ y(−1)]+X+(z)

Upon substitution for y(−1) and X+(z) and solving for Y+(z), we obtain the result

Y+(z) = α
1− αz−1 +

1
(1− αz−1)(1− z−1)

By performing a partial-fraction expansion and inverse transforming the result, we have

y(n) = αn+1u(n)+ 1− α
n+1

1− α u(n)

= 1
1− α (1− α

n+2)u(n)

6.3 Response of Pole–Zero Systems with Nonzero Initial Conditions

Suppose that the signal x(n) is applied to the pole–zero system at n = 0. Thus the
signal x(n) is assumed to be causal. The effects of all previous input signals to the
system are reflected in the initial conditions y(−1), y(−2), . . . , y(−N). Since the
input x(n) is causal and since we are interested in determining the output y(n) for
n ≥ 0, we can use the one-sided z-transform, which allows us to deal with the initial

Y+(z) = −
N∑
k=1

akz
−k

[
Y+(z)+

k∑
n=1

y(−n)zn
]
+

M∑
k=0

bkz
−kX+(z)

Since x(n) is causal, we can set X+(z) = X(z).
pressed as

Y+(z) =

M∑
k=0

bkz
−k

1+
N∑
k=1

akz
−k
X(z)−

N∑
k=1

akz
−k

k∑
n=1

y(−n)zn

1+
N∑
k=1

akz
−k

= H(z)X(z)+ N0(z)
A(z)

The z-Transform and Its Application to the Analysis of LTI Systems

(6.11)

(6.12)

(6.13)

(6.14)

(6.15)

EXAMPLE 6.6

By taking the one-sided z-transform of both sides of (6.11), we obtain

In any case (6.14) may be ex-

conditions. Thus the one-sided z-transform of (7) becomes

215



where

N0(z) = −
N∑
k=1

akz
−k

k∑
n=1

y(−n)zn

conditions can be subdivided into two parts. The first is the zero-state response of
the system, defined in the z-domain as

Yzs(z) = H(z)X(z)

The second component corresponds to the output resulting from the nonzero initial
conditions. This output is the zero-input response of the system, which is defined in
the z-domain as

Y+zi (z) =
N0(z)

A(z)

Hence the total response is the sum of these two output components, which can be
expressed in the time domain by determining the inverse z-transforms of Yzs(z) and
Yzi(z) separately, and then adding the results. Thus

y(n) = yzs(n)+ yzi(n)

Since the denominator of Y+zi (z), is A(z), its poles are p1 , p2, . . . , pN . Conse-
quently, the zero-input response has the form

yzi(n) =
N∑
k=1

Dk(pk)
nu(n)

k

to yield the total response in the form

y(n) =
N∑
k=1

A′k(pk)
nu(n)+

L∑
k=1

Qk(qk)
nu(n)

where, by definition,

A′k = Ak +Dk
This development indicates clearly that the effect of the initial conditions is to

alter the natural response of the system through modification of the scale factors {Ak}.
There are no new poles introduced by the nonzero initial conditions. Furthermore,
there is no effect on the forced response of the system. These important points are
reinforced in the following example.

The z-Transform and Its Application to the Analysis of LTI Systems

(6.16)

(6.17)

(6.18)

(6.19)

(6.20)

(6.21)

(6.22)

From (6.15) it is apparent that the output of the system with nonzero initial

This can be added to (6.4) and the terms involving the poles {p } can be combined

216



Determine the unit step response of the system described by the difference equation

y(n) = 0.9y(n− 1)− 0.81y(n− 2)+ x(n)
under the following initial conditions y(−1) = y(−2) = 1.
Solution. The system function is

H(z) = 1
1− 0.9z−1 + 0.81z−2

This system has two complex-conjugate poles at

p1 = 0.9ejπ/3, p2 = 0.9e−jπ/3

The z-transform of the unit step sequence is

X(z) = 1
1− z−1

Therefore,

Yzs(z) = 1
(1− 0.9ejπ/3z−1)(1− 0.9e−jπ/3z−1)(1− z−1)

= 0.0496− j0.542
1− 0.9ejπ/3z−1 +

0.0496+ j0.542
1− 0.9e−jπ/3z−1 +

1.099
1− z−1

and hence the zero-state response is

yzs(n) =
[
1.099+ 1.088(0.9)n cos

(π
3
n− 5.2◦

)]
u(n)

For the initial conditions y(−1) = y(−2) = 1, the additional component in the z-transform is

Yzi(z) = N0(z)
A(z)

= 0.09− 0.81z
−1

1− 0.9z−1 + 0.81z−2

= 0.045+ j0.4936
1− 0.9ejπ/3z−1 +

0.045− j0.4936
1− 0.9e−jπ/3z−1

Consequently, the zero-input response is

yzi(n) = 0.988(0.9)n cos
(π

3
n+ 87◦

)
u(n)

In this case the total response has the z-transform

Y (z) = Yzs(z)+ Yzi(z)

= 1.099
1− z−1 +

0.568+ j0.445
1− 0.9ejπ/3z−1 +

0.568− j0.445
1− 0.9e−jπ/3z−1

The inverse transform yields the total response in the form

y(n) = 1.099u(n)+ 1.44(0.9)n cos
(π

3
n+ 38◦

)
u(n)

The z-Transform and Its Application to the Analysis of LTI Systems

EXAMPLE 6.7

217



7 Summary and References

The z-transform plays the same role in discrete-time signals and systems as the
Laplace transform does in continuous-time signals and systems. In this chapter we
derived the important properties of the z-transform, which are extremely useful in
the analysis of discrete-time systems. Of particular importance is the convolution
property, which transforms the convolution of two sequences into a product of their
z-transforms.

In the context of LTI systems, the convolution property results in the product of
the z-transform X(z) of the input signal with the system function H(z), where the
latter is the z-transform of the unit sample response of the system. This relationship
allows us to determine the output of an LTI system in response to an input with
transform X(z) by computing the product Y (z) = H(z)X(z) and then determining
the inverse z-transform of Y (z) to obtain the output sequence y(n).

We observed that many signals of practical interest have rational z-transforms.
Moreover, LTI systems characterized by constant-coefficient linear difference equa-
tions also possess rational system functions. Consequently, in determining the inverse
z-transform, we naturally emphasized the inversion of rational transforms. For such
transforms, the partial-fraction expansion method is relatively easy to apply, in con-
junction with the ROC, to determine the corresponding sequence in the time domain.

We considered the characterization of LTI systems in the z-transform domain.
In particular, we related the pole–zero locations of a system to its time-domain char-
acteristics and restated the requirements for stability and causality of LTI systems
in terms of the pole locations. We demonstrated that a causal system has a system
function H(z) with a ROC |z| > r1 , where 0 < r1 ≤ ∞. In a stable and causal
system, the poles of H(z) lie inside the unit circle. On the other hand, if the system is
noncausal, the condition for stability requires that the unit circle be contained in the
ROC of H(z). Hence a noncausal stable LTI system has a system function with poles
both inside and outside the unit circle with an annular ROC that includes the unit
circle. Finally, the one-sided z-transform was introduced to solve for the response of
causal systems excited by causal input signals with nonzero initial conditions.

Problems

1 Determine the z-transform of the following signals.

(a) x(n) = {3, 0, 0, 0, 0, 6
↑
, 1,−4}

(b) x(n) =
{
( 12 )

n, n ≥ 5
0, n ≤ 4

2 Determine the z-transforms of the following signals and sketch the corresponding
pole–zero patterns.

(a) x(n) = (1+ n)u(n)
(b) x(n) = (an + a−n)u(n), a real
(c) x(n) = (−1)n2−nu(n)

The z-Transform and Its Application to the Analysis of LTI Systems

218



(d) x(n) = (nan sinω0n)u(n)
(e) x(n) = (nan cosω0n)u(n)
(f) x(n) = Arn cos(ω0n+ φ)u(n), 0 < r < 1
(g) x(n) = 12 (n2 + n)( 13 )n−1u(n− 1)
(h) x(n) = ( 12 )n[u(n)− u(n− 10)]

3 Determine the z-transforms and sketch the ROC of the following signals.

(a) x1(n) =
{
( 13 )

n, n ≥ 0
( 12 )
−n, n < 0

(b) x2(n) =
{
( 13 )

n − 2n, n ≥ 0
0, n < 0

(c) x3(n) = x1(n+ 4)
(d) x4(n) = x1(−n)

4 Determine the z-transform of the following signals.
(a) x(n) = n(−1)nu(n)
(b) x(n) = n2u(n)
(c) x(n) = −nanu(−n− 1)
(d) x(n) = (−1)n (cos π3 n) u(n)
(e) x(n) = (−1)nu(n)
(f) x(n) = {1

↑
, 0,−1, 0, 1,−1, . . .}

5 Determine the regions of convergence of right-sided, left-sided, and finite-duration
two-sided sequences.

6 Express the z-transform of

y(n) =
n∑

k=−∞
x(k)

in terms of X(z). [Hint: Find the difference y(n)− y(n− 1).]
7 Compute the convolution of the following signals by means of the z-transform.

x1(n) =
{
( 13 )

n, n ≥ 0
( 12 )
−n, n < 0

x2(n) = (12 )
nu(n)

8 Use the convolution property to:
(a) Express the z-transform of

y(n) =
n∑

k=−∞
x(k)

in terms of X(z).
(b) Determine the z-transform of x(n) = (n+ 1)u(n). [Hint: Show first that x(n) =

u(n) ∗ u(n).]

The z-Transform and Its Application to the Analysis of LTI Systems

219



9 The z-transform X(z) of a real signal x(n) includes a pair of complex-conjugate zeros
and a pair of complex-conjugate poles. What happens to these pairs if we multiply
x(n) by ejω0n? (Hint: Use the scaling theorem in the z-domain.)

10 Apply the final value theorem to determine x(∞) for the signal

x(n) =
{

1, if n is even
0, otherwise

11 Using long division, determine the inverse z-transform of

X(z) = 1+ 2z
−1

1− 2z−1 + z−2

if (a) x(n) is causal and (b) x(n) is anticausal.
12 Determine the causal signal x(n) having the z-transform

X(z) = 1
(1− 2z−1)(1− z−1)2

13 Let x(n) be a sequence with z-transform X(z). Determine, in terms of X(z), the
z-transforms of the following signals.

(a) x1(n) =
{
x
(
n
2

)
, if n even

0, if n odd
(b) x2(n) = x(2n)

14 Determine the causal signal x(n) if its z-transform X(z) is given by:

(a) X(z) = 1+ 3z−1
1+ 3z−1 + 2z−2

(b) X(z) = 1
1− z−1 + 12 z−2

(c) X(z) = z−6+ z−7
1− z−1

(d) X(z) = 1+ 2z−2
1+ z−2

(e) X(z) = 14 1+ 6z
−1+ z−2

(1− 2z−1 + 2z−2)(1− 0.5z−1)

(f) X(z) = 2− 1.5z−1
1− 1.5z−1+ 0.5z−2

1
4

1
2

1
2

π

4

√r =

−1
2

−

The z-Transform and Its Application to the Analysis of LTI Systems

Figure P14

220



(g) X(z) = 1+ 2z−1+ z−2
1+ 4z−1 + 4z−2

(h) 14 .

(i) X(z) = 1−
1
2 z
−1

1+ 12 z−1

(j) X(z) = 1− az−1
z−1− a

15 Determine all possible signals x(n) associated with the z-transform

X(z) = 5z
−1

(1− 2z−1)(3− z−1)
16 Determine the convolution of the following pairs of signals by means of the z-

transform.

(a) x1(n) = ( 14 )nu(n− 1), x2(n) = [1+ ( 12 )n]u(n)
(b) x1(n) = u(n), x2(n) = δ(n)+ ( 12 )nu(n)
(c) x1(n) = ( 12 )nu(n), x2(n) = cosπnu(n)
(d) x1(n) = nu(n), x2(n) = 2nu(n− 1)

17 Prove the final value theorem for the one-sided z-transform.
18 If X(z) is the z-transform of x(n), show that:

(a) Z{x∗(n)} = X∗(z∗)
(b) Z{Re[x(n)]} = 12 [X(z)+X∗(z∗)]
(c) Z{Im[x(n)]} = 12 [X(z)−X∗(z∗)]
(d) If

xk(n) =
{
x
(
n
k

)
, if n/k integer

0, otherwise

then
Xk(z) = X(zk)

(e) Z{ejω0nx(n)} = X(ze−jω0)
19 By first differentiatingX(z) and then using appropriate properties of the z-transform,

determine x(n) for the following transforms.

(a) X(z) = log(1− 2z), |z| < 12
(b) X(z) = log(1− z−1), |z| > 12

20
(a) Draw the pole–zero pattern for the signal

x1(n) = (rn sinω0n)u(n), 0 < r < 1
(b) Compute the z-transform X2(z), which corresponds to the pole–zero pattern in

part (a).

(c) Compare X1(z) with X2(z). Are they indentical? If not, indicate a method to
derive X1(z) from the pole–zero pattern.

The z-Transform and Its Application to the Analysis of LTI Systems

X(z) is specified by a pole–zero pattern in Fig. P14. The constant G =

221



21 Show that the roots of a polynomial with real coefficients are real or form complex-
conjugate pairs. The inverse is not true, in general.

22 Prove the convolution and correlation properties of the z-transform using only its
definition.

23 Determine the signal x(n) with z-transform

X(z) = ez + e1/z, |z| �= 0

24 Determine, in closed form, the causal signals x(n) whose z-transforms are given by:

(a) X(z) = 1
1+ 1.5z−1− 0.5z−2

(b) X(z) = 1
1− 0.5z−1+ 0.6z−2

Partially check your results by computing x(0), x(1), x(2), and x(∞) by an alternative
method.

25 Determine all possible signals that can have the following z-transforms.

(a) X(z) = 1
1− 1.5z−1+ 0.5z−2

(b) X(z) = 1
1− 12 z−1+ 14 z−2

26 Determine the signal x(n) with z-transform

X(z) = 3
1− 103 z−1 + z−2

if X(z) converges on the unit circle.
27
28 Prove the conjugation properties and Parseval’s relation for the z-transform given

29
each value of n. In general, this procedure proves to be tedious. It can be avoided
by making a transformation in the contour integral from z-plane to the w = 1/z
plane. Thus a circle of radius R in the z-plane is mapped into a circle of radius
1/R in the w-plane. As a consequence, a pole inside the unit circle in the z-plane is
mapped into a pole outside the unit circle in the w-plane. By making the change of
variable w = 1/z in the contour integral, determine the sequence x(n) for n < 0 in

30 Let x(n), 0 ≤ n ≤ N − 1 be a finite-duration sequence, which is also real-valued and
even. Show that the zeros of the polynomial X(z) occur in mirror-image pairs about
the unit circle. That is, if z = rejθ is a zero of X(z), then z = (1/r)ejθ is also a zero.

31 Prove that the Fibonacci sequence can be thought of as the impulse response of the
system described by the difference equation y(n) = y(n−1)+y(n−2)+x(n). Then
determine h(n) using z-transform techniques.

32 Show that the following systems are equivalent.

(a) y(n) = 0.2y(n− 1)+ x(n)− 0.3x(n− 1)+ 0.02x(n− 2)
(b) y(n) = x(n)− 0.1x(n− 1)

The z-Transform and Its Application to the Analysis of LTI Systems

Prove the complex convolution relation given by (2.22).

in Table 2.
In Example 4.1 we solved for x(n), n < 0, by performing contour integrations for

Example 4.1.

222



33 Consider the sequence x(n) = anu(n), −1 < a < 1. Determine at least two se-
quences that are not equal to x(n) but have the same autocorrelation.

34 Compute the unit step response of the system with impulse response

h(n) =
{

3n, n < 0
( 25 )

n, n ≥ 0

35 Compute the zero-state response for the following pairs of systems and input signals.

(a) h(n) = ( 13 )nu(n), x(n) = ( 12 )n
(
cos π3 n

)
u(n)

(b) h(n) = ( 12 )nu(n), x(n) = ( 13 )nu(n)+ ( 12 )−nu(−n− 1)
(c) y(n) = −0.1y(n− 1)+ 0.2y(n− 2)+ x(n)+ x(n− 1)x(n) = ( 13 )nu(n)
(d) y(n) = 12x(n)− 12x(n− 1)x(n) = 10

(
cos π2 n

)
u(n)

(e) y(n) = −y(n− 2)+ 10x(n)x(n) = 10(cos π2 n)u(n)
(f) h(n) = ( 25 )nu(n), x(n) = u(n)− u(n− 7)
(g) h(n) = ( 12 )nu(n), x(n) = (−1)n , −∞ < n <∞
(h) h(n) = ( 12 )nu(n), x(n) = (n+ 1)( 14 )nu(n)

36 Consider the system

H(z) = 1− 2z
−1 + 2z−2 − z−3

(1− z−1)(1− 0.5z−1)(1− 0.2z−1) , ROC: 0.5|z| > 1

(a) Sketch the pole–zero pattern. Is the system stable?

(b) Determine the impulse response of the system.

37 Compute the response of the system

y(n) = 0.7y(n− 1)− 0.12y(n− 2)+ x(n− 1)+ x(n− 2)

to the input x(n) = nu(n). Is the system stable?
38 Determine the impulse response and the step response of the following causal sys-

tems. Plot the pole–zero patterns and determine which of the systems are stable.

(a) y(n) = 34y(n− 1)− 18y(n− 2)+ x(n)
(b) y(n) = y(n− 1)− 0.5y(n− 2)+ x(n)+ x(n− 1)
(c) H(z) = z−1(1+ z−1)

(1− z−1)3
(d) y(n) = 0.6y(n− 1)− 0.08y(n− 2)+ x(n)
(e) y(n) = 0.7y(n− 1)− 0.1y(n− 2)+ 2x(n)− x(n− 2)

39 Let x(n) be a causal sequence with z-transform X(z) whose pole–zero plot is shown

(a) x1(n) = x(−n+ 2)
(b) x2(n) = ej (π/3)nx(n)

The z-Transform and Its Application to the Analysis of LTI Systems

in Fig. P39. Sketch the pole–zero plots and the ROC of the following sequence:

223



Im(z)

Re(z)
−1 1

2
1
2

−

1
2

−

1
2

40 We want to design a causal discrete-time LTI system with the property that if the
input is

x(n) = (1
2
)nu(n)− 1

4
(
1
2
)n−1u(n− 1)

then the output is

y(n) = (1
3
)nu(n)

(a) Determine the impulse response h(n) and the system function H(z) of a system
that satisfies the foregoing conditions.

(b) Find the difference equation that characterizes this system.

(c) Determine a realization of the system that requires the minimum possible amount
of memory.

(d) Determine if the system is stable.

41 Determine the stability region for the causal system

H(z) = 1
1+ a1z−1 + a2z−2

by computing its poles and restricting them to be inside the unit circle.
42 Consider the system

H(z) = z
−1 + 12z−2

1− 35z−1 + 225z−2

Determine:

(a) The impulse response

(b) The zero-state step response

(c) The step response if y(−1) = 1 and y(−2) = 2

The z-Transform and Its Application to the Analysis of LTI Systems

Figure P39

224



43 Determine the system function, impulse response, and zero-state step response of

x(n) y(n)
z−1

a

z−1+

44 Consider the causal system

y(n) = −a1y(n− 1)+ b0x(n)+ b1x(n− 1)
Determine:
(a) The impulse response
(b) The zero-state step response
(c) The step response if y(−1) = A �= 0
(d) The response to the input

x(n) = cosω0n, 0 ≤ n <∞
45 Determine the zero-state response of the system

y(n) = 1
2
y(n− 1)+ 4x(n)+ 3x(n− 1)

to the input
x(n) = ejω0nu(n)

What is the steady-state response of the system?
46

(a) Determine the system function and the impulse response of the system given
that H(z)|z=1 = 1.

(b) Is the system stable?
(c) Sketch a possible implementation of the system and determine the corresponding

difference equations.
Im(z)

Re(z)

r = 1.5
r

π

6
θ = 

θ 

−θ −0.8

The z-Transform and Its Application to the Analysis of LTI Systems

Figure P43

the system shown in Fig P43.

Consider the causal system defined by the pole–zero pattern shown in Fig. P46.

Figure P46

225



47 Compute the convolution of the following pair of signals in the time domain and by
using the one-sided z-transform.

(a) x1(n) = {1, 1, 1↑, 1, 1}, x2(n) = {1↑, 1, 1}

(b) x1(n) = ( 12 )nu(n), x2(n) = ( 13 )nu(n)
(c) x1(n) = {1, 2↑, 3, 4}, x2(n) = {4, 3, 2↑, 1}

(d) x1(n) = {1↑, 1, 1, 1, 1}, x2(n) = {1↑, 1, 1}

Did you obtain the same results by both methods? Explain.
48 Determine the one-sided z-transform of the constant signal x(n) = 1, −∞ < n <∞.
49 Use the one-sided z-transform to determine y(n), n ≥ 0 in the following cases.

(a) y(n)+ 12y(n− 1)− 14y(n− 2) = 0; y(−1) = y(−2) = 1
(b) y(n)− 1.5y(n− 1)+ 0.5y(n− 2) = 0; y(−1) = 1, y(−2) = 0
(c) y(n) = 12y(n− 1)+ x(n)x(n) = ( 13 )nu(n), y(−1) = 1
(d) y(n) = 14y(n− 2)+ x(n)x(n) = u(n)y(−1) = 0; y(−2) = 1

50 An FIR LTI system has an impulse response h(n), which is real valued, even, and
has finite duration of 2N + 1. Show that if z1 = rejω0 is a zero of the system, then
z1 = (1/r)ejω0 is also a zero.

51

(a) Determine the ROC of the system function H(z) if the system is known to be
stable.

(b) It is possible for the given pole–zero plot to correspond to a causal and stable
system? If so, what is the appropriate ROC?

(c) How many possible systems can be associated with this pole–zero pattern?

Im(z)

1−0.5−3 2
Re(z)

The z-Transform and Its Application to the Analysis of LTI Systems

Consider an LTI discrete-time system whose pole–zero pattern is shown in Fig. P51.

Figure P51

226



52 Let x(n) be a causal sequence.
(a) What conclusion can you draw about the value of its z-transform X(z) at z = ∞?
(b) Use the result in part (a) to check which of the following transforms cannot be

associated with a causal sequence.

(i)X(z) = (z−
1
2 )

4

(z− 13 )3
(ii)X(z) = (1−

1
2z
−1)2

(1− 13z−1)
(iii)X(z) = (z−

1
3 )

2

(z− 12 )3

53 A causal pole–zero system is BIBO stable if its poles are inside the unit circle. Con-
sider now a pole–zero system that is BIBO stable and has its poles inside the unit
circle. Is the system always causal? [Hint: Consider the systems h1(n) = anu(n) and
h2(n) = anu(n+ 3), |a| < 1.]

54 Let x(n) be an anticausal signal [i.e., x(n) = 0 for n > 0]. Formulate and prove an
initial value theorem for anticausal signals.

55 The step response of an LTI system is

s(n) = (1
3
)n−2u(n+ 2)

(a) Find the system function H(z) and sketch the pole–zero plot.
(b) Determine the impulse response h(n).
(c) Check if the system is causal and stable.

56 Use contour integration to determine the sequence x(n) whose z-transform is given
by
(a) X(z) = 1

1− 12 z−1
, |z| > 12

(b) X(z) = 1
1− 12 z−1

, |z| < 12
(c) X(z) = z− a1− az , |z| > |1/a|
(d) X(z) = 1−

1
4 z
−1

1− 16 z−1− 16 z−2
, |z| > 12

57 Let x(n) be a sequence with z-transform

X(z) = 1− a
2

(1− az)(1− az−1) , ROC: a > |z| > 1/a

with 0 < a < 1. Determine x(n) by using contour integration.
58 The z-transform of a sequence x(n) is given by

X(z) = z
20

(z− 12 )(z− 2)5(z+ 52 )2(z+ 3)
Furthermore it is known that X(z) converges for |z| = 1.
(a) Determine the ROC of X(z).
(b) Determine x(n) at n = −18. (Hint: Use contour integration.)

The z-Transform and Its Application to the Analysis of LTI Systems

227



The z-Transform and Its Application to the Analysis of LTI Systems

Answers to Selected Problems

1 (a) X(z) = 3z5 + 6+ z−1 − 4z−2 ROC : 0 < |z| <∞
2 (a) X(z) = 1

(1−z−1)2

(d) X(z) = [az
−1−(az−1)3] sinw0

[1−(2a cosw0)z−1+a2z−2]
2 , |z| < a

(h) X(z) = 1−
(

1
2 z
−1)10

1− 12 z−1
, |z| > 12

4 (a) X(z) = z−1
(1+z−1)2 , |z| > 1

(f) X(z) = 1− z−2 + z−4 − z−5 , z 	= 0
8 (a) Y (z) = X(z)

1−z−1

12 (a) x(n) = [4(2)n − 3− n]u(n)
14 (a) x(n) = [2(−1)n − (−2)n]u(n)

(c) x(n) = −
[

3
5

(
1√
2

)n
cos π4 n+ 2310

(
1
√

2
)n

sin π4 n+ 1720 (12)n
]
u(n)

(j) x(n) = (− 1
a

)n+1
u(n)+ ( 1

a

)n−1
u(n− 1)

16 (a) y(n) =
[
− 43

(
1
4

)n + 13 + ( 12 )n] u(n)
(d) y(n) = [−2(n+ 1)+ 2n+1]u(n)

19 (b) x(n) = − ( 1
n

) (
1
2

)n
u(n− 1)

24 (a) x(n) = [0.136(0.28)n + 0.864(−1.78)n]u(n)
35 (a) y(n) =

[
1
7

(
1
3

)n 6
7

(
1
2

)n
cos πn3 + 3

√
3

7

(
1
2

)n
sin πn3

]
u(n)

(d) y(n) = 10√
2

sin
(
πn

2 + π4
)
u(n)

(h) y(n) =
[
4
(

1
2

)n − n ( 14 )n − 3 ( 14 )n]u(n)
38 (a) h(n) =

[
2
(

1
2

)n − ( 14 )n]u(n)
y(n) =

[
8
3 − 2

(
1
2

)n + 13 ( 14 )n]u(n)
(d) h(n) =

[
2
(

2
5

)n − ( 15 )n]u(n)
y(n) =

[
25
12 + 14

(
1
5

)n − 43 ( 25 )n]u(n)
42 (a) h(n) =

[
− 72

(
1
5

)n−1 + 92 ( 25 )n−1]u(n− 1)
44 (a) h(n) = b0δ(n)+ (b1− b0a1)(−a1)n−1u(n− 1)

(b) y(n) =
[
b0+b1
1+a1 +

a1b0−b1
1+a1 (−a1)

n
]
u(n)

49 (d) y(n) =
[

4
3 − 38

(
1
2

)n + 724 (− 12 )n]u(n)
56 (a) x(n) = ( 12 )n u(n)

(d) x(n) =
[

3
10

(
1
2

)n 7
10

(− 13 )n]u(n)
58 ROC: a < |z| < 1/a

x(−18) = −32/15309

228



John G. Proakis, Dimitris G. Manolakis. Copyright © 2007 by Pearson Education, Inc. All rights reserved.
From Chapter 4   of Digital Signal Processing: Principles, Algorithms, and Applications, Fourth Edition.

Frequency Analysis of Signals

229



Frequency Analysis of
Signals

As we shall demonstrate, most signals of practical interest can be decomposed
into a sum of sinusoidal signal components. For the class of periodic signals, such
a decomposition is called a Fourier series. For the class of finite energy signals, the
decomposition is called the Fourier transform. These decompositions are extremely
important in the analysis of LTI systems because the response of an LTI system to a
sinusoidal input signal is a sinusoid of the same frequency but of different amplitude
and phase. Furthermore, the linearity property of the LTI system implies that a linear
sum of sinusoidal components at the input produces a similar linear sum of sinusoidal
components at the output, which differ only in the amplitudes and phases from the
input sinusoids. This characteristic behavior of LTI systems renders the sinusoidal
decomposition of signals very important. Although many other decompositions of
signals are possible, only the class of sinusoidal (or complex exponential) signals
possess this desirable property in passing through an LTI system.

We begin our study of frequency analysis of signals with the representation of
continuous-time periodic and aperiodic signals by means of the Fourier series and the
Fourier transform, respectively. This is followed by a parallel treatment of discrete-
time periodic and aperiodic signals. The properties of the Fourier transform are
described in detail and a number of time-frequency dualities are presented.

The Fourier transform is one of several mathematical tools that is useful in the anal-

These signal representations basically involve the decomposition of the signals in 
terms of sinusoidal (or complex exponential) components. With such a decomposi-
tion, a signal is said to be represented in the frequency domain.

ysis and design of linear time  invariant (LTI) systems. Another is the Fourier series. -

230



1 Frequency Analysis of Continuous-Time Signals

It is well known that a prism can be used to break up white light (sunlight) into the

Society, Isaac Newton used the term spectrum to describe the continuous bands of
colors produced by this apparatus. To understand this phenomenon, Newton placed
another prism upside-down with respect to the first, and showed that the colors

prisms and blocking one or more colors from hitting the second prism, he showed that
the remixed light is no longer white. Hence the light passing through the first prism
is simply analyzed into its component colors without any other change. However,
only if we mix again all of these colors do we obtain the original white light.

Later, Joseph Fraunhofer (1787–1826), in making measurements of light emitted
by the sun and stars, discovered that the spectrum of the observed light consists of
distinct color lines. A few years later (mid-1800s) Gustav Kirchhoff and Robert
Bunsen found that each chemical element, when heated to incandescence, radiated
its own distinct color of light. As a consequence, each chemical element can be
identified by its own line spectrum.

From physics we know that each color corresponds to a specific frequency of
the visible spectrum. Hence the analysis of light into colors is actually a form of
frequency analysis.

Frequency analysis of a signal involves the resolution of the signal into its fre-
quency (sinusoidal) components. Instead of light, our signal waveforms are basically
functions of time. The role of the prism is played by the Fourier analysis tools that
we will develop: the Fourier series and the Fourier transform. The recombination
of the sinusoidal components to reconstruct the original signal is basically a Fourier
synthesis problem. The problem of signal analysis is basically the same for the case
of a signal waveform and for the case of the light from heated chemical composi-

(a) Analysis and
(b) synthesis of the white
light (sunlight) using glass
prisms.

Glass prism

White light

Beam of
sunlight

(b)

Glass prism
Violet

Blue
Green
Yellow
Orange
Red

Spectrum

Beam of
sunlight

(a)

Frequency Analysis of Signals

colors of the rainbow (see Fig. 1.1(a)). In a paper submitted in 1672 to the Royal

blended back into white light, as in Fig. 1.1(b). By inserting a slit between the two

Figure 1.1

231



tions. Just as in the case of chemical compositions, different signal waveforms have
different spectra. Thus the spectrum provides an “identity” or a signature for the
signal in the sense that no other signal has the same spectrum. As we will see, this
attribute is related to the mathematical treatment of frequency-domain techniques.

If we decompose a waveform into sinusoidal components, in much the same way
that a prism separates white light into different colors, the sum of these sinusoidal
components results in the original waveform. On the other hand, if any of these
components is missing, the result is a different signal.

In our treatment of frequency analysis, we will develop the proper mathematical
tools (“prisms”) for the decomposition of signals (“light”) into sinusoidal frequency
components (colors). Furthermore, the tools (“inverse prisms”) for synthesis of a
given signal from its frequency components will also be developed.

The basic motivation for developing the frequency analysis tools is to provide
a mathematical and pictorial representation for the frequency components that are
contained in any given signal. As in physics, the term spectrum is used when referring
to the frequency content of a signal. The process of obtaining the spectrum of a
given signal using the basic mathematical tools described in this chapter is known as
frequency or spectral analysis. In contrast, the process of determining the spectrum
of a signal in practice, based on actual measurements of the signal, is called spectrum
estimation. This distinction is very important. In a practical problem the signal to
be analyzed does not lend itself to an exact mathematical description. The signal
is usually some information-bearing signal from which we are attempting to extract
the relevant information. If the information that we wish to extract can be obtained
either directly or indirectly from the spectral content of the signal, we can perform

of the signal spectrum. In fact, we can view spectral estimation as a type of spectral
analysis performed on signals obtained from physical sources (e.g., speech, EEG,
ECG, etc.). The instruments or software programs used to obtain spectral estimates
of such signals are known as spectrum analyzers.

The Fourier Series for Continuous-Time Periodic Signals

In this section we present the frequency analysis tools for continuous-time periodic
signals. Examples of periodic signals encountered in practice are square waves, rect-
angular waves, triangular waves, and of course, sinusoids and complex exponentials.

The basic mathematical representation of periodic signals is the Fourier series,
which is a linear weighted sum of harmonically related sinusoids or complex expo-
nentials. Jean Baptiste Joseph Fourier (1768–1830), a French mathematician, used
such trigonometric series expansions in describing the phenomenon of heat conduc-
tion and temperature distribution through bodies. Although his work was motivated
by the problem of heat conduction, the mathematical techniques that he developed
during the early part of the nineteenth century now find application in a variety of
problems encompassing many different fields, including optics, vibrations in mechan-
ical systems, system theory, and electromagnetics.

Frequency Analysis of Signals

Here, we will deal with spectral analysis. However, the subject of power spec-
trum estimation is beyond the scope of this chapter.

1.1

spectrum estimation on the information-bearing signal, and thus obtain an estimate

232



x(t) =
∞∑

k=−∞
cke

j2πkF0t

is a periodic signal with fundamental period Tp = 1/F0 . Hence we can think of the
exponential signals

{ej2πkF0t , k = 0,±1,±2, . . .}
as the basic “building blocks” from which we can construct periodic signals of various
types by proper choice of the fundamental frequency and the coefficients {ck}. F0
determines the fundamental period of x(t) and the coefficients {ck} specify the shape
of the waveform.

Suppose that we are given a periodic signal x(t)with period Tp

frequency F0 is selected to be the reciprocal of the given period Tp . To determine
k

complex exponential
e−j2πF0lt

where l is an integer and then integrate both sides of the resulting equation over a
single period, say from 0 to Tp , or more generally, from t0 to t0 + Tp , where t0 is an
arbitrary but mathematically convenient starting value. Thus we obtain

∫ t0+Tp
t0

x(t)e−j2πlF0t dt =
∫ t0+Tp
t0

e−j2πlF0t
( ∞∑
k=−∞

cke
+j2πkF0t

)
dt

of the summation and integration and combine the two exponentials. Hence

∞∑
k=−∞

ck

∫ t0+Tp
t0

ej2πF0(k−l)t dt =
∞∑

k=−∞
ck

[
ej2πF0(k−l)t

j2πF0(k − l)
]t0+Tp
t0

0
and t0 + Tp , respectively, yields zero. On the other hand, if k = l , we have

∫ t0+Tp
t0

dt = t
∣∣∣∣∣
t0+Tp

t0

= Tp

∫ t0+Tp
t0

x(t)e−j2πlF0t dt = clTp

Frequency Analysis of Signals

(1.1)

(1.2)

(1.3)

Recall that a linear combination of harmonically related complex exponentials 
of the form 

the periodic signal by the series (1.1), called a Fourier series, where the fundamental
. We can represent

the expression for the coefficients {c }, we first multiply both sides of (1.1) by the

To evaluate the integral on the right-hand side of (1.2), we interchange the order

For k �= l , the right-hand side of (1.3) evaluated at the lower and upper limits, t

Consequently, (1.2) reduces to

233



and therefore the expression for the Fourier coefficients in terms of the given periodic
signal becomes

cl = 1
Tp

∫ t0+Tp
t0

x(t)e−j2πlF0t dt

Since t0 is arbitrary, this integral can be evaluated over any interval of length Tp , that

for the Fourier series coefficients will be written as

cl = 1
Tp

∫
Tp

x(t)e−j2πlF0t dt

An important issue that arises in the representation of the periodic signal x(t)
by the Fourier series is whether or not the series converges to x(t) for every value of
t , that is, whether the signal x(t) and its Fourier series representation

∞∑
k=−∞

cke
j2πkF0t

are equal at every value of t . The so-called Dirichlet conditions guarantee that
for which x(t) is

discontinuous.
value) of the discontinuity. The Dirichlet conditions are:

1. The signal x(t) has a finite number of discontinuities in any period.

2. The signal x(t) contains a finite number of maxima and minima during any
period.

3. The signal x(t) is absolutely integrable in any period, that is,∫
Tp

|x(t)| dt <∞

All periodic signals of practical interest satisfy these conditions.

The weaker condition, that the signal has finite energy in one period,∫
Tp

|x(t)|2 dt <∞

guarantees that the energy in the difference signal

e(t) = x(t)−
∞∑

k=−∞
cke

j2πkF0t

is zero, although x(t) and its Fourier series may not be equal for all values of t . Note

Frequency Analysis of Signals

is, over any interval equal to the period of the signal x(t). Consequently, the integral

(1.4)

(1.5)

(1.6)

(1.7)

the series (1.5) will be equal to x(t), except at the values of t
At these values of t , (1.5) converges to the midpoint (average

that (1.6) implies (1.7), but not vice versa. Also, both (1.7) and the Dirichlet

234



conditions are sufficient but not necessary conditions (i.e., there are signals that have
a Fourier series representation but do not satisfy these conditions).

In summary, if x(t) is periodic and satisfies the Dirichlet conditions, it can be

Synthesis equation x(t) =
∞∑

k=−∞
cke

j2πkF0t

Analysis equation ck = 1
Tp

∫
Tp

x(t)e−j2πkF0t dt

In general, the Fourier coefficients ck are complex valued. Moreover, it is easily
shown that if the periodic signal is real, ck and c−k are complex conjugates. As a
result, if

ck = |ck|ejθk
then

c−k = |ck|−jθk
Consequently, the Fourier series may also be represented in the form

x(t) = c0 + 2
∞∑
k=1
|ck| cos(2πkF0t + θk)

where c0 is real valued when x(t) is real.
Finally, we should indicate that yet another form for the Fourier series can be

cos(2πkF0t + θk) = cos 2πkF0t cos θk − sin 2πkF0t sin θk

x(t) = a0 +
∞∑
k=1
(ak cos 2πkF0t − bk sin 2πkF0t)

where

a0 = c0
ak = 2|ck | cos θk
bk = 2|ck | sin θk

the
Fourier series representation of a real periodic signal.

Frequency Analysis of Signals

(1.8)

Frequency Analysis of Continuous-Time Periodic Signals

(1.9)

(1.10)

(1.11)

represented in a Fourier series as in (1.1), where the coefficients are specified by
(1.4). These relations are summarized below.

obtained by expanding the cosine function in (1.10) as

Consequently, we can rewrite (1.10) in the form

The expressions in (1.8), (1.10), and (1.11) constitute three equivalent forms for

235



1.2 Power Density Spectrum of Periodic Signals

A periodic signal has infinite energy and a finite average power, which is given as

Px = 1
Tp

∫
Tp

|x(t)|2 dt

∗
obtain

Px = 1
Tp

∫
Tp

x(t)

∞∑
k=−∞

c∗k e
−j2πkF0t dt

=
∞∑

k=−∞
c∗k

[
1
Tp

∫
Tp

x(t)e−j2πkF0t dt

]

=
∞∑

k=−∞
|ck|2

Therefore, we have established the relation

Px = 1
Tp

∫
Tp

|x(t)|2 dt =
∞∑

k=−∞
|ck|2

which is called Parseval’s relationfor power signals.

single complex exponential
x(t) = ckej2πkF0t

In this case, all the Fourier series coefficients except ck are zero. Consequently, the
average power in the signal is

Px = |ck|2

It is obvious that |ck|2 represents the power in the k th harmonic component of the
signal. Hence the total average power in the periodic signal is simply the sum of the
average powers in all the harmonics.

If we plot the |ck|2 as a function of the frequencies kF0 , k = 0,±1,±2, . . . , the di-
agram that we obtain shows how the power of the periodic signal is distributed among
the various frequency components.
is called the power density spectrum1of the periodic signal x(t). Since the power
in a periodic signal exists only at discrete values of frequencies (i.e., F = 0, ±F0 ,
±2F0, . . .), the signal is said to have a line spectrum. The spacing between two consec-
utive spectral lines is equal to the reciprocal of the fundamental period Tp , whereas
the shape of the spectrum (i.e., the power distribution of the signal), depends on the
time-domain characteristics of the signal.

1 This function is also called the power spectral density or, simply, the power spectrum.

Frequency Analysis of Signals

(1.12)

(1.13)

(1.14)

If we take the complex conjugate of (1.8) and substitute for x (t) in (1.12), we

To illustrate the physical meaning of (1.14), suppose that x(t) consists of a

This diagram, which is illustrated in Fig. 1.2,

236



Power density spectrum |ck|2

… …

0 F0 2F0 3F0−4F0 −3F0 −2F0 −F0 4F0 Frequency, F

Power density spectrum of a continuous-time periodic
signal.

As indicated in the preceding section, the Fourier series coefficients {ck} are
complex valued, that is, they can be represented as

ck = |ck|ejθk

where
θk = �ck

Instead of plotting the power density spectrum, we can plot the magnitude voltage
spectrum {|ck|} and the phase spectrum {θk} as a function of frequency. Clearly, the
power spectral density in the periodic signal is simply the square of the magnitude
spectrum. The phase information is totally destroyed (or does not appear) in the
power spectral density.

If the periodic signal is real valued, the Fourier series coefficients {ck} satisfy the
condition

c−k = c∗k
Consequently, |ck|2 = |c∗k |2 . Hence the power spectrum is a symmetric function
of frequency. This condition also means that the magnitude spectrum is symmetric
(even function) about the origin and the phase spectrum is an odd function. As a
consequence of the symmetry, it is sufficient to specify the spectrum of a real periodic
signal for positive frequencies only. Furthermore, the total average power can be
expressed as

Px = c20 + 2
∞∑
k=1
|ck|2

= a20 +
1
2

∞∑
k=1
(a2k + b2k )

k k

and {ck} coefficients in the Fourier series expressions.

Frequency Analysis of Signals

(1.15)

(1.16)

Figure 1.2

which follows directly from the relationships given in Section 1.1 among {a }, {b },

237



Continuous-time periodic
train of rectangular pulses.

x(t)

A

t

… …

0 τ
2

Tp−Tp τ
2

−

Determine the Fourier series and the power density spectrum of the rectangular pulse train

Solution. The signal is periodic with fundamental period Tp and, clearly, satisfies the Dirich-

Since x(t) is an even signal [i.e., x(t) = x(−t)], it is convenient to select the integration
interval from −Tp p

c0 = 1
Tp

∫ Tp/2
−Tp/2

x(t) dt = 1
Tp

∫ τ/2
−τ/2

A dt = Aτ
Tp

The term c0
have

ck = 1
Tp

∫ τ/2
−τ/2

Ae−j2πkF0t dt = A
Tp

[
e−j2πF0kt

−j2πkF0

]τ/2
−τ/2

= A
πF0kTp

ejπkF0τ − e−jπkF0τ
j2

= Aτ
Tp

sin πkF0τ
πkF0τ

, k = ±1,±2, . . .

φ = πkF0τ . In this case φ takes on discrete values since F0 and τ are fixed and the index
k varies. However, if we plot (sin φ)/φ with φ as a continuous parameter over the range

to zero as φ→±∞, has a maximum value of unity at φ = 0, and is zero at multiples of π (i.e.,

sample values of the (sinφ)/φ function for φ = πkF0τ and scaled in amplitude by Aτ/Tp .

1

0
−π−2π−3π−4π−5π−6π−7π 7π φ6π5π4π3π2ππ0

sin φ
φ

The function (sin φ)/φ .

Frequency Analysis of Signals

(1.17)

represents the average value (dc component) of the signal x(t) . For k �= 0 we

(1.18)

Figure 1.3

EXAMPLE 1.1

signal illustrated in Fig 1.3.

let conditions. Consequently, we can represent the signal in the Fourier series given by (1.8)
with the Fourier coefficients specified by (1.9).

/2 to T /2. Thus (1.9) evaluated for k = 0 yields

It is interesting to note that the right-hand side of (1.18) has the form (sin φ)/φ , where

−∞ < φ <∞, we obtain the graph shown in Fig 1.4. We observe that this function decays

at φ = mπ , m = ±1, ±2, . . .). It is clear that the Fourier coefficients given by (1.18) are the

Figure 1.4

238



Since the periodic function x(t) is even, the Fourier coefficients ck are real. Consequently,
the phase spectrum is either zero, when ck is positive, or π when ck is negative. Instead of
plotting the magnitude and phase spectra separately, we may simply plot {ck} on a single
graph, showing both the positive and negative values ck on the graph. This is commonly done
in practice when the Fourier coefficients {ck} are real.

p

is fixed and the pulse width τ is allowed to vary. In this case Tp = 0.25 second, so that
F0 = 1/Tp = 4 Hz and τ = 0.05Tp , τ = 0.1Tp , and τ = 0.2Tp . We observe that the effect
of decreasing τ while keeping Tp fixed is to spread out the signal power over the frequency
range. The spacing between adjacent spectral lines is F0 = 4 Hz, independent of the value of
the pulse width τ .

On the other hand, it is also instructive to fix τ and vary the period Tp when Tp > τ .
p = 5τ , Tp = 10τ , and Tp = 20τ . In this case, the

spacing between adjacent spectral lines decreases as Tp increases. In the limit as Tp →∞, the
Fourier coefficients ck approach zero due to the factor of Tp
This behavior is consistent with the fact that as Tp → ∞ and τ remains fixed, the resulting
signal is no longer a power signal. Instead, it becomes an energy signal and its average power
is zero. The spectra of finite energy signals are described in the next section.

We also note that if k �= 0 and sin(πkF0τ) = 0, then ck = 0. The harmonics with zero
power occur at frequencies kF0 such that π(kF0)τ = mπ , m = ±1, ±2, . . . , or at kF0 = m/τ .
For example, if F0 = 4 Hz and τ = 0.2Tp , it follows that the spectral components at ±20 Hz,
±40 Hz, . . . have zero power. These frequencies correspond to the Fourier coefficients ck ,
k = ±5, ±10, ±15, . . . . On the other hand, if τ = 0.1Tp
power are k = ±10, ±20, ±30, . . . .

The power density spectrum for the rectangular pulse train is

|ck|2 =




(
Aτ

Tp

)2
, k = 0(

Aτ

Tp

)2 ( sinπkF0τ
πkF0τ

)2
, k = ±1,±2, . . .

Fourier coefficients of the
rectangular pulse train when
Tp is fixed and the pulse
width τ varies.

ck

F

τ = 0.2Tp

ck

F

τ = 0.1Tp

τ = 0.05Tp
ck

F0

Frequency Analysis of Signals

, the spectral components with zero

(1.19)

Figure 1.5 illustrates the Fourier coefficients of the rectangular pulse train when T

Figure 1.6 illustrates this condition when T

in the denominator of (1.18).

Figure 1.5

239



Fourier coefficient of a
rectangular pulse train
with fixed pulse width
τ and varying period
Tp .

ck

F

Tp = 5τ

Tp = 10τ

Tp = 20τ

ck

F

ck

F

0

0

0

The Fourier Transform for Continuous-Time Aperiodic Signals

linear combination of harmonically related complex exponentials. As a consequence
of the periodicity, we saw that these signals possess line spectra with equidistant lines.
The line spacing is equal to the fundamental frequency, which in turn is the inverse
of the fundamental period of the signal. We can view the fundamental period as
providing the number of lines per unit of frequency (line density), as illustrated in

With this interpretation in mind, it is apparent that if we allow the period to
increase without limit, the line spacing tends toward zero. In the limit, when the
period becomes infinite, the signal becomes aperiodic and its spectrum becomes
continuous. This argument suggests that the spectrum of an aperiodic signal will be
the envelope of the line spectrum in the corresponding periodic signal obtained by
repeating the aperiodic signal with some period Tp .

From this aperiodic signal, we can create a periodic signal xp(t) with period Tp , as
p p →∞, that is,

x(t) = lim
Tp→∞

xp(t)

This interpretation implies that we should be able to obtain the spectrum of x(t)
from the spectrum of xp(t) simply by taking the limit as Tp →∞.

We begin with the Fourier series representation of xp(t),

xp(t) =
∞∑

k=−∞
cke

j2πkF0t , F0 = 1
Tp

Frequency Analysis of Signals

(1.20)

Figure 1.6

1.3

In Section 1.1 we developed the Fourier series to represent a periodic signal as a

Fig 1.6.

Let us consider an aperiodic signal x(t)with finite duration as shown in Fig 1.7(a).

shown in Fig 1.7(b). Clearly, x (t) = x(t) in the limit as T

240



(a) Aperiodic signal x(t)
and (b) periodic signal xp(t)
constructed by repeating
x(t) with a period Tp .

0

(a)

Tp/2

x(t)

t−Tp/2

0

(b)

Tp/2

x(t)

tTp/2−Tp/2−Tp

……

where

ck = 1
Tp

∫ Tp/2
−Tp/2

xp(t)e
−j2πkF0t dt

p p p

ck = 1
Tp

∫ Tp/2
−Tp/2

x(t)e−j2πkF0t dt

It is also true that x(t) = 0 for |t| > Tp/2. Consequently, the limits on the integral

ck = 1
Tp

∫ ∞
−∞

x(t)e−j2πkF0t dt

Let us now define a function X(F), called the Fourier transform of x(t), as

X(F) =
∫ ∞
−∞

x(t)e−j2πF t dt

X(F ) is a function of the continuous variable F . It does not depend on Tp or F0 .

ck can be expressed in terms of X(F) as

ck = 1
Tp
X(kF0)

or equivalently,

Tpck = X(kF0) = X
(
k

Tp

)

Frequency Analysis of Signals

(1.21)

(1.22)

(1.23)

(1.24)

(1.25)

Figure 1.7

Since x (t) = x(t) for −T /2 ≤ t ≤ T /2, (1.21) can be expressed as

in (1.22) can be replaced by −∞ and∞. Hence

However, if we compare (1.23) and (1.24), it is clear that the Fourier coefficients

241



Thus the Fourier coefficients are samples of X(F) taken at multiples of F0 and scaled
by F0 (multiplied by 1/Tp ). Substitution for ck

xp(t) = 1
Tp

∞∑
k=−∞

X

(
k

Tp

)
ej2πkF0t

p approaches infinity. First, we define
p

xp(t) =
∞∑

k=−∞
X(k�F)ej2πk �F t�F

It is clear that in the limit as Tp approaches infinity, xp(t) reduces to x(t). Also, �F
becomes the differential dF and k �F becomes the continuous frequency variable

F . Thus

lim
Tp→∞

xp(t) = x(t) = lim
�F→0

∞∑
k=−∞

X(k�F)e−j2πk �F t�F

x(t) =
∫ ∞
−∞

X(F)ej2πF t dF

This integral relationship yields x(t) when X(F) is known, and it is called the inverse
Fourier transform.

This concludes our heuristic derivation of the Fourier transform pair given by

ematically rigorous, it led to the desired Fourier transform relationships with rela-
tively simple intuitive arguments. In summary, the frequency analysis of continuous-
time aperiodic signals involves the following Fourier transform pair.

Frequency Analysis of Continuous-Time Aperiodic Signals

Synthesis equation (inverse transform) x(t) =
∫ ∞
−∞

X(F)ej2πF t dF

Analysis equation (direct transform) X(F) =
∫ ∞
−∞

x(t)e−j2πF t dt

It is apparent that the essential difference between the Fourier series and the
Fourier transform is that the spectrum in the latter case is continuous and hence
the synthesis of an aperiodic signal from its spectrum is accomplished by means of
integration instead of summation.

Frequency Analysis of Signals

(1.26)

(1.27)

(1.28)

(1.29)

(1.30)

from (1.25) into (1.20) yields

We wish to take the limit of (1.26) as T
�F = 1/T . With this substitution, (1.26) becomes

F . In turn, the summation in (1.27) becomes an integral over the frequency variable

(1.24) and (1.28) for an aperiodic signal x(t). Although the derivation is not math-

242



x(t) = 1
2π

∫ ∞
−∞

X(�)ej�t d�

X(�) =
∫ ∞
−∞

x(t)e−j�t dt

The set of conditions that guarantee the existence of the Fourier transform is the
Dirichlet conditions, which may be expressed as:

1. The signal x(t) has a finite number of finite discontinuities.

2. The signal x(t) has a finite number of maxima and minima.

3. The signal x(t) is absolutely integrable, that is,

∫ ∞
−∞
|x(t)| dt <∞

The third condition follows easily from the definition of the Fourier transform, given

|X(F)| =
∣∣∣∣
∫ ∞
−∞

x(t)e−j2πF t dt
∣∣∣∣ ≤

∫ ∞
−∞
|x(t)| dt

A weaker condition for the existence of the Fourier transform is that x(t) has
finite energy; that is, ∫ ∞

−∞
|x(t)|2 dt <∞

Note that if a signal x(t) is absolutely integrable, it will also have finite energy. That
is, if ∫ ∞

−∞
|x(t)| dt <∞

then

Ex =
∫ ∞
−∞
|x(t)|2 dt <∞

not be absolutely integrable. For example, the signal

x(t) = sin 2πF0t
πt

Frequency Analysis of Signals

can be expressed in terms of the radian frequency variable � = 2πF . Since dF =

(1.31)

(1.32)

(1.33)

(1.34)

(1.35)

However, the converse is not true. That is, a signal may have finite energy but may

(1.36)

Finally, we wish to indicate that the Fourier transform pair in (1.29) and (1.30)

d�/2π , (1.29) and (1.30) become

in (1.30). Indeed,

Hence |X(F)| <∞ if (1.33) is satisfied.

243



is square integrable but is not absolutely integrable. This signal has the Fourier
transform

X(F) =
{

1, |F | ≤ F0
0, |F | > F0

sufficient but not necessary for the existence of the Fourier transform. In any case,
nearly all finite energy signals have a Fourier transform, so that we need not worry
about the pathological signals, which are seldom encountered in practice.

1.4 Energy Density Spectrum of Aperiodic Signals

Let x(t) be any finite energy signal with Fourier transform X(F). Its energy is

Ex =
∫ ∞
−∞
|x(t)|2 dt

which, in turn, may be expressed in terms of X(F) as follows:

Ex =
∫ ∞
−∞

x(t)x∗(t) dt

=
∫ ∞
−∞

x(t) dt

[∫ ∞
−∞

X∗(F )e−j2πF t dF
]

=
∫ ∞
−∞

X∗(F ) dF
[∫ ∞
−∞

x(t)e−j2πF t dt
]

=
∫ ∞
−∞
|X(F)|2 dF

Therefore, we conclude that

Ex =
∫ ∞
−∞
|x(t)|2 dt =

∫ ∞
−∞
|X(F)|2 dF

This is Parseval’s relation for aperiodic, finite energy signals and expresses the prin-
ciple of conservation of energy in the time and frequency domains.

The spectrum X(F) of a signal is, in general, complex valued. Consequently, it
is usually expressed in polar form as

X(F) = |X(F)|ej�(F)

where |X(F)| is the magnitude spectrum and �(F) is the phase spectrum,
�(F) = �X(F)

On the other hand, the quantity

Sxx(F ) = |X(F)|2

Frequency Analysis of Signals

(1.37)

(1.38)

(1.39)

Since this signal violates (1.33), it is apparent that the Dirichlet conditions are

244



as a function of frequency. Hence Sxx(F ) is called the energy density spectrum of
x(t). The integral of Sxx(F ) over all frequencies gives the total energy in the signal.
Viewed in another way, the energy in the signal x(t) over a band of frequencies
F1 ≤ F ≤ F1 +�F is ∫ F1+�F

F1

Sxx(F ) dF ≥ 0

which implies that Sxx(f ) ≥ 0 for all F .
xx(F ) does not contain any phase information

[i.e., Sxx(F ) is purely real and nonnegative]. Since the phase spectrum of x(t) is not
contained in Sxx(F ), it is impossible to reconstruct the signal given Sxx(F ).

Finally, as in the case of Fourier series, it is easily shown that if the signal x(t) is
real, then

|X(−F)| = |X(F)|
�X(−F) = −�X(F)

Sxx(−F) = Sxx(F )
In other words, the energy density spectrum of a real signal has even symmetry.

Determine the Fourier transform and the energy density spectrum of a rectangular pulse signal
defined as

x(t) =
{
A, |t | ≤ τ/2
0, |t | > τ/2

Solution. Clearly, this signal is aperiodic and satisfies the Dirichlet conditions. Hence its

X(F) =
∫ τ/2
−τ/2

Ae−j2πF t dt = Aτ sinπFτ
πFτ

We observe that X(F) is real and hence it can be depicted graphically using only one diagram,

Hence the spectrum of the rectangular pulse is the envelope of the line spectrum
(Fourier coefficients) of the periodic signal obtained by periodically repeating the pulse with
period Tp In other words, the Fourier coefficients ck in the corresponding
periodic signal xp(t) are simply samples of X(F) at frequencies kF0 = k/Tp . Specifically,

ck = 1
Tp
X(kF0) = 1

Tp
X

(
k

Tp

)

the width of the main lobe, which contains most of the signal energy, is equal to 2/τ . As the

Frequency Analysis of Signals

(1.40)

(1.41)

(1.42)

(1.43)

(1.44)

(1.45)

which is the integrand in (1.38), represents the distribution of energy in the signal

From (1.39) we observe that S

By combining (1.40) and (1.39), we obtain

EXAMPLE 1.2

and illustrated in Fig 1.8(a).

Fourier transform exists. By applying (1.30), we find that

as shown in Fig 1.8(b). Obviously, X(F) has the shape of the (sinφ)/φ function shown in
Fig 1.4.

as in Fig 1.3.

From (1.44) we note that the zero crossings of X(F) occur at multiples of 1/τ . Furthermore,

245



(a) Rectangular
pulse and
(b) its Fourier
transform.

0

(a)

τ

x(t)

A

2
tτ

2

0

(b)

X(F)

Aτ

F1
τ

−2
τ

− 2
τ

1
τ

−

pulse duration τ decreases (increases), the main lobe becomes broader (narrower) and more

Fourier
transform of a
rectangular
pulse for
various width
values.

A

x(t)

0 tτ
2

τ

2
−

Aτ

X(F)

0 F1
τ

− 1
τ

A

x(t)

0 tτ
2

τ

2
−

A

x(t)

0 tτ
2

τ

2
−

Aτ

X(F)

0 F1
τ

− 1
τ

Aτ X(F)

0 F

τ
1−

τ
1

Frequency Analysis of Signals

Figure 1.8

energy is moved to the higher (lower) frequencies, as illustrated in Fig 1.9. Thus as the signal

Figure 1.9

246



pulse is expanded (compressed) in time, its transform is compressed (expanded) in frequency.
This behavior, between the time function and its spectrum, is a type of uncertainty principle
that appears in different forms in various branches of science and engineering.

Finally, the energy density spectrum of the rectangular pulse is

Sxx(F ) = (Aτ)2
(

sinπFτ
πFτ

)2

2 Frequency Analysis of Discrete-Time Signals

periodic (power) signals and the Fourier transform for finite energy aperiodic signals.
In this section we repeat the development for the class of discrete-time signals.

resentation of a continuous-time periodic signal can consist of an infinite number
of frequency components, where the frequency spacing between two successive har-
monically related frequencies is 1/Tp , and where Tp is the fundamental period. Since
the frequency range for continuous-time signals extends from−∞ to∞, it is possible
to have signals that contain an infinite number of frequency components. In contrast,
the frequency range for discrete-time signals is unique over the interval (−π, π) or
(0, 2π). A discrete-time signal of fundamental period N can consist of frequency
components separated by 2π/N radians or f = 1/N cycles. Consequently, the
Fourier series representation of the discrete-time periodic signal will contain at most
N frequency components. This is the basic difference between the Fourier series
representations for continuous-time and discrete-time periodic signals.

2.1 The Fourier Series for Discrete-Time Periodic Signals

Suppose that we are given a periodic sequence x(n) with period N , that is, x(n) =
x(n+N) for all n. The Fourier series representation for x(n) consists of N harmon-
ically related exponential functions

ej2πkn/N , k = 0, 1, . . . , N − 1

and is expressed as

x(n) =
N−1∑
k=0

cke
j2πkn/N

where the {ck} are the coefficients in the series representation.

mula:
N−1∑
n=0

ej2πkn/N =
{
N, k = 0,±N,±2N, . . .
0, otherwise

Frequency Analysis of Signals

(1.46)

(2.1)

To derive the expression for the Fourier coefficients, we use the following for-

(2.2)

In Section 1 we developed the Fourier series representation for continuous-time

As we have observed from the discussion of Section 1, the Fourier series rep-

247



The

formula
N−1∑
n=0

an =
{
N, a = 1
1− aN
1− a , a �= 1

The expression for the Fourier coefficients ck can be obtained by multiplying
−j2πln/N and summing the product from

n = 0 to n = N − 1. Thus
N−1∑
n=0

x(n)e−j2πln/N =
N−1∑
n=0

N−1∑
k=0

cke
j2π(k−l)n/N

obtain
N−1∑
n=0

ej2π(k−l)n/N =
{
N, k − l = 0,±N,±2N, . . .
0, otherwise

to Ncl and hence

cl = 1
N

N−1∑
n=0

x(n)e−j2πln/N , l = 0, 1, . . . , N − 1

Thus we have the desired expression for the Fourier coefficients in terms of the signal
x(n).

signals are summarized below.

Frequency Analysis of Discrete-Time Periodic Signals

Synthesis equation x(n) =
N−1∑
k=0

cke
j2πkn/N

Analysis equation ck = 1
N

N−1∑
n=0

x(n)e−j2πkn/N

Fourier coefficients {ck}, k = 0, 1, . . . , N − 1 provide the description of x(n) in the
frequency domain, in the sense that ck represents the amplitude and phase associated
with the frequency component

sk(n) = ej2πkn/N = ejωkn

where ωk = 2πk/N .

Frequency Analysis of Signals

(2.3)

(2.4)

(2.5)

(2.6)

(2.7)

(2.8)

Note the similarity of (2.2) with the continuous-time counterpart in (1.3).
proof of (2.2) follows immediately from the application of the geometric summation

both sides of (2.1) by the exponential e

If we perform the summation over n first, in the right-hand side of (2.4), we

where we have made use of (2.2). Therefore , the right-hand side of (2.4) reduces

The relationships (2.1) and (2.6) for the frequency analysis of discrete-time

Equation (2.7) is often called the discrete-time Fourier series (DTFS). The

248



ck+N = 1
N

N−1∑
n=0

x(n)e−j2π(k+N)n/N = 1
N

N−1∑
n=0

x(n)e−j2πkn/N = ck

Therefore, the Fourier series coefficients {ck} form a periodic sequence when ex-
tended outside of the range k = 0, 1, . . . , N − 1. Hence

ck+N = ck

that is, {ck} is a periodic sequence with fundamental period N . Thus the spectrum
of a signal x(n), which is periodic with period N , is a periodic sequence with period
N . Consequently, any N consecutive samples of the signal or its spectrum provide a
complete description of the signal in the time or frequency domains.

Although the Fourier coefficients form a periodic sequence, we will focus our
attention on the single period with range k = 0, 1, . . . , N − 1. This is convenient,
since in the frequency domain, this amounts to covering the fundamental range
0 ≤ ωk = 2πk/N < 2π , for 0 ≤ k ≤ N − 1. In contrast, the frequency range
−π < ωk = 2πk/N ≤ π corresponds to −N/2 < k ≤ N/2, which creates an
inconvenience when N is odd. Clearly, if we use a sampling frequency Fs , the range
0 ≤ k ≤ N − 1 corresponds to the frequency range 0 ≤ F < Fs .

Determine the spectra of the signals

(a) x(n) = cos√2πn
(b) x(n) = cosπn/3
(c) x(n) is periodic with period N = 4 and x(n) = {1

↑
, 1, 0, 0}

Solution.

(a) For ω0 =
√

2π , we have f0 = 1/
√

2. Since f0 is not a rational number, the signal is not
periodic. Consequently, this signal cannot be expanded in a Fourier series. Nevertheless,
the signal does possess a spectrum. Its spectral content consists of the single frequency
component at ω = ω0 =

√
2π .

(b) In this case f0 = 16 and hence x(n) is periodic with fundamental period N = 6. From

ck = 16
5∑
n=0

x(n)e−j2πkn/6, k = 0, 1, . . . , 5

Frequency Analysis of Signals

(2.9)

Recall that the functions sk(n) are periodic with period N. Hence sk(n) = sk(n + N).  
In view of this periodicity, it follows that the  Fourier  coefficients ck , when viewed beyond 
the

 
range k = 0, 1, . . . , N − 1, also satisfy a periodicity condition. Indeed, from (2.8), which 

holds for every value of k, we have

EXAMPLE 2.1

(2.8) we have

249



However, x(n) can be expressed as

x(n) = cos 2πn
6
= 1

2
ej2πn/6 + 1

2
e−j2πn/6

In comparing
1 = 12 . The second

also be written as

e−j2πn/6 = ej2π(5−6)n/6 = ej2π(5n)/6

which means that c−1 = c5 .
observation that the Fourier series coefficients form a periodic sequence of period N .
Consequently, we conclude that

c0 = c2 = c3 = c4 = 0

c1 = 12 , c5 =
1
2

(c)

ck = 14
3∑
n=0

x(n)e−j2πkn/4, k = 0, 1, 2, 3

or

ck = 14 (1+ e
−jπk/2), k = 0, 1, 2, 3

For k = 0, 1, 2, 3 we obtain

c0 = 12 , c1 =
1
4
(1− j), c2 = 0, c3 = 14 (1+ j)

The magnitude and phase spectra are

|c0| = 12 , |c1| =
√

2
4
, |c2| = 0, |c3| =

√
2

4

�c0 = 0, �c1 = −π4 , �c2 = undefined, �c3 =
π

4

Frequency Analysis of Signals

which is already in the form of the exponential Fourier series in (2.7).
the two exponential terms in x(n) with (2.7), it is apparent that c
exponential in x(n) corresponds to the term k = −1 in (2.7). However, this term can

But this is consistent with (2.9), and with our previous

From (2.8), we have

Figure 2.1 illustrates the spectral content of the signals in (b) and (c).

250



Spectra of the periodic
signals discussed in

1

0−1−5 1 2 3 4 5 6

2

ck

k

……

(a)

1

0−1−2−3 1 2 3 4

2

|ck|

k

……

(b)

π

0−1−2
−3 1 5

2 3 4

4

π

4

�ck

k

……

(c)

−

2.2 Power Density Spectrum of Periodic Signals

Px = 1
N

N−1∑
n=0
|x(n)|2

We shall now derive an expression for Px in terms of the Fourier coefficient {ck}.

Px = 1
N

N−1∑
n=0

x(n)x∗(n)

= 1
N

N−1∑
n=0

x(n)

(
N−1∑
k=0

c∗ke
−j2πkn/N

)

Frequency Analysis of Signals

(2.10)

The average power of a discrete-time periodic signal with period N is defined as

Figure 2.1

Example 2.1 (b) and (c).

If we use the relation (2.7) in (2.10), we have

251



obtaining

Px =
N−1∑
k=0

c∗k

[
1
N

N−1∑
n=0

x(n)e−j2πkn/N
]

=
N−1∑
k=0
|ck|2 = 1

N

N−1∑
n=0
|x(n)|2

which is the desired expression for the average power in the periodic signal. In other
words, the average power in the signal is the sum of the powers of the individual
frequency components.
periodic signals. The sequence |ck|2 for k = 0, 1, . . . , N − 1 is the distribution of
power as a function of frequency and is called the power density spectrum of the
periodic signal.

If we are interested in the energy of the sequence x(n) over a single period,

EN =
N−1∑
n=0
|x(n)|2 = N

N−1∑
k=0
|ck|2

which is consistent with our previous results for continuous-time periodic signals. If
∗

can easily show that
c∗k = c−k

or equivalently,

|c−k| = |ck| (even symmetry)
−�c−k = �ck (odd symmetry)

These symmetry properties for the magnitude and phase spectra of a periodic signal,
in conjunction with the periodicity property, have very important implications on the
frequency range of discrete-time signals.

|ck| = |cN−k|

and
�ck = −�cN−k

More specifically, we have

|c0| = |cN |, �c0 = −�cN = 0
|c1| = |cN−1|, �c1 = −�cN−1
|cN/2| = |cN/2|, �cN/2 = 0 if N is even
|c(N−1)/2| = |c(N+1)/2|, �c(N−1)/2 = −�c(N+1)/2 if N is odd

Frequency Analysis of Signals

(2.11)

(2.12)

(2.13)

(2.14)

(2.15)

(2.16)

(2.17)

(2.18)

Now, we can interchange the order of the two summations and make use of (2.8),

We view (2.11) as a Parseval’s relation for discrete-time

(2.11) implies that

the signal x(n) is real [i.e., x (n) = x(n)], then, proceeding as in Section 2.1, we

Indeed, by combining (2.9) with (2.14) and (2.15), we obtain

252



Thus, for a real signal, the spectrum ck , k = 0, 1, . . . , N/2 for N even, or k = 0,
1, . . . , (N − 1)/2 for N odd, completely specifies the signal in the frequency domain.
Clearly, this is consistent with the fact that the highest relative frequency that can be
represented by a discrete-time signal is equal to π . Indeed, if 0 ≤ ωk = 2πk/N ≤ π ,
then 0 ≤ k ≤ N/2.

By making use of these symmetry properties of the Fourier series coefficients

forms

x(n) = c0 + 2
L∑
k=1
|ck | cos

(
2π
N
kn+ θk

)

= a0 +
L∑
k=1

(
ak cos

2π
N
kn− bk sin 2π

N
kn

)

where a0 = c0 , ak = 2|ck| cos θk , bk = 2|ck| sin θk , and L = N/2 if N is even and
L = (N − 1)/2 if N is odd.

Finally, we note that as in the case of continuous-time signals, the power density
spectrum |ck|2 does not contain any phase information. Furthermore, the spectrum
is discrete and periodic with a fundamental period equal to that of the signal itself.

EXAMPLE 2.2 Periodic “Square-Wave” Signal

Determine the Fourier series coefficients and the power density spectrum of the periodic signal

Solution.

ck = 1
N

N−1∑
n=0

x(n)e−j2πkn/N = 1
N

L−1∑
n=0

Ae−j2πkn/N , k = 0, 1, . . . , N − 1

which is a geometric summation.
Thus we obtain

ck = A
N

L−1∑
n=0
(e−j2πk/N )n =



AL

N
, k = 0

A

N

1− e−j2πkL/N
1− e−j2πk/N , k = 1, 2, . . . , N − 1

Discrete-time periodic
square-wave signal. 0 L N−N n

x(n)

A

Frequency Analysis of Signals

(2.19)

(2.20)

of a real signal, the Fourier series in (2.7) can also be expressed in the alternative

shown in Fig 2.2.

By applying the analysis equation (2.8) to the signal shown in Fig 2.2, we obtain

Now we can use (2.3) to simplify the summation above.

Figure 2.2

253



The last expression can be simplified further if we note that

1− e−j2πkL/N
1− e−j2πk/N =

e−jπkL/N

e−jπk/N
ejπkL/N − e−jπkL/N
ejπk/N − e−jπk/N

= e−jπk(L−1)/N sin(πkL/N)
sin(πk/N)

Therefore,

ck =



AL

N
, k = 0,+N,±2N, . . .

A

N
e−jπk(L−1)/N

sin(πkL/N)
sin(πk/N)

, otherwise

The power density spectrum of this periodic signal is

|ck|2 =




(
AL

N

)2
, k = 0,+N,±2N, . . .(

A

N

)2 ( sinπkL/N
sinπk/N

)2
, otherwise

k
2 for L = 2, N = 10 and 40, and A = 1.

2.3 The Fourier Transform of Discrete-Time Aperiodic Signals

Just as in the case of continuous-time aperiodic energy signals, the frequency analysis
of discrete-time aperiodic finite-energy signals involves a Fourier transform of the
time-domain signal. Consequently, the development in this section parallels, to a

The Fourier transform of a finite-energy discrete-time signal x(n) is defined as

X(ω) =
∞∑

n=−∞
x(n)e−jωn

Physically, X(ω) represents the frequency content of the signal x(n). In other words,
X(ω) is a decomposition of x(n) into its frequency components.

We observe two basic differences between the Fourier transform of a discrete-
time finite-energy signal and the Fourier transform of a finite-energy analog signal.
First, for continuous-time signals, the Fourier transform, and hence the spectrum of
the signal, have a frequency range of (−∞,∞). In contrast, the frequency range for a
discrete-time signal is unique over the frequency interval of (−π, π) or, equivalently,
(0, 2π). This property is reflected in the Fourier transform of the signal. Indeed,X(ω)

Frequency Analysis of Signals

(2.21)

(2.22)

(2.23)

Figure 2.3 illustrates the plots of |c |

large extent, that given in Section 1.3.

254



1. 5 1 0. 5 0 0.5 1 1.5
2

1

0

1

2

3

4

5

Frequency (Cycles/Sampling interval)

N
 c

k

L = 2, N = 10

 1.5  1  0.5 0 0.5 1 1.5
 2

 1

0

1

2

3

4

5

Frequency (Cycles/Sampling interval)

N
| c

k

L = 2, N = 40

|
|

|
is periodic with period 2π , that is,

X(ω + 2πk) =
∞∑

n=−∞
x(n)e−j (ω+2πk)n

=
∞∑

n=−∞
x(n)e−jωne−j2πkn

=
∞∑

n=−∞
x(n)e−jωn = X(ω)

Hence X(ω) is periodic with period 2π . But this property is just a consequence of
the fact that the frequency range for any discrete-time signal is limited to (−π, π) or

Frequency Analysis of Signals

(2.24)

Figure 2.3 Plot of the power density spectrum given by (2.22).

255



(0, 2π), and any frequency outside this interval is equivalent to a frequency within
the interval.

The second basic difference is also a consequence of the discrete-time nature
of the signal. Since the signal is discrete in time, the Fourier transform of the signal
involves a summation of terms instead of an integral, as in the case of continuous-time
signals.

Since X(ω) is a periodic function of the frequency variable ω , it has a Fourier
series expansion, provided that the conditions for the existence of the Fourier se-
ries, described previously, are satisfied. In fact, from the definition of the Fourier

the form of a Fourier series. The Fourier coefficients in this series expansion are the
values of the sequence x(n).

To demonstrate this point, let us evaluate the sequence x(n) from X(ω). First,
jωm and integrate over the interval (−π, π). Thus

we have ∫ π
−π
X(ω)ejωm dω =

∫ π
−π

[ ∞∑
n=−∞

x(n)e−jωn
]
ejωm dω

the order of summation and integration. This interchange can be made if the series

XN(ω) =
N∑

n=−N
x(n)e−jωn

converges uniformly to X(ω) as N → ∞. Uniform convergence means that, for
every ω , XN(ω)→ X(ω), as N →∞. The convergence of the Fourier transform is
discussed in more detail in the following section. For the moment, let us assume that
the series converges uniformly, so that we can interchange the order of summation

∫ π
−π
ejω(m−n) dω =

{
2π, m = n
0, m �= n

Consequently,

∞∑
n=−∞

x(n)

∫ π
−π
ejω(m−n) dω =

{
2πx(m), m = n
0, m �= n

x(n) = 1
2π

∫ π
−π
X(ω)ejωn dω

expression for the Fourier series coefficient for a function that is periodic with period

Frequency Analysis of Signals

(2.25)

(2.26)

(2.27)

transform X(ω) of the sequence x(n), given by (2.23), we observe that X(ω) has

we multiply both sides (2.23) by e

The integral on the right-hand side of (2.25) can be evaluated if we can interchange

and integration in (2.25). Then

By combining (2.25) and (2.26), we obtain the desired result that

If we compare the integral in (2.27) with (1.9), we note that this is just the

256



the integrand, which is a consequence of our definition of the Fourier transform as

In summary, the Fourier transform pair for discrete-time signals is as follows.

Frequency Analysis of Discrete-Time Aperiodic Signals

Synthesis equation (inverse transform) x(n) = 1
2π

∫
2π
X(ω)ejωn dω

Analysis equation (direct transform) X(ω) =
∞∑

n=−∞
x(n)e−jωn

2.4 Convergence of the Fourier Transform

series

XN(ω) =
N∑

n=−N
x(n)e−jωn

→ ∞. By
uniform convergence we mean that for each ω ,

lim
N→∞

{
supω|X(ω)−XN(ω)|

} = 0

∞∑
n=−∞

|x(n)| <∞

then

|X(ω)| =
∣∣∣∣∣
∞∑

n=−∞
x(n)e−jωn

∣∣∣∣∣ ≤
∞∑

n=−∞
|x(n)| <∞

transform. We note that this is the discrete-time counterpart of the third Dirichlet
condition for the Fourier transform of continuous-time signals. The first two condi-
tions do not apply due to the discrete-time nature of {x(n)}.

Some sequences are not absolutely summable, but they are square summable.
That is, they have finite energy

Ex =
∞∑

n=−∞
|x(n)|2 <∞

Frequency Analysis of Signals

(2.28)

(2.29)

(2.30)

(2.31)

Uniform convergence is guaranteed if x(n) is absolutely summable. Indeed, if

(2.32)

(2.33)

2π . The only difference between (1.9) and (2.27) is the sign on the exponent in

given by (2.23). Therefore, the Fourier transform of the sequence x(n), defined by
(2.23), has the form of a Fourier series expansion.

In the derivation of the inverse transform given by (2.28), we assumed that the

converges uniformly to X(ω), given in the integral of (2.25), as N

Hence (2.32) is a sufficient condition for the existence of the discrete-time Fourier

257



We would like to define the Fourier
transform of finite-energy sequences, but we must relax the condition of uniform
convergence. For such sequences we can impose a mean-square convergence condi-
tion:

lim
N→∞

∫ π
−π
|X(ω)−XN(ω)|2 dω = 0

Thus the energy in the error X(ω)−XN(ω) tends toward zero, but the error |X(ω)−
XN(ω)| does not necessarily tend to zero. In this way we can include finite-energy
signals in the class of signals for which the Fourier transform exists.

Let us consider an example from the class of finite-energy signals. Suppose that

X(ω) =
{

1, |ω| ≤ ωc
0, ωc < |ω| ≤ π

represents only one period of X(ω). The inverse transform of X(ω) results in the
sequence

x(n) = 1
2π

∫ π
−π
X(ω)ejωn dω

= 1
2π

∫ ωc
−ωc

ejωn dω = sinωcn
πn

, n �= 0

For n = 0, we have
x(0) = 1

2π

∫ ωc
c dω = ωc

π

Hence

x(n) =



ωc

π
, n = 0

ωc

π

sinωcn
ωcn

, n �= 0

x(n) = sinωcn
πn

, −∞ < n <∞

with the understanding that at n = 0, x(n) = ωc/π . We should emphasize, however,
that (sinωcn)/πn is not a continuous function, and hence L’Hospital’s rule cannot
be used to determine x(0).

Frequency Analysis of Signals

(2.34)

(2.35)

The reader should remember that X(ω) is periodic with period 2π . Hence (2.35)

(2.36)

(2.37)

which is a weaker condition than (2.32).

This transform pair is illustrated in Fig 2.4.

−ω

Sometimes, the sequence {x(n)} in (2.36) is expressed as

258



x(n)

X(ω)

n
0

0

1

ωc
π

ωc

π
ωc

π

π−π −ωc ωc

−

(a)

(b)

Now let us consider the determination of the Fourier transform of the sequence

series
∞∑

n=−∞
x(n)e−jωn =

∞∑
n=−∞

sinωcn
πn

e−jωn

does not converge uniformly for all ω . However, the sequence {x(n)} has a finite
energy Ex = ωc/π as will be shown in Section 4.3.

To elaborate on this point, let us consider the finite sum

XN(ω) =
N∑

n=−N

sinωcn
πn

e−jωn

N(ω) for several values of N . We note that there is
a significant oscillatory overshoot at ω = ωc , independent of the value of N . As N
increases, the oscillations become more rapid, but the size of the ripple remains the
same. One can show that as N → ∞, the oscillations converge to the point of the

c

is satisfied, and therefore XN(ω) converges to X(ω) in the mean-square sense.

Frequency Analysis of Signals

(2.39)

(2.38)

Figure 2.4 Fourier transform pair in (2.35) and (2.36).

given by (2.37). The sequence {x(n)} is not absolutely summable. Hence the infinite

Hence the sum in (2.38) is
guaranteed to converge to the X(ω) given by (2.35) in the mean-square sense.

Figure 2.5 shows the function X

discontinuity at ω = ω , but their amplitude does not go to zero. However, (2.34)

259



Illustration of convergence of the Fourier transform
and the Gibbs phenomenon at the point of discontinuity.

The oscillatory behavior of the approximation XN(ω) to the function X(ω) at
a point of discontinuity of X(ω) is called the Gibbs phenomenon. A similar effect
is observed in the truncation of the Fourier series of a continuous-time periodic

For example, the truncation of the

the same oscillatory behavior in the finite-sum approximation of x(t). The Gibbs

systems.

2.5 Energy Density Spectrum of Aperiodic Signals

Recall that the energy of a discrete-time signal x(n) is defined as

Ex =
∞∑

n=−∞
|x(n)|2

Frequency Analysis of Signals

(2.40)

phenomenon is encountered again in the design of practical, discrete-time FIR

Figure 2.5

signal, given by the synthesis equation (1.8).
Fourier series for the periodic square-wave signal in Example 1.1 gives rise to

260



Let us now express the energy Ex in terms of the spectral characteristic X(ω). First
we have

Ex =
∞∑

n=−∞
x∗(n)x(n) =

∞∑
n=−∞

x(n)

[
1

2π

∫ π
−π
X∗(ω)e−jωn dω

]

If we interchange the order of integration and summation in the equation above, we
obtain

Ex = 12π
∫ π
−π
X∗(ω)

[ ∞∑
n=−∞

x(n)e−jωn
]
dω

= 1
2π

∫ π
−π
|X(ω)|2 dω

Therefore, the energy relation between x(n) and X(ω) is

Ex =
∞∑

n=−∞
|x(n)|2 = 1

2π

∫ π
−π
|X(ω)|2 dω

This is Parseval’s relation for discrete-time aperiodic signals with finite energy.
The spectrum X(ω) is, in general, a complex-valued function of frequency. It

may be expressed as
X(ω) = |X(ω)|ej�(ω)

where
�(ω) = �X(ω)

is the phase spectrum and |X(ω)| is the magnitude spectrum.
As in the case of continuous-time signals, the quantity

Sxx(ω) = |X(ω)|2

represents the distribution of energy as a function of frequency, and it is called the
energy density spectrum of x(n). Clearly, Sxx(ω) does not contain any phase infor-
mation.

Suppose now that the signal x(n) is real. Then it easily follows that

X∗(ω) = X(−ω)
or equivalently,

|X(−ω)| = |X(ω)|, (even symmetry)
and

�X(−ω) = −�X(ω), (odd symmetry)

Sxx(−ω) = Sxx(ω), (even symmetry)

Frequency Analysis of Signals

(2.47)

(2.46)

(2.45)

(2.44)

(2.43)

(2.42)

(2.41)

From (2.43) it also follows that

261



From these symmetry properties we conclude that the frequency range of real
discrete-time signals can be limited further to the range 0 ≤ ω ≤ π (i.e., one-half of
the period). Indeed, if we know X(ω) in the range 0 ≤ ω ≤ π , we can determine it
for the range −π ≤ ω < 0 using the symmetry properties given above. As we have
already observed, similar results hold for discrete-time periodic signals. Therefore,
the frequency-domain description of a real discrete-time signal is completely specified
by its spectrum in the frequency range 0 ≤ ω ≤ π .

Usually, we work with the fundamental interval 0 ≤ ω ≤ π or 0 ≤ F ≤ Fs/2,
expressed in hertz. We sketch more than half a period only when required by the
specific application.

Determine and sketch the energy density spectrum Sxx(ω) of the signal

x(n) = anu(n), −1 < a < 1

Solution. Since |a| < 1, the sequence x(n) is absolutely summable, as can be verified by
applying the geometric summation formula,

∞∑
n=−∞

|x(n)| =
∞∑
n=0
|a|n = 1

1− |a| <∞

X(ω) =
∞∑
n=0

ane−jωn =
∞∑
n=0
(ae−jω)n

Since |ae−jω| = |a| < 1, use of the geometric summation formula again yields

X(ω) = 1
1− ae−jω

The energy density spectrum is given by

Sxx(ω) = |X(ω)|2 = X(ω)X(ω) = 1
(1− ae−jω)(1− aejω)

or, equivalently, as

Sxx(ω) = 11− 2a cosω + a2

Note that Sxx(−ω) = Sxx

−0.5. Note that for a = −0.5 the signal has more rapid variations and as a result its spectrum
has stronger high frequencies.

Frequency Analysis of Signals

EXAMPLE 2.3

Hence the Fourier transform of x(n) exists and is obtained by applying (2.29). Thus

(ω) in accordance with (2.47).
Figure 2.6 shows the signal x(n) and its corresponding spectrum for a = 0.5 and a =

262



0

x(n)

5 10 15 20 n

0

x(n)

5 10 15 20 n

0
2

1
2
3

4

π

(b)(a)

−π − π
2
π

Sxx(ω)

0
2

1
2
3
4

π−π − π
2
π

Sxx(ω)

a = 0.5

a = −0.5

Figure 2.6 (a) Sequence x(n) = ( 12 )nu(n) and x(n) = (− 12 )nu(n); (b) their energy
density spectra.

Determine the Fourier transform and the energy density spectrum of the sequence

x(n) =
{
A, 0 ≤ n ≤ L− 1
0, otherwise

Solution. Before computing the Fourier transform, we observe that

∞∑
n=−∞

|x(n)| =
L−1∑
n=0
|A| = L|A| <∞

Hence x(n) is absolutely summable and its Fourier transform exists. Furthermore, we note
that x(n) is a finite-energy signal with Ex = |A|2L.

Discrete-time rectangular
pulse. 0 L − 1 n

x(n)

A
…

Frequency Analysis of Signals

(2.48)

EXAMPLE 2.4

which is illustrated in Fig 2.7.

Figure 2.7

263



The Fourier transform of this signal is

X(ω) =
L−1∑
n=0

Ae−jωn

= A1− e
−jωL

1− e−jω

= Ae−j (ω/2)(L−1) sin(ωL/2)
sin(ω/2)

the indeterminate form when ω = 0.
The magnitude and phase spectra of x(n) are

|X(ω)| =


|A|L, ω = 0
|A|

∣∣∣∣ sin(ωL/2)sin(ω/2)
∣∣∣∣ , otherwise

and

�X(ω) = �A− ω
2
(L− 1)+� sin(ωL/2)

sin(ω/2)

where we should remember that the phase of a real quantity is zero if the quantity is positive
and π if it is negative.

Magnitude and phase of
Fourier transform of the
discrete-time rectangular

0
2

1

π

2

3

4

5

−π − π
2
π

ω

|X(ω)|

Θ(ω)

2
π−π − π

2
− π

−π

2
π

2
π

π

ω

Frequency Analysis of Signals

(2.49)

(2.50)

(2.51)

For ω = 0 the transform in (2.49) yields X(0) = AL, which is easily established by setting
ω = 0 in the defining equation for X(ω), or by using L’Hospital’s rule in (2.49) to resolve

The spectra |X(ω)| and �X(ω) are shown in Fig 2.8 for the case A = 1 and L = 5. The
energy density spectrum is simply the square of the expression given in (2.50).

Figure 2.8

pulse in Fig 2.7.

264



There is an interesting relationship between the Fourier transform of the constant

equally spaced (harmonically related) frequencies

ωk = 2π
N
k, k = 0, 1, . . . , N − 1

we obtain

X

(
2π
N
k

)
= Ae−j (π/N)k(L−1) sin[(π/N)kL]

sin[(π/N)k]

If we compare this result with the expression for the Fourier series coefficients given

X

(
2π
N
k

)
= Nck, k = 0, 1, . . . , N − 1

To elaborate, we have established that the Fourier transform of the rectangular pulse,
which is identical with a single period of the periodic rectangular pulse train, evalu-
ated at the frequencies ω = 2πk/N, k = 0, 1, . . . , N − 1, which are identical to the
harmonically related frequency components used in the Fourier series representa-
tion of the periodic signal, is simply a multiple of the Fourier coefficients {ck} at the
corresponding frequencies.

pulse evaluated at ω = 2πk/N , k = 0, 1, . . . , N − 1, and the Fourier coefficients of
the corresponding periodic signal, is not only true for these two signals but, in fact,
holds in general.

2.6 Relationship of the Fourier Transform to the z-Transform

The z-transform of a sequence x(n) is defined as

X(z) =
∞∑

n=−∞
x(n)z−n, ROC: r2 < |z| < r1

where r2 < |z| < r1 is the region of convergence of X(z). Let us express the complex
variable z in polar form as

z = rejω

where r = |z| and ω = �z. Then, within the region of convergence of X(z), we can
substitute z = rejω

X(z)|z=rejω =
∞∑

n=−∞
[x(n)r−n]e−jωn

Frequency Analysis of Signals

(2.52)

(2.53)

(2.54)

(2.55)

(2.56)

amplitude pulse in Example 2.4 and the periodic rectangular wave considered in
Example 2.2. If we evaluate the Fourier transform as given in (2.49) at a set of

in (2.21) for the periodic rectangular wave, we find that

The relationship given in (2.53) for the Fourier transform of the rectangular

into (2.54). This yields

265



Fourier transform of the signal sequence x(n)r−n . The weighting factor r−n is grow-
ing with n if r < 1 and decaying if r > 1. Alternatively, if X(z) converges for |z| = 1,
then

X(z)|z=ejω ≡ X(ω) =
∞∑

n=−∞
x(n)e−jωn

Therefore, the Fourier transform can be viewed as the z-transform of the sequence
evaluated on the unit circle. If X(z) does not converge in the region |z| = 1 [i.e.,
if the unit circle is not contained in the region of convergence of X(z)], the Fourier

We should note that the existence of the z-transform requires that the sequence
{x(n)r−n} be absolutely summable for some value of r , that is,

∞∑
n=−∞

|x(n)r−n| <∞

0
but the Fourier transform does not exist. This is the case, for example, for causal
sequences of the form x(n) = anu(n), where |a| > 1.

2
1

0
1

2 2
1

0
1

2

0

5

10

15

20

Im(z)

ln
|X

(z
) |

Re(z)

0
0

2

4

6

8

10
|X(ω)|

π/2 π−π  −π/2

ω = ±π

ω = π/2

ω = 0

ω = −π
/2

0

X(z)

Imz

X(ejω)

ejω

Unit Circle    Rez
ω

relationship between X(z) and X(ω) for the sequence in Exam-

Frequency Analysis of Signals

(2.57)

(2.58)

From the relationship in (2.56) we note that X(z) can be interpreted as the

transform X(ω) does not exist. Figure 2.9 illustrates the relationship between X(z)
and X(ω) for the rectangular sequence in Example 2.4, where A = 1 and L = 10.

Hence if (2.58) converges only for values of r > r > 1, the z-transform exists,

Figure 2.9
ple 2.4, with A = 1 and L = 10

266



for example, the sequence

x(n) = sinωcn
πn

, −∞ < n <∞

This sequence does not have a z-transform. Since it has a finite energy, its Fourier
transform converges in the mean-square sense to the discontinuous function X(ω),
defined as

X(ω) =
{

1, |ω| < ωc
0, ωc < |ω| ≤ π

for some region in the z-plane. If this region contains the unit circle, the Fourier
transform X(ω) exists. However, the existence of the Fourier transform, which is
defined for finite energy signals, does not necessarily ensure the existence of the
z-transform.

2.7 The Cepstrum

Let us consider a sequence {x(n)} having a z-transform X(z). We assume that {x(n)}
is a stable sequence so that X(z) converges on the unit circle. The complex cepstrum
of the sequence {x(n)} is defined as the sequence {cx(n)}, which is the inverse z-
transform of Cx(z), where

Cx(z) = lnX(z)
The complex cepstrum exists if Cx(z) converges in the annular region r1 < |z| <

r2 , where 0 < r1 < 1 and r2 > 1. Within this region of convergence, Cx(z) can be
represented by the Laurent series

Cx(z) = lnX(z) =
∞∑

n=−∞
cx(n)z

−n

where

cx(n) = 12πj
∫
C

lnX(z)zn−1 dz

C is a closed contour about the origin and lies within the region of convergence.
x

{cx(n)} is stable. Furthermore, if the complex cepstrum exists, Cx(z) converges on
the unit circle and hence we have

Cx(ω) = lnX(ω) =
∞∑

n=−∞
cx(n)e

−jωn

where {cx(n)} is the sequence obtained from the inverse Fourier transform of lnX(ω),
that is,

cx(n) = 12π
∫ π
−π

lnX(ω)ejωn dω

Frequency Analysis of Signals

(2.59)

(2.60)

(2.61)

(2.62)

(2.63)

(2.64)

(2.65)

There are sequences, however, that do not satisfy the requirement in (2.58),

In conclusion, the existence of the z-transform requires that (2.58) be satisfied

Clearly, if C (z) can be represented as in (2.62), the complex cepstrum sequence

267



If we express X(ω) in terms of its magnitude and phase, say

X(ω) = |X(ω)|ejθ(ω)

then
lnX(ω) = ln |X(ω)| + jθ(ω)

cx(n) = 12π
∫ π
−π

[ln |X(ω)| + jθ(ω)]ejωn dω

transforms of ln |X(ω)| and θ(ω):

cm(n) = 12π
∫ π
−π

ln |X(ω)|ejωn dω

cθ(n) = 12π
∫ π
−π
θ(ω)ejωn dω

In some applications, such as speech signal processing, only the component cm(n) is
computed. In such a case the phase of X(ω) is ignored. Therefore, the sequence
{x(n)} cannot be recovered from {cm(n)}. That is, the transformation from {x(n)} to
{cm(n)} is not invertible.

In speech signal processing, the (real) cepstrum has been used to separate and
thus to estimate the spectral content of the speech from the pitch frequency of the
speech. The complex cepstrum is used in practice to separate signals that are con-
volved. The process of separating two convolved signals is called deconvolution and
the use of the complex cepstrum to perform the separation is called homomorphic
deconvolution.

2.8 The Fourier Transform of Signals with Poles on the Unit Circle

determined by evaluating its z-transform X(z) on the unit circle, provided that the
unit circle lies within the region of convergence of X(z). Otherwise, the Fourier
transform does not exist.

There are some aperiodic sequences that are neither absolutely summable nor
square summable. Hence their Fourier transforms do not exist. One such sequence
is the unit step sequence, which has the z-transform

X(z) = 1
1− z−1

Another such sequence is the causal sinusoidal signal sequence x(n) = (cosω0n)u(n).
This sequence has the z-transform

X(z) = 1− z
−1 cosω0

1− 2z−1 cosω0 + z−2

Frequency Analysis of Signals

(2.66)

(2.67)

(2.68)

(2.69)

(2.70)

By substituting (2.67) into (2.65), we obtain the complex cepstrum in the form

We can separate the inverse Fourier transform in (2.68) into the inverse Fourier

As was shown in Section 2.6, the Fourier transform of a sequence x(n) can be

268



Note that both of these sequences have poles on the unit circle.
For sequences such as these two examples, it is sometimes useful to extend the

Fourier transform representation. This can be accomplished, in a mathematically
rigorous way, by allowing the Fourier transform to contain impulses at certain fre-
quencies corresponding to the location of the poles of X(z) that lie on the unit circle.
The impulses are functions of the continuous frequency variable ω and have infinite
amplitude, zero width, and unit area. An impulse can be viewed as the limiting form
of a rectangular pulse of height 1/a and width a , in the limit as a → 0. Thus, by
allowing impulses in the spectrum of a signal, it is possible to extend the Fourier trans-
form representation to some signal sequences that are neither absolutely summable
nor square summable.

The following example illustrates the extension of the Fourier transform repre-
sentation for three sequences.

(a) x1(n) = u(n)
(b) x2(n) = (−1)nu(n)
(c) x3(n) = (cosω0n)u(n)
by evaluating their z-transforms on the unit circle.

Solution.

(a) From

X1(z) = 11− z−1 =
z

z− 1 , ROC: |z| > 1

X1(z) has a pole, p1 = 1, on the unit circle, but converges for |z| > 1.
If we evaluate X1(z) on the unit circle, except at z = 1, we obtain

X1(ω) = e
jω/2

2j sin(ω/2)
= 1

2 sin(ω/2)
ej (ω−π/2), ω �= 2πk, k = 0, 1, . . .

At ω = 0 and multiples of 2π , X1(ω) contains impulses of area π .
Hence the presence of a pole at z = 1 (i.e., at ω = 0) creates a problem only when we

want to compute |X1(ω)| at ω = 0,because |X1(ω)| → ∞ as ω→ 0. For any other value
of ω , X1(ω) is finite (i.e., well behaved). Although at first glance one might expect the
signal to have zero-frequency components at all frequencies except at ω = 0, this is not
the case. This happens because the signal x1(n) is not a constant for all −∞ < n < ∞.
Instead, it is turned on at n = 0. This abrupt jump creates all frequency components
existing in the range 0 < ω ≤ π . Generally, all signals which start at a finite time have
nonzero-frequency components everywhere in the frequency axis from zero up to the
folding frequency.

(b) that the z-transform of anu(n) with a = −1 reduces to

X2(z) = 11+ z−1 =
z

z+ 1 , ROC: |z| > 1

Frequency Analysis of Signals

common

Determine the Fourier transform of the following signals.

EXAMPLE 2.5

Given

z-transform  pairs we find that

269



which has a pole at z = −1 = ejπ . The Fourier transform evaluated at frequencies other
than ω = π and multiples of 2π is

X2(ω) = e
jω/2

2 cos(ω/2)
, ω �= 2π(k + 1

2
), k = 0, 1, . . .

In this case the impulse occurs at ω = π + 2πk .
Hence the magnitude is

|X2(ω)| = 12| cos(ω/2)| , ω �= 2πk + π, k = 0, 1, . . .

and the phase is

�X2(ω) =




ω
2 , if cos

ω

2
≥ 0

ω
2 + π, if cos

ω

2
< 0

Note that due to the presence of the pole at a = −1 (i.e., at frequency ω = π ), the
magnitude of the Fourier transform becomes infinite. Now |X(ω)| → ∞ as ω→ π . We
observe that (−1)nu(n) = (cosπn)u(n), which is the fastest possible oscillating signal in
discrete time.

(c) From the discussion above, it follows that X3(ω) is infinite at the frequency component
ω = ω0 . Indeed, we find that

x3(n) = (cosω0n)u(n) z←→ X3(z) = 1− z
−1 cosω0

1− 2z−1 cosω0 + z−2 , ROC: |z| > 1

The Fourier transform is

X3(ω) = 1− e
−jω cosω0

(1− e−j (ω−ω0))(1− ej (ω+ω0)) , ω �= ±ω0 + 2πk, k = 0, 1, . . .

The magnitude of X3(ω) is given by

|X3(ω)| = |1− e
−jω cosω0|

|1− e−j (ω−ω0 )||1− e−j (ω+ω0 )| , ω �= ±ω0 + 2πk, k = 0, 1, . . .

Now if ω = −ω0 or ω = ω0 , |X3(ω)| becomes infinite. For all other frequencies, the
Fourier transform is well behaved.

Frequency Analysis of Signals

270



0
F

Xa(F)

(a)

0−π
ω

X(ω)

π

0
F

Xa(F)

(b)

0−π
ω

X(ω)

π

0
F

Xa(F)

(c)

0−π
ω

X(ω)

π

(a) Low-frequency, (b) high-frequency, and (c) medium-frequency
signals.

2.9 Frequency-Domain Classification of Signals: The Concept
of Bandwidth

Just as we have classified signals according to their time-domain characteristics, it is
also desirable to classify signals according to their frequency-domain characteristics.
It is common practice to classify signals in rather broad terms according to their
frequency content.

In particular, if a power signal (or energy signal) has its power density spectrum
(or its energy density spectrum) concentrated about zero frequency, such a signal is

of such a signal. On the other hand, if the signal power density spectrum (or the
energy density spectrum) is concentrated at high frequencies, the signal is called a

having a power density spectrum (or an energy density spectrum) concentrated some-
where in the broad frequency range between low frequencies and high frequencies

Frequency Analysis of Signals

Figure 2.10

called a low-frequency signal. Figure 2.10(a) illustrates the spectral characteristics

high-frequency signal. Such a signal spectrum is illustrated in Fig 2.10(b). A signal

271



0

X(F)

Aperiodic signals Periodic signals

B
F

−B 0

ck

MF0
kF0−MF0

0

X(ω)

ω0
ω

−ω0−π π 0−π π

ck

ω
M2π
N

M2π
N

C
on

tin
uo

us
-

tim
e

D
is

cr
et

e-
tim

e

−

Figure 2.11 Some examples of bandlimited signals.

such a signal spectrum.
In addition to this relatively broad frequency-domain classification of signals,

it is often desirable to express quantitatively the range of frequencies over which
the power or energy density spectrum is concentrated. This quantitative measure is
called the bandwidth of a signal. For example, suppose that a continuous-time signal
has 95% of its power (or energy) density spectrum concentrated in the frequency
range F1 ≤ F ≤ F2 . Then the 95% bandwidth of the signal is F2 − F1 . In a similar
manner, we may define the 75% or 90% or 99% bandwidth of the signal.

In the case of a bandpass signal, the term narrowband is used to describe the
signal if its bandwidth F2 − F1 is much smaller (say, by a factor of 10 or more) than
the median frequency (F2 + F1)/2. Otherwise, the signal is called wideband.

We shall say that a signal is bandlimited if its spectrum is zero outside the fre-
quency range |F | ≥ B . For example, a continuous-time finite-energy signal x(t)
is bandlimited if its Fourier transform X(F) = 0 for |F | > B . A discrete-time
finite-energy signal x(n) is said to be (periodically) bandlimited if

|X(ω)| = 0, for ω0 < |ω| < π

Similarly, a periodic continuous-time signal xp(t) is periodically bandlimited if its
Fourier coefficients ck = 0 for |k| > M , where M is some positive integer. A
periodic discrete-time signal with fundamental period N is periodically bandlimited
if the Fourier coefficients ck = 0 for k0
types of bandlimited signals.

By exploiting the duality between the frequency domain and the time domain, we
can provide similar means for characterizing signals in the time domain. In particular,
a signal x(t) will be called time-limited if

x(t) = 0, |t | > τ

Frequency Analysis of Signals

is called a medium-frequency signal or a bandpass signal.Figure 2.10(c) illustrates

< |k| < N . Figure 2.11 illustrates the four

272



If the signal is periodic with period Tp , it will be called periodically time-limited if

xp(t) = 0, τ < |t | < Tp/2

If we have a discrete-time signal x(n) of finite duration, that is,

x(n) = 0, |n| > N

it is also called time-limited. When the signal is periodic with fundamental period
N , it is said to be periodically time-limited if

x(n) = 0, n0 < |n| < N

We state, without proof, that no signal can be time-limited and bandlimited simul-
taneously. Furthermore, a reciprocal relationship exists between the time duration
and the frequency duration of a signal. To elaborate, if we have a short-duration
rectangular pulse in the time domain, its spectrum has a width that is inversely pro-
portional to the duration of the time-domain pulse. The narrower the pulse becomes
in the time domain, the larger the bandwidth of the signal becomes. Consequently,
the product of the time duration and the bandwidth of a signal cannot be made arbi-
trarily small. A short-duration signal has a large bandwidth and a small bandwidth
signal has a long duration. Thus, for any signal, the time–bandwidth product is fixed
and cannot be made arbitrarily small.

Finally, we note that we have discussed frequency analysis methods for periodic
and aperiodic signals with finite energy. However, there is a family of deterministic
aperiodic signals with finite power. These signals consist of a linear superposition of
complex exponentials with nonharmonically related frequencies, that is,

x(n) =
M∑
k=1

Ake
jωkn

where ω1 , ω2, . . . , ωM are nonharmonically related. These signals have discrete
spectra but the distances among the lines are nonharmonically related. Signals with
discrete nonharmonic spectra are sometimes called quasi-periodic.

2.10 The Frequency Ranges of Some Natural Signals

The frequency analysis tools that we have developed in this chapter are usually
applied to a variety of signals that are encountered in practice (e.g., seismic, biological,
and electromagnetic signals). In general, the frequency analysis is performed for the
purpose of extracting information from the observed signal. For example, in the case
of biological signals, such as an ECG signal, the analytical tools are used to extract
information relevant for diagnostic purposes. In the case of seismic signals, we may
be interested in detecting the presence of a nuclear explosion or in determining the
characteristics and location of an earthquake. An electromagnetic signal, such as a
radar signal reflected from an airplane, contains information on the position of the

Frequency Analysis of Signals

273



plane and its radial velocity. These parameters can be estimated from observation
of the received radar signal.

In processing any signal for the purpose of measuring parameters or extracting
other types of information, one must know approximately the range of frequencies
contained by the signal.
limits in the frequency domain for biological, seismic, and electromagnetic signals.

3 Frequency-Domain and Time-Domain Signal Properties

In the previous sections of the chapter we have introduced several methods for the
frequency analysis of signals. Several methods were necessary to accommodate the
different types of signals. To summarize, the following frequency analysis tools have
been introduced:

1. The Fourier series for continuous-time periodic signals.

2. The Fourier transform for continuous-time aperiodic signals.

3. The Fourier series for discrete-time periodic signals.

4. The Fourier transform for discrete-time aperiodic signals.

Frequency Ranges of Some Biological Signals

Type of Signal Frequency Range (Hz)

Electroretinograma 0–20

Electronystagmogramb 0–20

Pneumogramc 0–40

Electrocardiogram (ECG) 0–100

Electroencephalogram (EEG) 0–100

Electromyogramd 10–200

Sphygmomanograme 0–200

Speech 100–4000
a A graphic recording of retina characteristics.
b A graphic recording of involuntary movement of the eyes.
c A graphic recording of respiratory activity.
d A graphic recording of muscular action, such as muscular contraction.
e A recording of blood pressure.

TABLE 2 Frequency Ranges of Some Seismic Signals

Type of Signal Frequency Range (Hz)

Wind noise 100–1000

Seismic exploration signals 10–100

Earthquake and nuclear explosion signals 0.01–10

Seismic noise 0.1–1

Frequency Analysis of Signals

For reference, Tables 1, 2, and 3 give approximate

TABLE 1

274



TABLE 3 Frequency Ranges of Electromagnetic Signals

Type of Signal Wavelength (m) Frequency Range (Hz)

Radio broadcast 104 –102 3× 104 –3× 106
Shortwave radio signals 102 –10−2 3× 106 –3× 1010
Radar, satellite communications,

space communications,

common-carrier microwave 1–10−2 3× 108 –3× 1010
Infrared 10−3 –10−6 3× 1011 –3× 1014
Visible light 3.9× 10−7 –8.1× 10−7 3.7× 1014 –7.7× 1014
Ultraviolet 10−7 –10−8 3× 1015 –3× 1016
Gamma rays and X rays 10−9 –10−10 3× 1017 –3× 1018

As we have already indicated several times, there are two time-domain charac-
teristics that determine the type of signal spectrum we obtain. These are whether
the time variable is continuous or discrete, and whether the signal is periodic or
aperiodic. Let us briefly summarize the results of the previous sections.

Continuous-time signals have aperiodic spectra A close inspection of the Fourier
series and Fourier transform analysis formulas for continuous-time signals does not
reveal any kind of periodicity in the spectral domain. This lack of periodicity is a
consequence of the fact that the complex exponential exp(j2πF t) is a function of
the continuous variable t, and hence it is not periodic in F . Thus the frequency range
of continuous-time signals extends from F = 0 to F = ∞.
Discrete-time signals have periodic spectra Indeed, both the Fourier series and the
Fourier transform for discrete-time signals are periodic with period ω = 2π . As a
result of this periodicity, the frequency range of discrete-time signals is finite and
extends from ω = −π to ω = π radians, where ω = π corresponds to the highest
possible rate of oscillation.

Periodic signals have discrete spectra As we have observed, periodic signals are
described by means of Fourier series. The Fourier series coefficients provide the
“lines” that constitute the discrete spectrum. The line spacing �F or �f is equal
to the inverse of the period Tp or N , respectively, in the time domain. That is,
�F = 1/Tp for continuous-time periodic signals and �f = 1/N for discrete-time
signals.

Aperiodic finite energy signals have continuous spectra This property is a direct
consequence of the fact that both X(F) and X(ω) are functions of exp(j2πF t) and
exp(jωn), respectively, which are continuous functions of the variables F and ω . The
continuity in frequency is necessary to break the harmony and thus create aperiodic
signals.

Frequency Analysis of Signals

Figure 3.1 summarizes the analysis and synthesis formulas for these types of signals.

275



0

Continuous and periodic

Fo
ur

ie
r 

se
ri

es

Pe
ri

od
ic

 s
ig

na
ls

Fo
ur

ie
r 

tr
an

sf
or

m
s

A
pe

ri
od

ic
 s

ig
na

ls

Discrete and aperiodic Discrete and periodic Discrete and periodic

Continuous and aperiodic Continuous and aperiodic Discrete and aperiodic Continuous and periodic

Time-domain Time-domain Frequency-domainFrequency-domain

Continuous-time signals Discrete-time signals

Tp−Tp
t

xa(t)

……

0

F0 = Tp

1

F

ck

0
t

xa(t)

ck = ∫Tp xa(t)e−j2πkF0t dt1Tp
xa(t) =   Σ ckej2πkF0t

Xa(F) = ∫− xa(t)e−j2πFt dt

xa(t) = ∫− Xa(F)e j2πFt dF

0
F

Xa(F)

N
1

0−N N
n

x(n)

… …

0 1 2−1−2−3

x(n)

n
… …

0−2π 2π−π π

X(ω)

ω
… …

−N N
k

0

ck

… …

n = 0

N − 1

k=−

X(ω) =  Σ  x(n)e−jωn
n=−

x(n) =  ∫2π X(ω)e jωn dω

ck =         Σ  x(n)e−j(2π/N)kn

k = 0

N − 1
x(n) =  Σ  cke j(2π/N)kn

2π
1

Summary of analysis and synthesis formulas.

Freq
uency

A
nalysis

ofSignals

Figure 3.1

276



In summary, we can conclude that periodicity with “period” α in one domain
automatically implies discretization with “spacing” of 1/α in the other domain, and
vice versa.

If we keep in mind that “period” in the frequency domain means the frequency
range, “spacing” in the time domain is the sampling period T , line spacing in the
frequency domain is �F , then α = Tp implies that 1/α = 1/Tp = �F , α = N
implies that �f = 1/N , and α = Fs implies that T = 1/Fs .

stress, however, that the illustrations used in this figure do not correspond to any
actual transform pairs. Thus any comparison among them should be avoided.

dualities among the several frequency analysis relationships. In particular, we ob-
serve that there are dualities between the following analysis and synthesis equations:

1. The analysis and synthesis equations of the continuous-time Fourier transform.

2. The analysis and synthesis equations of the discrete-time Fourier series.

3. The analysis equation of the continuous-time Fourier series and the synthesis
equation of the discrete-time Fourier transform.

4. The analysis equation of the discrete-time Fourier transform and the synthesis
equation of the continuous-time Fourier series.

Note that all dual relations differ only in the sign of the exponent of the corresponding
complex exponential. It is interesting to note that this change in sign can be thought
of either as a folding of the signal or a folding of the spectrum, since

e−j2πF t = ej2π(−F)t = ej2πF(−t)

If we turn our attention now to the spectral density of signals, we recall that we
have used the term energy density spectrum for characterizing finite-energy aperiodic
signals and the term power density spectrum for periodic signals. This terminology is
consistent with the fact that periodic signals are power signals and aperiodic signals
with finite energy are energy signals.

4 Properties of the Fourier Transform for Discrete-Time Signals

The Fourier transform for aperiodic finite-energy discrete-time signals described in
the preceding section possesses a number of properties that are very useful in reduc-
ing the complexity of frequency analysis problems in many practical applications. In
this section we develop the important properties of the Fourier transform. Similar
properties hold for the Fourier transform of aperiodic finite-energy continuous-time
signals.

Frequency Analysis of Signals

These time-frequency dualities are apparent from observation of Fig 3.1. We

A careful inspection of Fig 3.1 also reveals some mathematical symmetries and

277



For convenience, we adopt the notation

X(ω) ≡ F {x(n)} =
∞∑

n=−∞
x(n)e−jωn

for the direct transform (analysis equation) and

x(n) ≡ F−1{X(ω)} = 1
2π

∫
2π
X(ω)ejωn dω

for the inverse transform (synthesis equation). We also refer to x(n) and X(ω) as a
Fourier transform pair and denote this relationship with the notation

x(n)
F←→ X(ω)

Recall thatX(ω) is periodic with period 2π . Consequently, any interval of length
2π is sufficient for the specification of the spectrum. Usually, we plot the spectrum
in the fundamental interval [−π, π ]. We emphasize that all the spectral information
contained in the fundamental interval is necessary for the complete description or
characterization of the signal.
is always 2π , independent of the specific characteristics of the signal within the
fundamental interval.

4.1 Symmetry Properties of the Fourier Transform

When a signal satisfies some symmetry properties in the time domain, these proper-
ties impose some symmetry conditions on its Fourier transform. Exploitation of any
symmetry characteristics leads to simpler formulas for both the direct and inverse
Fourier transform. A discussion of various symmetry properties and the implications
of these properties in the frequency domain is given here.

Suppose that both the signal x(n) and its transform X(ω) are complex-valued
functions. Then they can be expressed in rectangular form as

x(n) = xR(n)+ jxI (n)
X(ω) = XR(ω)+ jXI (ω)
−jω = cosω− j sinω into

and imaginary parts, we obtain

XR(ω) =
∞∑

n=−∞
[xR(n) cosωn+ xI (n) sinωn]

XI(ω) = −
∞∑

n=−∞
[xR(n) sinωn− xI (n) cosωn]

Frequency Analysis of Signals

(4.1)

(4.2)

(4.3)

(4.4)

(4.5)

(4.6)

(4.7)

For this reason, the range of integration in(4.2)

By substituting (4.4) and e (4.1) and separating the real

278



jω

obtain

xR(n) = 12π
∫

2π
[XR(ω) cosωn−XI (ω) sinωn] dω

xI (n) = 12π
∫

2π
[XR(ω) sinωn+XI(ω) cosωn] dω

Now, let us investigate some special cases.

Real signals. If x(n) is real, then xR I

XR(ω) =
∞∑

n=−∞
x(n) cosωn

and

XI (ω) = −
∞∑

n=−∞
x(n) sinωn

XR(−ω) = XR(ω) , (even)
XI(−ω) = −XI (ω), (odd)

X∗(ω) = X(−ω)

In this case we say that the spectrum of a real signal has Hermitian symmetry.

real signals are

|X(ω)| =
√
X2R(ω)+X2I (ω)

�X|ω| = tan−1 XI (ω)
XR(ω)

possess the symmetry properties

|X(ω)| = |X(−ω)| , (even)
�X(−ω) = −�X(ω), (odd)

Frequency Analysis of Signals

(4.8)

(4.9)

(4.10)

(4.11)

(4.12)

(4.13)

(4.14)

(4.15)

(4.16)

(4.17)

(4.18)

In a similar manner, by substituting (4.5) and e = cosω+ j sinω into (4.2), we

(n) = x(n) and x (n) = 0. Hence (4.6) and
(4.7) reduce to

Since cos(−ωn) = cosωn and sin(−ωn) = − sinωn, it follows from (4.10) and
(4.11) that

If we combine (4.12) and (4.13) into a single equation, we have

As a consequence of (4.12) and (4.13), the magnitude and phase spectra also

With the aid of Fig 4.1, we observe that the magnitude and phase spectra for

279



Magnitude and phase
functions.

0

Imaginary axis

Real axis

XI(ω)

XR(ω)

X(ω)

�X(ω)

X(ω
)

In the case of the inverse transform of a real-valued signal [i.e., x(n) = xR(n)],

x(n) = 1
2π

∫
2π

[XR(ω) cosωn−XI (ω) sinωn] dω

Since both products XR(ω) cosωn and XI (ω) sinωn are even functions of ω , we have

x(n) = 1
π

∫ π
0

[XR(ω) cosωn−XI(ω) sinωn] dω

Real and even signals. If x(n) is real and even [i.e., x(−n) = x(n)], then x(n) cosωn

XR(ω) = x(0)+ 2
∞∑
n=1

x(n) cosωn, (even)

XI (ω) = 0

x(n) = 1
π

∫ π
0
XR(ω) cosωn dω

Thus real and even signals possess real-valued spectra, which, in addition, are even
functions of the frequency variable ω .

Real and odd signals. If x(n) is real and odd [i.e., x(−n) = −x(n)], then x(n) cosωn

XR(ω) = 0

XI(ω) = −2
∞∑
n=1

x(n) sinωn, (odd)

x(n) = − 1
π

∫ π
0
XI (ω) sinωn dω

Frequency Analysis of Signals

(4.26)

(4.25)

(4.24)

(4.23)

(4.22)

(4.21)

(4.20)

(4.19)

Figure 4.1

(4.8) implies that

is even and x(n) sinωn is odd. Hence, from (4.10), (4.11), and (4.20) we obtain

is odd and x(n) sinωn is even. Consequently, (4.10), (4.11) and (4.20) imply that

280



Thus real-valued odd signals possess purely imaginary-valued spectral characteristics,
which, in addition, are odd functions of the frequency variable ω .

Purely imaginary signals. In this case xR I

XR(ω) =
∞∑

n=−∞
xI (n) sinωn, (odd)

XI(ω) =
∞∑

n=−∞
xI (n) cosωn, (even)

xI (n) = 1
π

∫ π
0

[XR(ω) sinωn+XI (ω) cosωn] dω

If xI (n) is odd [i.e., xI (−n) = −xI (n)], then

XR(ω) = 2
∞∑
n=1

xI (n) sinωn, (odd)

XI(ω) = 0

xI (n) = 1
π

∫ π
0
XR(ω) sinωn dω

Similarly, if xI (n) is even [i.e., xI (−n) = xI (n)], we have

XR(ω) = 0

XI(ω) = xI (0)+ 2
∞∑
n=1

xI (n) cosωn, (even)

xI (n) = 1
π

∫ π
0
XI(ω) cosωn dω

An arbitrary, possibly complex-valued signal x(n) can be decomposed as

x(n) = xR(n)+ jxI (n) = xeR(n)+ xoR(n)+ j [xeI (n)+ xoI (n)] = xe(n)+ xo(n)

Frequency Analysis of Signals

(4.36)

(4.35)

(4.33)

(4.34)

(4.32)

(4.31)

(4.30)

(4.29)

(4.28)

(4.27)

(n) = 0 and x(n) = jx (n). Thus (4.6),
(4.7), and (4.9) reduce to

281



where, by definition,

xe(n) = xeR(n)+ jxeI (n) =
1
2

[x(n)+ x∗(−n)]

xo(n) = xoR(n)+ jxoI (n) =
1
2

[x(n)− x∗(−n)]

The superscripts e and o denote the even and odd signal components, respectively.
e e o o

transform properties established above, we obtain the following relationships:

X(ω) = [Xe(ω) + jXe(ω)] + [Xo(ω) − jXo(ω)] = Xe(ω) + Xo(ω)R R II

x(n) = [xe(n) + jxe(n)] + [xo(n) + jxo(n)] = xe(n) + xo(n)R R II 

They are often used to simplify Fourier transform calculations in
practice.

Symmetry Properties of the Discrete-Time Fourier Transform
Sequence DTFT
x(n) X(ω)

x∗(n) X∗(−ω)
x∗(−n) X∗(ω)
xR(n) Xe(ω) = 12 [X(ω)+X∗(−ω)]
jxI (n) Xo(ω) = 12 [X(ω)−X∗(−ω)]

xe(n) = 12 [x(n)+ x∗(−n)] XR(ω)
xo(n) = 12 [x(n)− x∗(−n)] jXI (ω)

Real Signals
X(ω) = X∗(−ω)

Any real signal XR(ω) = XR(−ω)
x(n) XI (ω) = −XI(−ω)

|X(ω)| = |X(−ω)|
�X(ω) = −�X(−ω)

xe(n) = 12 [x(n)+ x(−n)] XR(ω)
(real and even) (real and even)

xo(n) = 12 [x(n)− x(−n)] jXI (ω)
(real and odd) (imaginary and odd)

Frequency Analysis of Signals

(4.37)

We note that x (n) = x (−n) and x (−n) = −x (n). From (4.36) and the Fourier

These symmetry properties of the Fourier transform are summarized in Table 4
and in Fig 4.2.

TABLE 4

282



Even

Time domain Frequency domain

Odd
Real

Signal

Even

Odd
Real

Fourier Transform

Odd

Even
Imaginary

Odd

Even
Imaginary

Summary of symmetry properties for the Fourier transform.

Determine and sketch XR(ω), XI(ω), |X(ω)|, and �X(ω) for the Fourier transform

X(ω) = 1
1− ae−jω , −1 < a < 1

Solution.
conjugate of the denominator, we obtain

X(ω) = 1− ae
jω

(1− ae−jω)(1− aejω) =
1− a cosω − ja sinω

1− 2a cosω + a2
This expression can be subdivided into real and imaginary parts. Thus we obtain

XR(ω) = 1− a cosω1− 2a cosω + a2

XI(ω) = − a sinω1− 2a cosω + a2

phase spectra as

|X(ω)| = 1√
1− 2a cosω + a2

and

�X(ω) = − tan−1 a sinω
1− a cosω

The reader can easily verify that as expected, all symmetry properties for the spectra of real
signals apply to this case.

Frequency Analysis of Signals

(4.38)

(4.39)

(4.40)

Figure 4.2

EXAMPLE 4.1

By multiplying both the numerator and denominator of (4.38) by the complex

Substitution of the last two equations into (4.15) and (4.16) yields the magnitude and

Figures 4.3 and 4.4 show the graphical representation of these spectra for a = 0.8.

283



Graph of XR(ω) and
XI(ω) for the transform in  0. 5 0 0.5

ω
ππππ

 0. 5 0 0.5
ω

ππππ
0

1

2

3

4

5

6

X
R
(ω

)

 3

 2

 1

0

1

2

3

X
1(

ω
)

Magnitude and phase
spectra of the transform in

 0. 5 0 0.5
0

1

2

3

4

5

6

ω
ππππ

 0. 5 0 0.5
ω

ππππ

|X
(ω

)|

 0.4π

 0.2π

0

0.4π

0.2π

∠
X

(ω
)

Frequency Analysis of Signals

Figure 4.3

Example 4.1.

Figure 4.4

Example 4.1.

284



Determine the Fourier transform of the signal

x(n) =
{
A, −M ≤ n ≤ M
0, elsewhere

Solution. Clearly, x(−n) = x(n). Thus x(n) is a real and even signal.
obtain

X(ω) = XR(ω) = A
(

1+ 2
M∑
n=1

cosωn

)

X(ω) = A sin(M +
1
2 )ω

sin(ω/2)

Since X(ω) is real, the magnitude and phase spectra are given by

|X(ω)| =
∣∣∣∣∣A sin(M +

1
2 )ω

sin(ω/2)

∣∣∣∣∣
and

�X(ω) =
{

0, if X(ω) > 0
π, if X(ω) < 0

4.2 Fourier Transform Theorems and Properties

In this section we introduce several Fourier transform theorems and illustrate their
use in practice by examples.

Linearity. If

x1(n)
F←→ X1(ω)

and
x2(n)

F←→ X2(ω)

then
a1x1(n)+ a2x2(n) F←→ a1X1(ω)+ a2X2(ω)

Simply stated, the Fourier transformation, viewed as an operation on a signal
x(n), is a linear transformation. Thus the Fourier transform of a linear combination of
two or more signals is equal to the same linear combination of the Fourier transforms

property makes the Fourier transform suitable for the study of linear systems.

Frequency Analysis of Signals

(4.42)

(4.43)

(4.44)

EXAMPLE 4.2

(4.41)

From (4.21) we

If we use the identity given in Problem 13, we obtain the simpler form

Figure 4.5 shows the graphs for X(ω).

of the individual signals. This property is easily proved by using (4.1). The linearity

285



0

x(n)

M
n

−M

X(ω)

ω
−π π−2π 2π

−2

2

4

|X(ω)|

ω
−π π−2π 2π

2

0

4

�X(ω)

ω
−π π

π

−π

−2π 2π

Spectral characteristics of rectangular pulse in

Determine the Fourier transform of the signal

x(n) = a|n|, −1 < a < 1
Solution. First, we observe that x(n) can be expressed as

x(n) = x1(n)+ x2(n)
where

x1(n) =
{
an, n ≥ 0
0, n < 0

Frequency Analysis of Signals

(4.45)

Figure 4.5
Example 4.2.

EXAMPLE 4.3

286



and

x2(n) =
{
a−n, n < 0
0, n ≥ 0

X1(ω) =
∞∑

n=−∞
x1(n)e

−jωn =
∞∑
n=0

ane−jωn =
∞∑
n=0
(ae−jω)n

The summation is a geometric series that converges to

X1(ω) = 11− ae−jω

provided that
|ae−jω| = |a| · |e−jω| = |a| < 1

which is a condition that is satisfied in this problem. Similarly, the Fourier transform of x2(n) is

X2(ω) =
∞∑

n=−∞
x2(n)e

−jωn =
−1∑

n=−∞
a−ne−jωn

=
−1∑

n=−∞
(aejω)−n =

∞∑
k=1
(aejω)k

= ae
jω

1− aejω

By combining these two transforms, we obtain the Fourier transform of x(n) in the form

X(ω) = X1(ω)+X2(ω)

= 1− a
2

1− 2a cosω + a2

Time shifting. If

x(n)
F←→ X(ω)

then
x(n− k) F←→ e−jωkX(ω)

The proof of this property follows immediately from the Fourier transform of x(n−k)
by making a change in the summation index. Thus

F {x(n− k)} = X(ω)e−jωk

= |X(ω)|ej [�X(ω)−ωk]

Frequency Analysis of Signals

(4.47)

(4.46)

Beginning with the definition of the Fourier transform in (4.1), we have

Figure 4.6 illustrates x(n) and X(ω) for the case in which a = 0.8.

287



Sequence x(n) and its
Fourier transform in

0

x(n)

n

X(ω)

ω
−π π−2π 2π0

1 + a
1 − a

This relation means that if a signal is shifted in the time domain by k samples, its
magnitude spectrum remains unchanged. However, the phase spectrum is changed
by an amount −ωk . This result can easily be explained if we recall that the frequency
content of a signal depends only on its shape. From a mathematical point of view,
we can say that shifting by k in the time domain is equivalent to multiplying the
spectrum by e−jωk in the frequency domain.

Time reversal. If
x(n)

F←→ X(ω)

then
x(−n) F←→ X(−ω)

x(−n) and making a simple change in the summation index. Thus

F {x(−n)} =
∞∑

l=−∞
x(l)ejωl = X(−ω)

F {x(−n)} = X(−ω) = |X(−ω)|ej�X(−ω)

= |X(ω)|e−j�X(ω)

Frequency Analysis of Signals

This property can be established by performing the Fourier transformation of

(4.48)

Figure 4.6

Example 4.3 with a = 0.8.

If x(n) is real, then from (4.17) and (4.18) we obtain

288



This means that if a signal is folded about the origin in time, its magnitude spec-
trum remains unchanged, and the phase spectrum undergoes a change in sign (phase
reversal).

Convolution theorem. If
x1(n)

F←→ X1(ω)
and

x2(n)
F←→ X2(ω)

then
x(n) = x1(n) ∗ x2(n) F←→ X(ω) = X1(ω)X2(ω)

x(n) = x1(n) ∗ x2(n) =
∞∑

k=−∞
x1(k)x2(n− k)

By multiplying both sides of this equation by the exponential exp(−jωn) and sum-
ming over all n, we obtain

X(ω) =
∞∑

n=−∞
x(n)e−jωn =

∞∑
n=−∞

[ ∞∑
k=−∞

x1(k)x2(n− k)
]
e−jωn

After interchanging the order of the summations and making a simple change in
the summation index, the right-hand side of this equation reduces to the product

1 2
The convolution theorem is one of the most powerful tools in linear systems

analysis. That is, if we convolve two signals in the time domain, then this is equivalent

x1(n) = x2(n) = {1, 1↑, 1}

Solution.

X1(ω) = X2(ω) = 1+ 2 cosω
Then

X(ω) = X1(ω)X2(ω) = (1+ 2 cosω)2

= 3+ 4 cosω + 2 cos 2ω
= 3+ 2(ejω + e−jω)+ (ej2ω + e−j2ω)

Frequency Analysis of Signals

(4.49)

X (ω)X (ω). Thus (4.49) is established.

EXAMPLE 4.4

By use of (4.49), determine the convolution of the sequences

By using (4.21), we obtain

To prove (4.49), we recall the convolution formula

to multiplying their spectra in the frequency domain. We will see that the convo-
lution theorem provides an important computational tool for many digital signal 
processing applications.

289



Hence the convolution of x1(n) with x2(n) is

x(n) = {1 2 3
↑

2 1}

0 1−1

1

x1(n)

n

F

F

F

X1(ω)

ω
π−π

3

0 1−1

x2(n)

n

0 1 2−1−2

3

x(n)

n

X2(ω)

ω
π−π −1

3

X1(ω)

ω
π−π

10

0

Graphical representation of the convolution property.

The correlation theorem. If

x1(n)
F←→ X1(ω)

and

x2(n)
F←→ X2(ω)

then

rx1x2(m)
F←→ Sx1x2(ω) = X1(ω)X2(−ω)

rx1x2(n) =
∞∑

k=−∞
x1(k)x2(k − n)

Frequency Analysis of Signals

(4.50)

Figure 4.7 illustrates the foregoing relationships.

Figure 4.7

The proof of (4.50) is similar to the proof of (4.49). In this case, we have

290



By multiplying both sides of this equation by the exponential exp(−jωn) and sum-
ming over all n, we obtain

Sx1x2(ω) =
∞∑

n=−∞
rx1x2(n)e

−jωn =
∞∑

n=−∞

[ ∞∑
k=−∞

x1(k)x2(k − n)
]
e−jωn

Finally, we interchange the order of the summations and make a change in the sum-
mation index. Thus we find that the right-hand side of the equation above reduces
to X1(ω)X2(−ω). The function Sx1x2(ω) is called the cross-energy density spectrum
of the signals x1(n) and x2(n).

The Wiener–Khintchine theorem. Let x(n) be a real signal. Then

rxx(l)
F←→ Sxx(ω)

That is, the energy spectral density of an energy signal is the Fourier transform of its

This is a very important result. It means that the autocorrelation sequence of a
signal and its energy spectral density contain the same information about the signal.
Since neither of these contains any phase information, it is impossible to uniquely
reconstruct the signal from the autocorrelation function or the energy density spec-
trum.

Determine the energy density spectrum of the signal

x(n) = anu(n), −1 < a < 1

Solution.

rxx(l) = 11− a2a
|l|, ∞ < l <∞

|l|
have

F {rxx(l)} = 11− a2F {a
|l|} = 1

1− 2a cosω + a2

Thus, according to the Wiener–Khintchine theorem,

Sxx(ω) = 11− 2a cosω + a2

Frequency Analysis of Signals

(4.51)

autocorrelation sequence. This is a special case of (4.50).

EXAMPLE 4.5

By using the result in (4.46) for the Fourier transform of a , derived in Example 4.3, we

We found that the autocorrelation function for this signal is

291



X(ω − ω0)

X(ω)

−2π

−2π −2π + ω0 −2π + ω0 

2π

2π

1

1

ωm−ωmπ π

π

0

0

ω

ωω0 +ωmω0 − ωm ω0

(a)

(b)

Fourier transform (ω0 ≤ 2π − ωm).

Frequency shifting. If

x(n)
F←→ X(ω)

then

ejω0nx(n)
F←→ X(ω − ω0)

According to this property, multiplication of a sequence x(n) by ejω0n is
equivalent to a frequency translation of the spectrum X(ω) by ω0 . This frequency

ω0 applies to the spectrum of the signal in every period.

The modulation theorem. If

x(n)
F←→ X(ω)

then

x(n) cosω0n
F←→ 1

2
[X(ω + ω0)+X(ω − ω0)]

To prove the modulation theorem, we first express the signal cosω0n as

cosω0n = 12 (e
jω0n + e−jω0n)

Upon multiplying x(n) by these two exponentials and using the frequency-shifting

Frequency Analysis of Signals

(4.53)

This property is easily proved by direct substitution into the analysis equation

(4.52)

Illustration of the frequency-shifting property of theFigure 4.8

(4.1).

translation is illustrated in Fig 4.8. Since the spectrum X(ω) is periodic, the shift

property described in the preceding section, we obtain the desired result in (4.53).

292



0

(a)

ω
π−−2π

−2π 2π

2π

1

X(ω)

−π π
2

π

2

0

(b)

ω
π−

1

Y1(ω)

−π π
2

π

2

0

(c)

ω
π−

1

Y2(ω) = X(ω − π)

−π π
2

− 3π
2

3π
2

π

2

1
2

Graphical representation of the modulation theorem.

0

spectra of the signals x(n), y1(n) = x(n) cos 0.5πn and y2(n) = x(n) cosπn.

Parseval’s theorem. If

x1(n)
F←→ X1(ω)

and

x2(n)
F←→ X2(ω)

then
∞∑

n=−∞
x1(n)x

∗
2 (n) =

1
2π

∫ π
−π
X1(ω)X

∗
2(ω) dω

Frequency Analysis of Signals

(4.54)

Although the property given in (4.52) can also be viewed as (complex) mod-
ulation, in practice we prefer to use (4.53) because the signal x(n) cosω n is real.
Clearly, in this case the symmetry properties (4.12) and (4.13) are preserved.

The modulation theorem is illustrated in Fig 4.9, which contains a plot of the

Figure 4.9

293



1

1
2π

∫
2π

[ ∞∑
n=−∞

x1(n)e
−jωn

]
X∗2(ω) dω

=
∞∑

n=−∞
x1(n)

1
2π

∫
2π
X∗2(ω)e

−jωn dω =
∞∑

n=−∞
x1(n)x

∗
2 (n)

In the special case where x2(n) = x1(n) =
reduces to

∞∑
n=−∞

|x(n)|2 = 1
2π

∫
2π
|X(ω)|2 dω

x of the signal
x(n). It is also equal to the autocorrelation of x(n), rxx(l), evaluated at l = 0. The

the integral over the interval −π ≤ ω ≤ π yields the total signal energy. Therefore,
we conclude that

Ex = rxx(0) =
∞∑

n=−∞
|x(n)|2 = 1

2π

∫
2π
|X(ω)|2 dω = 1

2π

∫ π
−π
Sxx(ω) dω

Multiplication of two sequences (Windowing theorem). If

x1(n)
F←→ X1(ω)

and

x2(n)
F←→ X2(ω)

then

x3(n) ≡ x1(n)x2(n) F←→ X3(ω) = 12π
∫ π
−π
X1(λ)X2(ω − λ)dλ

Fourier transforms X1(ω) and X2(ω). This relation is the dual of the time-domain
convolution. In other words, the multiplication of two time-domain sequences is
equivalent to the convolution of their Fourier transforms. On the other hand, the
convolution of two time-domain sequences is equivalent to the multiplication of their
Fourier transforms.

Frequency Analysis of Signals

(4.57)

(4.56)

(4.55)

x(n), Parseval’s relation (4.54)

To prove this theorem, we use (4.1) to eliminate X (ω) on the right-hand side
of (4.54). Thus we have

We observe that the left-hand side of (4.55) is simply the energy E

integrand in the right-hand side of (4.55) is equal to the energy density spectrum, so

The integral on the right-hand side of (4.57) represents the convolution of the

294



To prove (4.57) we begin with the Fourier transform of x3(n) = x1(n)x2(n) and
use the formula for the inverse transform, namely,

x1(n) = 12π
∫ π
−π
X1(λ)e

jλndλ

Thus, we have

X3(ω) =
∞∑

n=−∞
x3(n)e

−jωn =
∞∑

n=−∞
x1(n)x2(n)e

−jωn

=
∞∑

n=−∞

[
1

2π

∫ π
−π
X1(λ)e

jλndλ

]
x2(n)e

−jωn

= 1
2π

∫ π
−π
X1(λ)dλ

[ ∞∑
n=−∞

x2(n)e
−j (ω−λ)n

]

= 1
2π

∫ π
−π
X1(λ)X2(ω − λ)dλ

1
andX2(ω) because it is the convolution of two periodic functions having the same pe-
riod. We note that the limits of integration extend over a single period. Furthermore,
we note that due to the periodicity of the Fourier transform for discrete-time signals,
there is no “perfect” duality between the time and frequency domains with respect
to the convolution operation, as in the case of continuous-time signals. Indeed, con-
volution in the time domain (aperiodic summation) is equivalent to multiplication
of continuous periodic Fourier transforms. However, multiplication of aperiodic
sequences is equivalent to periodic convolution of their Fourier transforms.

filter design based on the window technique.

Differentiation in the frequency domain. If

x(n)
F←→ X(ω)

then

nx(n)
F←→ j dX(ω)

dω

Frequency Analysis of Signals

(4.58)

The convolution integral in (4.57) is known as the periodic convolution ofX (ω)

The Fourier transform pair in (4.57) will prove useful in our treatment of FIR

295



and differentiate the series term by term with respect to ω . Thus we obtain

dX(ω)

dω
= d
dω

[ ∞∑
n=−∞

x(n)e−jωn
]

=
∞∑

n=−∞
x(n)

d

dω
e−jωn

= −j
∞∑

n=−∞
nx(n)e−jωn

Now we multiply both sides of the equation by j to obtain the desired result in

Properties of the Fourier Transform for Discrete-Time Signals

Property Time Domain Frequency Domain

Notation x(n) X(ω)

x1(n) X1(ω)

x2(n) X2(ω)

Linearity a1x1(n)+ a2x2(n) a1X1(ω)+ a2X2(ω)
Time shifting x(n− k) e−jωkX(ω)
Time reversal x(−n) X(−ω)
Convolution x1(n) ∗ x2(n) X1(ω)X2(ω)
Correlation rx1x2 (l) = x1(l) ∗ x2(−l) Sx1x2 (ω) = X1(ω)X2(−ω)

= X1(ω)X∗2(ω)
[if x2(n) is real]

Wiener–Khintchine theorem rxx(l) Sxx(ω)

Frequency shifting ejω0nx(n) X(ω − ω0)
Modulation x(n) cosω0n 12X(ω + ω0)+ 12X(ω − ω0)
Multiplication x1(n)x2(n) 12π

∫ π
−π X1(λ)X2(ω− λ)dλ

Differentiation in

the frequency domain nx(n) j dX(ω)
dω

Conjugation x∗(n) X∗(−ω)
Parseval’s theorem

∑∞
n=−∞ x1(n)x

∗
2 (n) = 12π

∫ π
−π X1(ω)X

∗
2(ω)dω

Frequency Analysis of Signals

     that are encountered in further study.

To prove this property, we use the definition of the Fourier transform in (4.1)

(4.58).
The properties derived in this section are summarized in Table 5, which serves

TABLE 5

as a convenient reference. Table 6 illustrates some useful Fourier transform pairs

296



Signal x(n) Spectrum X(ω)

0 1 2 3−1−2−3

1

n

x(n) = δ(n)

π−π

1

ω

X(π) = 1

0 L−L

1

n

A

x(n) = 
A,   |n| ≤ L
0,   |n| > L

0 π−π
ω

X(ω) = A
  sin (L +    )ω12

sin ω
2

0π
n

−
ωc

π
ωc

π

ωc

π

ωc   n = 0

πn
sin ωcn   n ≠ 0

x(n) = 
,

,

0

X(ω)

1

ωc−ωc−π
ω

X(ω) =
1,   |ω| < ωc

0,   ωc ≤ |ω| ≤ π

x(n) =
{
an, n ≥ 0
0, n > 0

X(ω) = 1
1−ae−jω

5 Summary and References

The Fourier series and the Fourier transform are the mathematical tools for analyzing
the characteristics of signals in the frequency domain. The Fourier series is appro-
priate for representing a periodic signal as a weighted sum of harmonically related
sinusoidal components, where the weighting coefficients represent the strengths of
each of the harmonics, and the magnitude squared of each weighting coefficient rep-
resents the power of the corresponding harmonic. As we have indicated, the Fourier
series is one of many possible orthogonal series expansions for a periodic signal. Its
importance stems from the characteristic behavior of LTI systems.

The Fourier transform is appropriate for representing the spectral characteris-
tics of aperiodic signals with finite energy. The important properties of the Fourier
transform were also presented in this chapter.

There are many excellent texts on Fourier series and Fourier transforms. For
reference, we include the texts by Bracewell (1978), Davis (1963), Dym and McKean
(1972), and Papoulis (1962).

Frequency Analysis of Signals

TABLE 6 Some Useful Fourier Transform Pairs for Discrete-Time Aperiodic Signals

297



Problems

1
(a) Determine its spectrum Xa(F ).
(b) Compute the power of the signal.
(c) Plot the power spectral density.
(d) Check the validity of Parseval’s relation for this signal.

0−τ τ 2τ
t

A

xa(t)

2 Compute and sketch the magnitude and phase spectra for the following signals (a >
0).

(a) xa(t) =
{
Ae−at , t ≥ 0
0, t < 0

(b) xa(t) = Ae−a|t |
3 Consider the signal

x(t) =
{

1− |t |/τ, |t| ≤ τ
0, elsewhere

(a) Determine and sketch its magnitude and phase spectra, |Xa(F )| and �Xa(F ),
respectively.

(b) Create a periodic signal xp(t) with fundamental period Tp ≥ 2τ , so that x(t) =
xp(t) for |t| < Tp/2. What are the Fourier coefficients ck for the signal xp(t)?

(c) Using the results in parts (a) and (b), show that ck = (1/Tp)Xa(k/Tp).
4 Consider the following periodic signal:

x(n) = {. . . , 1, 0, 1, 2, 3
↑
, 2, 1, 0, 1, . . .}

(a) Sketch the signal x(n) and its magnitude and phase spectra.
(b) Using the results in part (a), verify Parseval’s relation by computing the power

in the time and frequency domains.
5 Consider the signal

x(n) = 2+ 2 cos πn
4
+ cos πn

2
+ 1

2
cos

3πn
4

(a) Determine and sketch its power density spectrum.
(b) Evaluate the power of the signal.

Frequency Analysis of Signals

Consider the full-wave rectified sinusoid in Fig. P1.

Figure P1

298



6 Determine and sketch the magnitude and phase spectra of the following periodic
signals.

(a) x(n) = 4 sin π(n−2)3
(b) x(n) = cos 2π3 n+ sin 2π5 n
(c) x(n) = cos 2π3 n sin 2π5 n
(d) x(n) = {. . . ,−2,−1, 0

↑
, 1, 2,−2,−1, 0, 1, 2, . . .}

(e) x(n) = {. . . ,−1, 2, 1
↑
, 2,−1, 0,−1, 2, 1, 2, . . .}

(f) x(n) = {. . . , 0, 0, 1
↑
, 1, 0, 0, 0, 1, 1, 0, 0, . . .}

(g) x(n) = 1,−∞ < n <∞
(h) x(n) = (−1)n,−∞ < n <∞

7 Determine the periodic signals x(n), with fundamental period N = 8, if their Fourier
coefficients are given by:

(a) ck = cos kπ4 + sin 3kπ4
(b) ck =

{
sin kπ3 , 0 ≤ k ≤ 6
0, k = 7

(c) {ck} = {. . . , 0, 14 , 12 , 1, 2↑, 1,
1
2 ,

1
4 , 0 . . .}

8 Two DT signals, sk(n) and sl(n), are said to be orthogonal over an interval [N1, N2] if

N2∑
n=N1

sk(n)s
∗
l (n) =

{
Ak, k = l
0, k �= l

If Ak = 1, the signals are called orthonormal.
(a) Prove the relation

N−1∑
n=0

ej2πkn/N =
{
N, k = 0,±N,±2N, . . .
0, otherwise

(b) Illustrate the validity of the relation in part (a) by plotting for every value of
k = 1, 2, . . . , 6, the signals sk(n) = ej (2π/6)kn , n = 0, 1, . . . , 5. [Note: For a given
k , n the signal sk(n) can be represented as a vector in the complex plane.]

(c) Show that the harmonically related signals

sk(n) = ej (2π/N)kn

are orthogonal over any interval of length N .

Frequency Analysis of Signals

299



9 Compute the Fourier transform of the following signals.

(a) x(n) = u(n)− u(n− 6)
(b) x(n) = 2nu(−n)
(c) x(n) = ( 14 )nu(n+ 4)
(d) x(n) = (αn sinω0n)u(n), |α| < 1
(e) x(n) = |α|n sinω0n, |α| < 1

(f) x(n) =
{

2− ( 12 )n, |n| ≤ 4
0, elsewhere

(g) x(n) = {−2,−1, 0
↑
, 1, 2}

(h) x(n) =
{
A(2M + 1− |n|), |n| ≤ M
0, |n| > M

Sketch the magnitude and phase spectra for parts (a), (f), and (g).

10 Determine the signals having the following Fourier transforms.

(a) X(ω) =
{

0, 0 ≤ |ω| ≤ ω0
1, ω0 < |ω| ≤ π

(b) X(ω) = cos2 ω

(c) X(ω) =
{

1, ω0 − δω/2 ≤ |ω| ≤ ω0 + δω/2
0, elsewhere

(d)

0− − π
8

π

8
3π
8

3π−π
8

6π
8

7π π
8

ω

1

2

X(ω)

11 Consider the signal

x(n) = {1, 0,−1, 2
↑
, 3}

with Fourier transform X(ω) = XR(ω)+ j (XI (ω)). Determine and sketch the signal
y(n) with Fourier transform

Y (ω) = XI (ω)+XR(ω)ej2ω

Frequency Analysis of Signals

Figure P10

The signal shown in Fig. P10.

300



12

0
(a)

(b)

(c)

− π−π 8π10
8π
10

9π
10

ω
1
2

X(ω)

0 π−π
ω

ωc

1
2

X(ω)

W

0 π
ω

X(ω)

−π

−ωc

x(n) =
{

1, −M ≤ n ≤ M
0, otherwise

was shown to be

X(ω) = 1+ 2
M∑
n=1

cosωn

Show that the Fourier transform of

x1(n) =
{

1, 0 ≤ n ≤ M
0, otherwise

and

x2(n) =
{

1, −M ≤ n ≤ −1
0, otherwise

are, respectively,

X1(ω) = 1− e
−jω(M+1)

1− e−jω

X2(ω) = e
jω − ejω(M+1)

1− ejω

Frequency Analysis of Signals

Determine the signal x(n) if its Fourier transform is as given in Fig. P12.

In Example 4.2, the Fourier transform of the signal

Figure P12

13

301



Thus prove that

X(ω) = X1(ω)+X2(ω)

= sin(M +
1
2 )ω

sin(ω/2)

and therefore,

1+ 2
M∑
n=1

cosωn = sin(M +
1
2 )ω

sin(ω/2)

14 Consider the signal
x(n) = {−1, 2,−3

↑
, 2,−1}

with Fourier transform X(ω). Compute the following quantities, without explicitly
computing X(ω):

(a) X(0)

(b) � X(ω)

(c)
∫ π
−π X(ω) dω

(d) X(π)

(e)
∫ π
−π |X(ω)|2 dω

15 The center of gravity of a signal x(n) is defined as

c =

∞∑
n=−∞

nx(n)

∞∑
n=−∞

x(n)

and provides a measure of the “time delay” of the signal.

(a) Express c in terms of X(ω).

(b)

0
ω

−

1

X(ω)

π

2
π

2
π

Frequency Analysis of Signals

Compute c for the signal x(n) whose Fourier transform is shown in Fig. P15.

Figure P15

302



16 Consider the Fourier transform pair

anu(n)
F←→ 1

1− ae−jω , |a| < 1

Use the differentiation in frequency theorem and induction to show that

x(n) = (n+ l − 1)!
n!(l − 1)! a

nu(n)
F←→ X(ω) = 1

(1− ae−jω)l

17 Let x(n) be an arbitrary signal, not necessarily real valued, with Fourier transform
X(ω). Express the Fourier transforms of the following signals in terms of X(ω).

(a) x∗(n)

(b) x∗(−n)
(c) y(n) = x(n)− x(n− 1)
(d) y(n) =∑nk=−∞ x(k)
(e) y(n) = x(2n)
(f) y(n) =

{
x(n/2), n even
0, n odd

18 Determine and sketch the Fourier transforms X1(ω), X2(ω), and X3(ω) of the fol-
lowing signals.

(a) x1(n) = {1, 1, 1↑, 1, 1}
(b) x2(n) = {1, 0, 1, 0, 1↑, 0, 1, 0, 1}
(c) x3(n) = {1, 0, 0, 1, 0, 0, 1↑, 0, 0, 1, 0, 0, 1}
(d) Is there any relation between X1(ω), X2(ω), and X3(ω)? What is its physical

meaning?

(e) Show that if

xk(n) =
{
x
(n
k

)
, if n/k integer

0, otherwise

then
Xk(ω) = X(kω)

19
sketch the Fourier transforms of the following signals.

(a) x1(n) = x(n) cos(πn/4)
(b) x2(n) = x(n) sin(πn/2)
(c) x3(n) = x(n) cos(πn/2)
(d) x4(n) = x(n) cosπn

Frequency Analysis of Signals

Let x(n) be a signal with Fourier transform as shown in Fig. P19. Determine and

303



0
ω

−

1

X(ω)

π

2
π

2
π−π

Note that these signal sequences are obtained by amplitude modulation of a carrier
cosωcn or sinωcn by the sequence x(n).

20 Consider an aperiodic signal x(n) with Fourier transform X(ω). Show that the
Fourier series coefficients Cyk of the periodic signal

y(n) =
∞∑

l=−∞
x(n− lN)

are given by

C
y

k =
1
N
X

(
2π
N
k

)
, k = 0, 1, . . . , N − 1

21 Prove that

XN(ω) =
N∑

n=−N

sinωcn
πn

e−jωn

may be expressed as

XN(ω) = 12π
∫ ωc
−ωc sin[(2N + 1)(ω − θ/2)]

sin[(ω − θ)/2] dθ

22 A signal x(n) has the following Fourier transform:

X(ω) = 1
1− ae−jω

Determine the Fourier transforms of the following signals:

(a) x(2n+ 1)
(b) eπn/2x(n+ 2)
(c) x(−2n)
(d) x(n) cos(0.3πn)

(e) x(n) ∗ x(n− 1)
(f) x(n) ∗ x(−n)

Frequency Analysis of Signals

Figure P19

304



23
determine and sketch the Fourier transform of the following signals:

(a) y1(n) =
{
x(n), n even
0, n odd

(b) y2(n) = x(2n)
(c) y3(n) =

{
x(n/2), n even
0, n odd

Note that y1(n) = x(n)s(n), where s(n) = {. . . 0, 1, 0, 1↑, 0, 1, 0, 1, . . .}

0
ω

−

X(ω)

π

4
π

4
π−π

Frequency Analysis of Signals

Answers to Selected Problems

2 (a) Xa(f ) = Aa+j2ππF ; |Xa(F )| = A√
a2+(2πF)

2 ; 	 Xa(f ) = − tan−1 2πfa

5 (a) x(n) =
{

11
2 , 2+ 34

√
2, 1, 2− 34

√
2, 12 , 2− 34

√
2, 1, 2+ 34

√
2
}

Hence, c0 = 2, c1 = c7 = 1, c2 = c6 = 12 , c3 = c5 = 14 , c4 = 0
(b) P = 538

6 (a)

ck =




1
2l , k = 3
1
2 , k = 5
1
2 , k = 10
−1
2j , k = 12
0, otherwise

(d) c0 = 0,
c1 = 2j5

[− sin ( 2π5 )+ 2 sin ( 4π5 )]
c2 = 2j5

[
sin

(
4π
5

)− 2 sin ( 2π5 )] ;
c3 = −c2 ; c4 = −c1

(h) N = 2; ck = 12 (1− e−jπk); c0 = 0, c1 = −1
9 (a) X(ω) = 1−e−j6ω1−e−jω

(e)
∑∞

n=−∞ |x(n)| → ∞.
Therefore, the Fourier transform does not exist.

(h) X(ω) = (2M + 1)A+ 2A∑Mk=1(2M + 1− k) coswk
14 (a) X(0) =∑n x(n) = −1;

(e)
∫ π
−π |X(ω|2dω = 2π

∑
n |x(n)|2 = (2π)(19) = 38π

17 (a)
∑

n x
∗(n)e−jωn = (∑n x(n)e−j (−ω)n))∗ = X∗(−ω)

(c) Y (ω) = X(ω)+X(ω)e−jω = (1− e−jω)X(ω)
(f) Y (ω) = X(2ω)

19 (a) X1(ω) = 12j
[
X
(
ω − π4

)+X (ω + π4 )]
(d) X4(ω) = 12 [X(ω − π)+X(ω + π)] = X(ω − π)

22 (a) X1(ω) = ejω/21−aejω/2
(d) X4(ω) = 12 [X(ω − 0.3π)+X(ω + 0.3π)]

23 (b) Y2(ω) = X
(
ω

2

)

From a discrete-time signal x(n) with Fourier transform X(ω), shown in Fig. P23,

Figure P23

305



This page intentionally left blank 



John G. Proakis, Dimitris G. Manolakis. Copyright © 2007 by Pearson Education, Inc. All rights reserved.
From Chapter 5   of Digital Signal Processing: Principles, Algorithms, and Applications, Fourth Edition.

Frequency-Domain
Analysis of LTI Systems

307



Frequency-Domain
Analysis of LTI Systems

We also develop frequency-domain relationships between the spectra of the
input and output sequences of an LTI system. The final section of this chapter
is focused on the application of LTI systems for performing inverse filtering and
deconvolution.

1 Frequency-Domain Characteristics of Linear Time-Invariant
Systems

In this section we develop the characterization of linear time-invariant systems in the
frequency domain. The basic excitation signals in this development are the complex
exponentials and sinusoidal functions. The characteristics of the system are described
by a function of the frequency variable ω called the frequency response, which is the
Fourier transform of the impulse response h(n) of the system.

The frequency response function completely characterizes a linear time-invariant
system in the frequency domain. This allows us to determine the steady-state re-
sponse of the system to any arbitrary weighted linear combination of sinusoids or
complex exponentials. Since periodic sequences, in particular, lend themselves to
a Fourier series decomposition as a weighted sum of harmonically related complex

In this chapter we treat the characterization of linear time-invariant systems in the
frequency domain. The basic excitation signals in this development are the complex
exponentials and sinusoidal functions. We will observe that a linear time-invariant 
(LTI) system performs a discrimination or filtering on the various frequency compo-
nents at its input. This observation leads us to characterize and classify some simple 
LTI systems according to the type of filtering they perform on any input signal. The 
design of these simple filters is described and some applications are given.

308



exponentials, it becomes a simple matter to determine the response of a linear time-
invariant system to this class of signals. This methodology is also applied to aperiodic
signals since such signals can be viewed as a superposition of infinitesimal size com-
plex exponentials.

1.1 Response to Complex Exponential and Sinusoidal Signals:
The Frequency Response Function

y(n) =
∞∑

k=−∞
h(k)x(n− k)

In this input–output relationship, the system is characterized in the time domain by
its unit sample response {h(n),−∞ < n <∞}.

To develop a frequency-domain characterization of the system, let us excite the
system with the complex exponential

x(n) = Aejωn, −∞ < n <∞

where A is the amplitude and ω is any arbitrary frequency confined to the frequency

y(n) =
∞∑

k=−∞
h(k)[Aejω(n−k)]

= A
[ ∞∑
k=−∞

h(k)e−jωk
]
ejωn

variable ω . In fact, this term is the Fourier transform of the unit sample response
h(k) of the system. Hence we denote this function as

H(ω) =
∞∑

k=−∞
h(k)e−jωk

Clearly, the function H(ω) exists if the system is BIBO stable, that is, if

∞∑
n=−∞

|h(n)| <∞

y(n) = AH(ω)ejωn

(1.1)

(1.2)

(1.3)

(1.4)

(1.5)

Frequency-Domain Analysis of LTI Systems

interval [−π, π ]. By substituting (1.2) into (1.1), we obtain the response

We observe that the term in brackets in (1.3) is a function of the frequency

With the definition in (1.4), the response of the system to the complex expo-
nential given in (1.2) is

x(n)

Recall that the response of any relaxed linear time-invariant system to an arbitrary 
input signal  is given by the convolution sum formula

309



We note that the response is also in the form of a complex exponential with the same
frequency as the input, but altered by the multiplicative factor H(ω).

called an eigenfunction of the system. In other words, an eigenfunction of a system
is an input signal that produces an output that differs from the input by a constant

linear time-invariant system, and H(ω) evaluated at the frequency of the input signal
is the corresponding eigenvalue.

Determine the output sequence of the system with impulse response

h(n) = (1
2
)nu(n)

when the input is the complex exponential sequence

x(n) = Aejπn/2, −∞ < n <∞
Solution. First we evaluate the Fourier transform of the impulse response h(n), and then

H(ω) =
∞∑

n=−∞
h(n)e−jωn = 1

1− 12 e−jω

H
(π

2

)
= 1

1+ j 12
= 2√

5
e−j26.6

◦

and therefore the output is

y(n) = A
(

2√
5
e−j26.6

◦
)
ejπn/2

y(n) = 2√
5
Aej(πn/2−26.6

◦), −∞ < n <∞

This example clearly illustrates that the only effect of the system on the input
signal is to scale the amplitude by 2/

√
5 and shift the phase by −26.6◦ . Thus the

output is also a complex exponential of frequency π/2, amplitude 2A/
√

5, and phase
−26.6◦ .

If we alter the frequency of the input signal, the effect of the system on the input
also changes and hence the output changes. In particular, if the input sequence is a
complex exponential of frequency π , that is,

x(n) = Aejπn, −∞ < n <∞

(1.6)

(1.7)

(1.8)

(1.9)

EXAMPLE 1.1

Frequency-Domain Analysis of LTI Systems

multiplicative factor. The multiplicative factor is called an eigenvalue of the system.

As a result of this characteristic behavior, the exponential signal in (1.2) is

In this case, a complex exponential signal of the form (1.2) is an eigenfunction of a

we use (1.5) to determine y(n).

At ω = π/2, (1.7) yields

We recall that

310



then, at ω = π ,
H(π) = 1

1− 12e−jπ
= 13

2

= 2
3

and the output of the system is

y(n) = 2
3
Aejπn, −∞ < n <∞

We note that H(π) is purely real [i.e., the phase associated with H(ω) is zero at
ω = π ]. Hence, the input is scaled in amplitude by the factor H(π) = 23 , but the
phase shift is zero.

In general, H(ω) is a complex-valued function of the frequency variable ω .
Hence it can be expressed in polar form as

H(ω) = |H(ω)|ej�(ω)

where |H(ω)| is the magnitude of H(ω) and

�(ω) = �H(ω)

which is the phase shift imparted on the input signal by the system at the frequency ω .
Since H(ω) is the Fourier transform of {h(k)}, it follows that H(ω) is a periodic

series expansion for H(ω), with h(k) as the Fourier series coefficients. Consequently,
the unit impulse h(k) is related to H(ω) through the integral expression

h(k) = 1
2π

∫ π
−π
H(ω)ejωkdω

For a linear time-invariant system with a real-valued impulse response, the mag-
nitude and phase functions possess symmetry properties which are developed as
follows. From the definition of H(ω), we have

H(ω) =
∞∑

k=−∞
h(k)e−jωk

=
∞∑

k=−∞
h(k) cosωk − j

∞∑
k=−∞

h(k) sinωk

= HR(ω)+ jHI (ω)

=
√
H 2R(ω)+H 2I (ω)ej tan

−1[HI (ω)/HR(ω)]

(1.10)

(1.11)

(1.12)

(1.13)

Frequency-Domain Analysis of LTI Systems

function with period 2π . Furthermore, we can view (1.4) as the exponential Fourier

311



where HR(ω) and HI(ω) denote the real and imaginary components of H(ω), de-
fined as

HR(ω) =
∞∑

k=−∞
h(k) cosωk

HI (ω) = −
∞∑

k=−∞
h(k) sinωk

of HR(ω) and HI(ω), are

|H(ω)| =
√
H 2R(ω)+H 2I (ω)

�(ω) = tan−1 HI(ω)
HR(ω)

We note thatHR(ω) = HR(−ω) andHI(ω) = −HI(−ω), so thatHR(ω) is an even
function of ω and HI (ω) is an odd function of ω . As a consequence, it follows that
|H(ω)| is an even function of ω and �(ω) is an odd function of ω . Hence, if we know
|H(ω)| and �(ω) for 0 ≤ ω ≤ π , we also know these functions for −π ≤ ω ≤ 0.

Moving Average Filter

Determine the magnitude and phase ofH(ω) for the three-point moving average (MA) system

y(n) = 1
3

[x(n+ 1)+ x(n)+ x(n− 1)]

and plot these two functions for 0 ≤ ω ≤ π .

Solution. Since

h(n) = {1
3
,

1
3
↑
,

1
3
}

it follows that

H(ω) = 1
3
(ejω + 1+ e−jω) = 1

3
(1+ 2 cosω)

Hence

|H(ω)| = 1
3
|1+ 2 cosω|

�(ω) =
{

0, 0 ≤ ω ≤ 2π/3
π, 2π/3 ≤ ω < π

|H(ω)| is an even function of frequency and�(ω) is an odd function of frequency. It is apparent
from the frequency response characteristic H(ω) that this moving average filter smooths the
input data, as we would expect from the input–output equation.

(1.14)

(1.15)

(1.16)

EXAMPLE 1.2

Frequency-Domain Analysis of LTI Systems

It is clear from (1.12) that the magnitude and phase of H(ω), expressed in terms

Figure 1.1 illustrates the graphs of the magnitude and phase ofH(ω). As indicated previously,

312



−π −π/2 0 π/2 π

0.2
0.4
0.6
0.8

1
1.2
1.4

ω

|H
(ω

)|

−π −π/2 0 π/2 π
−π

−π/2

0

π/2

π

Θ
(ω

) 
(R

ad
s)

ω

The symmetry properties satisfied by the magnitude and phase functions of
H(ω), and the fact that a sinusoid can be expressed as a sum or difference of two
complex-conjugate exponential functions, imply that the response of a linear time-
invariant system to a sinusoid is similar in form to the response when the input is a
complex exponential. Indeed, if the input is

x1(n) = Aejωn

the output is
y1(n) = A|H(ω)|ej�(ω)ejωn

On the other hand, if the input is

x2(n) = Ae−jωn

the response of the system is

y2(n) = A|H(−ω)|ej�(−ω)e−jωn

= A|H(ω)|e−j�(ω)e−jωn

where, in the last expression, we have made use of the symmetry properties |H(ω)| =
|H(−ω)| and �(ω) = −�(−ω). Now, by applying the superposition property of the
linear time-invariant system, we find that the response of the system to the input

Figure 1.1 Magnitude and phase responses for the MA system in Example 1.2.

Frequency-Domain Analysis of LTI Systems

313



x(n) = 1
2

[x1(n)+ x2(n)] = A cosωn

is

y(n) = 1
2

[y1(n)+ y2(n)]

y(n) = A|H(ω)| cos[ωn+�(ω)]
Similarly, if the input is

x(n) = 1
j2

[x1(n)− x2(n)] = A sinωn

the response of the system is

y(n) = 1
j2

[y1(n)− y2(n)]

y(n) = A|H(ω)| sin[ωn+�(ω)]
It is apparent from this discussion that H(ω), or equivalently, |H(ω)| and �(ω),

completely characterize the effect of the system on a sinusoidal input signal of
any arbitrary frequency. Indeed, we note that |H(ω)| determines the amplification
(|H(ω)| > 1) or attenuation (|H(ω)| < 1) imparted by the system on the input sinu-
soid. The phase �(ω) determines the amount of phase shift imparted by the system
on the input sinusoid. Consequently, by knowing H(ω), we are able to determine
the response of the system to any sinusoidal input signal. Since H(ω) specifies the
response of the system in the frequency domain, it is called the frequency response
of the system. Correspondingly, |H(ω)| is called the magnitude response and �(ω)
is called the phase response of the system.

If the input to the system consists of more than one sinusoid, the superposition
property of the linear system can be used to determine the response. The following
examples illustrate the use of the superposition property.

x(n) = 10− 5 sin π
2
n+ 20 cosπn, −∞ < n <∞

Solution.

H(ω) = 1
1− 12e−jω

The first term in the input signal is a fixed signal component corresponding to ω = 0. Thus

H(0) = 1
1− 12

= 2

(1.17)

(1.18)

EXAMPLE 1.3

Frequency-Domain Analysis of LTI Systems

Determine the response of the system in Example 1.1 to the input signal

The frequency response of the system is given in (1.7) as

314



The second term in x(n) has a frequency π/2. At this frequency the frequency response of
the system is

H
(π

2

)
= 2√

5
e−j26.6

◦

Finally, the third term in x(n) has a frequency ω = π . At this frequency

H(π) = 2
3

Hence the response of the system to x(n) is

y(n) = 20− 10√
5

sin
(π

2
n− 26.6◦

)
+ 40

3
cosπn, −∞ < n <∞

A linear time-invariant system is described by the following difference equation:

y(n) = ay(n− 1)+ bx(n), 0 < a < 1

(a) Determine the magnitude and phase of the frequency response H(ω) of the system.
(b) Choose the parameter b so that the maximum value of |H(ω)| is unity, and sketch |H(ω)|

and �H(ω) for a = 0.9.
(c) Determine the output of the system to the input signal

x(n) = 5+ 12 sin π
2
n− 20 cos

(
πn+ π

4

)

Solution. The impulse response of the system is

h(n) = banu(n)

Since |a| < 1, the system is BIBO stable and hence H(ω) exists.
(a) The frequency response is

H(ω) =
∞∑

n=−∞
h(n)e−jωn

= b
1− ae−jω

Since
1− ae−jω = (1− a cosω)+ ja sinω

it follows that

|1− ae−jω| =
√
(1− a cosω)2 + (a sinω)2

=
√

1+ a2 − 2a cosω

EXAMPLE 1.4

Frequency-Domain Analysis of LTI Systems

315



and

�(1− ae−jω) = tan−1 a sinω
1− a cosω

Therefore,

|H(ω)| = |b|√
1+ a2 − 2a cosω

�H(ω) = �(ω) = �b − tan−1 a sinω
1− a cosω

(b) Since the parameter a is positive, the denominator of |H(ω)| attains a minimum at ω = 0.
Therefore, |H(ω)| attains its maximum value at ω = 0. At this frequency we have

|H(0)| = |b|
1− a = 1

which implies that b = ±(1− a). We choose b = 1− a , so that

|H(ω)| = 1− a√
1+ a2 − 2a cosω

and

�(ω) = − tan−1 a sinω
1− a cosω

observe that this system attenuates high-frequency signals.

−π −π/2 0 π/2 π

1

2

3

4

ω

|H
( ω

)|

−π −π/2 0 π/2 π
−π

2

0

π/2

π

Θ
( ω

) 
(R

ad
s)

ω

Magnitude and phase responses for the system inFigure 1.2
Example 1.4 with a = 0.9.

Frequency-Domain Analysis of LTI Systems

The frequency response plots for |H(ω)| and �(ω) are illustrated in Fig. 1.2. We

316



(c) The input signal consists of components at frequencies ω = 0, π/2, and π . For ω = 0,
|H(0)| = 1 and �(0) = 0. For ω = π/2,

∣∣∣H (π
2

)∣∣∣ = 1− a√
1+ a2 =

0.1√
1.81
= 0.074

�
(π

2

)
= − tan−1 a = −42◦

For ω = π ,

|H(π)| = 1− a
1+ a =

0.1
1.9
= 0.053

�(π) = 0

Therefore, the output of the system is

y(n) = 5|H(0)| + 12
∣∣∣H (π

2

)∣∣∣ sin [π
2
n+�

(π
2

)]

− 20|H(π)| cos
[
πn+ π

4
+�(π)

]

= 5+ 0.888 sin
(π

2
n− 42◦

)
− 1.06 cos

(
πn+ π

4

)
, −∞ < n <∞

In the most general case, if the input to the system consists of an arbitrary linear
combination of sinusoids of the form

x(n) =
L∑
i=1

Ai cos(ωin+ φi), −∞ < n <∞

where {Ai} and {φi} are the amplitudes and phases of the corresponding sinusoidal
components, then the response of the system is simply

y(n) =
L∑
i=1

Ai |H(ωi)| cos[ωin+ φi +�(ωi)]

where |H(ωi)| and �(ωi) are the magnitude and phase, respectively, imparted by
the system to the individual frequency components of the input signal.

It is clear that depending on the frequency response H(ω) of the system, input
sinusoids of different frequencies will be affected differently by the system. For
example, some sinusoids may be completely suppressed by the system if H(ω) = 0
at the frequencies of these sinusoids. Other sinusoids may receive no attenuation (or
perhaps, some amplification) by the system. In effect, we can view the linear time-
invariant system functioning as a filter to sinusoids of different frequencies, passing
some of the frequency components to the output and suppressing or preventing other
frequency components from reaching the output.
lem involves determining the parameters of a linear time-invariant system to achieve
a desired frequency response H(ω).

(1.19)

Frequency-Domain Analysis of LTI Systems

-The basic digital filter design prob

317



Steady-State and Transient Response to Sinusoidal
Input Signals

In the discussion in the preceding section, we determined the response of a lin-
ear time-invariant system to exponential and sinusoidal input signals applied to the
system at n = −∞. We usually call such signals eternal exponentials or eternal si-
nusoids, because they were applied at n = −∞. In such a case, the response that we
observe at the output of the system is the steady-state response. There is no transient
response in this case.

On the other hand, if the exponential or sinusoidal signal is applied at some
finite time instant, say at n = 0, the response of the system consists of two terms,
the transient response and the steady-state response. To demonstrate this behavior,
let us consider, as an example, the system described by the first-order difference
equation

y(n) = ay(n− 1)+ x(n)

y(n) = an+1y(−1)+
n∑
k=0

akx(n− k), n ≥ 0

where y(−1) is the initial condition.
Now, let us assume that the input to the system is the complex exponential

x(n) = Aejωn, n ≥ 0

y(n) = an+1y(−1)+ A
n∑
k=0

akejω(n−k)

= an+1y(−1)+ A
[

n∑
k=0
(ae−jω)k

]
ejωn

= an+1y(−1)+ A1− a
n+1e−jω(n+1)

1− ae−jω e
jωn, n ≥ 0

= an+1y(−1)− Aa
n+1e−jω(n+1)

1− ae−jω e
jωn + A

1− ae−jω e
jωn, n ≥ 0

In this case
the two terms involving an+1
Consequently, we are left with the steady-state response

yss(n) = lim
n→∞ y(n) =

A

1− ae−jω e
jωn

= AH(ω)ejωn

(1.20)

(1.21)

(1.22)

(1.23)

(1.24)

1.2

Frequency-Domain Analysis of LTI Systems

applied at n = 0. When we substitute (1.22) into (1.21), we obtain

We recall that the system in (1.20) is BIBO stable if |a| < 1.
in (1.23) decay toward zero as n approaches infinity.

x(n) n = 0This system’s response to any input  applied at  is given as

318



that is,

ytr(n) = an+1y(−1)− Aa
n+1e−jω(n+1)

1− ae−jω e
jωn, n ≥ 0

which decay toward zero as n approaches infinity. The first term in the transient
response is the zero-input response of the system and the second term is the transient
produced by the exponential input signal.

In general, all linear time-invariant BIBO systems behave in a similar fashion
when excited by a complex exponential, or by a sinusoid at n = 0 or at some other
finite time instant. That is, the transient response decays toward zero as n → ∞,
leaving only the steady-state response that we determined in the preceding section.
In many practical applications, the transient response of the system is unimportant,
and therefore it is usually ignored in dealing with the response of the system to
sinusoidal inputs.

Steady-State Response to Periodic Input Signals

Suppose that the input to a stable linear time-invariant system is a periodic signal
x(n) with fundamental period N . Since such a signal exists from −∞ < n <∞, the
total response of the system at any time instant n is simply equal to the steady-state
response.

To determine the response y(n) of the system, we make use of the Fourier series
representation of the periodic signal, which is

x(n) =
N−1∑
k=0

cke
j2πkn/N , k = 0, 1, . . . , N − 1

where the {ck} are the Fourier series coefficients. Now the response of the system to
the complex exponential signal

xk(n) = ckej2πkn/N , k = 0, 1, . . . , N − 1

is

yk(n) = ckH
(

2π
N
k

)
ej2πkn/N , k = 0, 1, . . . , N − 1

where

H

(
2πk
N

)
= H(ω)|ω=2πk/N , k = 0, 1, . . . , N − 1

By using the superposition principle for linear systems, we obtain the response of the

y(n) =
N−1∑
k=0

ckH

(
2πk
N

)
e , −∞ < n <∞

(1.25)

(1.26)

(1.27)

(1.28)

1.3

Frequency-Domain Analysis of LTI Systems

The first two terms in (1.23) constitute the transient response of the system,

j2πkn/N

system to the periodic signal x(n) in (1.26) as

319



This result implies that the response of the system to the periodic input signal
x(n) is also periodic with the same period N . The Fourier series coefficients for y(n)
are

dk ≡ ckH
(

2πk
N

)
, k = 0, 1, . . . , N − 1

Hence, the linear system can change the shape of the periodic input signal by scaling
the amplitude and shifting the phase of the Fourier series components, but it does
not affect the period of the periodic input signal.

Response to Aperiodic Input Signals

Y (ω) = H(ω)X(ω)
where Y (ω), X(ω), and H(ω) are the corresponding Fourier transforms of {y(n)},
{x(n)}, and {h(n)}, respectively. From this relationship we observe that the spectrum
of the output signal is equal to the spectrum of the input signal multiplied by the
frequency response of the system.

If we express Y (ω), H(ω), and X(ω) in polar form, the magnitude and phase of
the output signal can be expressed as

|Y (ω)| = |H(ω)||X(ω)|
�Y (ω) = �X(ω)+�H(ω)

where |H(ω)| and �H(ω) are the magnitude and phase responses of the system.
By its very nature, a finite-energy aperiodic signal contains a continuum of fre-

quency components. The linear time-invariant system, through its frequency re-
sponse function, attenuates some frequency components of the input signal and am-
plifies other frequency components. Thus the system acts as a filterto the input signal.
Observation of the graph of |H(ω)| shows which frequency components are ampli-
fied and which are attenuated. On the other hand, the angle of H(ω) determines the
phase shift imparted in the continuum of frequency components of the input signal
as a function of frequency. If the input signal spectrum is changed by the system in an
undesirable way, we say that the system has caused magnitude and phase distortion.

We also observe that the output of a linear time-invariant system cannot contain
frequency components that are not contained in the input signal. It takes either a
linear time-variant system or a nonlinear system to create frequency components
that are not necessarily contained in the input signal.

can be used in the analysis of BIBO-stable LTI systems. We observe that in time-
domain analysis, we deal with the convolution of the input signal with the impulse

(1.29)

(1.30)

(1.31)

(1.32)

1.4

Figure 1.3 illustrates the time-domain and frequency-domain relationships that

Frequency-Domain Analysis of LTI Systems

{x(n)}
{y(n)} {h(n)}

The convolution theorem provides the desired frequency-domain relationship for 
determining the output of anLTI system to an aperiodic finite-energy signal. If
denotes the input sequence,  denotes the output sequence, and  denotes 
the unit sample response of the system, then from the convolution theorem, we have

320



Time- and frequency-
domain input–output
relationships in LTI systems.

x(n)
X(ω)

y(n) = h(n)*x(n)
Y(ω) = H(ω)X(ω)

Linear
time-invariant

system
h(n), H(ω)

Input Output

response of the system to obtain the output sequence of the system. On the other
hand, in frequency-domain analysis, we deal with the input signal spectrum X(ω) and
the frequency responseH(ω) of the system, which are related through multiplication,
to yield the spectrum of the signal at the output of the system.

signal. Then the output sequence {y(n)} can be determined from the inverse Fourier
transform

y(n) = 1
2π

∫ π
−π
Y (ω)ejωndω

However, this method is seldom used. Instead, the z-transform is a simpler method
for solving the problem of determining the output sequence {y(n)}.

squared magnitude of both sides. Thus we obtain

|Y (ω)|2 = |H(ω)|2|X(ω)|2

Syy(ω) = |H(ω)|2Sxx(ω)

where Sxx(ω) and Syy(ω) are the energy density spectra of the input and output
signals, respectively.
obtain the energy of the output signal as

Ey = 12π
∫ π
−π
Syy(ω)dω

= 1
2π

∫ π
−π
|H(ω)|2Sxx(ω)dω

A linear time-invariant system is characterized by its impulse response

h(n) = (1
2
)nu(n)

Determine the spectrum and the energy density spectrum of the output signal when the system
is excited by the signal

x(n) = (1
4
)nu(n)

(1.33)

(1.34)

(1.35)

Figure 1.3

EXAMPLE 1.5

Frequency-Domain Analysis of LTI Systems

We can use the relation in (1.30) to determine the spectrum Y (ω) of the output

Let us return to the basic input–output relation in (1.30) and compute the

By integrating (1.34) over the frequency range (−π, π), we

321



Solution. The frequency response function of the system

H(ω) =
∞∑
n=0
(

1
2
)ne−jωn

= 1
1− 12 e−jω

Similarly, the input sequence {x(n)} has a Fourier transform

X(ω) = 1
1− 14e−jω

Hence the spectrum of the signal at the output of the system is

Y (ω) = H(ω)X(ω)

= 1
(1− 12 e−jω)(1− 14e−jω)

The corresponding energy density spectrum is

Syy(ω) = |Y (ω)|2 = |H(ω)|2|X(ω)|2

= 1
( 54 − cosω)( 1716 − 12 cosω)

Frequency Response of LTI Systems

In this section we focus on determining the frequency response of LTI systems that
have rational system functions. Recall that this class of LTI systems is described in
the time domain by constant-coefficient difference equations.

2.1 Frequency Response of a System with a Rational System
Function

H(ω) = H(z)|z=ejω =
∞∑

n=−∞
h(n)e−jωn (2.1)

2

Frequency-Domain Analysis of LTI Systems

H(z)If the system function  converges on the unit circle, we can obtain the frequency 
response of the system by evaluating  on the unit circle. ThusH(z)

322



In the case where H(z) is a rational function of the form H(z) = B(z)/A(z), we have

H(ω) = B(ω)
A(ω)

=

M∑
k=0

bke
−jωk

1+
N∑
k=1

ake
−jωk

= b0

M∏
k=1
(1− zke−jω)

N∏
k=1
(1− pke−jω)

where the {ak} and {bk} are real, but {zk} and {pk} may be complex valued.
It is sometimes desirable to express the magnitude squared of H(ω) in terms of

H(z). First, we note that
|H(ω)|2 = H(ω)H ∗(ω)

H ∗(ω) = b0

M∏
k=1
(1− z∗kejω)

N∏
k=1
(1− p∗k ejω)

It follows that H ∗(ω) is obtained by evaluating H ∗(1/z∗) on the unit circle, where
for a rational system function,

H ∗(1/z∗) = b0

M∏
k=1
(1− z∗kz)

N∏
k=1
(1− p∗k z)

However, when {h(n)} is real or, equivalently, the coefficients {ak} and {bk} are
real, complex-valued poles and zeros occur in complex-conjugate pairs. In this case,
H ∗(1/z∗) = H(z−1). Consequently, H ∗(ω) = H(−ω), and

|H(ω)|2 = H(ω)H ∗(ω) = H(ω)H(−ω) = H(z)H(z−1)|z=ejω

(2.2)

(2.3)

(2.4)

(2.5)

(2.6)

Frequency-Domain Analysis of LTI Systems

For the rational system function given by (2.3), we have

z-transform
{rhh(m)}H(z)H(z−1)

According to the correlation theorem for the , the function  
 is the z-transform of the autocorrelation sequence  of 

323



{h(n)}.
|H(ω)|2 {rhh(m)}.

Similarly, if H(z) = B(z)/A(z), the transforms D(z) = B(z)B(z−1) and C(z) =
A(z)A(z−1) are the z-transforms of the autocorrelation sequences {cl} and {dl}, where

cl =
N−|l|∑
k=0

akak+l , −N ≤ l ≤ N

dl =
M−|l|∑
k=0

bkbk+l , −M ≤ l ≤ M

Since the system parameters {ak} and {bk} are real valued, it follows that cl = c−l
and dl = d−l . By using this symmetry property, |H(ω)|2 may be expressed as

|H(ω)|2 =
d0 + 2

M∑
k=1

dk cos kω

c0 + 2
N∑
k=1

ck cos kω

Finally, we note that cos kω can be expressed as a polynomial function of cosω .
That is,

cos kω =
k∑

m=0
βm(cosω)m

where {βm} are the coefficients in the expansion. Consequently, the numerator and
denominator of |H(ω)|2 can be viewed as polynomial functions of cosω . The fol-
lowing example illustrates the foregoing relationships.

Determine |H(ω)|2 for the system

y(n) = −0.1y(n− 1)+ 0.2y(n− 2)+ x(n)+ x(n− 1)

Solution. The system function is

H(z) = 1+ z
−1

1+ 0.1z−1 − 0.2z−2

and its ROC is |z| > 0.5. Hence H(ω) exists. Now

H(z)H(z−1) = 1+ z
−1

1+ 0.1z−1 − 0.2z−2 ·
1+ z

1+ 0.1z− 0.2z2

= 2 + z+ z
−1

1.05+ 0.08(z + z−1)− 0.2(z−2 + z−2)

(2.7)

(2.8)

(2.9)

(2.10)

EXAMPLE 2.1

Frequency-Domain Analysis of LTI Systems

the unit  sample response  Then it follows from the Wiener–Khintchine the
orem that  is the Fourier transform of

-

324



By evaluating H(z)H(z−1) on the unit circle, we obtain

|H(ω)|2 = 2 + 2 cosω
1.05+ 0.16 cosω − 0.4 cos 2ω

However, cos 2ω = 2 cos2 ω − 1. Consequently, |H(ω)|2 may be expressed as

|H(ω)|2 = 2(1+ cosω)
1.45+ 0.16 cosω − 0.8 cos2 ω

We note that given H(z), it is straightforward to determine H(z−1) and then
|H(ω)|2 . However, the inverse problem of determining H(z), given |H(ω)|2 or the
corresponding impulse response {h(n)}, is not straightforward. Since |H(ω)|2 does
not contain the phase information in H(ω), it is not possible to uniquely determine
H(z).

To elaborate on the point, let us assume that the N poles and M zeros of H(z)
are {pk} and {zk}, respectively. The corresponding poles and zeros of H(z−1) are
{1/pk} and {1/zk}, respectively. Given |H(ω)|2 or, equivalently, H(z)H(z−1), we
can determine different system functions H(z) by assigning to H(z), a pole pk or
its reciprocal 1/pk , and a zero zk or its reciprocal 1/zk . For example, if N = 2 and
M = 1, the poles and zeros of H(z)H(z−1) are {p1 , p2 , 1/p1 , 1/p2} and {z1, 1/z1}. If
p1 and p2 are real, the possible poles for H(z) are {p1, p2}, {1/p1, 1/p2}, {p1, 1/p2},
and {p2, 1/p1} and the possible zeros are {z1} or {1/z1}. Therefore, there are eight
possible choices of system functions, all of which result in the same |H(ω)|2 . Even if
we restrict the poles of H(z) to be inside the unit circle, there are still two different
choices for H(z), depending on whether we pick the zero {z1} or {1/z1}. Therefore,
we cannot determine H(z) uniquely given only the magnitude response |H(ω)|.

Computation of the Frequency Response Function

In evaluating the magnitude response and the phase response as functions of fre-
quency, it is convenient to express H(ω) in terms of its poles and zeros. Hence we
write H(ω) in factored form as

H(ω) = b0

M∏
k=1
(1− zke−jωk)

N∏
k=1
(1− pke−jωk)

or, equivalently, as

H(ω) = b0ejω(N−M)

M∏
k=1
(ejω − zk)

N∏
k=1
(ejω − pk)

(2.11)

(2.12)

2.2

Frequency-Domain Analysis of LTI Systems

325



ejω − zk = Vk(ω)ej�k(ω)

and
ejω − pk = Uk(ω)ej�k(ω)

where
Vk(ω) ≡ |ejω − zk|, �k(ω) ≡ �(ejω − zk)

and
Uk(ω) ≡ |ejω − pk|, �k(ω) = �(ejω − pk)

The magnitude of H(ω) is equal to the product of magnitudes of all terms in

|H(ω)| = |b0| V1(ω) · · ·VM(ω)
U1(ω)U2(ω) · · ·UN(ω)

since the magnitude of ejω(N−M) is 1.
The phase of H(ω) is the sum of the phases of the numerator factors, minus the

have

�H(ω) = �b0 + ω(N −M)+�1(ω)+�2(ω)+ · · · +�M(ω)
− [�1(ω)+�2(ω)+ · · · +�N(ω)]

The phase of the gain term b0 is zero or π , depending on whether b0 is positive or
negative. Clearly, if we know the zeros and the poles of the system function H(z),

Let us consider a pole pk and a zero zk located at points A and B of the

value of frequency ω . The given value of ω determines the angle of ejω with the
positive real axis. The tip of the vector ejω specifies a point L on the unit circle.
The evaluation of the Fourier transform for the given value of ω is equivalent to
evaluating the z-transform at the point L of the complex plane. Let us draw the
vectors AL and BL from the pole and zero locations to the point L, at which we wish

CL = CA+ AL

and
CL = CB+ BL

However, CL = ejω , CA = pk and CB = zk . Thus

AL = ejω − pk

(2.13)

(2.14)

(2.15)

(2.16)

(2.17)

(2.18)

(2.19)

Frequency-Domain Analysis of LTI Systems

Let us express the complex-valued factors in (2.12) in polar form as

phases of the denominator factors. Thus, by combining (2.13) through (2.16), we

we can evaluate the frequency response from (2.17) and (2.18).
There is a geometric interpretation of the quantities appearing in (2.17) and

(2.18).
z-plane, as shown in Fig. 2.1(a). Assume that we wish to compute H(ω) at a specific

(2.12). Thus, using (2.13) through (2.16), we obtain

to compute the Fourier transform. From Fig. 2.1(a) it follows that

326



Geometric interpretation of
the contribution of a pole
and a zero to the Fourier
transform (1) magnitude:
the factor Vk/Uk , (2) phase:
the factor �k −�k .

Im(z)

Re(z)

A
pk

zk

C

ejω

0 1

ω

B

LUnit circle
z = ejω

or |z| = 1

(a)

Im(z)

Re(z)

pk

Uk
Vk

zk

0 1

L
Unit circle

(b)

Θk(ω)Φk(ω)

and
BL = ejω − zk

AL = ejω − pk = Uk(ω)ej�k(ω)

BL = ejω − zk = Vk(ω)ej�k(ω)

Thus Uk(ω) is the length of AL, that is, the distance of the pole pk from the point
L corresponding to ejω , whereas Vk(ω) is the distance of the zero zk from the same

(2.20)

(2.21)

(2.22)

Figure 2.1

Frequency-Domain Analysis of LTI Systems

By combining these relations with (2.13) and (2.14), we obtain

327



A zero on the unit circle
causes |H(ω)| = 0 and
ω = �zk . In contrast, a
pole on the unit circle
results in |H(ω)| = ∞ at
ω = �pk .

Im(z)

Re(z)

�pk
�zk

zk = e j�zk
Vk

Uk

ejω

0 1

ω

L
Unit circle

Pk = ej�Pk

point L. The phases �k(ω) and �k(ω) are the angles of the vectors AL and BL
with the positive real axis, respectively. These geometric interpretations are shown

Geometric interpretations are very useful in understanding how the location of
poles and zeros affects the magnitude and phase of the Fourier transform. Suppose

k k

We note that at ω = �zk , Vk(ω) and consequently |H(ω)| become zero. Similarly, at
ω = �pk the length Uk(ω) becomes zero and hence |H(ω)| becomes infinite. Clearly,
the evaluation of phase in these cases has no meaning.

From this discussion we can easily see that the presence of a zero close to the unit
circle causes the magnitude of the frequency response, at frequencies that correspond
to points of the unit circle close to that point, to be small. In contrast, the presence of
a pole close to the unit circle causes the magnitude of the frequency response to be
large at frequencies close to that point. Thus poles have the opposite effect of zeros.
Also, placing a zero close to a pole cancels the effect of the pole, and vice versa. This

k = pk , the terms ejω − zk and ejω − pk
cancel. Obviously, the presence of both poles and zeros in a transform results in a
greater variety of shapes for |H(ω)| and �H(ω). This observation is very important
in the design of digital filters. We conclude our discussion with the following example
illustrating these concepts.

Evaluate the frequency response of the system described by the system function

H(z) = 1
1− 0.8z−1 =

z

z− 0.8
Solution. Clearly, H(z) has a zero at z = 0 and a pole at p = 0.8. Hence the frequency
response of the system is

H(ω) = e
jω

ejω − 0.8

Figure 2.2

in Fig. 2.1(b).

EXAMPLE 2.2

Frequency-Domain Analysis of LTI Systems

that a zero, say z , and a pole, say p , are on the unit circle as shown in Fig. 2.2.

can be also seen from (2.12), since if z

328



Magnitude and
phase of system with
H(z) = 1/(1− 0.8z−1).

π
2

π
2

π
2

−

π
2

−

0

0

π

π

−π
−π

Ph
as

e 
(r

ad
ia

ns
)

|H
(ω

) |

π
2

π
2

− 0
0

0.4

0.2

0.6

0.8

1.0

1.2

π−π

The magnitude response is

|H(ω)| = |e
jω|

|ejω − 0.8| =
1√

1.64− 1.6 cosω
and the phase response is

θ(ω) = ω − tan−1 sinω
cosω − 0.8

Note that the peak of the
magnitude response occurs at ω = 0, the point on the unit circle closest to the pole located
at 0.8.

|H(ω)|dB = 20 log10 |b0| + 20
M∑
k=1

log10 Vk(ω)− 20
N∑
k=1

log10 Uk(ω)

Thus the magnitude response is expressed as a sum of the magnitude factors in
|H(ω)|.

3 Correlation Functions and Spectra at the Output of LTI
Systems

In this section, we derive the spectral relationships between the input and output

(2.23)

Figure 2.3

The magnitude and phase responses are illustrated in Fig. 2.3.

Frequency-Domain Analysis of LTI Systems

If the magnitude response in (2.17) is expressed in decibels,

signals of LTI systems. Section 3.1 describes the relationships for the energy density

relationships for the power density spectra of random input and output signals.
spectra of deterministic input and output signals. Section 3.2 is focused on the

329



3.1 Input–Output Correlation Functions and Spectra

ryy(m) = rhh(m) ∗ rxx(m)
ryx(m) = h(m) ∗ rxx(m)

where rxx(m) is the autocorrelation sequence of the input signal {x(n)}, ryy(m) is the
autocorrelation sequence of the output {y(n)}, rhh(m) is the autocorrelation sequence
of the impulse response {h(n)}, and ryx
the output and the input signals.
operation, the z-transform of these equations yields

Syy(z) = S (z)Sxx(z)
= H(z)H(z−1)Sxx(z)

Syx(z) = H(z)Sxx(z)

If we substitute z = ejω

Syx(ω) = H(ω)Sxx(ω)
= H(ω)|X(ω)|2

where Syx(ω) is the cross-energy density spectrum of {y(n)} and {x(n)}. Similarly,
evaluating Syy(z) on the unit circle yields the energy density spectrum of the output
signal as

Syy(ω) = |H(ω)|2Sxx(ω)
where Sxx(ω) is the energy density spectrum of the input signal.

Since ryy(m) and Syy(ω) are a Fourier transform pair, it follows that

ryy(m) = 12π
∫ π
−π
Syy(ω)e

jωmdω

The total energy in the output signal is simply

Ey = 12π
∫ π
−π
Syy(ω)dω = ryy(0)

= 1
2π

∫ π
−π
|H(ω)|2 Sxx(ω)dω

y ≥ 0.

(3.1)

(3.2)

(3.3)

(3.4)

(3.5)

(3.6)

(3.7)

(3.8)

Frequency-Domain Analysis of LTI Systems

(m) is the crosscorrelation sequence between

hh

Since (3.1) and (3.2) involve the convolution

in (3.4), we obtain

The result in (3.8) may be used to easily prove that E

Consider several correlation relationships between the input and the output se-
quences of an LTI system. Specifically, the following equations:

330



Finally, we note that if the input signal has a flat spectrum [i.e., Sxx(ω) = Sx =

Syx(ω) = H(ω)Sx
where Sx is the constant value of the spectrum. Hence

H(ω) = 1
Sx
Syx(ω)

or, equivalently,

h(n) = 1
Sx
ryx(m)

the system by a spectrally flat signal {x(n)}, and crosscorrelating the input with the
output of the system. This method is useful in measuring the impulse response of an
unknown system.

Correlation Functions and Power Spectra for Random Input
Signals

we now deal with the statistical mean and autocorrelation of the input and output
signals of an LTI system.

Let us consider a discrete-time linear time-invariant system with unit sample
response {h(n)} and frequency response H(f ). For this development we assume
that {h(n)} is real. Let x(n) be a sample function of a stationary random process

From the convolution summation that relates the output to the input we have

y(n) =
∞∑

k=−∞
h(k)x(n− k)

Since x(n) is a random input signal, the output is also a random sequence. In other
words, for each sample sequence x(n) of the process X(n), there is a correspond-
ing sample sequence y(n) of the output random process Y (n). We wish to relate
the statistical characteristics of the output random process Y (n) to the statistical
characterization of the input process and the characteristics of the system.

The expected value of the output y(n) is

my ≡ E[y(n)] = E[
∞∑

k=−∞
h(k)x(n− k)]

=
∞∑

k=−∞
h(k)E[x(n− k)]

my = mx
∞∑

k=−∞
h(k)

(3.9)

(3.10)

(3.11)

(3.12)

(3.13)

3.2

Frequency-Domain Analysis of LTI Systems

constant for π ≤ ω ≤ −π ], (3.5) reduces to

The relation in (3.11) implies that h(n) can be determined by exciting the input to

X(n) that excites the system and let y(n) denote the response of the system to x(n).

This development parallels the derivations in Section 3.1, with the exception that

331



From the Fourier transform relationship

H(ω) =
∞∑

k=−∞
h(k)e−jωk

we have

H(0) =
∞∑

k=−∞
h(k)

my = mxH(0)
The autocorrelation sequence for the output random process is defined as

γyy(m) = E[y∗(n)y(n+m)]

= E

 ∞∑
k=−∞

h(k)x∗(n− k)
∞∑

j=−∞
h(j)x(n+m− j)




=
∞∑

k=−∞

∞∑
j=−∞

h(k)h(j)E[x∗(n− k)x(n+m− j)]

=
∞∑

k=−∞

∞∑
j=−∞

h(k)h(j)γxx(k − j +m)

This is the general form for the autocorrelation of the output in terms of the auto-
correlation of the input and the impulse response of the system.

that is, when mx = 0 and
γxx(m) = σ 2x δ(m)

where σ 2x ≡ γxx

γyy(m) = σ 2x
∞∑

k=−∞
h(k)h(k +m)

Under this condition the output process has the average power

γyy(0) = σ 2x
∞∑

n=−∞
h2(n) = σ 2x

∫ 1/2
−1/2
|H(f )|2df

where we have applied Parseval’s theorem.

(3.14)

(3.15)

(3.16)

(3.17)

(3.18)

(3.19)

(3.20)

Frequency-Domain Analysis of LTI Systems

which is the dc gain of the system. The relationship in (3.15) allows us to express
the mean value in (3.13) as

A special form of (3.17) is obtained when the input random process is white,

(0) is the input signal power. Then (3.17) reduces to

332



determining the power density spectrum of γyy(m). We have

�yy(ω) =
∞∑

m=−∞
γyy(m)e

−jωm

=
∞∑

m=−∞

[ ∞∑
k=−∞

∞∑
l=−∞

h(k)h(l)γxx(k − l +m)
]
e−jωm

=
∞∑

k=−∞

∞∑
l=−∞

h(k)h(l)

[ ∞∑
m=−∞

γxx(k − l +m)e−jωm
]

= �xx(f )
[ ∞∑
k=−∞

h(k)ejωk

][ ∞∑
l=−∞

h(l)e−jωl
]

= |H(ω)|2�xx(ω)

This is the desired relationship for the power density spectrum of the output pro-
cess, in terms of the power density spectrum of the input process and the frequency
response of the system.

The equivalent expression for continuous-time systems with random inputs is

�yy(F ) = |H(F)|2�xx(F )

where the power density spectra �yy(F ) and �xx(F ) are the Fourier transforms of
the autocorrelation functions γyy(τ ) and γxx(τ ), respectively, and where H(F) is the
frequency response of the system, which is related to the impulse response by the
Fourier transform, that is,

H(F) =
∫ ∞
−∞

h(t)e−j2πF t dt

As a final exercise, we determine the crosscorrelation of the output y(n) with
∗

expected value, we obtain

E[y(n)x∗(n−m)] = E
[ ∞∑
k=−∞

h(k)x∗(n−m)x(n− k)
]

γyx(m) =
∞∑

k=−∞
h(k)E[x∗(n−m)x(n− k)]

=
∞∑

k=−∞
h(k)γxx(m− k)

(3.21)

(3.22)

(3.23)

(3.24)

Frequency-Domain Analysis of LTI Systems

The relationship in (3.17) can be transformed into the frequency domain by

the input signal x(n). If we multiply both sides of (3.12) by x (n−m) and take the

333



pression is
�yx(ω) = H(ω)�xx(ω)

�yx(ω) = σ 2x H(ω)
where σ 2x is the input noise power. This result means that an unknown system with
frequency response H(ω) can be identified by exciting the input with white noise,
crosscorrelating the input sequence with the output sequence to obtain γyx(m), and
finally, computing the Fourier transform of γyx(m). The result of these computations
is proportional to H(ω).

4 Linear Time-Invariant Systems as Frequency-Selective Filters

The term filter is commonly used to describe a device that discriminates, according to
some attribute of the objects applied at its input, what passes through it. For example,
an air filter allows air to pass through it but prevents dust particles that are present
in the air from passing through. An oil filter performs a similar function, with the
exception that oil is the substance allowed to pass through the filter, while particles
of dirt are collected at the input to the filter and prevented from passing through. In
photography, an ultraviolet filter is often used to prevent ultraviolet light, which is
present in sunlight and which is not a part of visible light, from passing through and
affecting the chemicals on the film.

As we have observed in the preceding section, a linear time-invariant system
also performs a type of discrimination or filtering among the various frequency com-
ponents at its input. The nature of this filtering action is determined by the fre-
quency response characteristics H(ω), which in turn depends on the choice of the
system parameters (e.g., the coefficients {ak} and {bk} in the difference equation
characterization of the system). Thus, by proper selection of the coefficients, we
can design frequency-selective filters that pass signals with frequency components in
some bands while they attenuate signals containing frequency components in other
frequency bands.

In general, a linear time-invariant system modifies the input signal spectrum
X(ω) according to its frequency response H(ω) to yield an output signal with spec-
trum Y (ω) = H(ω)X(ω). In a sense, H(ω) acts as a weighting function or a spectral
shaping function to the different frequency components in the input signal. When
viewed in this context, any linear time-invariant system can be considered to be a
frequency-shaping filter, even though it may not necessarily completely block any or
all frequency components. Consequently, the terms “linear time-invariant system”
and “filter” are synonymous and are often used interchangeably.

We use the term filter to describe a linear time-invariant system used to perform
spectral shaping or frequency-selective filtering. Filtering is used in digital signal
processing in a variety of ways, such as removal of undesirable noise from desired
signals, spectral shaping such as equalization of communication channels, signal de-
tection in radar, sonar, and communications, and for performing spectral analysis of
signals, and so on.

(3.25)

(3.26)

Frequency-Domain Analysis of LTI Systems

Since (3.24) has the form of a convolution, the frequency-domain equivalent ex-

In the special case where x(n) is white noise, (3.25) reduces to

334



Ideal Filter Characteristics

Filters are usually classified according to their frequency-domain characteristics as
lowpass, highpass, bandpass, and bandstop or band-elimination filters. The ideal mag-

shown, these ideal filters have a constant-gain (usually taken as unity-gain) passband
characteristic and zero gain in their stopband.

Another characteristic of an ideal filter is a linear phase response. To demon-
strate this point, let us assume that a signal sequence {x(n)} with frequency compo-
nents confined to the frequency range ω1 < ω < ω2 is passed through a filter with

Magnitude responses
for some ideal
frequency-selective
discrete-time filters.

1

|H(ω)|

B

B

0−ωc−π ωc
ω

π

1

|H(ω)|

0−ωc−π ωc
ω

π

Lowpass

Highpass

1

|H(ω)|

0−ω0 −ω1−ω2 ω0 ω2ω1−π
ω

π

Bandpass

1

|H(ω)|

0−ω0 ω0−π
ω

π

Bandstop

1

|H(ω)|

0−π
ω

π

All-pass

4.1

Figure 4.1

Frequency-Domain Analysis of LTI Systems

nitude response characteristics of these types of filters are illustrated in Fig. 4.1. As

335



frequency response

H(ω) =
{
Ce−jωn0, ω1 < ω < ω2
0, otherwise

where C and n0 are constants. The signal at the output of the filter has a spectrum

Y (ω) = X(ω)H(ω)
= CX(ω)e−jωn0, ω1 < ω < ω2

By applying the scaling and time-shifting properties of the Fourier transform, we
obtain the time-domain output

y(n) = Cx(n− n0)

Consequently, the filter output is simply a delayed and amplitude-scaled version of
the input signal. A pure delay is usually tolerable and is not considered a distortionof
the signal. Neither is amplitude scaling. Therefore, ideal filters have a linear phase
characteristic within their passband, that is,

�(ω) = −ωn0
The derivative of the phase with respect to frequency has the units of delay.

Hence we can define the signal delay as a function of frequency as

τg(ω) = −d�(ω)
dω

τg(ω) is usually called the envelope delayor the group delay of the filter. We interpret
τg(ω) as the time delay that a signal component of frequency ω undergoes as it passes

τg(ω) = n0 = constant. In this case all frequency components of the input signal
undergo the same time delay.

In conclusion, ideal filters have a constant magnitude characteristic and a linear
phase characteristic within their passband. In all cases, such filters are not physically
realizable but serve as a mathematical idealization of practical filters. For example,
the ideal lowpass filter has an impulse response

hlp(n) = sinωcπn
πn

, −∞ < n <∞

We note that this filter is not causal and it is not absolutely summable and therefore
it is also unstable. Consequently, this ideal filter is physically unrealizable. Never-
theless, its frequency response characteristics can be approximated very closely by

In the following discussion, we treat the design of some simple digital filters by
the placement of poles and zeros in the z-plane. We have already described how

(4.1)

(4.2)

(4.3)

(4.4)

(4.5)

(4.6)

Frequency-Domain Analysis of LTI Systems

from the input to the output of the system. Note that when�(ω) is linear as in (4.4),

practical, physically realizable filters.

336



the location of poles and zeros affects the frequency response characteristics of the

the frequency response characteristics from the pole–zero plot. This same approach
can be used to design a number of simple but important digital filters with desirable
frequency response characteristics.

The basic principle underlying the pole–zero placement method is to locate poles
near points of the unit circle corresponding to frequencies to be emphasized, and to
place zeros near the frequencies to be deemphasized. Furthermore, the following
constraints must be imposed:

1. All poles should be placed inside the unit circle in order for the filter to be stable.
However, zeros can be placed anywhere in the z-plane.

2. All complex zeros and poles must occur in complex-conjugate pairs in order for
the filter coefficients to be real.

From our previous discussion we recall that for a given pole–zero pattern, the system
function H(z) can be expressed as

H(z) =

M∑
k=0

bkz
−k

1+
N∑
k=1

akz
−k
= b0

M∏
k=1
(1− zkz−1)

N∏
k=1
(1− pkz−1)

where b0 is a gain constant selected to normalize the frequency response at some
specified frequency. That is, b0 is selected such that

|H(ω0)| = 1

where ω0 is a frequency in the passband of the filter. Usually, N is selected to equal
or exceed M , so that the filter has more nontrivial poles than zeros.

In the next section, we illustrate the method of pole–zero placement in the design
of some simple lowpass, highpass, and bandpass filters, digital resonators, and comb
filters. The design procedure is facilitated when carried out interactively on a digital
computer with a graphics terminal.

4.2 Lowpass, Highpass, and Bandpass Filters

In the design of lowpass digital filters, the poles should be placed near the unit circle
at points corresponding to low frequencies (near ω = 0) and zeros should be placed
near or on the unit circle at points corresponding to high frequencies (near ω = π ).
The opposite holds true for highpass filters.

pass filters. The magnitude and phase responses for the single-pole filter with system
function

H1(z) = 1− a1− az−1

(4.7)

(4.8)

(4.9)

Frequency-Domain Analysis of LTI Systems

system. In particular, in Section 2.2 we presented a graphical method for computing

Figure 4.2 illustrates the pole–zero placement of three lowpass and three high-

337



Lowpass

Highpass

Pole–zero patterns for several lowpass and highpass
filters.

filter has unity gain at ω = 0. The gain of this filter at high frequencies is relatively
small.

The addition of a zero at z = −1 further attenuates the response of the filter at
high frequencies. This leads to a filter with a system function

Magnitude and phase
response of (1) a
single-pole filter and (2) a
one-pole, one-zero filter;
H1(z) = (1− a)/(1 − az−1),
H2(z) = [(1 − a)/2][(1 +
z−1)/(1 − az−1)] and
a = 0.9.

π
2

π
2

π
2−

π
2−

0

0

π

π

−π
−π

|H
(ω

) |
Θ

(ω
)

|H2(ω)|

Θ2(ω)

Θ1(ω)

|H1(ω)|

π
2

π
2− 0

0

0.4

0.2

0.6

0.8

1.0

1.2

π−π

Figure 4.2

are illustrated in Fig. 4.3 for a = 0.9. The gain G was selected as 1− a , so that the

Figure 4.3

Frequency-Domain Analysis of LTI Systems

338



Magnitude and phase

highpass filter; H(z) = [(1−
a)/2][(1 − z−1)/(1+ az−1)]
with a = 0.9.

π
2

π
2

π
2

−

π
2

−

0

0

π

π

−π
−π

Θ
(ω

)
|H

(ω
) |

π
2

π
2

− 0
0

0.4

0.2

0.6

0.8

1.0

1.2

π−π

H2(z) = 1− a2
1+ z−1

1− az−1
In this

case the magnitude of H2(ω) goes to zero at ω = π .
Similarly, we can obtain simple highpass filters by reflecting (folding) the pole–

zero locations of the lowpass filters about the imaginary axis in the z-plane. Thus we
obtain the system function

H3(z) = 1− a2
1− z−1

1+ az−1

A two-pole lowpass filter has the system function

H(z) = b0
(1− pz−1)2

Determine the values of b0 and p such that the frequency response H(ω) satisfies the condi-
tions

H(0) = 1
and ∣∣∣H (π

4

)∣∣∣2 = 1
2

(4.10)

(4.11)

response of a simple

Figure 4.4

EXAMPLE 4.1

Frequency-Domain Analysis of LTI Systems

and a frequency response characteristic that is also illustrated in Fig. 4.3.

which has the frequency response characteristics illustrated in Fig. 4.4 for a = 0.9.

339



Solution. At ω = 0 we have
H(0) = b0

(1− p)2 = 1

Hence
b0 = (1− p)2

At ω = π/4,

H
(π

4

)
= (1− p)

2

(1− pe−jπ/4)2

= (1− p)
2

(1− p cos(π/4)+ jp sin(π/4))2

= (1− p)
2

(1− p/√2+ jp/√2)2

Hence
(1− p)4

[(1− p/√2)2 + p2/2]2 =
1
2

or, equivalently, √
2(1− p)2 = 1+ p2 −

√
2p

The value of p = 0.32 satisfies this equation. Consequently, the system function for the desired
filter is

H(z) = 0.46
(1− 0.32z−1)2

The same principles can be applied for the design of bandpass filters. Basically,
the bandpass filter should contain one or more pairs of complex-conjugate poles near
the unit circle, in the vicinity of the frequency band that constitutes the passband of
the filter. The following example serves to illustrate the basic ideas.

Design a two-pole bandpass filter that has the center of its passband at ω = π/2, zero in its
frequency response characteristic at ω = 0 and ω = π , and a magnitude response of 1/√2 at
ω = 4π/9.
Solution. Clearly, the filter must have poles at

p1,2 = re±jπ/2

and zeros at z = 1 and z = −1. Consequently, the system function is

H(z) = G (z− 1)(z + 1)
(z − jr)(z+ jr)

= G z
2 − 1

z2 + r2

EXAMPLE 4.2

Frequency-Domain Analysis of LTI Systems

340



Magnitude and phase
response of a simple
bandpass filter in

0.15[(1− z−2)/(1+ 0.7z−2)].
π
2

π
2

π
2

−

π
2

−

0

0

π

π

−π
−π

Ph
as

e 
(r

ad
ia

ns
)

|H
(ω

) |

π
2

π
2

− 0
0

0.4

0.2

0.6

0.8

1.0

1.2

π−π

The gain factor is determined by evaluating the frequency response H(ω) of the filter at
ω = π/2. Thus we have

H
(π

2

)
= G 2

1− r2 = 1

G = 1− r
2

2

The value of r is determined by evaluating H(ω) at ω = 4π/9. Thus we have
∣∣∣∣H

(
4π
9

)∣∣∣∣
2

= (1− r
2)2

4
2− 2 cos(8π/9)

1+ r4 + 2r2 cos(8π/9) =
1
2

or, equivalently,
1.94(1− r2)2 = 1− 1.88r2 + r4

The value of r2 = 0.7 satisfies this equation. Therefore, the system function for the desired
filter is

H(z) = 0.15 1− z
−2

1+ 0.7z−2

It should be emphasized that the main purpose of the foregoing methodology
for designing simple digital filters by pole–zero placement is to provide insight into
the effect that poles and zeros have on the frequency response characteristic of

Figure 4.5

Its frequency response is illustrated in Fig. 4.5.

Frequency-Domain Analysis of LTI Systems

Example 4.2; H(z) =

341



systems. The methodology is not intended as a good method for designing digital
filters with well-specified passband and stopband characteristics.

If hlp(n) denotes the impulse response of a lowpass filter with frequency re-
sponse Hlp(ω), a highpass filter can be obtained by translating Hlp(ω) by π radians
(i.e., replacing ω by ω − π ). Thus

Hhp(ω) = Hlp(ω − π)
where Hhp(ω) is the frequency response of the highpass filter. Since a frequency
translation of π radians is equivalent to multiplication of the impulse response hlp(n)
by ejπn , the impulse response of the highpass filter is

hhp(n) = (ejπ )nhlp(n) = (−1)nhlp(n)
Therefore, the impulse response of the highpass filter is simply obtained from the
impulse response of the lowpass filter by changing the signs of the odd-numbered
samples in hlp(n). Conversely,

hlp(n) = (−1)nhhp(n)
If the lowpass filter is described by the difference equation

y(n) = −
N∑
k=1

aky(n− k)+
M∑
k=0

bkx(n− k)

its frequency response is

Hlp(ω) =

M∑
k=0

bke
−jωk

1+
N∑
k=1

ake
−jωk

Hhp(ω) =

M∑
k=0
(−1)kbke−jωk

1+
N∑
k=1
(−1)kake−jωk

(4.12)

(4.13)

(4.14)

(4.15)

(4.16)

(4.17)

Frequency-Domain Analysis of LTI Systems

Now, if we replace ω by ω − π , in (4.16), then

hlp(n).
A simple lowpass-to-highpass filter transformation. Suppose that we have de-
signed a prototype lowpass filter with impulse response By using the 
frequency translation property of the Fourier transform, it is possible to convert 
the prototype filter to either a bandpass or a highpass filter. In this section we 
present a simple frequency transformation for converting a lowpass filter into a 
highpass filter, and vice versa.

342



which corresponds to the difference equation

y(n) = −
N∑
k=1
(−1)kaky(n− k)+

M∑
k=0
(−1)kbkx(n− k)

Convert the lowpass filter described by the difference equation

y(n) = 0.9y(n− 1)+ 0.1x(n)

into a highpass filter.

Solution.

y(n) = −0.9y(n− 1)+ 0.1x(n)

and its frequency response is

Hhp(ω) = 0.11+ 0.9e−jω
The reader may verify that Hhp(ω) is indeed highpass.

Digital Resonators

A digital resonator is a special two-pole bandpass filter with the pair of complex-

the vicinity of the pole location. The angular position of the pole determines the
resonant frequency of the filter. Digital resonators are useful in many applications,
including simple bandpass filtering and speech generation.

In the design of a digital resonator with a resonant peak at or near ω = ω0 , we
select the complex-conjugate poles at

p1,2 = re±jω0 , 0 < r < 1
In addition, we can select up to two zeros. Although there are many possible choices,
two cases are of special interest. One choice is to locate the zeros at the origin. The
other choice is to locate a zero at z = 1 and a zero at z = −1. This choice completely
eliminates the response of the filter at frequencies ω = 0 and ω = π , and it is useful
in many practical applications.

The system function of the digital resonator with zeros at the origin is

H(z) = b0
(1− rejω0z−1)(1− re−jω0z−1)

H(z) = b0
1− (2r cosω0)z−1 + r2z−2

(4.18)

(4.19)

(4.20)

EXAMPLE 4.3

4.3

Frequency-Domain Analysis of LTI Systems

The difference equation for the highpass filter, according to (4.18), is

conjugate poles located near the unit circle as shown in Fig. 4.6(a). The magnitude

refers to the fact that the filter has a large magnitude response (i.e., it resonates) in
of the frequency response of the filter is shown in Fig. 4.6(b). The name resonator

343



(a) Pole–zero pattern and

magnitude and phase
response of a digital
resonator with (1) r = 0.8
and (2) r = 0.95.

r

r

ω0

−ω0

(a)

(b)

0
0

r = 0.8

r = 0.95 

π

2
π−π

0.2

0.4

0.6

0.8

1.0

1.2

|H
(ω

)|

− π
2

r = 0.8

r = 0.95

(c)

0π
2

π−π
−π

π

0

Θ
(ω

)

−

π

2
−

π

2

π

2

Re(z)

Im(z)

Since |H(ω)| has its peak at or near ω = ω0 , we select the gain b0 so that
0

H(ω0) = b0
(1− rejω0e−jω0)(1− re−jω0e−jω0)

= b0
(1− r)(1− re−j2ω0)

and hence

|H(ω0)| = b0
(1− r)

√
1+ r2 − 2r cos 2ω0

= 1

(4.21)

(b) the corresponding

Figure 4.6

Frequency-Domain Analysis of LTI Systems

|H(ω )| = 1. From (4.19) we obtain

344



Thus the desired normalization factor is

b0 = (1− r)
√

1+ r2 − 2r cos 2ω0

|H(ω)| = b0
U1(ω)U2(ω)

�(ω) = 2ω −�1(ω)−�2(ω)

where U1(ω) and U2(ω) are the magnitudes of the vectors from p1 and p2 to the
point ω in the unit circle and �1(ω) and �2(ω) are the corresponding angles of these
two vectors. The magnitudes U1(ω) and U2(ω) may be expressed as

U1(ω) =
√

1+ r2 − 2r cos(ω0 − ω)

U2(ω) =
√

1+ r2 − 2r cos(ω0 + ω)

For any value of r , U1(ω) takes its minimum value (1 − r) at ω = ω0 . The
product U1(ω)U2(ω) reaches a minimum value at the frequency

ωr = cos−1
(

1+ r2
2r

cosω0

)

which defines precisely the resonant frequency of the filter. We observe that when
r is very close to unity, ωr ≈ ω0 , which is the angular position of the pole. We also
observe that as r approaches unity, the resonance peak becomes sharper because
U1(ω) changes more rapidly in relative size in the vicinity of ω0 . A quantitative
measure of the sharpness of the resonance is provided by the 3-dB bandwidth ω
of the filter. For values of r close to unity,

ω ≈ 2(1− r)

0 =
π/3, r = 0.8 and ω0 = π/3, r = 0.95. We note that the phase response undergoes
its greatest rate of change near the resonant frequency.

If the zeros of the digital resonator are placed at z = 1 and z = −1, the resonator
has the system function

H(z) = G (1− z
−1)(1+ z−1)

(1− rejω0z−1)(1− re−jω0z−1)

= G 1− z
−2

1− (2r cosω0)z−1 + r2z−2

(4.22)

(4.23)

(4.24)

(4.25)

(4.26)

(4.27)

Figure 4.6 illustrates the magnitude and phase of digital resonators with ω

Frequency-Domain Analysis of LTI Systems

The frequency response of the resonator in (4.19) can be expressed as

345



and a frequency response characteristic

H(ω) = b0 1− e
−j2ω

[1− rej (ω0−ω)][1− re−j (ω0+ω)]
We observe that the zeros at z = ±1 affect both the magnitude and phase response
of the resonator. For example, the magnitude response is

|H(ω)| = b0 N(ω)
U1(ω)U2(ω)

where N(ω) is defined as
N(ω) =

√
2(1− cos 2ω)

Due to the presence of the zero factor, the resonant frequency is altered from

Although exact values for these two parameters are rather tedious to derive, we can

previous case in which the zeros are located at the origin.
0 = π/3,

r = 0.8 and ω0 = π/3, r = 0.95. We observe that this filter has a slightly smaller
bandwidth than the resonator, which has zeros at the origin. In addition, there
appears to be a very small shift in the resonant frequency due to the presence of
the zeros.

Magnitude and phase
response of digital
resonator with zeros
at ω = 0 and ω = π
and (1) r = 0.8 and
(2) r = 0.95.

(4.28)

(4.29)

Figure 4.7 illustrates the magnitude and phase characteristics for ω

Figure 4.7

Frequency-Domain Analysis of LTI Systems

that given by the expression in (4.25). The bandwidth of the filter is also altered.

easily compute the frequency response in (4.28) and compare the result with the

346



Frequency response
characteristic of a notch
filter.

0
0 ω0

ω
ω1 π

|H(ω)|

4.4 Notch Filters

A notch filter is a filter that contains one or more deep notches or, ideally, perfect
nulls in its frequency response characteristic.
response characteristic of a notch filter with nulls at frequencies ω0 and ω1 . Notch
filters are useful in many applications where specific frequency components must be
eliminated. For example, instrumentation and recording systems require that the
power-line frequency of 60 Hz and its harmonics be eliminated.

To create a null in the frequency response of a filter at a frequency ω0 , we simply
introduce a pair of complex-conjugate zeros on the unit circle at an angle ω0 . That is,

z1,2 = e±jω0

Thus the system function for an FIR notch filter is simply

H(z) = b0(1− ejω0z−1)(1− e−jω0z−1)
= b0(1− 2 cosω0z−1 + z−2)

a null at ω = π/4.

Suppose that we place a pair of complex-conjugate poles at

p1,2 = re±jω0

The effect of the poles is to introduce a resonance in the vicinity of the null and thus
to reduce the bandwidth of the notch. The system function for the resulting filter is

H(z) = b0 1− 2 cosω0z
−1 + z−2

1− 2r cosω0z−1 + r2z−2

ω0 0 = π/4, r = 0.95. When compared with the frequency

the bandwidth of the notch.

(4.30)

(4.31)

Frequency-Domain Analysis of LTI Systems

Figure 4.8

Figure 4.8 illustrates the frequency

As an illustration, Fig. 4.9 shows the magnitude response for a notch filter having

The magnitude response |H(ω)| of the filter in (4.31) is plotted in Fig. 4.10 for
= π/4, r = 0.85, and for ω

response of the FIR filter in Fig. 4.9, we note that the effect of the poles is to reduce

The problem with the FIR notch filter is that the notch has a relatively large 
bandwidth, which means that other frequency components around the desired 
null are severely attenuated. To reduce the bandwidth of the null, we can resort 
to a more sophisticated, longer FIR filter. Alternatively, we could, in an ad hoc 
manner, attempt to improve on the frequency response characteristics by intro-
ducing poles in the system function.

347



Frequency response
characteristics of a
notch filter with a
notch at ω = π/4
or f = 1/8; H(z) =
G[1− 2 cosω0z−1 + z−2].

Frequency response
characteristics of two
notch filters with poles
at (1) r = 0.85 and
(2) r = 0.95; H(z) =
b0[(1−2 cosω0z−1+z−2)/(1−
2r cosω0z−1 + r2z−2)].

Figure 4.9

Figure 4.10

Frequency-Domain Analysis of LTI Systems

348



In addition to reducing the bandwidth of the notch, the introduction of a pole
in the vicinity of the null may result in a small ripple in the passband of the filter
due to the resonance created by the pole. The effect of the ripple can be reduced by
introducing additional poles and/or zeros in the system function of the notch filter.
The major problem with this approach is that it is basically an ad hoc, trial-and-error
method.

4.5 Comb Filters

In its simplest form, a comb filter can be viewed as a notch filter in which the nulls
occur periodically across the frequency band, hence the analogy to an ordinary comb
that has periodically spaced teeth. Comb filters find applications in a wide range of
practical systems such as in the rejection of power-line harmonics, in the separation of
solar and lunar components from ionospheric measurements of electron concentra-
tion, and in the suppression of clutter from fixed objects in moving-target-indicator
(MTI) radars.

To illustrate a simple form of a comb filter, consider a moving average (FIR)
filter described by the difference equation

y(n) = 1
M + 1

M∑
k=0

x(n− k)

The system function of this FIR filter is

H(z) = 1
M + 1

M∑
k=0

z−k

= 1
M + 1

[1− z−(M+1)]
(1− z−1)

and its frequency response is

H(ω) = e
−jωM/2

M + 1
sinω(M+12 )

sin(ω/2)

z = ej2πk/(M+1), k = 1, 2, 3, . . . ,M

Note that the pole at z = 1 is actually canceled by the zero at z = 1, so that in effect
the FIR filter does not contain poles outside z = 0.

istence of the periodically spaced zeros in frequency at ωk = 2πk/(M + 1) for

(4.32)

(4.33)

(4.34)

(4.35)

Frequency-Domain Analysis of LTI Systems

From (4.33) we observe that the filter has zeros on the unit circle at

A plot of the magnitude characteristic of (4.34) clearly illustrates the ex-

k = 1, 2, . . . ,M . Figure 4.11 shows |H(ω)| for M = 10.

349



characteristic for the comb

M = 10.
In more general terms, we can create a comb filter by taking an FIR filter with

system function

H(z) =
M∑
k=0

h(k)z−k

and replacing z by zL , where L is a positive integer. Thus the new FIR filter has a
system function

HL(z) =
M∑
k=0

h(k)z−kL

If the frequency response of the original FIR filter is H(ω), the frequency response

HL(ω) =
M∑
k=0

h(k)e−jkLω

= H(Lω)
Consequently, the frequency response characteristic HL(ω) is simply an L-order

between HL(ω) and H(ω) for L = 5.
Now, suppose that the original FIR filter with system functionH(z) has a spectral

null (i.e., a zero), at some frequency ω0 . Then the filter with system function HL(z)
has periodically spaced nulls at ωk = ω0 + 2πk/L, k = 0, 1, 2, . . . , L− 1. As

This
FIR filter can be viewed as an FIR filter of length 10, but only four of the 10 filter
coefficients are nonzero.

Let us now return to the moving average filter with system function given by
Suppose that we replace z by zL . Then the resulting comb filter has the

system function

HL(z) = 1
M + 1

1− z−L(M+1)
1− z−L

and a frequency response

HL(ω) = 1
M + 1

sin[ωL(M + 1)/2]
sin(ωL/2)

e−jωLM/2

(4.36)

(4.37)

(4.38)

(4.39)

(4.40)

Figure 4.11

Frequency-Domain Analysis of LTI Systems

Magnitude response

filter given by (4.34) with

of the FIR in (4.37) is

repetition of H(ω) in the range 0 ≤ ω ≤ 2π . Figure 4.12 illustrates the relationship

an illustration, Fig. 4.13 shows an FIR comb filter with M = 3 and L = 3.

(4.33).

350



0
(a)

ω
π−π−2π 2π

1

H(ω)

0

(b)

ω
2π
5

HL(ω)

2π
5

4π
5

6π
5

8π
5

2π−2π −4π5−
6π
5

−8π
5

−

Comb filter with frequency response HL(ω)
obtained from H(ω).

This filter has zeros on the unit circle at

zk = ej2πk/L(M+1)

for all integer values of k except k = 0, L, 2L, . . . ,ML.
|HL(ω)| for L = 3 and M = 10.

and lunar spectral components in ionospheric measurements of electron concen-
tration as described in the paper by Bernhardt et al. (1976). The solar period is
Ts
monics. The lunar period is TL = 24.84 hours and provides spectral lines at 0.96618

spectrum of the unfiltered ionospheric measurements of the electron concentration.
Note that the weak lunar spectral components are almost hidden by the strong solar
spectral components.

The two sets of spectral components can be separated by the use of comb filters.
If we wish to obtain the solar components, we can use a comb filter with a narrow
passband at multiples of one cycle per day. This can be achieved by selecting L such
that Fs/L = 1 cycle per day, where Fs is the corresponding sampling frequency. The

+ + +

x(n)

h(0) h(1) h(2) h(3)

y(n)

z−1 z−1z−1 z−1 z−1 z−1 z−1 z−1 z−1

Realization of an FIR comb filter having M = 3 and L = 3.

(4.41)

Figure 4.12

Figure 4.13

Frequency-Domain Analysis of LTI Systems

= 24 hours and results in a solar component of one cycle per day and its har-

Figure 4.14 illustrates

The comb filter described by (4.39) finds application in the separation of solar

cycle per day and its harmonics. Figure 4.15(a) shows a plot of the power density

351



Magnitude response
characteristic for a comb

L = 3 and M = 10.

result is a filter that has peaks in its frequency response at multiples of one cycle per
day. By selecting M = 58, the filter will have nulls at multiples of (Fs/L)/(M + 1) =
1/59 cycle per day. These nulls are very close to the lunar components and result in

the comb filter that isolates the solar components. A comb filter that rejects the solar
components and passes the lunar components can be designed in a similar manner.

filter.

(a) Spectrum of unfiltered electron content data; (b) spec-
trum of output of solar filter; (c) spectrum of output of lunar filter. [From
paper by Bernhardt et al. (1976). Reprinted with permission of the Amer-
ican Geophysical Union.]

Figure 4.14

Figure 4.15

Frequency-Domain Analysis of LTI Systems

filter given by (4.40), with

good rejection. Figure 4.15(b) illustrates the power spectral density of the output of

Figure 4.15(c) illustrates the power spectral density at the output of such a lunar

352



4.6 All-Pass Filters

An all-pass filter is defined as a system that has a constant magnitude response for
all frequencies, that is,

|H(ω)| = 1, 0 ≤ ω ≤ π
The simplest example of an all-pass filter is a pure delay system with system function

H(z) = z−k

This system passes all signals without modification except for a delay of k samples.
This is a trivial all-pass system that has a linear phase response characteristic.

A more interesting all-pass filter is described by the system function

H(z) = aN + aN−1z
−1 + · · · + a1z−N+1 + z−N

1+ a1z−1 + · · · + aNz−N

=
∑N

k=0 akz
−N+k∑N

k=0 akz−k
, a0 = 1

where all the filter coefficients {ak} are real. If we define the polynomial A(z) as

A(z) =
N∑
k=0

akz
−k, a0 = 1

H(z) = z−N A(z
−1)

A(z)

Since
|H(ω)|2 = H(z)H(z−1)|z=ejω = 1

0 is a pole of H(z),
then 1/z0 is a zero of H(z) (i.e., the poles and zeros are reciprocals of one another).

and a two-pole, two-zero filter. A plot of the phase characteristics of these filters is
0 = π/4.

(   , −ω0)

0

(a)

a 1 1
a

0

(b)

r

r

1

1
r

r(   , ω0)
1

ω0

ω0

(r, −ω0)

(r, ω0)

Pole–zero patterns of (a) a first-order and (b) a
second-order all-pass filter.

(4.42)

(4.43)

(4.44)

Figure 4.16

Frequency-Domain Analysis of LTI Systems

then (4.43) can be expressed as

the system given by (4.44) is an all-pass system. Furthermore, if z

Figure 4.16 illustrates typical pole–zero patterns for a single-pole, single-zero filter

shown in Fig. 4.17 for a = 0.6 and r = 0.9, ω

353



Frequency response
characteristics of an
all-pass filter with system
functions (1) H(z) =
(0.6 + z−1)/(1 + 0.6z−1),
(2) H(z) =
(r2 − 2r cosω0z−1 + z−2)/
(1 − 2r cosω0z−1 + r2z−2),
r = 0.9, ω0 = π/4.

H
(ω

)

The most general form for the system function of an all-pass system with real
coefficients, expressed in factored form in terms of poles and zeros, is

Hap(z) =
NR∏
k=1

z−1 − αk
1− αkz−1

Nc∏
k=1

(z−1 − βk)(z−1 − β∗k )
(1− βkz−1)(1− β∗k z−1)

where there are NR real poles and zeros and Nc complex-conjugate pairs of poles
and zeros. For causal and stable systems we require that −1 < αk < 1 and |βk| < 1.

Expressions for the phase response and group delay of all-pass systems can easily

all-pass system we have

Hap(ω) = e
jω − re−jθ

1− rejθ e−jω
Hence

�ap(ω) = −ω − 2 tan−1 r sin(ω − θ)1− r cos(ω − θ)
and

τg(ω) = −
d�ap(ω)

dω
= 1− r

2

1+ r2 − 2r cos(ω − θ)
We note that for a causal and stable system, r < 1 and hence τg(ω) ≥ 0. Since the
group delay of a higher-order pole–zero system consists of a sum of positive terms

(4.45)

(4.46)

Figure 4.17

Frequency-Domain Analysis of LTI Systems

be obtained using the method described in Section 2.1. For a single pole–single zero

as in (4.46), the group delay will always be positive.

354



All-pass filters find application as phase equalizers. When placed in cascade
with a system that has an undesired phase response, a phase equalizer is designed to
compensate for the poor phase characteristics of the system and therefore to produce
an overall linear-phase response.

4.7 Digital Sinusoidal Oscillators

A digital sinusoidal oscillatorcan be viewed as a limiting form of a two-pole resonator
for which the complex-conjugate poles lie on the unit circle. From our previous
discussion of second-order systems, we recall that a system with system function

H(z) = b0
1+ a1z−1 + a2z−2

and parameters
a1 = −2r cosω0 and a2 = r2

has complex-conjugate poles at p = re±jω0 , and a unit sample response

h(n) = b0r
n

sinω0
sin(n+ 1)ω0u(n)

If the poles are placed on the unit circle (r = 1) and b0 is set to A sinω0 , then

h(n) = A sin(n+ 1)ω0u(n)

Thus the impulse response of the second-order system with complex-conjugate poles
on the unit circle is a sinusoid and the system is called a digital sinusoidal oscillator
or a digital sinusoidal generator.

A digital sinusoidal generator is a basic component of a digital frequency syn-
thesizer.

y(n) = −a1y(n− 1)− y(n− 2)+ b0δ(n)

Digital sinusoidal generator.

+

+

(A sin ω0)δ(n) y(n) = A sin (n + 1)ω0

a1 = −2 cos ω0
a2 = 1

z−1

−a1

−a2

z−1

(4.47)

(4.48)

(4.49)

(4.50)

(4.51)

Figure 4.18

illustrated in Fig. 4.18. The corresponding difference equation for this system is

Frequency-Domain Analysis of LTI Systems

The block diagram representation of the system function given by (4.47) is

355



where the parameters are a1 = −2 cosω0 and b0 = A sinω0 , and the initial conditions

y(0) = A sinω0
y(1) = 2 cosω0y(0) = 2A sinω0 cosω0 = A sin 2ω0
y(2) = 2 cosω0y(1)− y(0)
= 2A cosω0 sin 2ω0 − A sinω0
= A(4 cos2 ω0 − 1) sinω0
= 3A sinω0 − 4 sin3 ω0 = A sin 3ω0

and so forth. We note that the application of the impulse at n = 0 serves the purpose
of beginning the sinusoidal oscillation. Thereafter, the oscillation is self-sustaining
because the system has no damping (i.e., r = 1).

It is interesting to note that the sinusoidal oscillation obtained from the system

conditions to y(−1) = 0, y(−2) = −A sinω0 . Thus the zero-input response to the
second-order system described by the homogeneous difference equation

y(n) = −a1y(n− 1)− y(n− 2)

with initial conditions y(−1) = 0 and y(−2) = −A sinω0 , is exactly the same as

sin α + sin β = 2 sin α + β
2

cos
α − β

2

where, by definition, α = (n+ 1)ω0 , β = (n− 1)ω0 , and y(n) = sin(n+ 1)ω0 .
In some practical applications involving modulation of two sinusoidal carrier

signals in phase quadrature, there is a need to generate the sinusoids A sinω0n and
A cosω0 n. These signals can be generated from the so-called coupled-form oscillator,
which can be obtained from the trigonometric formulas

cos(α + β) = cosα cosβ − sin α sin β
sin(α + β) = sin α cosβ + cosα sin β

where, by definition, α = nω0 , β = ω0 , and

yc(n) = cos nω0u(n)
ys(n) = sin nω0u(n)

(4.52)

(4.53)

(4.54)

(4.55)

Frequency-Domain Analysis of LTI Systems

are y(−1) = y(−2) = 0. By iterating the difference equation in (4.51), we obtain

in (4.51) can also be obtained by setting the input to zero and setting the initial

the response of (4.51) to an impulse excitation. In fact, the difference equation in
(4.52) can be obtained directly from the trigonometric identity

356



Realization of the
coupled-form oscillator.

yc(n) = cos ω0n

cos ω0

sin ω0

+

−sin ω0

z−1

ys(n) = sin ω0ncos ω0

+
z−1

Thus we obtain the two coupled difference equations

yc(n) = (cosω0)yc(n− 1)− (sinω0)ys(n− 1)
ys(n) = (sinω0)yc(n− 1)+ (cosω0)ys(n− 1)

which can also be expressed in matrix form as

[
yc(n)

ys(n)

]
=
[

cosω0 − sinω0
sinω0 cosω0

] [
yc(n− 1)
ys(n− 1)

]

The structure for the realization of the coupled-form oscillator is illustrated in

but which requires the initial conditions yc(−1) = A cosω0 and ys(−1) = −A sinω0
in order to begin its self-sustaining oscillations.

two-dimensional coordinate system with coordinates yc(n) and ys(n). As a conse-
quence, the coupled-form oscillator can also be implemented by use of the so-called
CORDIC algorithm [see the book by Kung et al. (1985)].

5 Inverse Systems and Deconvolution

As we have seen, a linear time-invariant system takes an input signal x(n) and pro-
duces an output signal y(n), which is the convolution of x(n) with the unit sample
response h(n) of the system. In many practical applications we are given an output
signal from a system whose characteristics are unknown and we are asked to de-
termine the input signal. For example, in the transmission of digital information at
high data rates over telephone channels, it is well known that the channel distorts
the signal and causes intersymbol interference among the data symbols. The inter-
symbol interference may cause errors when we attempt to recover the data. In such
a case the problem is to design a corrective system which, when cascaded with the
channel, produces an output that, in some sense, corrects for the distortion caused

(4.56)

(4.57)

(4.58)

Figure 4.19

Frequency-Domain Analysis of LTI Systems

Fig. 4.19. We note that this is a two-output system which is not driven by any input,

Finally, it is interesting to note that (4.58) corresponds to vector rotation in the

357



by the channel, and thus yields a replica of the desired transmitted signal. In digi-
tal communications such a corrective system is called an equalizer. In the general
context of linear systems theory, however, we call the corrective system an inverse
system, because the corrective system has a frequency response which is basically the
reciprocal of the frequency response of the system that caused the distortion. Fur-
thermore, since the distortive system yields an output y(n) that is the convolution
of the input x(n) with the impulse response h(n), the inverse system operation that
takes y(n) and produces x(n) is called deconvolution.

If the characteristics of the distortive system are unknown, it is often neces-
sary, when possible, to excite the system with a known signal, observe the output,
compare it with the input, and in some manner, determine the characteristics of the
system. For example, in the digital communication problem just described, where
the frequency response of the channel is unknown, the measurement of the channel
frequency response can be accomplished by transmitting a set of equal-amplitude
sinusoids, at different frequencies with a specified set of phases, within the frequency
band of the channel. The channel will attenuate and phase shift each of the sinusoids.
By comparing the received signal with the transmitted signal, the receiver obtains a
measurement of the channel frequency response which can be used to design the in-
verse system. The process of determining the characteristics of the unknown system,
either h(n) or H(ω), by a set of measurements performed on the system is called
system identification.

The term “deconvolution” is often used in seismic signal processing, and more
generally, in geophysics to describe the operation of separating the input signal from
the characteristics of the system which we wish to measure. The deconvolution
operation is actually intended to identify the characteristics of the system, which in
this case, is the earth, and can also be viewed as a system identification problem. The
“inverse system,” in this case, has a frequency response that is the reciprocal of the
input signal spectrum that has been used to excite the system.

5.1 Invertibility of Linear Time-Invariant Systems

A system is said to be invertible if there is a one-to-one correspondence between its
input and output signals. This definition implies that if we know the output sequence
y(n), −∞ < n <∞, of an invertible system T , we can uniquely determine its input
x(n), −∞ < n <∞. The inverse system with input y(n) and output x(n) is denoted
by T −1 . Clearly, the cascade connection of a system and its inverse is equivalent to
the identity system, since

w(n) = T −1[y(n)] = T −1{T [x(n)]} = x(n)
For example, the systems defined by the input–output

relations y(n) = ax(n) and y(n) = x(n− 5) are invertible, whereas the input–output
relations y(n) = x2(n) and y(n) = 0 represent noninvertible systems.

As indicated above, inverse systems are important in many practical applications,
including geophysics and digital communications. Let us begin by considering the
problem of determining the inverse of a given system. We limit our discussion to the
class of linear time-invariant discrete-time systems.

(5.1)

Frequency-Domain Analysis of LTI Systems

as illustrated in Fig. 5.1.

358



System T in cascade with
−1 .

x(n) y(n)
w(n) = x(n)T T −1

Direct
system

Inverse
system

Identity system

Now, suppose that the linear time-invariant system T has an impulse response
h(n) and let hI (n) denote the impulse response of the inverse system T −1 . Then

w(n) = hI (n) ∗ h(n) ∗ x(n) = x(n)

h(n) ∗ hI (n) = δ(n)
I

h(n). A
−1. Thus

H(z)HI (z) = 1

and therefore the system function for the inverse system is

HI(z) = 1
H(z)

If H(z) has a rational system function

H(z) = B(z)
A(z)

then

HI (z) = A(z)
B(z)

Thus the zeros of H(z) become the poles of the inverse system, and vice versa.
Furthermore, if H(z) is an FIR system, then HI(z) is an all-pole system, or if H(z)
is an all-pole system, then HI(z) is an FIR system.

Determine the inverse of the system with impulse response

h(n) = (1
2
)nu(n)

(5.2)

(5.3)

(5.4)

(5.5)

(5.6)

its inverse T

Figure 5.1

EXAMPLE 5.1

Frequency-Domain Analysis of LTI Systems

(5.1) is equivalent to the convolution equation

But (5.2) implies that

The convolution equation in (5.3) can be used to solve for h (n) for a given
However, the solution of (5.3) in the time domain is usually difficult.

simpler approach is to transform (5.3) into the z-domain and solve for T
in the z-transform domain, (5.3) becomes

359



Solution. The system function corresponding to h(n) is

H(z) = 1
1− 12 z−1

, ROC: |z| > 1
2

This system is both causal and stable. Since H(z) is an all-pole system, its inverse is FIR and
is given by the system function

HI(z) = 1− 12 z
−1

Hence its impulse response is

hI (n) = δ(n)− 12 δ(n− 1)

Determine the inverse of the system with impulse response

h(n) = δ(n)− 1
2
δ(n− 1)

This is an FIR system and its system function is

H(z) = 1− 1
2
z−1, ROC: |z| > 0

The inverse system has the system function

HI (z) = 1
H(z)

= 1
1− 12 z−1

= z
z− 12

Thus HI (z) has a zero at the origin and a pole at z = 12 . In this case there are two possible

we take the ROC of HI (z) as |z| > 12 , the inverse transform yields

hI (n) = (12 )
nu(n)

which is the impulse response of a causal and stable system. On the other hand, if the ROC is
assumed to be |z| < 12 , the inverse system has an impulse response

hI (n) = −
(

1
2

)n
u(−n− 1)

In this case the inverse system is anticausal and unstable.

EXAMPLE 5.2

Frequency-Domain Analysis of LTI Systems

regions of convergence and hence two possible inverse systems, as illustrated in Fig. 5.2. If

360



0

(a)

ROC

z-plane

1
2

(b)

z-plane

1
2

ROC

Two possible regions of convergence for H(z) = z/(z− 12 ).

specify the region of convergence for the system function of the inverse system.
In some practical applications the impulse response h(n) does not possess a z-

transform that can be expressed in closed form. As an alternative we may solve

simplifies to the equation

n∑
k=0

h(k)hI (n− k) = δ(n)

By assumption, hI (n) = 0 for n < 0. For n = 0 we obtain

hI (0) = 1/h(0)

The values of hI (n) for n ≥ 1 can be obtained recursively from the equation

hI (n) =
n∑
k=1

h(n)hI (n− k)
h(0)

, n ≥ 1

This recursive relation can easily be programmed on a digital computer.
First, the method does not

work if h(0) = 0. However, this problem can easily be remedied by introducing

δ(n−m), where m = 1 if h(0) = 0 and h(1) �= 0, and so on. Second, the recursion in

accuracy of h(n) deteriorates for large n.

(5.7)

(5.8)

(5.9)

Figure 5.2

Frequency-Domain Analysis of LTI Systems

We observe that (5.3) cannot be solved uniquely by using (5.6) unless we

(5.3) directly using a digital computer. Since (5.3) does not, in general, possess a
unique solution, we assume that the system and its inverse are causal. Then (5.3)

There are two problems associated with (5.9).

an appropriate delay in the right-hand side of (5.7), that is, by replacing δ(n) by

(5.9) gives rise to round-off errors which grow with n and, as a result, the numerical

361



Determine the causal inverse of the FIR system with impulse response

h(n) = δ(n)− αδ(n− 1)

Since h(0) = 1, h(1) = −α , and h(n) = 0 for n ≥ α , we have

hI (0) = 1/h(0) = 1

and
hI (n) = αhI (n− 1), n ≥ 1

Consequently,
hI (1) = α, hI (2) = α2, . . . , hI (n) = αn

which corresponds to a causal IIR system as expected.

5.2 Minimum-Phase, Maximum-Phase, and Mixed-Phase Systems

The invertibility of a linear time-invariant system is intimately related to the charac-
teristics of the phase spectral function of the system. To illustrate this point, let us
consider two FIR systems, characterized by the system functions

H1(z) = 1+ 12z
−1 = z−1(z+ 1

2
)

H2(z) = 12 + z
−1 = z−1(1

2
z+ 1)

1
2

h(1) = 1/2.

1 2

In the frequency domain, the two systems are characterized by their frequency
response functions, which can be expressed as

|H1(ω)| = |H2(ω)| =
√

5
4
+ cosω

and

�1(ω) = −ω + tan−1 sinω1
2 + cosω

�2(ω) = −ω + tan−1 sinω2+ cosω

(5.10)

(5.11)

(5.12)

(5.13)

(5.14)

EXAMPLE 5.3

Frequency-Domain Analysis of LTI Systems

and an impulse response h(0) = 1,The system in (5.10) has a zero at z = −
The system in (5.11) has a zero at z = −2 and an impulse response

the reciprocal relationship between the zeros of H (z) and H (z).
h(0) = 1/2, h(1) = 1, which is the reverse of the system in (5.10). This is due to

362



θ1(ω)

ω

π

−π

(a)

θ2(ω)

ω

π

−π

(b)

The magnitude characteristics for the two systems are identical because the zeros of
H1(z) and H2(z) are reciprocals.

1 2
phase characteristic �1(ω) for the first system begins at zero phase at the frequency
ω = 0 and terminates at zero phase at the frequency ω = π . Hence the net phase
change, �1(π)−�1(0), is zero. On the other hand, the phase characteristic for the
system with the zero outside the unit circle undergoes a net phase change �2(π) −
�2(0) = π radians. As a consequence of these different phase characteristics, we call
the first system a minimum-phase system and the second system a maximum-phase
system.

These definitions are easily extended to an FIR system of arbitrary length. To
be specific, an FIR system of length M + 1 has M zeros. Its frequency response can
be expressed as

H(ω) = b0(1− z1e−jω)(1− z2e−jω) · · · (1− zMe−jω)
where {zi} denote the zeros and b0 is an arbitrary constant. When all the zeros are

valued zero, will undergo a net phase change of zero between ω = 0 and ω = π .
Also, each pair of complex-conjugate factors inH(ω)will undergo a net phase change
of zero. Therefore,

�H(π)−�H(0) = 0
and hence the system is called a minimum-phase system. On the other hand, when
all the zeros are outside the unit circle, a real-valued zero will contribute a net phase
change of π radians as the frequency varies from ω = 0 to ω = π , and each pair of
complex-conjugate zeros will contribute a net phase change of 2π radians over the
same range of ω . Therefore,

�H(π)−�H(0) = Mπ
which is the largest possible phase change for an FIR system with M zeros. Hence
the system is called maximum phase. It follows from the discussion above that

�Hmax(π) ≥ �Hmin(π)

(5.15)

(5.16)

(5.17)

(5.18)

Figure 5.3

Frequency-Domain Analysis of LTI Systems

Phase response characteristics for the systems in (5.10). and (5.11).

The graphs of � (ω) and � (ω) are illustrated in Fig. 5.3. We observe that the

inside the unit circle, each term in the product of (5.15), corresponding to a real-

363



If the FIR system with M zeros has some of its zeros inside the unit circle and
the remaining zeros outside the unit circle, it is called a mixed-phase system or a
nonminimum-phase system.

Since the derivative of the phase characteristic of the system is a measure of
the time delay that signal frequency components undergo in passing through the
system, a minimum-phase characteristic implies a minimum delay function, while a
maximum-phase characteristic implies that the delay characteristic is also maximum.

Now suppose that we have an FIR system with real coefficients. Then the mag-
nitude square value of its frequency response is

|H(ω)|2 = H(z)H(z−1)|z=ejω

This relationship implies that if we replace a zero zk of the system by its inverse 1/zk ,
the magnitude characteristic of the system does not change. Thus if we reflect a zero
zk that is inside the unit circle into a zero 1/zk outside the unit circle, we see that the
magnitude characteristic of the frequency response is invariant to such a change.

It is apparent from this discussion that if |H(ω)|2 is the magnitude square fre-
quency response of an FIR system having M zeros, there are 2M possible configu-
rations for the M zeros, of which some are inside the unit circle and the remaining
are outside the unit circle. Clearly, one configuration has all the zeros inside the unit
circle, which corresponds to the minimum-phase system. A second configuration
has all the zeros outside the unit circle, which corresponds to the maximum-phase
system. The remaining 2M − 2 configurations correspond to mixed-phase systems.
However, not all 2M − 2 mixed-phase configurations necessarily correspond to FIR
systems with real-valued coefficients. Specifically, any pair of complex-conjugate ze-
ros results in only two possible configurations, whereas a pair of real-valued zeros
yields four possible configurations.

Determine the zeros for the following FIR systems and indicate whether the system is minimum
phase, maximum phase, or mixed phase.

H1(z) = 6+ z−1 − z−2

H2(z) = 1− z−1 − 6z−2

H3(z) = 1− 52 z
−1 − 3

2
z−2

H4(z) = 1+ 53 z
−1 − 2

3
z−2

Solution. By factoring the system functions we find the zeros for the four systems are

H1(z) −→ z1,2 = −12 ,
1
3
−→ minimum phase

H2(z) −→ z1,2 = −2, 3 −→ maximum phase

(5.19)

EXAMPLE 5.4

Frequency-Domain Analysis of LTI Systems

364



H3(z) −→ z1,2 = −12 , 3 −→ mixed phase

H4(z) −→ z1,2 = −2, 13 −→ mixed phase

Since the zeros of the four systems are reciprocals of one another, it follows that all four
systems have identical magnitude frequency response characteristics but different phase char-
acteristics.

The minimum-phase property of FIR systems carries over to IIR systems that
have rational system functions. Specifically, an IIR system with system function

H(z) = B(z)
A(z)

is called minimum phase if all its poles and zeros are inside the unit circle. For a
stable and causal system [all roots of A(z) fall inside the unit circle] the system is
called maximum phase if all the zeros are outside the unit circle, and mixed phase if
some, but not all, of the zeros are outside the unit circle.

This discussion brings us to an important point that should be emphasized. That
is, a stable pole–zero system that is minimum phase has a stable inverse which is also
minimum phase. The inverse system has the system function

H−1(z) = A(z)
B(z)

Hence the minimum-phase property of H(z) ensures the stability of the inverse
system H−1(z) and the stability of H(z) implies the minimum-phase property of
H−1(z). Mixed-phase systems and maximum-phase systems result in unstable inverse
systems.

Decomposition of nonminimum-phase pole–zero systems. Any nonminimum-phase
pole–zero system can be expressed as

H(z) = Hmin(z)Hap(z)
where Hmin(z) is a minimum-phase system and Hap(z) is an all-pass system. We
demonstrate the validity of this assertion for the class of causal and stable systems
with a rational system functionH(z) = B(z)/A(z). In general, if B(z) has one or more
roots outside the unit circle, we factor B(z) into the product B1(z)B2(z), where B1(z)
has all its roots inside the unit circle and B2(z) has all its roots outside the unit circle.
Then B2(z−1) has all its roots inside the unit circle. We define the minimum-phase
system

Hmin(z) = B1(z)B2(z
−1)

A(z)

and the all-pass system

Hap(z) = B2(z)
B2(z−1)

Thus H(z) = Hmin(z)Hap(z). Note that Hap(z) is a stable, all-pass, maximum-phase
system.

(5.20)

(5.21)

(5.22)

Frequency-Domain Analysis of LTI Systems

365



Group delay of nonminimum-phase system. Based on the decomposition of a non-

τg(ω) = τming (ω)+ τ apg (ω)

Since τ ap(ω) ≥ 0 for 0 ≤ ω ≤ π , it follows that τg(ω) ≥ τming (ω), 0 ≤ ω ≤ π . From

Partial energy of nonminimum-phase system. The partial energy of a causal system
with impulse response h(n) is defined as

E(n) =
n∑
k=0
|h(k)|2

It can be shown that among all systems having the same magnitude response and the
same total energy E(∞), the minimum-phase system has the largest partial energy
[i.e., Emin(n) ≥ E(n), where Emin(n) is the partial energy of the minimum-phase
system].

System Identification and Deconvolution

Suppose that we excite an unknown linear time-invariant system with an input se-
quence x(n) and we observe the output sequence y(n). From the output sequence
we wish to determine the impulse response of the unknown system. This is a problem
in system identification, which can be solved by deconvolution. Thus we have

y(n) = h(n) ∗ x(n)

=
∞∑

k=−∞
h(k)x(n− k)

An analytical solution of the deconvolution problem can be obtained by working

Y (z) = H(z)X(z)

and hence

H(z) = Y (z)
X(z)

X(z) and Y (z) are the z-transforms of the available input signal x(n) and the observed
output signal y(n), respectively. This approach is appropriate only when there are
closed-form expressions for X(z) and Y (z).

(5.23)

(5.24)

(5.25)

(5.26)

5.3

Frequency-Domain Analysis of LTI Systems

minimum-phase system given by (5.22), we can express the group delay of H(z) as

response, the minimum-phase system has the smallest group delay.

g

(5.23) we conclude that among all pole–zero systems having the same magnitude

with the z-transform of (5.25). In the z-transform domain we have

366



A causal system produces the output sequence

y(n) =



1, n = 0
7

10 , n = 1
0, otherwise

when excited by the input sequence

x(n) =




1, n = 0
− 710 , n = 1
1

10 , n = 2
0, otherwise

Determine its impulse response and its input–output equation.

Solution. The system function is easily determined by taking the z-transforms of x(n) and
y(n). Thus we have

H(z) = Y (z)
X(z)

= 1+
7

10 z
−1

1− 710 z−1 + 110 z−2

= 1+
7

10 z
−1

(1− 12 z−1)(1− 15z−1)

Since the system is causal, its ROC is |z| > 12 . The system is also stable since its poles lie inside
the unit circle.

The input–output difference equation for the system is

y(n) = 7
10
y(n− 1)− 1

10
y(n− 2)+ x(n)+ 7

10
x(n− 1)

Its impulse response is determined by performing a partial-fraction expansion of H(z) and
inverse transforming the result. This computation yields

h(n) = [4(1
2
)n − 3(1

5
)n]u(n)

that the system is causal. However, the example above is artificial, since the system
response {y(n)} is very likely to be infinite in duration. Consequently, this approach
is usually impractical.

As an alternative, we can deal directly with the time-domain expression given

y(n) =
n∑
k=0

h(k)x(n− k), n ≥ 0

EXAMPLE 5.5

Frequency-Domain Analysis of LTI Systems

We observe that (5.26) determines the unknown system uniquely if it is known

by (5.25). If the system is causal, we have

367



and hence

h(0) = y(0)
x(0)

h(n) =
y(n)−

n−1∑
k=0
h(k)x(n− k)

x(0)
, n ≥ 1

This recursive solution requires that x(0) �= 0. However, we note again that when
{h(n)} has infinite duration, this approach may not be practical unless we truncate
the recursive solution at same stage [i.e., truncate {h(n)}].

Another method for identifying an unknown system is based on a crosscorre-
lation technique. Recall that the input–output crosscorrelation function is given as

ryx(m) =
∞∑
k=0

h(k)rxx(m− k) = h(n) ∗ rxx(m)

where ryx(m) is the crosscorrelation sequence of the input {x(n)} to the system with
the output {y(n)} of the system, and rxx(m) is the autocorrelation sequence of the
input signal. In the frequency domain, the corresponding relationship is

Syx(ω) = H(ω)Sxx(ω) = H(ω)|X(ω)|2

Hence

H(ω) = Syx(ω)
Sxx(ω)

= Syx(ω)|X(ω)|2
These relations suggest that the impulse response {h(n)} or the frequency re-

sponse of an unknown system can be determined (measured) by crosscorrelating
the input sequence {x(n)} with the output sequence {y(n)}, and then solving the

sequence {x(n)} such that its autocorrelation sequence {rxx(n)}, is a unit sample

H(ω), the values of the impulse response {h(n)} are simply equal to the values of the
crosscorrelation sequence {ryx(n)}.

In general, the crosscorrelation method described above is an effective and prac-
tical method for system identification.

5.4 Homomorphic Deconvolution

(5.27)

(5.28)

(5.29)

Frequency-Domain Analysis of LTI Systems

deconvolution problem in (5.28) by means of the recursive equation in (5.27).
Alternatively, we could simply compute the Fourier transform of (5.28) and deter-

sequence, or equivalently, that its spectrum is flat (constant) over the passband of

mine the frequency response given by (5.29). Furthermore, if we select the input

The complex cepstrum  is  a useful tool for performing deconvolution in some 
applications such as seismic signal processing. To describe this method, let us

368



{y(n)} Y(z) lnY(z)

Cy(z)
z-Transform

Complex
logarithm

{cy(z)}Inverse
z-transform

Homomorphic system for obtaining the cepstrum {cy(n)}
of the sequence {y(n)}.

us suppose that {y(n)} is the output sequence of a linear time-invariant system
which is excited by the input sequence {x(n)}. Then

Y (z) = X(z)H(z)
where H(z) is the system function. The logarithm of Y (z) is

Cy(z) = ln Y (z)
= lnX(z)+ lnH(z)
= Cx(z)+ Ch(z)

Consequently, the complex cepstrum of the output sequence {y(n)} is expressed as
the sum of the cepstrum of {x(n)} and {h(n)}, that is,

cy(n) = cx(n)+ ch(n)
Thus we observe that convolution of the two sequences in the time domain corre-
sponds to the summation of the cepstrum sequences in the cepstral domain. The
system for performing these transformations is called a homormorphic systemand is

In some applications, such as seismic signal processing and speech signal process-
x h

different so that they can be separated in the cepstral domain. Specifically, suppose
that {ch(n)} has its main components (main energy) in the vicinity of small values of
n, whereas {cx(n)} has its components concentrated at large values of n. We may
say that {ch(n)} is “lowpass” and {cx(n)} is “highpass.” We can then separate {ch(n)}
from {cx(n)} using appropriate “lowpass” and “highpass” windows, as illustrated in

Separating the two cepstral

and “highpass” windows.

cx(n)

whp(n)

cy(n) = cx(n) + ch(n)

ch(n)

wlp(n)

cy(n) = cx(n) + ch(n) ˆ

ˆ

(5.30)

(5.31)

(5.32)

Figure 5.4

ing, the characteristics of the cepstral sequences {c (n)} and {c (n)} are sufficiently

illustrated in Fig. 5.4.

Fig. 5.5. Thus

components by “lowpass”

Figure 5.5

Frequency-Domain Analysis of LTI Systems

369



cx(n)

ch(n)

Cx(x)

Ch(x)

X(z)

H(z)

x(n)

h(n)

z-Transform
Complex

exponential
Inverse

z-transform

ˆ ˆ ˆ ˆ

ˆ ˆ ˆ ˆ

Inverse homomorphic system for recovering the se-
quences {x(n)} and {h(n)} from the corresponding cepstra.

ĉh(n) = cy(n)wlp(n)
and

ĉx(n) = cy(n)whp(n)
where

wlp(n) =
{

1, |n| ≤ N1
0, otherwise

whp(n) =
{

0, |n| ≤ N1
1, |n| > N1

Once we have separated the cepstrum sequences {ĉh(n)} and {ĉx(n)} by windowing,
the sequences {x̂(n)} and {ĥ(n)} are obtained by passing {ĉh(n)} and {ĉx(n)} through

In practice, a digital computer would be used to compute the cepstrum of the
sequence {y(n)}, to perform the windowing functions, and to implement the inverse

transform, we would substitute a special form of the Fourier transform and its inverse.

Summary and References

In this chapter we considered the frequency-domain characteristics of LTI systems.
We showed that an LTI system is characterized in the frequency domain by its fre-
quency response function H(ω), which is the Fourier transform of the impulse re-
sponse of the system. We also observed that the frequency response function de-
termines the effect of the system on any input signal. In fact, by transforming the
input signal into the frequency domain, we observed that it is a simple matter to
determine the effect of the system on the signal and to determine the system output.
When viewed in the frequency domain, an LTI system performs spectral shaping or
spectral filtering on the input signal.

(5.33)

(5.34)

(5.35)

(5.36)

Figure 5.6

6

Frequency-Domain Analysis of LTI Systems

the inverse homomorphic system, shown in Fig. 5.6.

homomorphic system shown in Fig. 5.6. In place of the z-transform and inverse z-

This special form is called the discrete Fourier transform.

The design of some simple IIR filters was also considered in this chapter from 
the viewpoint of pole–zero placement. By means of this method, we were able to 
design simple digital resonators, notch filters, comb filters, all-pass filters, and digital 
sinusoidal generators. Digital sinusoidal generators find use in frequency synthesis 
applications. A comprehensive treatment of frequency synthesis techniques is given 
in the text edited by Gorski-Popiel (1975).

370



Finally, we characterized LTI systems as either minimum-phase, maximum-phase,
or mixed-phase, depending on the position of their poles and zeros in the frequency
domain. Using these basic characteristics of LTI systems, we considered practical
problems in inverse filtering, deconvolution, and system identification. We con-
cluded with the description of a deconvolution method based on cepstral analysis of
the output signal from a linear system.

A vast amount of technical literature exists on the topics of inverse filtering,
deconvolution, and system identification. In the context of communications, system
identification and inverse filtering as they relate to channel equalization are treated
in the book by Proakis (2001). Deconvolution techniques are widely used in seismic
signal processing. For reference, we suggest the papers by Wood and Treitel (1975),
Peacock and Treitel (1969), and the books by Robinson and Treitel (1978, 1980).
Homomorphic deconvolution and its applications to speech processing aretreated in
the book by Oppenheim and Schafer (1989).

Problems

1 The following input–output pairs have been observed during the operation of various
systems:

(a) x(n) = ( 12 )n
T1−→ y(n) = ( 18)n

(b) x(n) = ( 12 )nu(n)
T2−→ y(n) = ( 18 )nu(n)

(c) x(n) = ejπ/5 T3−→ y(n) = 3ejπ/5

(d) x(n) = ejπ/5u(n) T4−→ y(n) = 3ejπ/5u(n)

(e) x(n) = x(n+N1) T5−→ y(n) = y(n+N2), N1 �= N2, N1, N2 prime
Determine their frequency response if each of the above systems is LTI.

2 (a) Determine and sketch the Fourier transform WR(ω) of the rectangular sequence

wR(n) =
{

1, 0 ≤ n ≤M
0, otherwise

(b) Consider the triangular sequence

wT (n) =
{
n, 0 ≤ n ≤ M/2
M − n, M/2 < 2 ≤M
0, otherwise

Determine and sketch the Fourier transform WT (ω) of wT (n) by expressing it
as the convolution of a rectangular sequence with itself.

(c) Consider the sequence

wc(n) = 12
(

1+ cos 2πn
M

)
wR(n)

Determine and sketch Wc(ω) by using WR(ω).

Frequency-Domain Analysis of LTI Systems

371



3 Consider an LTI system with impulse response h(n) = ( 12 )nu(n).
(a) Determine and sketch the magnitude and phase response |H(ω)| and � H(ω),

respectively.

(b) Determine and sketch the magnitude and phase spectra for the input and output
signals for the following inputs:

1. x(n) = cos 3πn
10

,−∞ < n <∞
2. x(n) = {. . . , 1, 0, 0, 1

↑
, 1, 1, 0, 1, 1, 1, 0, 1, . . .}

4 Determine and sketch the magnitude and phase response of the following systems:

(a) y(n) = 12 [x(n)+ x(n− 1)]
(b) y(n) = 12 [x(n)− x(n− 1)]
(c) y(n) = 12 [x(n+ 1)− x(n− 1)]
(d) y(n) = 12 [x(n+ 1)+ x(n− 1)]
(e) y(n) = 12 [x(n)+ x(n− 2)]
(f) y(n) = 12 [x(n)− x(n− 2)]
(g) y(n) = 13 [x(n)+ x(n− 1)+ x(n− 2)]
(h) y(n) = x(n)− x(n− 8)
(i) y(n) = 2x(n− 1)− x(n− 2)
(j) y(n) = 14 [x(n)+ x(n− 1)+ x(n− 2)+ x(n− 3)]
(k) y(n) = 18 [x(n)+ 3x(n− 1)+ 3x(n− 2)+ x(n− 3)]
(l) y(n) = x(n− 4)

(m) y(n) = x(n+ 4)
(n) y(n) = 14 [x(n)− 2x(n− 1)+ x(n− 2)]

5 An FIR filter is described by the difference equation

y(n) = x(n)+ x(n− 10)

(a) Compute and sketch its magnitude and phase response.

(b) Determine its response to the inputs

1. x(n) = cos π
10
n+ 3 sin

(π
3
n+ π

10

)
, −∞ < n <∞

2. x(n) = 10+ 5 cos
(

2π
5
n+ π

2

)
, −∞ < n <∞

Frequency-Domain Analysis of LTI Systems

372



to the input signal x(n) = 10ejπn/2u(n). Let b = 2 and y(−1) = y(−2) = y(−3) =
y(−4) = 0.

+

x(n)

y(n)

b

z −1 z −1 z −1 z −1

−

7 Consider the FIR filter

y(n) = x(n)+ x(n− 4)
(a) Compute and sketch its magnitude and phase response.

(b) Compute its response to the input

x(n) = cos π
2
n+ cos π

4
n, −∞ < n <∞

(c) Explain the results obtained in part (b) in terms of the magnitude and phase
responses obtained in part (a).

8 Determine the steady-state and transient responses of the system

y(n) = 1
2

[x(n)− x(n− 2)]

to the input signal

x(n) = 5+ 3 cos
(π

2
n+ 60◦

)
, −∞ < n <∞

9 From our discussions it is apparent that an LTI system cannot produce frequencies
at its output that are different from those applied in its input. Thus, if a system
creates “new” frequencies, it must be nonlinear and/or time varying. Determine the
frequency content of the outputs of the following systems to the input signal

x(n) = A cos π
4
n

(a) y(n) = x(2n)
(b) y(n) = x2(n)
(c) y(n) = (cosπn)x(n)

Figure P6

Frequency-Domain Analysis of LTI Systems

Determine the transient and steady-state responses of the FIR filter shown in Fig. P66

373



10 Determine and sketch the magnitude and phase response of the systems shown in

+x(n) y(n)z −1 +z −1 +z −1

(c)

+
+

x(n) y(n)z −1

−

(b)

1
2

1
8

+
+

x(n) y(n)z −1

+

(a)

1
2

11 Determine the magnitude and phase response of the multipath channel

y(n) = x(n)+ x(n−M)

At what frequencies does H(ω) = 0?
12 Consider the filter

y(n) = 0.9y(n− 1)+ bx(n)
(a) Determine b so that |H(0)| = 1.
(b) Determine the frequency at which |H(ω)| = 1/√2.
(c) Is this filter lowpass, bandpass, or highpass?
(d) Repeat parts (b) and (c) for the filter y(n) = −0.9y(n− 1)+ 0.1x(n).

13 Harmonic distortion in digital sinusoidal generators An ideal sinusoidal generator
produces the signal

x(n) = cos 2πf0n, −∞ < n <∞

which is periodic with fundamental period N if f0 = k0/N and k0 , N are relatively
prime numbers. The spectrum of such a “pure” sinusoid consist of two lines at k = k0
and k = N − k0 (we limit ourselves in the fundamental interval 0 ≤ k ≤ N − 1). In
practice, the approximations made in computing the samples of a sinusoid of relative
frequency f0 result in a certain amount of power falling into other frequencies. This
spurious power results in distortion, which is referred to as harmonic distortion.

Figure P10

Frequency-Domain Analysis of LTI Systems

Fig. P10(a) through (c).

374



Harmonic distortion is usually measured in terms of the total harmonic distortion
(THD), which is defined as the ratio

THD = spurious harmonic power
total power

(a) Show that

THD = 1− 2 |ck0 |
2

Px

where

ck0 =
1
N

N−1∑
n=0

x(n)e−j (2π/N)k0n

Px = 1
N

N−1∑
n=0
|x(n)|2

(b) By using the Taylor approximation

cosφ = 1− φ
2

2!
+ φ

4

4!
− φ

6

6!
+ · · ·

compute one period of x(n) for f0 = 1/96, 1/32, 1/256 by increasing the number
of terms in the Taylor expansion from 2 to 8.

(c) Compute the THD and plot the power density spectrum for each sinusoid in
part (b) as well as for the sinusoids obtained using the computer cosine function.
Comment on the results.

14 Measurement of the total harmonic distortion in quantized sinusoids Let x(n) be a
periodic sinusoidal signal with frequency f0 = k/N , that is,

x(n) = sin 2πf0n
(a) Write a computer program that quantizes the signal x(n) into b bits or equiva-

lently into L = 2b levels by using rounding. The resulting signal is denoted by
xq(n).

(b) For f0 = 1/50 compute the THD of the quantized signals xq(n) obtained by
using b = 4, 6, 8, and 16 bits.

(c) Repeat part (b) for f0 = 1/100.
(d) Comment on the results obtained in parts (b) and (c).

15 Consider the discrete-time system

y(n) = ay(n− 1)+ (1− a)x(n), n ≥ 0

where a = 0.9 and y(−1) = 0.

Frequency-Domain Analysis of LTI Systems

375



(a) Compute and sketch the output yi(n) of the system to the input signals

xi(n) = sin 2πfin, 0 ≤ n ≤ 100

where f1 = 14 , f2 = 15 , f3 = 110 , f4 = 120 .
(b) Compute and sketch the magnitude and phase response of the system and use

these results to explain the response of the system to the signals given in part (a).

16 Consider an LTI system with impulse response h(n) = ( 13 )|n| .
(a) Determine and sketch the magnitude and phase response H(ω)| and �H(ω),

respectively.

(b) Determine and sketch the magnitude and phase spectra for the input and output
signals for the following inputs:

1. x(n) = cos 3πn
8
,−∞ < n <∞

2. x(n) = {. . . ,−1, 1,−1, 1
↑
,−1, 1,−1, 1,−1, 1,−1, 1, . . .}

17

(a) Determine the input–output relation and the impulse response h(n).

(b) Determine and sketch the magnitude |H(ω)| and the phase response �H(ω) of
the filter and find which frequencies are completely blocked by the filter.

(c) When ω0 = π/2, determine the output y(n) to the input

x(n) = 3 cos
(π

3
n+ 30◦

)
, −∞ < n <∞

+x(n) y(n)z −1 z −1

a =  −2 cos ω0

18 Consider the FIR filter
y(n) = x(n)− x(n− 4)

(a) Compute and sketch its magnitude and phase response.

(b) Compute its response to the input

x(n) = cos π
2
n+ cos π

4
n, −∞ < n <∞

(c) Explain the results obtained in part (b) in terms of the answer given in part (a).

Figure P17

Frequency-Domain Analysis of LTI Systems

Consider the digital filter shown in Fig. P17.

376



19 Determine the steady-state response of the system

y(n) = 1
2

[x(n)− x(n− 2)]

to the input signal

x(n) = 5+ 3 cos
(π

2
n+ 60◦

)
+ 4 sin(πn+ 45◦), −∞ < n <∞

20 Recall from Problem 9 that an LTI system cannot produce frequencies at its output
that are different from those applied in its input. Thus if a system creates “new”
frequencies, it must be nonlinear and/or time varying. Indicate whether the following
systems are nonlinear and/or time varying and determine the output spectra when
the input spectrum is

X(ω) =
{

1, |ω| ≤ π/4
0, π/4 ≤ |ω| ≤ π

(a) y(n) = x(2n)
(b) y(n) = x2(n)
(c) y(n) = (cosπn)x(n)

21 Consider an LTI system with impulse response

h(n) =
[(

1
4

)n
cos

(π
4
n
)]
u(n)

(a) Determine its system function H(z).

(b) Is it possible to implement this system using a finite number of adders, multipliers,
and unit delays? If yes, how?

(c) Provide a rough sketch of |H(ω)| using the pole–zero plot.
(d) Determine the response of the system to the input

x(n) = (1
4
)nu(n)

22 An FIR filter is described by the difference equation

y(n) = x(n)− x(n− 6)

(a) Compute and sketch its magnitude and phase response.

(b) Determine its response to the inputs

1. x(n) = cos π
10
n+ 3 sin

(π
3
n+ π

10

)
, −∞ < n <∞

2. x(n) = 5+ 6 cos
(

2π
5
n+ π

2

)
, −∞ < n <∞

Frequency-Domain Analysis of LTI Systems

377



23 The frequency response of an ideal bandpass filter is given by

H(ω) =




0, |ω| ≤ π
8

1,
π

8
< |ω| < 3π

8

0,
3π
8
≤ |ω| ≤ π

(a) Determine its impulse response

(b) Show that this impulse response can be expressed as the product of cos(nπ/4)
and the impulse response of a lowpass filter.

24 Consider the system described by the difference equation

y(n) = 1
2
y(n− 1)+ x(n)+ 1

2
x(n− 1)

(a) Determine its impulse response.

(b) Determine its frequency response:

1. From the impulse response

2. From the difference equation

(c) Determine its response to the input

x(n) = cos
(π

2
n+ π

4

)
, −∞ < n <∞

25 Sketch roughly the magnitude |H(ω)| of the Fourier transforms corresponding to

(a)

Double pole

Unit circle
(b)

Double zero

Unit circle
Pole at 0.9ejθ

0.9

(c)

0.9

8th-order pole
(d)

1

Figure P25

Frequency-Domain Analysis of LTI Systems

the pole–zero patterns of systems given in Fig. P25.

378



26 Design an FIR filter that completely blocks the frequency ω0 = π/4 and then com-
pute its output if the input is

x(n) =
(

sin
π

4
n
)
u(n)

for n = 0, 1, 2, 3, 4. Does the filter fulfill your expectations? Explain.
27 A digital filter is characterized by the following properties:

1. It is highpass and has one pole and one zero.

2. The pole is at a distance r = 0.9 from the origin of the z-plane.
3. Constant signals do not pass through the system.

(a) Plot the pole–zero pattern of the filter and determine its system function H(z).

(b) Compute the magnitude response |H(ω)| and the phase response �H(ω) of the
filter.

(c) Normalize the frequency response H(ω) so that |H(π)| = 1.
(d) Determine the input–output relation (difference equation) of the filter in the

time domain.

(e) Compute the output of the system if the input is

x(n) = 2 cos
(π

6
n+ 45◦

)
, −∞ < n <∞

(You can use either algebraic or geometrical arguments.)

28 A causal first-order digital filter is described by the system function

H(z) = b0 1+ bz
−1

1+ az−1

(a) Sketch the direct form I and direct form II realizations of this filter and find the
corresponding difference equations.

(b) For a = 0.5 and b = −0.6, sketch the pole–zero pattern. Is the system stable?
Why?

(c) For a = −0.5 and b = 0.5, determine b0 , so that the maximum value of |H(ω)|
is equal to 1.

(d) Sketch the magnitude response |H(ω)| and the phase response �H(ω) of the
filter obtained in part (c).

(e) In a specific application it is known that a = 0.8. Does the resulting filter amplify
high frequencies or low frequencies in the input? Choose the value of b so as to
improve the characteristics of this filter (i.e., make it a better lowpass or a better
highpass filter).

29 Derive the expression for the resonant frequency of a two-pole filter with poles at
p1 = rejθ and p2 ∗1

Frequency-Domain Analysis of LTI Systems

= p , given by (4.25).

379



30 Determine and sketch the magnitude and phase responses of the Hanning filter
characterized by the (moving average) difference equation

y(n) = 1
4
x(n)+ 1

2
x(n− 1)+ 1

4
x(n− 2)

31 A causal LTI system excited by the input

x(n) = (1
4
)nu(n)+ u(−n− 1)

produces an output y(n) with z-transform

Y (z) = −
3
4z
−1

(1− 14z−1)(1+ z−1)
(a) Determine the system function H(z) and its ROC.
(b) Determine the output y(n) of the system.

(Hint: Pole cancellation increases the original ROC.)
32 Determine the coefficients of a linear-phase FIR filter

y(n) = b0x(n)+ b1x(n− 1)+ b2x(n− 2)
such that:

(a) It rejects completely a frequency component at ω0 = 2π/3.
(b) Its frequency response is normalized so that H(0) = 1.
(c) Compute and sketch the magnitude and phase response of the filter to check if

it satisfies the requirements.
33 Determine the frequency response H(ω) of the following moving average filters.

(a) y(n) = 1
2M + 1

M∑
k=−M

x(n− k)

(b) y(n) = 1
4M

x(n+M)+ 1
2M

M−1∑
k=−M+1

x(n− k)+ 1
4M

x(n−M)

Which filter provides better smoothing? Why?
34 Compute the magnitude and phase response of a filter with system function

H(z) = 1+ z−1 + z−2 + · · · + z−8

If the sampling frequency is Fs = 1 kHz, determine the frequencies of the analog
sinusoids that cannot pass through the filter.

35 A second-order system has a double pole at p1,2 = 0.5 and two zeros at

z1,2 = e±j3π/4

Using geometric arguments, choose the gain G of the filter so that |H(0)| = 1.

Frequency-Domain Analysis of LTI Systems

380



36 In this problem we consider the effect of a single zero on the frequency response of
a system. Let z = rejθ be a zero inside the unit circle (r < 1). Then

Hz(ω) = 1− rejθ e−jω

= 1− r cos(ω − θ)+ jr sin(ω − θ)

(a) Show that the magnitude response is

|Hz(ω)| = [1− 2r cos(ω − θ)+ r2]1/2

or, equivalently,

20 log10 |Hz(ω)| = 10 log10[1− 2r cos(ω − θ)+ r2]

(b) Show that the phase response is given as

�z(ω) = tan−1 r sin(ω − θ)1− r cos(ω − θ)

(c) Show that the group delay is given as

τg(ω) = r
2 − r cos(ω − θ)

1+ r2 − 2r cos(ω − θ)

(d) Plot the magnitude |H(ω)|dB, the phase �(ω) and the group delay τg(ω) for
r = 0.7 and θ = 0, π/2, and π .

37 In this problem we consider the effect of a single pole on the frequency response of
a system. Hence, we let

Hp(ω) = 11− rejθ e−jω , r < 1

Show that

|Hp(ω)|dB = −|Hz(ω)|dB
�Hp(ω) = −�Hz(ω)
τpg (ω) = −τ zg (ω)

z
z
g

38 In this problem we consider the effect of complex-conjugate pairs of poles and zeros
on the frequency response of a system. Let

Hz(ω) = (1− rejθe−jω)(1− re−jθ e−jω)

Frequency-Domain Analysis of LTI Systems

where H (ω) and τ (ω) are defined in Problem 36.

381



(a) Show that the magnitude response in decibels is

|Hz(ω)|dB = 10 log10[1+ r2 − 2r cos(ω − θ)]
+ 10 log10[1+ r2 − 2r cos(ω + θ)]

(b) Show that the phase response is given as

�z(ω) = tan−1 r sin(ω − θ)1− r cos(ω − θ) + tan
−1 r sin(ω + θ)

1− r cos(ω + θ)
(c) Show that the group delay is given as

τ zg (ω) =
r2 − r cos(ω − θ)

1+ r2 − 2r cos(ω − θ) +
r2 − r cos(ω + θ)

1+ r2 − 2r cos(ω + θ)
(d) If Hp(ω) = 1/Hz(ω), show that

|Hp(ω)|dB = −|Hz(ω)|dB
�p(ω) = −�z(ω)
τpg (ω) = −τ zg (ω)

(e) Plot |Hp(ω)|, �p(ω) and τpg (ω) for τ = 0.9, and θ = 0, π/2.
39 Determine the 3-dB bandwidth of the filters (0 < a < 1)

H1(z) = 1− a1− az−1

H2(z) = 1− a2
1+ z−1

1− az−1

Which is a better lowpass filter?
40 Design a digital oscillator with adjustable phase, that is, a digital filter which produces

the signal
y(n) = cos(ω0n+ θ)u(n)

41 This problem provides another derivation of the structure for the coupled-form os-
cillator by considering the system

y(n) = ay(n− 1)+ x(n)

for a = ejω0 .
Let x(n) be real. Then y(n) is complex. Thus

y(n) = yR(n)+ jyI (n)
(a) Determine the equations describing a system with one input x(n) and the two

outputs yR(n) and yI (n).

Frequency-Domain Analysis of LTI Systems

382



(b) Determine a block diagram realization

(c) Show that if x(n) = δ(n), then

yR(n) = (cosω0n)u(n)
yI (n) = (sinω0n)u(n)

(d) Compute yR(n), yI (n), n = 0, 1, . . . , 9 for ω0 = π/6. Compare these with the
true values of the sine and cosine.

42 Consider a filter with system function

H(z) = b0 (1− e
jω0z−1)(1− e−jω0z−1)

(1− rejω0z−1)(1− re−jω0z−1)
(a) Sketch the pole–zero pattern.

(b) Using geometric arguments, show that for r � 1, the system is a notch filter and
provide a rough sketch of its magnitude response if ω0 = 60◦ .

(c) For ω0 = 60◦ , choose b0 so that the maximum value of |H(ω)| is 1.
(d) Draw a direct form II realization of the system.

(e) Determine the approximate 3-dB bandwidth of the system.
43 Design an FIR digital filter that will reject a very strong 60-Hz sinusoidal interference

contaminating a 200-Hz useful sinusoidal signal. Determine the gain of the filter so
that the useful signal does not change amplitude. The filter works at a sampling
frequency Fs = 500 samples/s. Compute the output of the filter if the input is a 60-
Hz sinusoid or a 200-Hz sinusoid with unit amplitude. How does the performance
of the filter compare with your requirements?

44 Determine the gain b0 0 1.

45
plying the trigonometric identity

cosα + cosβ = 2 cos α + β
2

cos
α − β

2

where α = (n + 1)ω0 , β = (n − 1)ω0 , and y(n) = cosω0n. Thus show that the
sinusoidal signal y(n) = 0
initial conditions y(−1) = A cosω0 and y(−2) = A cos 2ω0 .

46 0 and β = (n − 2)ω0 to
derive the difference equation for generating the sinusoidal signal y(n) = A sin nω0 .
Determine the corresponding initial conditions.

47

48 Determine the structure for the coupled-form oscillator by combining the structure

Frequency-Domain Analysis of LTI Systems

for the digital resonator described by (4.28) so that |H(ω )| =
Demonstrate that the difference equation given in (4.52) can be obtained by ap-

A cosω n can be generated from (4.52) by use of the

= nωUse the trigonometric identity in (4.53) with α

z-transform
h(n) = Acosnω0u(n) and h(n) = A sin nω0u(n),

Using  pairs, determine the difference equations for the digital oscil-
lators that have impulse responses  
respectively.

for the digital oscillators obtained in Problem 47.

383



49 Convert the highpass filter with system function

H(z) = 1− z
−1

1− az−1 , a < 1

into a notch filter that rejects the frequency ω0 = π/4 and its harmonics.
(a) Determine the difference equation.
(b) Sketch the pole–zero pattern.
(c) Sketch the magnitude response for both filters.

50 Choose L and M for a lunar filter that must have narrow passbands at (k ± F)
cycles/day, where k = 1, 2, 3, . . . and F = 0.067726.

51 (a)
all-pass.

(b) What is the number of delays and multipliers required for the efficient imple-
mentation of a second-order all-pass system?

52 A digital notch filter is required to remove an undesirable 60-Hz hum associated with
a power supply in an ECG recording application. The sampling frequency used is
Fs = 500 samples/s. (a) Design a second-order FIR notch filter and (b) a second-
order pole–zero notch filter for this purpose. In both cases choose the gain b0 so that
|H(ω)| = 1 for ω = 0.

53 Determine the coefficients {h(n)} of a highpass linear phase FIR filter of length
M = 4 which has an antisymmetric unit sample response h(n) = −h(M − 1− n) and
a frequency response that satisfies the condition∣∣∣H (π

4

)∣∣∣ = 1
2
,

∣∣∣∣H
(

3π
4

)∣∣∣∣ = 1
54 In an attempt to design a four-pole bandpass digital filter with desired magnitude

response

|Hd(ω)| =
{

1,
π

6
≤ ω ≤ π

2
0, elsewhere

we select the four poles at

p1,2 = 0.8e±j2π/9

p3,4 = 0.8e±j4π/9

and four zeros at
z1 = 1, z2 = −1, z3,4 = e±3π/4

(a) Determine the value of the gain so that∣∣∣∣H
(

5π
12

)∣∣∣∣ = 1
(b) Determine the system function H(z).
(c) Determine the magnitude of the frequency response H(ω) for 0 ≤ ω ≤ π and

compare it with the desired response |Hd(ω)|.

Frequency-Domain Analysis of LTI Systems

Show that the systems corresponding to the pole–zero patterns of Fig. 4.16 are

384



55 A discrete-time system with input x(n) and output y(n) is described in the frequency
domain by the relation

Y (ω) = e−j2πωX(ω)+ dX(ω)
dω

(a) Compute the response of the system to the input x(n) = δ(n).
(b) Check if the system is LTI and stable.

56 Consider an ideal lowpass filter with impulse response h(n) and frequency response

H(ω) =
{

1, |ω| ≤ ωc
0, ωc < |ω| < π

What is the frequency response of the filter defined by

g(n) =
{
h
(n

2

)
, n even

0, n odd

57 Determine its impulse response and its fre
quency response if the system H(ω) is:

(a) Lowpass with cutoff frequency ωc .

(b) Highpass with cutoff frequency ωc .

x(n) − y(n)H(ω) +

58 Frequency inverters have been used for many years for speech scrambling. Indeed,
a voice signal x(n) becomes unintelligible if we invert its spectrum as shown in

(a) Determine how frequency inversion can be performed in the time domain.

(b) Design an unscrambler. (Hint: The required operations are very simple and can
easily be done in real time.)

Figure P57

Frequency-Domain Analysis of LTI Systems

Consider the system shown in Fig. P57. -

Fig. P58.

385



(a) Original spectrum;
(b) frequency-inverted
spectrum.

0
(a)

2 1

π
ω

X(ω)

−π

0
(b)

1 2

π
ω

Y(ω)

−π

59 A lowpass filter is described by the difference equation

y(n) = 0.9y(n− 1)+ 0.1x(n)
(a) By performing a frequency translation of π/2, transform the filter into a bandpass

filter.

(b) What is the impulse response of the bandpass filter?
(c) What is the major problem with the frequency translation method for transform-

ing a prototype lowpass filter into a bandpass filter?
60 Consider a system with a real-valued impulse response h(n) and frequency response

H(ω) = |H(ω)|ejθ(ω)

The quantity

D =
∞∑

n=−∞
n2h2(n)

provides a measure of the “effective duration” of h(n).

(a) Express D in terms of H(ω).
(b) Show that D is minimized for θ(ω) = 0.

61 Consider the lowpass filter

y(n) = ay(n− 1)+ bx(n), 0 < a < 1
(a) Determine b so that |H(0)| = 1.
(b) Determine the 3-dB bandwidth ω3 for the normalized filter in part (a).
(c) How does the choice of the parameter a affect ω3 ?
(d) Repeat parts (a) through (c) for the highpass filter obtained by choosing −1 <

a < 0.

Figure P58

Frequency-Domain Analysis of LTI Systems

386



62 Sketch the magnitude and phase response of the multipath channel

y(n) = x(n)+ αx(n−M), α > 0

for α << 1.
63 Determine the system functions and the pole-zero locations for the systems shown

+x(n) y(n)z −1 +z −1 +z −1

(c)

+
+

x(n) y(n)z −1

−

(b)

1
2

1
8

+
+

x(n) y(n)z −1

+

(a)

1
2

64 Determine and sketch the impulse response and the magnitude and phase responses

+

x(n)

y(n)

b

z −1 z −1z −1 z −1

65 Consider the system
y(n) = x(n)− 0.95x(n− 6)

(a) Sketch its pole–zero pattern.

(b) Sketch its magnitude response using the pole–zero plot.

(c) Determine the system function of its causal inverse system.

(d) Sketch the magnitude response of the inverse system using the pole–zero plot.

Figure P63

Figure P64

Frequency-Domain Analysis of LTI Systems

in Fig. P63(a) through (c), and indicate whether or not the systems are stable.

of the FIR filter shown in Fig. P64 for b = 1 and b = −1.

387



66 Determine the impulse response and the difference equation for all possible systems
specified by the system functions

(a) H(z) = z
−1

1− z−1 − z−2
(b) H(z) = 1

1− e−4az−4 , 0 < a < 1
67 Determine the impulse response of a causal LTI system which produces the response

y(n) = {1
↑
,−1, 3,−1, 6}

when excited by the input signal

x(n) = {1
↑
, 1, 2}

68 The system

y(n) = 1
2
y(n− 1)+ x(n)

is excited with the input

x(n) = (1
4
)nu(n)

Determine the sequences rxx(l), rhh(l), rxy(l), and ryy(l).
69 Determine if the following FIR systems are minimum phase.

(a) h(n) = {10
↑
, 9,−7,−8, 0, 5, 3}

(b) h(n) = {5
↑
, 4,−3,−4, 0, 2, 1}

70 Can you determine the coefficients of the all-pole system

H(z) = 1

1+
N∑
k=1

akz
−k

if you know its orderN and the values h(0), h(1), . . . , h(L−1) of its impulse response?
How? What happens if you do not know N ?

71 Consider a system with impulse response

h(n) = b0δ(n)+ b1δ(n−D)+ b2δ(n− 2D)
(a) Explain why the system generates echoes spaced D samples apart.
(b) Determine the magnitude and phase response of the system.
(c) Show that for |b0+ b2| << |b1|, the locations of maxima and minima of |H(ω)|2

are at

ω = ± k
D
π, k = 0, 1, 2, . . .

(d) Plot |H(ω)| and �H(ω) for b0 = 0.1, b1 = 1, and b2 = 0.05 and discuss the
results.

Frequency-Domain Analysis of LTI Systems

388



72 Consider the pole–zero system

H(z) = B(z)
A(z)

= 1+ bz
−1

1+ az−1 =
∞∑
n=0

h(n)z−n

(a) Determine h(0), h(1), h(2), and h(3) in terms of a and b .

(b) Let rhh(l) be the autocorrelation sequence of h(n). Determine rhh(0), rhh(1),
rhh(2), and rhh(3) in terms of a and b .

73 Let x(n) be a real-valued minimum-phase sequence. Modify x(n) to obtain another
real-valued minimum-phase sequence y(n) such that y(0) = x(0) and y(n) = |x(n)|.

74 The frequency response of a stable LTI system is known to be real and even. Is the
inverse system stable?

75 Let h(n) be a real filter with nonzero linear or nonlinear phase response. Show that
the following operations are equivalent to filtering the signal x(n) with a zero-phase
filter.

(a) g(n) = h(n) ∗ x(n)
f (n) = h(n) ∗ g(−n)
y(n) = f (−n)

(b) g(n) = h(n) ∗ x(n)
f (n) = h(n) ∗ x(−n)
y(n) = g(n)+ f (−n)

(Hint: Determine the frequency response of the composite system y(n) = H [x(n)].)
76 Check the validity of the following statements:

(a) The convolution of two minimum-phase sequences is always a minimum-phase
sequence.

(b) The sum of two minimum-phase sequences is always minimum phase.

77 Determine the minimum-phase system whose squared magnitude response is given
by:

(a) |H(ω)|2 =
5
4
− cosω

10
9
− 2

3
cosω

(b) |H(ω)|2 = 2(1− a
2)

(1+ a2)− 2a cosω, |a| < 1
78 Consider an FIR system with the following system function:

H(z) = (1− 0.8ejπ/2z−1)(1− 0.8e−jπ/2z−1)(1− 1.5ejπ/4z−1)(1− 1.5e−jπ/4z−1)
(a) Determine all systems that have the same magnitude response. Which is the

minimum-phase system?

Frequency-Domain Analysis of LTI Systems

389



(b) Determine the impulse response of all systems in part (a).

(c) Plot the partial energy

E(n) =
n∑
k=0

h2(n)

for every system and use it to identify the minimum- and maximum-phase sys-
tems.

79 The causal system

H(z) = 1

1+
N∑
k=1

akz
−k

is known to be unstable.
We modify this system by changing its impulse response h(n) to

h′(n) = λnh(n)u(n)

(a) Show that by properly choosing λ we can obtain a new stable system.

(b) What is the difference equation describing the new system?

80 Given a signal x(n), we can create echoes and reverberations by delaying and scaling
the signal as follows

y(n) =
∞∑
k=0

gkx(n− kD)

where D is positive integer and gk > gk−1 > 0.
(a) Explain why the comb filter

H(z) = 1
1− az−D

can be used as a reverberator (i.e., as a device to produce artificial reverbera-
tions). (Hint: Determine and sketch its impulse response.)

(b) The all-pass comb filter

H(z) = z
−D − a

1− az−D

is used in practice to build digital reverberators by cascading three to five such
filters and properly choosing the parameters a and D . Compute and plot the
impulse response of two such reverberators each obtained by cascading three
sections with the following parameters.

Frequency-Domain Analysis of LTI Systems

390



UNIT 1

Section D a

1 50 0.7

2 40 0.665

3 32 0.63175

UNIT 2

Section D a

1 50 0.7

2 17 0.77

3 6 0.847

(c) The difference between echo and reverberation is that with pure echo there are
clear repetitions of the signal, but with reverberations, there are not. How is this
reflected in the shape of the impulse response of the reverberator? Which unit
in part (b) is a better reverberator?

(d) If the delaysD1 ,D2 ,D3 in a certain unit are prime numbers, the impulse response
of the unit is more “dense.” Explain why.

(e) Plot the phase response of units 1 and 2 and comment on them.
(f) Plot h(n) for D1 , D2 , and D3 being nonprime. What do you notice?
More details about this application can be found in a paper by J. A. Moorer, “Signal
Processing Aspects of Computer Music: A Survey,” Proc. IEEE, Vol. 65, No. 8,
Aug. 1977, pp. 1108–1137.

81 By trial-and-error design a third-order lowpass filter with cutoff frequency at ωc =
π/9 radians/sample interval. Start your search with
(a) z1 = z2 = z3 = 0, p1 = r, p2,3 = re±jωc , r = 0.8
(b) r = 0.9, z1 = z2 = z3 = −1

82 A speech signal with bandwidth B = 10 kHz is sampled at F2 = 20 kHz. Suppose
that the signal is corrupted by four sinusoids with frequencies

F1 = 10,000 Hz, F3 = 7778 Hz
F2 = 8889 Hz, F4 = 6667 Hz

(a) Design a FIR filter that eliminates these frequency components.
(b) Choose the gain of the filter so that |H(0)| = 1 and then plot the log magnitude

response and the phase response of the filter.
(c) Does the filter fulfill your objectives? Do you recommend the use of this filter

in a practical application?
83 Compute and sketch the frequency response of a digital resonator with ω = π/6 and

r = 0.6, 0.9, 0.99. In each case, compute the bandwidth and the resonance frequency
from the graph, and check if they are in agreement with the theoretical results.

84 The system function of a communication channel is given by

H(z) = (1− 0.9ej0.4πz−1)(1− 0.9e−j0.4πz−1)(1− 1.5ej0.6πz−1)(1− 1.5e−j0.6πz−1)
Determine the system function Hc(z) of a causal and stable compensating system
so that the cascade interconnection of the two systems has a flat magnitude re-
sponse. Sketch the pole–zero plots and the magnitude and phase responses of all
systems involved into the analysis process. [Hint: Use the decomposition H(z) =
Hap(z)Hmin(z).]

Frequency-Domain Analysis of LTI Systems

391



Frequency-Domain Analysis of LTI Systems

Answers to Selected Problems

1 (a) Because the range of n is (−∞,∞), the fourier transforms of x(n) and y(n) do not exist.
However, the relationship implied by the forms of x(n) and y(n) is y(n) = x3(n). In this
case, the system H1 is non-linear.

(b) In this case, H(ω) = 1−
1
2 e
−jω

1− 18 e−jω
, so the system is LTI.

3 (a) |H(ω)| = 1
[ 54−cosω]

1
2

	 H(ω) = − tan−1 12 sinω
1− 12 cosω

4 (a) H(ω) = (sinω)ejπ/2
(j) H(ω) = (cosω) (cos ω2 ) e−j3ω/2
(n) H(ω) = (sin2 ω/2)e−j (ω−π)

8 yss(n) = 3 cos
(
π

2 n+ 600
)
; ytr (n) = 0

12 (a) H(ω) = b
1−0.9e−jω ; |H(0)| = 1, ⇒ b = ±0.1

�(ω) =
{
− ωM2 , cos ωM2 > 0
π − ωM2 , cos ωM2 < 0

(b) |H(ω0)|2 = 12 ⇒ b
2

1.81−1.8 cosω0 =
1
2 ⇒ ω0 = 0.105

16 (a) |H(ω)| = 45−3 cosω ; 	 H(ω) = 0
18 (a) H(ω) = 2e−j2ωejπ/2 sin 2ω

(b) y(n) = 2 cosπn/4, H ( π4 ) = 2, 	 H ( π4 ) = 0
20 (a) Y (ω) = X ( ω2 ) =

{
1, |ω| ≤ π2
0, π2 ≤ |ω| ≤ p

(b) Y (ω) = 12 [X(ω − π)+X(ω + π)] =
{

0, |ω| ≤ 3π4
1
2 ,

3π
4 ≤ |ω| < π

22 (a) |H(ω)| = 2| sin 5ω} ,
�(ω) =

{ π
2 − 5ω for sin 5ω > 0
π

2 − 5ω + π, for sin 5ω < 0
(b)

∣∣H ( π10 )∣∣ = 2, 	 H ( (π 10) = 0; ∣∣H ( π3 )∣∣ = √3, � ( π3 ) = 	 H ( π3 ) = − π6
(1) Hence, y(n) = 2 cos π10n+ 3

√
3 sin

(
π

3 n− π15
)

(2) H(0) = 0, H ( 2π5 ) = 0; Hence, y(n) = 0)
24 (a) H(z) = 2

1− 12 z−1
− 1; h(n) = 2 ( 12 )n u(n)− δ(n)

30 |H(ω)| = cos2 ω2 ; �(ω) = 	 H(ω) = −ω
33 (a) H(ω) = 12M+1

[
1+ 2∑M−1k=1 cosωk]

(b) H(ω) = 12M cosMω + 12M
[
1+ 2∑M−1k=1 cosωk]

39 (a) |H1(ω)|2 = 12 ⇒ cosw2 = 4a−1−a
2

2a

(b) |H2(ω)|2 = ( 12 ⇒ cosω 2a1+a2
By comparing the results of (a) and (b) we conclude that ω2 < ω1 . Therefore, the second filter
has a smaller 3dB bandwidth.

49 (a) Replace z by z8 . We need 8 zeros at the frequencies ω = 0, ± π4 , ± π3 , ± 3π4 , π . Hence,
H(z) = 1−z−8

1−az−8 ; y(n) = ay(n− 8)+ x(n)− x(n− 8)
53

Hr(ω) = 2
[
h(0) sin 3ω2 + h(1) sin ω2

]
; Hr

(
π

4

) = 12 ; Hr ( 3π4 ) = 1
Hr
(

3π
4

) = 2h(0) sin 9π8 + 2h(1) sin 3π8 = 1
1.85h(0)+ 0.765h(1) = 12
−0.765h(0)+ 1.85h(1) = 1
h(1) = 0.56, h(0) = 0.04

392



Frequency-Domain Analysis of LTI Systems

59 (a) H(z) = 0.1
1−0.9z−1 ; Hbp(ω) = H

(
ω − π2

) = 0.1
1−0.9e−jπ−

π
2

(b) h(n) = 0.1 (0.9 ejπ/2)n u(n)
61 (a) H(z) = b

1=az−1 ; H(ω) = b1=ae−jω ; b = ±(1− a)
(b) |H(ω)|2 = b2

1+a2−2a cosω = 12 ; ω3 = cos−1
(

4a−1−a2
2a

)
66 (a) h(n) =

[
− 1√

5

(
1+√5

2

)n
+ 1√

5

(
1−√5

2

)n]
u(−n− 1)

y(n) = y(n− 1)+ y(n− 2)+ x(n− 1)
68 rxx(n) = 1615

(
1
4

)n
u(n)+ 1615 (4)nu(−n− 1); rhh(n) = 43

(
1
2

)n
u(n)+ 43 (2)nu(−n− 1)

rxy(n) = 1617 (2)nu(n− 1)− 1615 (4)nu(−n− 1)− 128105
(

1
4

)n
u(n)

ryy(n) = 6421 (2)nu(−n− 1)− 128105 (4)nu(−n− 1)+ 6421
(

1
2

)n
u(n)− 128105

(
1
4

)n
u(n)

77 (a) H(z)H(z−1) = 54 − 12 (z+z−1)10
9 − 13 (z+z−1)

H(z) = 1−
1
2 z
−1

1− 13 z−1

82 (a) B = 10 kHz; Fs = 20 kHz; z1 = 10k20k = 0.5; z2 = 7.777k20k = 0.3889; z3 = 8.889k20k = 0.4445
z4 = 6.667k20k = 0.3334;
H(z) = (z− 0.5)(z− 0.3889)(z− 0.4445)(z− 0.3334)

393



This page intentionally left blank 



John G. Proakis, Dimitris G. Manolakis. Copyright © 2007 by Pearson Education, Inc. All rights reserved.
From Chapter 6   of Digital Signal Processing: Principles, Algorithms, and Applications, Fourth Edition.

Sampling and Reconstruction
of Signals

395



Sampling and
Reconstruction
of Signals

1 Ideal Sampling and Reconstruction of Continuous-Time Signals

x(n) = xa(nT ), −∞ < n <∞ (1.1)

xa(t),
x(n)

T

Fs = 1/T

In the sampling of continuous-time signals, if the signals are bandlimited, it is pos-
sible to reconstruct the original signal from the samples, provided that the sampling 
rate is at least twice the highest frequency contained in the signal. Recall the subse-
quent operations of quantization and coding that are necessary to convert an analog 
signal to a digital signal appropriate for digital processing.

In this chapter we consider time-domain sampling, analog-to-digital (A/D) con-
version (quantization and coding), and digital-to-analog (D/A) conversion (signal 
reconstruction). We also consider the sampling of signals that are characterized as 
bandpass signals. The final topic deals with the use of oversampling and sigma-delta 
modulation in the design of high precision A/D converters.

To process a continuous-time signal using digital signal processing techniques, it is 
necessary to convert the signal into a sequence of numbers. This is usually done 
by sampling the analog signal, say  periodically every  seconds to produce a 
discrete-time signal  given by

The relationship (1.1) describes the sampling process in the time domain. The 
sampling frequency  must be selected large enough such that the sampling 

396



If xa(t) is an aperiodic signal with finite energy, its (voltage) spectrum is given
by the Fourier transform relation

Xa(F ) =
∫ ∞
−∞

xa(t)e
−j2πF t dt

whereas the signal xa(t) can be recovered from its spectrum by the inverse Fourier
transform

xa(t) =
∫ ∞
−∞

Xa(F )e
j2πF t dF

Note that utilization of all frequency components in the infinite frequency range
−∞ < F < ∞ is necessary to recover the signal xa(t) if the signal xa(t) is not
bandlimited.

The spectrum of a discrete-time signal x(n), obtained by sampling xa(t), is given
by the Fourier transform relation

X(ω) =
∞∑

n=−∞
x(n)e−jωn

or, equivalently,

X(f ) =
∞∑

n=−∞
x(n)e−j2πf n

The sequence x(n) can be recovered from its spectrum X(ω) or X(f ) by the inverse
transform

x(n) = 1
2π

∫ π
−π
X(ω)ejωndω

=
∫ 1/2
−1/2

X(f )ej2πf n df

In order to determine the relationship between the spectra of the discrete-time
signal and the analog signal, we note that periodic sampling imposes a relationship
between the independent variables t and n in the signals xa(t) and x(n), respectively.
That is,

t = nT = n
Fs

This relationship in the time domain implies a corresponding relationship between
the frequency variables F and f in Xa(F ) and X(f ), respectively.

(1.2)

(1.3)

(1.4)

(1.5)

(1.6)

(1.7)

Sampling and Reconstruction of Signals

xa(t) and x(n).

does not cause any loss of spectral information (no aliasing). Indeed, if the spectrum 
of the analog signal can be recovered from the spectrum of the discrete-time signal, 
there is no loss of information. Consequently, we investigate the sampling process by 
finding the relationship between the spectra of signals

397



x(n) ≡ xa(nT ) =
∫ ∞
−∞

Xa(F )e
j2πnF/Fs dF

∫ 1/2
−1/2

X(f )ej2πf n df =
∫ ∞
−∞

Xa(F )e
j2πnF/Fs dF

f = F
Fs

obtain the result

1
Fs

∫ Fs/2
−Fs/2

X(F)ej2πnF/Fs dF =
∫ ∞
−∞

Xa(F )e
j2πnF/Fs dF

integration range of this integral can be divided into an infinite number of intervals
of width Fs . Thus the integral over the infinite range can be expressed as a sum of
integrals, that is,

∫ ∞
−∞

Xa(F )e
j2πnF/Fs dF =

∞∑
k=−∞

∫ (k+1/2)Fs
(k−1/2)Fs

Xa(F )e
j2πnF/Fs dF

We observe that Xa(F ) in the frequency interval (k − 12 )Fs to (k + 12 )Fs is identical
to Xa(F − kFs) in the interval −Fs/2 to Fs/2. Consequently,
∞∑

k=−∞

∫ (k+1/2)Fs
(k−1/2)Fs

Xa(F )e
j2πnF/Fs dF =

∞∑
k=−∞

∫ Fs/2
−Fs/2

Xa(F − kFs)ej2πnF/Fs dF

=
∫ Fs/2
−Fs/2

[ ∞∑
k=−∞

Xa(F − kFs)
]
ej2πnF/Fs dF

where we have used the periodicity of the complex exponential, namely,

ej2πn(F+kFs)/Fs = ej2πnF/Fs

(1.9)

(1.10)

(1.11)

(1.12)

(1.13)

(1.8)

Sampling and Reconstruction of Signals

Indeed, substitution of (1.7) into (1.3) yields

If we compare (1.6) with (1.8), we conclude that

With the aid of (1.10), we can make a simple change in variable in (1.9), and

We now turn our attention to the integral on the right-hand side of (1.11). The

Periodic  sampling  imposes  a  relationship between the  frequency  variables  and  
of the  corresponding  analog and  discrete-time  signals, respectively.  That is,

F

f

398



X(F) = Fs
∞∑

k=−∞
Xa(F − kFs)

or, equivalently,

X(f ) = Fs
∞∑

k=−∞
Xa[(f − k)Fs]

This is the desired relationship between the spectrum X(F) or X(f ) of the
discrete-time signal and the spectrum Xa(F ) of the analog signal. The right-hand

FsXa(F ) with period Fs . This periodicity is necessary because the spectrum X(f )
of the discrete-time signal is periodic with period fp = 1 or Fp = Fs .

For example, suppose that the spectrum of a band-limited analog signal is as
The spectrum is zero for |F | ≥ B . Now, if the sampling fre

quency Fs is selected to be greater than 2B , the spectrum X(Fs) of the discrete-
s

is selected such that Fs ≥ 2B , where 2B is the Nyquist rate, then
X(F) = FsXa(F ), |F | ≤ Fs/2

In this case there is no aliasing and therefore the spectrum of the discrete-time signal
is identical (within the scale factor Fs ) to the spectrum of the analog signal, within
the fundamental frequency range |F | ≤ Fs/2 or |f | ≤ 12 .

On the other hand, if the sampling frequency Fs is selected such that Fs <
2B , the periodic continuation of Xa(F ) results in spectral overlap, as illustrated in

Thus the spectrum X(F) of the discrete-time signal contains
aliased frequency components of the analog signal spectrum Xa(F ). The end result
is that the aliasing which occurs prevents us from recovering the original signal xa(t)
from the samples.

Given the discrete-time signal x(n) with the spectrum X(F), as illustrated in

signal from the samples x(n). Since in the absence of aliasing

Xa(F ) =
{ 1
Fs
X(F), |F | ≤ Fs/2

0, |F | > Fs/2

X(F) =
∞∑

n=−∞
x(n)e−j2πFn/Fs

the inverse Fourier transform of Xa(F ) is

xa(t) =
∫ Fs/2
−Fs/2

Xa(F )e
j2πF t dF

(1.14)

(1.15)

(1.16)

(1.17)

(1.18)

(1.19)

Sampling and Reconstruction of Signals

Comparing (1.13), (1.12), and (1.11), we conclude that

side of (1.14) or (1.15) consists of a periodic repetition of the scaled spectrum

shown in Fig. 1.1(a).

time signal will appear as shown in Fig. 1.1(b). Thus, if the sampling frequency F

Fig. 1.1(c) and (d).

Fig. 1 .1 (b), with no aliasing, it is now possible to reconstruct the original analog

and by the Fourier transform relationship (1.5),

-

399



xa(t)

t
0

Xa(F)

F
0 B−B

1

xa(t)

t
0

n
0

x(n) = xa(nT )

|T |

n
0

x(n)

|  T  |

n
0

x(n)

ˆ ˆ

X(F)

X(F)

X(F)

F
0 Fs

2

Fs−Fs

Fs
Fs Xa(F + Fs)

Fs Xa(F − Fs)Fs Xa(F)

F
0 Fs

2

Fs−Fs

Fs

F
0 Fs−Fs

Xa(F)

F
0 Fs/2−Fs/2

(a)

(b)

(c)

(d)

(e)

|  T  |

Sampling of an analog bandlimited signal and aliasing of spec-
tral components.

Let us assume that Fs
have

xa(t) = 1
Fs

∫ Fs/2
−Fs/2

[ ∞∑
n=−∞

x(n)e−j2πFn/Fs
]
ej2πF t dF

= 1
Fs

∞∑
n=−∞

x(n)

∫ Fs/2
−Fs/2

ej2πF(t−n/Fs ) dF

=
∞∑

n=−∞
xa(nT )

sin(π/T )(t − nT )
(π/T )(t − nT )

(1.20)

Figure 1.1

Sampling and Reconstruction of Signals

≥ 2B . With th e substitution of (1.17) into (1.19), we

400



Reconstruction of a
continuous-time signal
using ideal interpolation.

Reconstructed signal

0−T T 2T 3T 4T

where x(n) = xa(nT ) and where T = 1/Fs is the sampling interval.

g(t) = sin(π/T )t
(π/T )t

appropriately shifted by nT , n = 0, ±1, ±2, . . . , and multiplied or weighted by
the corresponding samples xa(nT ) of the signal.

a

interpolation function. We note that at t = kT , the interpolation function g(t − nT )
is zero except at k = n. Consequently, xa(t) evaluated at t = kT is simply the
sample xa(kT ). At all other times the weighted sums of the time-shifted versions
of the interpolation function combine to yield exactly xa(t). This combination is

a

is called the ideal interpolation formula. It forms the basis for the sampling theorem,
which can be stated as follows.

Sampling Theorem. A bandlimited continuous-time signal, with highest frequency
(bandwidth) B hertz, can be uniquely recovered from its samples provided that the
sampling rate Fs ≥ 2B samples per second.

the recovery of xa(t) from its samples x(n) requires an infinite number of samples.
However, in practice we use a finite number of samples of the signal and deal with
finite-duration signals. As a consequence, we are concerned only with reconstructing
a finite-duration signal from a finite number of samples.

When aliasing occurs due to too low a sampling rate, the effect can be described
by a multiple folding of the frequency axis of the frequency variable F for the analog
signal. a According

s results in a periodic
repetition ofXa(F ) with period Fs . If Fs < 2B , the shifted replicas ofXa(F ) overlap.
The overlap that occurs within the fundamental frequency range −Fs/2 ≤ F ≤ Fs/2

within the fundamental frequency range is obtained by adding all the shifted portions
within the range |f | ≤ 12

(1.21)

Figure 1.2

Sampling and Reconstruction of Signals

The reconstruction formula in (1.20) involves the function

We call (1.20) an interpolation
formula for reconstructing x (t) from its samples, and g(t), given in (1.21), is the

illustrated in Fig. 1.2.
The formula in (1.20) for reconstructing the analog signal x (t) from its samples

According to the sampling theorem and the reconstruction formula in (1.20),

Figure 1.3(a) shows the spectrum X (F) of an analog signal.
to (1.14), sampling of the signal with a sampling frequency F

is illustrated in Fig. 1.3(b). The corresponding spectrum of the discrete-time signal

, to yield the spectrum shown in Fig. 1.3(c).

This is a
reconstruction formula fro m the sampling theorem.

401



0

Xa(F)

B
F

−B
(a)

0
F

−
(c)

0 Fs 2Fs−Fs−2Fs

X

Xa(F − Fs)

F

(b)

F
Fs

Fs
2

Fs
2

1
T

1
T

Illustration of aliasing around the folding frequency.

−

−

Fs
2

Fs Fs
F

f

−Fs
2

2

0

1

2
1

Relationship between frequency variables F and f .

Figure 1.3

Figure 1.4

Sampling and Reconstruction of Signals

A careful inspection of Fig. 1.3(a) and (b) reveals that the aliased spectrum in 
Fig. 1.3 (c) can be obtained by folding the original spectrum like an accordion with 
pleats at every odd multiple of  Consequently, the frequency  is called the 
folding frequency. Clearly, then, periodic sampling automatically forces a folding of 
the frequency axis of an analog signal at odd multiples of , and this results in 
the relationship  between the frequencies for continuous-time signals and 
discrete-time signals. Due to the folding of the frequency axis, the relationship 

 is not truly linear, but piecewise linear, to accommodate for the aliasing effect. 
This relationship is illustrated in Fig. 1.4.

Fs/2. Fs/2

F = fFs
Fs/2

 
 

F =
fFs

402



Xa(F )xa(t)

Fourier transform
pair

Reconstruction:

Sampling:
x(n) = xa(nT)

Xa(F) = ∫ xa(t)e−j2πFt dt−

Xa(F) = TX    

|F | <

xa(t) = ∫ Xa(F)ej2πFt dF−

X( f )x(n)
Fourier transform

pair

X( f ) =  Σ x(n)e−j2πfn
n = − 

X( f) = Fs    Σ Xa[( f − k)Fs]
k = −

xa(t) = Σ x(n)
n −

x(n) = ∫ X( f )ej2πfn df

sin π(t − nT)/T
π(t − nT)/T

Fs
2

A
/D

D
/A

A
/D

D
/A

1 2/
−1 2/

Time-domain and frequency-domain relationships for sampled
signals.

If the analog signal is bandlimited to B ≤ Fs/2, the relationship between f and F
is linear and one-to-one. In other words, there is no aliasing. In practice, prefiltering
with an antialiasing filter is usually employed prior to sampling. This ensures that
frequency components of the signal above F ≥ B are sufficiently attenuated so that,
if aliased, they cause negligible distortion on the desired signal.

The relationships among the time-domain and frequency-domain functions xa(t),
a

ing the continuous-time functions, xa(t) and Xa(F ), from the discrete-time quantities
x(n) and X(f ), assume that the analog signal is bandlimited and that it is sampled
at the Nyquist rate (or faster).

The following examples serve to illustrate the problem of the aliasing of fre-
quency components.

Aliasing in Sinusoidal Signals

The continuous-time signal

xa(t) = cos 2πF0t = 12 e
j2πF0t + 1

2
e−j2πF0t

0

Figure 1.5

EXAMPLE 1.1

Sampling and Reconstruction of Signals

x(n), X (F), and X(f ) are summarized in Fig. 1.5. The relationships for recover-

has a discrete spectrum with spectral lines at F = ±F , as shown in Fig. 1.6(a). The process

(F )

403



0

0

Spectrum

(a)
−F0 F0

F

1
2

Spectrum

(b)
F0 − Fs−Fs Fs − F0 Fs

F

1
2T

…

…

…

…

0

Spectrum

(d)

F0 − Fs F0 + FsFs − F0

Fs − F0 F0 − Fs

Fs
F

−F0 − Fs −Fs

Spectrum

(c)

Fs − F0−(Fs − F0)
F

Fs

1
2

2

0

Spectrum

(e)

Fs
F

Fs

1
2

22
−

Fs
2

−

1
2T

Aliasing of sinusoidal signals.

of sampling this signal with a sampling frequency Fs introduces replicas of the spectrum about
s s 0 s

To reconstruct the continuous-time signal, we should select the frequency components
inside the fundamental frequency range |F | ≤ Fs/2. The resulting spectrum is shown in

x̂a(t) = cos 2π(Fs − F0)t

Figure 1.6

Sampling and Reconstruction of Signals

multiples of F . This is illustrated in Fig. 1.6(b) for F /2 < F < F .

Fig. 1.6(c). The reconstructed signal is

404



Now, if Fs is selected such that Fs < F0 < 3Fs/2, the spectrum of the sampled signal is

xa(t) = cos 2π(F0 − Fs)t
In both cases, aliasing has occurred, so that the frequency of the reconstructed signal is an
aliased version of the frequency of the original signal.

Sampling and Reconstruction of a Nonbandlimited Signal

Consider the following continuous-time two-sided exponential signal:

xa(t) = e−A|t | F←→ Xa(F ) = 2A
A2 + (2πF)2 , A > 0

(a) Determine the spectrum of the sampled signal x(n) = xa(nT ). (b) Plot the signals xa(t) and
x(n) = xa(nT ), for T = 1/3 sec and T = 1 sec, and their spectra. (c) Plot the continuous-time
signal x̂a(t) after reconstruction with ideal bandlimited interpolation.

Solution.
(a) If we sample xa(nT ) with a sampling frequency Fs = 1/T , we have

x(n) = xa(nT ) = e−AT |n| = (e−AT )|n|, −∞ < n <∞
The spectrum of x(n) can be found easily if we use a direct computation of the discrete-
time Fourier transform. We find that

X(F) = 1− a
2

1− 2a cos 2π(F/Fs)+ a2 , a = e
−AT

−5 0 5
0

0.5

1

−4 −2 0 2 4
0

1

2xa(t) Xa(F)

F (Hz)t (sec)
(a)

(b)

(c)
−5 0 5
0

0.5

1

−4 −2 0 2 4
0

1

2

F (Hz)t (sec)

x(n) = xa(nt) T = 1 sec

−5 0 5
0

0.5

1

−4 −2 0 2 4
0

1

2

Fs = 3 Hz

Fs = 1 Hz

Xa(F)X(F)

F (Hz)t (sec)

x(n) = xa(nt) T =    sec3
1

Xa(F)

X(F)

(a) Analog signal xa(t) and its spectrum Xa(F ); (b) x(n) =
xa(nT ) and its spectrum for Fs = 3 Hz; and (c) x(n) = xa(nT ) and its
spectrum for Fs = 1 Hz.

EXAMPLE 1.2

Figure 1.7

Sampling and Reconstruction of Signals

shown in Fig. 1.6(d). The reconstructed signal, shown in Fig. 1.6(e), is

405



Clearly, since cos 2π(F/Fs) is periodic with period Fs , so is the spectrum X(F).

(b) Since Xa (F ) is not bandlimited, aliasing cannot be avoided.
original signal xa(t) and its spectrum Xa(F ) for A = 1. The sampled signal x(n) and its
spectrum X(F) are shown for Fs = 3 Hz and Fs
The aliasing distortion is clearly noticeable in the frequency domain when Fs = 1 Hz and
almost unnoticeable when Fs = 3 Hz.

(c) The spectrum X̂a(F ) of the reconstructed signal x̂a(t) is given by

X̂a(F ) =
{
TX(F), |F | ≤ Fs/2
0, otherwise

The values of x̂a(t) can be evaluated for plotting purposes using the ideal bandlimited

s = 3 Hz and Fs = 1
Hz. It is interesting to note that in every case x̂a(nT ) = xa(nt), but x̂a(nt) �= xa(nT ) for
t �= nT . The results of aliasing are clearly evident in the spectrum of x̂a(t) for Fs = 1 Hz,
where we note how the folding of the spectrum about F = ±0.5 Hz increases the high
frequency content of X̂a(F ).

-5 0 5
0

0.5

1

-4 -2 0 2 4
0

1

2xa(t) Xa(F)

(a)

(b)

(c)

-5 0 5
0

0.5

1

-4 -2 0 2 4
0

1

2 X(F)Fs = 3 Hz Xa(F)
T =    sec

3
1 xa(t)

^

-5 0 5
0

0.5

1

-4 -2 0 2 4
0

1

2

Xa(F)

T = 1 sec

Fs = 1 Hz

X(F)xa(t)
^

Xa(F)
^

(a) Analog signal xa(t) and its spectrum Xa(F ); (b) reconstructed signal x̂a(t) and
its spectrum for Fs = 3 Hz; and (c) reconstructed signal x̂a(t) and its spectrum for Fs = 1 Hz
Figure 1.8

Sampling and Reconstruction of Signals

Figure 1.7(a) shows the

= 1 Hz in Figures 1.7(b) and 1.7(c).

interpolation formula (1.20) for all significant values of x(n) and sin(πt/T)/(π t/T ).
Figure 1.8 illustrates the reconstructed signal and its spectrum for F

406



2 Discrete-Time Processing of Continuous-Time Signals

Many practical applications require the discrete-time processing of continuous-time
signals.
this objective. In designing the processing to be performed, we must first select the
bandwidth of the signal to be processed, since the signal bandwidth determines the
minimum sampling rate. For example, a speech signal, which is to be transmitted dig-
itally, can contain frequency components above 3000 Hz, but for purposes of speech
intelligibility and speaker identification, the preservation of frequency components
bellow 3000 Hz is sufficient. Consequently, it would be inefficient from a process-
ing viewpoint to preserve the higher-frequency components and wasteful of channel
bandwidth to transmit the extra bits needed to represent these higher-frequency
components in the speech signal. Once the desired frequency band is selected we
can specify the sampling rate and the characteristics of the prefilter, which is also
called an antialiasing filter.

The prefilter is an analog filter which has a twofold purpose. First, it ensures that
the bandwidth of the signal to be sampled is limited to the desired frequency range.

s

so that the amount of signal distortion due to aliasing is negligible. For example, the
speech signal to be transmitted digitally over a telephone channel would be filtered
by a lowpass filter having a passband extending to 3000 Hz, a transition band of
approximately 400 to 500 Hz, and a stopband above 3400 to 3500 Hz. The speech
signal may be sampled at 8000 Hz and hence the folding frequency would be 4000
Hz. Thus aliasing would be negligible. Another reason for using a prefilter is to limit
the additive noise spectrum and other interference, which often corrupts the desired
signal. Usually, additive noise is wideband and exceeds the bandwidth of the desired
signal. By prefiltering we reduce the additive noise power to that which falls within
the bandwidth of the desired signal and we reject the out-of-band noise.

Once we have specified the prefilter requirements and have selected the desired
sampling rate, we can proceed with the design of the digital signal processing opera-
tions to be performed on the discrete-time signal. The selection of the sampling rate
Fs = 1/T , where T is the sampling interval, not only determines the highest fre-
quency (Fs/2) that is preserved in the analog signal, but also serves as a scale factor
that influences the design specifications for digital filters and any other discrete-time
systems through which the signal is processed.

The ideal A/D converter and the ideal D/A converter provide the interface be-
tween the continuous-time and discrete-time domains. The overall system is equiva-
lent to a continuous-time system, which may or may not be linear and time-invariant

Discrete-time
system

Ideal
A/D

Analog

signal
Ideal
D/A

Fs Fs

xa(t)
Prefilter

ya(t)x(n) y(n)

System for the discrete-time processing of continuous-time signals.Figure 2.1

Sampling and Reconstruction of Signals

Figure 2.1 shows the configuration of a general system used to achieve

Thus any frequency components of the signal above F /2 are sufficiently attenuated

407



x(n)xa(t)

xa(t)

Xa(F)

−Fs Fs

x(n)     xa(nT)

     1 0 1 2 3 4 5 T n0 t

( )X F

sF ( )X F1/T

F0

( )aX F1

0−B B F

Ideal
A/D

Characteristics of an ideal A/D converter in the time
and frequency domains.

(even if the discrete-time system is linear and time invariant) because ideal A/D and
D/A converters are time-varying operations.

verter in the time and frequency domains. We recall that if xa(t) is the input signal
and x(n) is the output signal, we have

x(n) = xa(t)|t=nT = xa(nT ) (Time domain)

X(F ) = 1
T

∞∑
k=−∞

Xa(F − kFs) (Frequency domain)

Basically, the ideal A/D converter is a linear time-varying system that (a) scales the
analog spectrum by a factor Fs = 1/T and (b) creates a periodic repetition of the
scaled spectrum with period Fs .

The input–output characteristics of the ideal D/A converter are illustrated in

ya(t) =
∞∑

n=−∞
y(n)ga(t − nT ), (Time domain)

( )y n

     1 0 1 2 3 4 5 T n 0

( )ay t

t

( )

( )

y n

Y F

( )

( )

a

a

y t

Y F

Ideal
D/A

sF
( )Y F1

sF0 FsF

( )aY FT

0 F

Characteristics of an ideal D/A converter in the time
and frequency domains.

(2.1)

(2.2)

(2.3)

Figure 2.2

Figure 2.3

Sampling and Reconstruction of Signals

Figure 2.2 summarizes the input-output characteristics of an ideal A/D con-

Figure 2.3. In the time domain the input and output signals are related by

408



where

ga(t) = sin(πt/T )
πt/T

F←→ Ga(F ) =
{
T , |F | ≤ Fs/2
0, otherwise

but it is not, a convolution because the D/A is a linear time-varying system with a
discrete-time input and a continuous-time output. To obtain a frequency-domain
description, we evaluate the Fourier transform of the output signal

Ya(F ) =
∞∑

n=−∞
y(n)

∫ ∞
−∞

ga(t − nT )e−j2πF t dt

=
∞∑

n=−∞
y(n)Ga(F )e

−j2πFnT

After taking Ga(F ) outside the summation, we obtain

Ya(F ) = Ga(F )Y (F ) (Frequency domain)

where Y (F ) is the discrete-time Fourier transform of y(n). We note that the ideal
D/A converter (a) scales the input spectrum by a factor T = 1/Fs and (b) removes
the frequency components for |F | > Fs/2. Basically, the ideal D/A acts as a fre-
quency window that “removes” the discrete-time spectral periodicity to generate an
aperiodic continuous-time signal spectrum. We stress once again that the ideal D/A

convolution integral.
Suppose now that we are given a continuous-time LTI system defined by

ỹa(t) =
∫ ∞
−∞

ha(τ )xa(t − τ) dt

Ỹa(F ) = Ha(F )Xa(F )

and we wish to determine whether there is a discrete-time system H(F) such that
a

If xa(t) is not bandlimited or it is bandlimited but Fs < 2B , it is impossible to find
such a system due to the presence of aliasing. However, if xa(t) is bandlimited and
Fs > 2B , we have X(F) = Xa(F )/T for |F | ≤ Fs/2. Therefore, the output of the

Ya(F ) = H(F)X(F)Ga(F ) =
{
H(F)Xa(F ), |F | ≤ Fs/2
0, |F | > Fs/2

To assure that ya(t) = ŷa(t), we should choose the discrete-time system so that

H(F) =
{
Ha(F ), |F | ≤ Fs/2
0, |F | > Fs/2

(2.4)

(2.5)

(2.6)

(2.7)

(2.8)

(2.9)

Sampling and Reconstruction of Signals

is the interpolation function of the i deal D/A. We emphasize that (2.3) looks like,

is not a continuous-time ideal lowpass filter because (2.3) is not a continuous-time

the entire system i n Figure 2.1 is equivalent to the continuous-time system H (F).

system in Figure 2.1 is given by

409



Simulation of an analog integrator

Its input–output relation is
given by

RC
dya(t)

dt
+ ya(t) = xa(t)

Taking the Fourier transform of both sides, we can show that the frequency response of the
integrator is

Ha(F ) = Ya(F )
Xa(F )

= 1
1+ jF/Fc , Fc =

1
2πRC

Evaluating the inverse Fourier transform yields the impulse response

ha(t) = Ae−Atu(t), A = 1
RC

Clearly the impulse response ha(t) is a nonbandlimited signal. We now define a discrete-time
system by sampling the continuous-time impulse response as follows:

h(n) = ha(nT ) = A(e−AT )nu(n)

H(z) =
∞∑
n=0

A(e−AT )nz−n = 1
1− e−AT z−1

y(n) = e−AT y(n− 1)+ Ax(n)

The system is causal and has a pole p = e−AT . Since A > 0, |p| < 1 and the system is always
stable. The frequency response of the system is obtained by evaluating H(z) for z = ej2πF/Fs .

discrete-time simulator for Fs =50, 100, 200, and 1000 Hz. We note that the effects of aliasing,
caused by the sampling of ha(t), become negligible only for sampling frequencies larger than
1 kHz. The discrete-time implementation is accurate for input signals with bandwidth much
less than the sampling frequency.

EXAMPLE 2.1

Sampling and Reconstruction of Signals

Consider the analog integrator circuit shown in Figure 2.4(a).

Figure 2.4(b) shows the magnitude frequency responses of the analog integrator and the

We note that, in this special case, the cascade connection of the A/D converter  

converter (linear time-varying system) is equivalent to a continuous-timeLTI  
system. This important result provides the theoretical basis for the discrete-time 
filtering of continuous-time signals. These concepts are illustrated in the following 
examples.

(linear time-varying system), an LTI (linear time-invariant) system, and the D/A 

We say that the discrete-time system is obtained from the continuous-time system through 

discrete-time system are
transformationan impulse-invariance . The system function and the difference equation of the 

410



1

0.8

0.6

0.4

0.2

Magnitude

Frequency  F (Hz)
0 20 40 60 80 100 120 140 160 180 200

( )aH F

KHz1sF

Hz50sF

Hz200sF

Hz100sF

( )x n ( )y n

_1z
eAT

A

( )h n

0 tT

( )ah t

R

C( )ax t ( )ay t

=

=
=

=

Discrete-time implementation of an analog integrator using
impulse response sampling. The approximation is satisfactory when the
bandwidth of the input signal is much less than the sampling frequency.

Ideal bandlimited differentiator

The ideal continuous-time differentiator is defined by

ya(t) = dxa(t)
dt

and has frequency response function

Ha(F ) = Ya(F )
Xa(F )

= j2πF

For processing bandlimited signals, it is sufficient to use the ideal bandlimited differentiator
defined by

Ha(F ) =
{
j2πF, |F | ≤ Fc
0, |F | > Fc

If we choose Fs = 2Fc , we can define an ideal discrete time differentiator by

H(F) = Ha(F ) = j2πF, |F | ≤ Fs/2

(2.10)

(2.11)

(2.12)

(2.13)

Figure 2.4

EXAMPLE 2.2

Sampling and Reconstruction of Signals

411



Since by definitionH(F) =∑k Ha(F−kFs), we have h(n) = ha(nT ) . In terms of ω = 2πF/Fs ,
H(ω) is periodic with period 2π . Therefore, the discrete-time impulse response is given by

h(n) = 1
2π

∫ π
−π
H(ω)ejωndω = πn cosπn− sin πn

πn2T

or in a more compact form

h(n) =
{

0, n = 0
cosπn
nT

, n �= 0
The magnitude and phase responses of the continuous-time and discrete-time ideal differen-

|Ha(F)|

�Ha(F)

�H( )

|H( )|

F

F

−π

−π

π

π

ω

ω

−2π

−2π

2π

2π

2
−

π
2

π
2

π
2

−

π
2

−

Fs

2
Fs

2
Fs

−
2
Fs

(b)

(a)

Frequency responses of the ideal bandlimited continuous-
time differentiator (a) and its discrete-time counterpart (b).

ω

ω

Figure 2.5

Sampling and Reconstruction of Signals

(2.14)

(2.15)

tiators are shown in Figure 2.5.

412



Fractional delay
A continuous-time delay system is defined by

ya(t) = xa(t − td )
for any td > 0. Although the concept is simple, its practical implementation is quite compli-
cated. If xa(t) is bandlimited and sampled at the Nyquist rate, we obtain

y(n) = ya(nT ) = xa(nT − td ) = xa[(n−�)T ] = x(n−�)
where � = td/T . If � is an integer, delaying the sequence x(n) is a simple process. For
noninteger values of �, the delayed value of x(n) would lie somewhere between two samples.
However, this value is unavailable and the only way to generate an appropriate value is by
ideal bandlimited interpolation. One way to approach this problem is by considering the
frequency response

Hid(ω) = e−jω�

hid(n) = 12π
∫ π
−π
H(ω)ejωndω = sinπ(n−�)

π(n−�)
When the delay � assumes integer values, hid(n) reduces to δ(n−�) because the sin function
is sampled at the zero crossings. When � is noninteger, hid(n) is infinitely long because the
sampling times fall between the zero crossings. Unfortunately, the ideal impulse response
for fractional delay systems is noncausal and has infinite duration. Therefore, the frequency

More details
about fractional delay filter design can be found in Laakso et al. (1996).

3 Analog-to-Digital and Digital-to-Analog Converters

In the previous section we assumed that the A/D and D/A converters in the processing
of continuous-time signals are ideal. The one implicit assumption that we have made
in the discussion on the equivalence of continuous-time and discrete-time signal
processing is that the quantization error in analog-to-digital conversion and round-off
errors in digital signal processing are negligible. These issues are further discussed in
this section. However, we should emphasize that analog signal processing operations
cannot be done very precisely either, since electronic components in analog systems
have tolerances and they introduce noise during their operation. In general, a digital
system designer has better control of tolerances in a digital signal processing system
than an analog system designer who is designing an equivalent analog system.

nals to discrete-time signals using an ideal sampler and ideal interpolation. In this
section we deal with the devices for performing these conversions from analog to
digital.

3.1 Analog-to-Digital Converters

Recall that the process of converting a continuous-time (analog) signal to a digital
sequence that can be processed by a digital system requires that we quantize the
sampled values to a finite number of levels and represent each level by a number
of bits.

(2.16)

(2.17)

(2.18)

(2.19)

EXAMPLE 2.3

Sampling and Reconstruction of Signals

of the delay system in (2.17) and its impulse response

response (2.18) has to be approximated with realizable FIR or IIR filters.

The discussion in Section 1 focused on the conversion of continuous-time sig-

413



Analog
preamp

Sample-
hold

S/H
control

Convert
command

Status

A/D
converter

Buffer
or bus

To computer or
communication
channel

H H H H H

H

S S S S S S

Input

Tracking
in "sample"

Holding

S/H output

(a)

(b)

(a) Block diagram of basic elements of an A/D con-
verter; (b) time-domain response of an ideal S/H circuit.

In this section we consider the performance requirements for these elements. Al-
though we focus mainly on ideal system characteristics, we shall also mention some
key imperfections encountered in practical devices and indicate how they affect the
performance of the converter. We concentrate on those aspects that are more rele-
vant to signal processing applications. The practical aspects of A/D converters and
related circuitry can be found in the manufacturers’ specifications and data sheets.

In practice, the sampling of an analog signal is performed by a sample-and-hold
(S/H) circuit. The sampled signal is then quantized and converted to digital form.
Usually, the S/H is integrated into the A/D converter. The S/H is a digitally controlled
analog circuit that tracks the analog input signal during the sample mode, and then
holds it fixed during the hold mode to the instantaneous value of the signal at the

shows the time-domain response of an ideal S/H circuit (i.e., an S/H that responds
instantaneously and accurately).

The goal of the S/H is to continuously sample the input signal and then to hold
that value constant as long as it takes for the A/D converter to obtain its digital
representation. The use of an S/H allows the A/D converter to operate more slowly
compared to the time actually used to acquire the sample. In the absence of an S/H,
the input signal must not change by more than one-half of the quantization step
during the conversion, which may be an impractical constraint. Consequently, the
S/H is crucial in high-resolution (12 bits per sample or higher) digital conversion of
signals that have large bandwidths (i.e., they change very rapidly).

Figure 3.1

Figure 3.1(a) shows a block diagram of the basic elements of an A/D converter.

Sampling and Reconstruction of Signals

time the system is switched from the sample mode to the hold mode. Figure 3.1(b)

414



An ideal S/H introduces no distortion in the conversion process and is accurately
modeled as an ideal sampler. However, time-related degradations such as errors in
the periodicity of the sampling process (“jitter”), nonlinear variations in the dura-
tion of the sampling aperture, and changes in the voltage held during conversion
(“droop”) do occur in practical devices.

The A/D converter begins the conversion after it receives a convert command.
The time required to complete the conversion should be less than the duration of the
hold mode of the S/H. Furthermore, the sampling period T should be larger than
the duration of the sample mode and the hold mode.

In the following sections we assume that the S/H introduces negligible errors
and we focus on the digital conversion of the analog samples.

3.2 Quantization and Coding

The basic task of the A/D converter is to convert a continuous range of input ampli-
tudes into a discrete set of digital code words. This conversion involves the processes
of quantization and coding. Quantization is a nonlinear and noninvertible process
that maps a given amplitude x(n) ≡ x(nT ) at time t = nT into an amplitude xk ,

the signal amplitude range is divided into L intervals

Ik = {xk < x(n) ≤ xk+1}, k = 1, 2, . . . , L

by the L+ 1 decision levels x1 , x2, . . . , xL+1 . The possible outputs of the quantizer
(i.e., the quantization levels) are denoted as x̂1 , x̂2, . . . , x̂L . The operation of the
quantizer is defined by the relation

xq(n) ≡ Q[x(n)] = x̂k, if x(n) ∈ Ik

…
…

…
x3 x3 x4 x4 x5 xk xk xk + 1

x1 = −∞ x9 = ∞x1 x2 x2 x3 x3 x4 x4 x5 x5 x6 x6 x7 x7 x8 x8

−4∆ −3∆ −2∆ −∆ ∆ 2∆ 3∆0

Quantization
levels

Decision
levels

Ik

Instantaneous amplitude

Instantaneous amplitude

Range of quantizer

(a)

(b)

^ ^ ^ ^ ^ ^ ^ ^

^^^

Quantization process and an example of a midtread
quantizer.

(3.1)

(3.2)

Figure 3.2

Sampling and Reconstruction of Signals

taken from a finite set of values. The procedure is illustrated in Fig. 3.2(a), where

415



of n (i.e., the quantization is memoryless and is simply denoted as xq = Q[x]). Fur-
thermore, in signal processing we often use uniform or linear quantizers defined by

x̂k+1 − x̂k = �, k = 1, 2, . . . , L− 1
xk+1 − xk = �, for finite xk, xk+1

where � is the quantizer step size. Uniform quantization is usually a requirement
if the resulting digital signal is to be processed by a digital system. However, in
transmission and storage applications of signals such as speech, nonlinear and time-
variant quantizers are frequently used.

If zero is assigned a quantization level, the quantizer is of the midtread type. If

illustrates a midtread quantizer with L = 8 levels. In theory, the extreme decision
levels are taken as x1 = −∞ and xL+1 = ∞, to cover the total dynamic range of
the input signal. However, practical A/D converters can handle only a finite range.
Hence we define the range R of the quantizer by assuming that I1 = IL = �. For

the term full-scale range (FSR) is used to describe the range of an A/D converter for
bipolar signals (i.e., signals with both positive and negative amplitudes). The term
full scale (FS) is used for unipolar signals.

It can be easily seen that the quantization error eq(n) is always in the range−�/2
to �/2:

−�
2
< eq(n) ≤ �2

In other words, the instantaneous quantization error cannot exceed half of the quan-
tization step. If the dynamic range of the signal, defined as xmax− xmin , is larger than
the range of the quantizer, the samples that exceed the quantizer range are clipped,
resulting in a large (greater than �/2) quantization error.

The operation of the quantizer is better described by the quantization character-

levels. This characteristic is preferred in practice over the midriser because it pro-
vides an output that is insensitive to infinitesimal changes of the input signal about
zero. Note that the input amplitudes of a midtread quantizer are rounded to the
nearest quantization levels.

The coding process in an A/D converter assigns a unique binary number to each
quantization level. If we have L levels, we need at least L different binary numbers.
With a word length of b + 1 bits we can represent 2b+1 distinct binary numbers.
Hence we should have 2b+1 ≥ L or, equivalently, b+ 1 ≥ log2 L. Then the step size
or the resolution of the A/D converter is given by

� = R
2b+1

where R is the range of the quantizer.

(3.3)

(3.4)

(3.5)

Sampling and Reconstruction of Signals

In most digital signal processing operations the mapping in (3.2) is independent

zero is assigned a decision level, the quantizer is called a midrise type. Figure 3.2(b)

example, the range of the quantizer shown in Fig. 3.2(b) is equal to 8�. In practice,

istic function, illustrated in Fig. 3.3 for a midtread quantizer with eight quantization

416



x = Q[x]

−2∆

−3∆

−4∆

∆

2∆

3∆

−∆

Quantization
levels

Decision levels

Output

Input
9∆
2

− 7∆
2

− 5∆
2

− 3∆
2

−

∆
2

− ∆
2

3∆
2

5∆
2

7∆
2

9∆
2

x

Two's-complement
code words

011
010
001
000
111
110
101
100

Range R = FSR

(Peak-to-peak range)

^

Example of a midtread quantizer.

There are various binary coding schemes, each with its advantages and disad-

The two’s-complement representation is used in most digital signal processors.
Thus it is convenient to use the same system to represent digital signals because we
can operate on them directly without any extra format conversion. In general, a
(b + 1)-bit binary fraction of the form β0β1β2 · · ·βb has the value

−β0 · 20 + β1 · 2−1 + β2 · 2−2 + · · · + βb · 2−b

if we use the two’s-complement representation. Note that β0 is the most significant
bit (MSB) and βb is the least significant bit (LSB). Although the binary code used
to represent the quantization levels is important for the design of the A/D converter
and the subsequent numerical computations, it does not have any effect in the per-
formance of the quantization process. Thus in our subsequent discussions we ignore
the process of coding when we analyze the performance of A/D converters.

The only degradation introduced by an ideal converter is the quantization error,
which can be reduced by increasing the number of bits. This error, which dominates
the performance of practical A/D converters, is analyzed in the next section.

Practical A/D converters differ from ideal converters in several ways. Various
degradations are usually encountered in practice. Specifically, practical A/D convert-
ers may have an offset error (the first transition may not occur at exactly + 12 LSB),

Figure 3.3

Sampling and Reconstruction of Signals

vantages. Table 1 illustrates some existing schemes for 3-bit binary coding.

417



Commonly Used Bipolar Codes

Decimal Fraction

Positive Negative Sign + Two’s Offset One’s
Number Reference Reference Magnitude Complement Binary Complement

+7 + 78 − 78 0 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1
+6 + 68 − 68 0 1 1 0 0 1 1 0 1 1 1 0 0 1 1 0
+5 + 58 − 58 0 1 0 1 0 1 0 1 1 1 0 1 0 1 0 1
+4 + 48 − 48 0 1 0 0 0 1 0 0 1 1 0 0 0 1 0 0
+3 + 38 − 38 0 0 1 1 0 0 1 1 1 0 1 1 0 0 1 1
+2 + 28 − 28 0 0 1 0 0 0 1 0 1 0 1 0 0 0 1 0
+1 + 18 − 18 0 0 0 1 0 0 0 1 1 0 0 1 0 0 0 1

0 0+ 0− 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0
0 0− 0+ 1 0 0 0 (0 0 0 0) (1 0 0 0) 1 1 1 1
−1 − 18 + 18 1 0 0 1 1 1 1 1 0 1 1 1 1 1 1 0
−2 − 28 + 28 1 0 1 0 1 1 1 0 0 1 1 0 1 1 0 1
−3 − 38 + 38 1 0 1 1 1 1 0 1 0 1 0 1 1 1 0 0
−4 − 48 + 48 1 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1
−5 − 58 + 58 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 0
−6 − 68 + 68 1 1 1 0 1 0 1 0 0 0 1 0 1 0 0 1
−7 − 78 + 78 1 1 1 1 1 0 0 1 0 0 0 1 1 0 0 0
−8 − 88 + 88 (1 0 0 0) (0 0 0 0)

scale-factor (or gain) error (the difference between the values at which the first tran-
sition and the last transition occur is not equal to FS − 2LSB), and a linearity error
(the differences between transition values are not all equal or uniformly changing).
If the differential linearity error is large enough, it is possible for one or more code
words to be missed. Performance data on commercially available A/D converters
are specified in manufacturers’ data sheets.

3.3 Analysis of Quantization Errors

To determine the effects of quantization on the performance of an A/D converter,
we adopt a statistical approach. The dependence of the quantization error on the
characteristics of the input signal and the nonlinear nature of the quantizer make a
deterministic analysis intractable, except in very simple cases.

In the statistical approach, we assume that the quantization error is random in
nature. We model this error as noise that is added to the original (unquantized) signal.
If the input analog signal is within the range of the quantizer, the quantization error

TABLE 1

Sampling and Reconstruction of Signals

418



Mathematical model of
quantization noise.

+

Quantizer
Q[x(n)]

x(n)

x(n)

xq(n)

xq(n) = x(n) + eq(n)

eq(n)

(a) Actual system

(b) Mathematical model

eq(n) is bounded in magnitude [i.e., |eq(n)| < �/2], and the resulting error is called
granular noise. When the input falls outside the range of the quantizer (clipping),
eq(n) becomes unbounded and results in overload noise. This type of noise can
result in severe signal distortion when it occurs. Our only remedy is to scale the
input signal so that its dynamic range falls within the range of the quantizer. The
following analysis is based on the assumption that there is no overload noise.

q

To carry out the analysis, we make the following assumptions about the statistical
properties of eq(n):

1. The error eq(n) is uniformly distributed over the range −�/2 < eq(n) < �/2.
2. The error sequence {eq(n)} is a stationary white noise sequence. In other words,

the error eq(n) and the error eq(m) for m �= n are uncorrelated.
3. The error sequence {eq(n)} is uncorrelated with the signal sequence x(n).
4. The signal sequence x(n) is zero mean and stationary.

These assumptions do not hold, in general. However, they do hold when the quanti-
zation step size is small and the signal sequence x(n) traverses several quantization
levels between two successive samples.

Under these assumptions, the effect of the additive noise eq(n) on the desired
signal can be quantified by evaluating the signal-to-quantization-noise (power) ratio
(SQNR), which can be expressed on a logarithmic scale (in decibels or dB) as

SQNR = 10 log10
Px

Pn

where Px = σ 2x = E[x2(n)] is the signal power and Pn = σ 2e = E[e2q(n)] is the power
of the quantization noise.

If the quantization error is uniformly distributed in the range (−�/2,�/2) as

zation noise power) is

Pn = σ 2e =
∫ �/2
−�/2

e2p(e) de = 1
�

∫ �/2
−�/2

e2 de = �
2

12

(3.6)

(3.7)

Figure 3.4

Sampling and Reconstruction of Signals

The mathematical model for the quantization error e (n) is shown in Fig. 3.4.

shown in Fig. 3.5, the mean value of the error is zero and the variance (the quanti-

419



Probability density function
for the quantization error.

∆
2

− ∆
2

1
∆

0

p(e)

e

expression for the SQNR becomes

SQNR = 10 log Px
Pn
= 20 log σx

σe

= 6.02b + 16.81− 20 log R
σx

dB

of the input signal. For example, if we assume that x(n) is Gaussian distributed and
the range of the quantizer extends from −3σx to 3σx (i.e., R = 6σx ), then less than 3
out of every 1000 input signal amplitudes would result in an overload on the average.
For R = 6σx

SQNR 6.02b 1.25 dB

Due to limitations in the fabrication of A/D converters, their performance falls

may be somewhat less than the number of bits in the A/D converter. For instance, a
16-bit converter may have only an effective 14 bits of accuracy.

3.4 Digital-to-Analog Converters

In practice, D/A conversion is usually performed by combining a D/A converter
with a sample-and-hold (S/H) followed by a lowpass (smoothing) filter, as shown in

Digital
input
signal

Digital-
to-analog
converter

Sample
and
hold

Lowpass
smoothing

filter

Analog
output
signal

Basic operations in converting a digital signal into an analog signal.

(3.8)

(3.9)

Figure 3.5

Figure 3.6

Sampling and Reconstruction of Signals

By combining (3.5) with (3.7) and substituting the result

The last term in (3.8) depends on the range R of the A/D converter and the statistics

, (3.8) becomes

short of the theoretical value given by (3.8). As a result, the effective number of bits

Fig. 3.6. The D/A converter accepts, at its input, electrical signals that correspond

into (3.6), the

The formula in (3.8) is frequently used to specify the precision needed in an 
A/D converter. It simply means that each additional bit in the quantizer increases 
the signal-to-quantization-noise ratio by 6 dB. (It is interesting to note that the same 
result is derived for a sinusoidal signal using a deterministic approach.) However, 
we should bear in mind the conditions under which this result has been derived.

420



Ideal D/A converter
characteristic.

Analog output voltage
Ideal D/A

100 101 110 111 000 001 010 011 Input code words

−2∆

−3∆

−4∆

∆

2∆

3∆

−∆

to a binary word, and produces an output voltage or current that is proportional to
the value of the binary word. Ideally, its input–output characteristic is as shown in

The line connecting the dots is a straight line
through the origin. In practical D/A converters, the line connecting the dots may
deviate from the ideal. Some of the typical deviations from ideal are offset errors,
gain errors, and nonlinearities in the input–output characteristic.

An important parameter of a D/A converter is its settling time, which is defined
as the time required for the output of the D/A converter to reach and remain within
a given fraction (usually, ± 12 LSB) of the final value, after application of the input
code word. Often, the application of the input code word results in a high-amplitude
transient, called a “glitch.” This is especially the case when two consecutive code
words to the A/D differ by several bits. The usual way to remedy this problem is
to use an S/H circuit designed to serve as a “deglitcher.” Hence the basic task of
the S/H is to hold the output of the D/A converter constant at the previous output
value until the new sample at the output of the D/A reaches steady state. Then
it samples and holds the new value in the next sampling interval. Thus the S/H
approximates the analog signal by a series of rectangular pulses whose height is equal

an S/H to a discrete-time sinusoidal signal. As shown, the approximation, is basically
a staircase function which takes the signal sample from the D/A converter and holds
it for T seconds. When the next sample arrives, it jumps to the next value and holds
it for T seconds, and so on.

Response of an S/H
interpolator to a
discrete-time sinusoidal
signal.

0 20 40 60 80 100

−1

0

1

Analog 
signal

S/H input S/H output

t 

Figure 3.7

Figure 3.8

Sampling and Reconstruction of Signals

Fig. 3.7 for a 3-bit bipolar signal.

to the corresponding value of the signal pulse. Figure 3.8 illustrates the response of

421



Frequency responses of
sample-and-hold and
the ideal bandlimited
interpolator.

|GSH(F)|

|GBL(F)|
T

F0 1
2T

1
2T

1
T

1
T

− −

4 dB

The interpolation function of the S/H system is a square pulse defined by

gSH(t) =
{

1, 0 ≤ t ≤ T
0, otherwise

The frequency-domain characteristics are obtained by evaluating its Fourier trans-
form

GSH(F ) =
∫ ∞
−∞

gSH(t)e
−j2πF t dt = T sin πFT

πFT
e−2πF(T /2)

The magnitude of GSH
magnitude response of the ideal bandlimited interpolator for comparison purposes.
It is apparent that the S/H does not possess a sharp cutoff frequency characteristic.
This is due to a large extent to the sharp transitions of its interpolation function
gSH(t). As a consequence, the S/H passes undesirable aliased frequency components
(frequencies above Fs/2) to its output. This effect is sometimes referred to as post-
aliasing. To remedy this problem, it is common practice to filter the output of the S/H
by passing it through a lowpass filter, which highly attenuates frequency components
above Fs/2. In effect, the lowpass filter following the S/H smooths its output by
removing sharp discontinuities. Sometimes, the frequency response of the lowpass
filter is defined by

Ha(F ) =
{

πFT
sinπFT e

2πF(T /2), |F | ≤ Fs/2
0, |F | > Fs/2

to compensate for the sinx/x distortion of the S/H (aperture effect). The aperture
effect attenuation compensation, which reaches a maximum of 2/π or 4 dB at F =
Fs/2, is usually neglected. However, it may be introduced using a digital filter before
the sequence is applied to the D/A converter. The half-sample delay introduced by
the S/H cannot be compensated because we cannot design analog filters that can
introduce a time advance.

4 Sampling and Reconstruction of Continuous-Time Bandpass Signals

A continuous-time bandpass signal with bandwidth B and center frequency Fc has its
frequency content in the two frequency bands defined by 0 < FL < |F | < FH , where
Fc = (FL+FH

(3.10)

(3.11)

(3.12)

Figure 3.9

Sampling and Reconstruction of Signals

(F ) is shown in Figure 3.9, where we superimpose the

)/2 (see Figure 4.1(a)). A naive application of the sampling theorem

422



would suggest a sampling rate Fs ≥ 2FH ; however, as we show in this section, there
are sampling techniques that allow sampling rates consistent with the bandwidth B ,
rather than the highest frequency, FH , of the signal spectrum. Sampling of bandpass
signals is of great interest in the areas of digital communications, radar, and sonar
systems.

Uniform or First-Order Sampling

Uniform or first-order sampling is the typical periodic sampling introduced in Sec-
s = 1/T produces

a sequence x(n) = xa(nT ) with spectrum

X(F) = 1
T

∞∑
k=−∞

Xa(F − kFs)

The positioning of the shifted replicas X(F − kFs) is controlled by a single param-
eter, the sampling frequency Fs . Since bandpass signals have two spectral bands, in
general, it is more complicated to control their positioning, in order to avoid aliasing,
with the single parameter Fs .

Integer Band Positioning. We initially restrict the higher frequency of the band to be
an integer multiple of the bandwidth, that is, FH = mB (integer band positioning).
The numberm = FH/B , which is in general fractional, is known as the band position.

positioned bandpass signals, choosing Fs = 2B results in a sequence with a spectrum

be reconstructed using the reconstruction formula

xa(t) =
∞∑

n=−∞
xa(nT )ga(t − nT )

where

ga(t) = sinπBt
πBt

cos 2πFct

is the inverse Fourier transform of the bandpass frequency gating function shown in
We note that ga(t) is equal to the ideal interpolation function for

c

It is worth noticing that, by properly choosing the center frequency Fc of Ga(F ),
we can reconstruct a continuous-time bandpass signal with spectral bands centered
at Fc = ±(kB + B/2), k = 0, 1, . . .. For k = 0 we obtain the equivalent baseband
signal, a process known as down-conversion.
demonstrates that the baseband spectrum for m = 3 has the same spectral structure
as the original spectrum; however, the baseband spectrum for m = 4 has been
“inverted.” In general, when the band position is an even integer the baseband
spectral images are inverted versions of the original ones. Distinguishing between
these two cases is important in communications applications.

(4.1)

(4.2)

(4.3)

4.1

Sampling and Reconstruction of Signals

tion 1. Sampling the bandpass signal in Figure 4.1(a) at a rate F

(m = 3) band positioning. It c an be easily seen from Figure 4.1(b) that, for integer-
Figures 4.1 (a) and 4.1 (d) show two bandpass signals with even (m = 4) and odd

without aliasing. From Figure 4.1(c), we see that the original bandpass signal can

Figure 4.1(c).
lowpass signals [see (1.21)], modulated by a carrier with frequency F .

A simple inspection of Figure 4.1

423



-Fc Fc

B 2B 3B 4B

3B 4B

0-B-2B-3B-4B

|X(F)|

|Xa(F)|

F

F

FFL FH

-Fc Fc

B 2B 3B 4B0

0

0

-Fc Fc

Ga(F)

F0

-B-2B-3B-4B

|X(F)|

|Xa(F)|

F

FL FH

2FsFs-Fs-2Fs

2FsFs-Fs-2Fs

T

1
T
_

1

1

1
T
_

(a)

(b)

(c)

(d)

(e)

Nyquist zones

1st 2nd 3rd

1st 2nd 3rd

4th

Illustration of bandpass signal sampling for integer band positioning.

Arbitrary Band Positioning. Consider now a bandpass signal with arbitrarily po-
To avoid aliasing, the sampling

frequency should be such that the (k−1)th and kth shifted replicas of the “negative”

we see that this is possible if there is an integer k and a sampling frequency Fs that
satisfy the following conditions:

2FH ≤ kFs
(k − 1)Fs ≤ 2FL

(4.4)

(4.5)

Figure 4.1

Sampling and Reconstruction of Signals

sitioned spectral bands, as shown in Figure 4.2.

spectral band do not overlap with the “positive” spectral band. From Figure 4.2(b)

424



F−Fc Fc

−Fc Fc
0

0

|X(F)|

|Xa(F)|

F

FL FH

1

1
T
_

(a)

(b)

(k−1)Fs
2FL
2FH

kFs

BB

(k−1)th replica kth replica

Illustration of bandpass signal sampling for arbitrary band positioning.

s

s should be in the range

2FH
k
≤ Fs ≤ 2FL

k − 1

1
Fs
≤ k

2FH

(k − 1)Fs ≤ 2FH − 2B

we obtain

kmax ≤ FH
B

The maximum value of integer k is the number of bands that we can fit in the range
from 0 to FH , that is

kmax =
⌊
FH

B

⌋

where 	b
 denotes the integer part of b. The minimum sampling rate required to
avoid aliasing is Fsmax = 2FH/kmax . Therefore, the range of acceptable uniform
sampling rates is determined by

2FH
k
≤ Fs ≤ 2FL

k − 1

(4.6)

(4.7)

(4.8)

(4.9)

(4.10)

(4.11)

Figure 4.2

Sampling and Reconstruction of Signals

which is a system of two inequalities with two unknowns, k and F . From (4.4) and
(4.5) we can easily see that F

To determine the integer k we rewrite (4.4) and (4.5) as follows:

By multiplying (4.7) and (4.8) by sides and solving the resulting inequality for k

425



where k is an integer number given by

1 ≤ k ≤
⌊
FH

B

⌋

are valid for both integer and arbitrary band positioning.

al. (1991). The plot shows the sampling frequency, normalized by B , as a function
of the band position, FH

2
k

FH

B
≤ Fs
B
≤ 2
k − 1

(
FH

B
− 1

)

The shaded areas represent sampling rates that result in aliasing. The allowed range
of sampling frequencies is inside the white wedges. For k = 1, we obtain 2FH ≤
Fs ≤ ∞, which is the sampling theorem for lowpass signals. Each wedge in the plot
corresponds to a different value of k .

To determine the allowed sampling frequencies, for a given FH and B , we draw
a vertical line at the point determined by FH/B . The segments of the line within the
allowed areas represent permissible sampling rates. We note that the theoretically
minimum sampling frequency Fs = 2B , corresponding to integer band positioning,

1 2 3 4 5 6 7 8 9 10
1

2

3

4

5

6

Forbidden (aliasing producing) region

k = 2 k = 3k = 1

N
yq

ui
st

 ra
te

: F
s 

 =
 2

F H

F s 
 =

 F H

FH
B

Fs
B

Allowed (white) and forbidden (shaded) sampling frequency re-
gions for bandpass signals. The minimum sampling frequency Fs = 2B , which
corresponds to the corners of the alias-free wedges, is possible for integer-
positioned bands only.

(4.12)

(4.13)

Figure 4.3

Sampling and Reconstruction of Signals

As long as there is no aliasing, reconstruction is done using (4.2) and (4.3), which

Choosing a Sampling Frequency. To appreciate the implications of conditions (4.11)
and (4.12), we depict them graphically in Figure 4.3, as suggested by Vaughan et

/B . This is facilitated by rewriting (4.11) as follows:

426



occurs at the tips of the wedges. Therefore, any small variation of the sampling
rate or the carrier frequency of the signal will move the sampling frequency into the
forbidden area. A practical solution is to sample at a higher sampling rate, which
is equivalent to augmenting the signal band with a guard band �B = �BL +�BH .
The augmented band locations and bandwidth are given by

F ′L = FL −�BL
F ′H = FH +�BH
B ′ = B +�B

The lower-order wedge and the corresponding range of allowed sampling are given by

2F ′H
k′
≤ Fs ≤ 2F

′
L

k′ − 1 where k
′ =

⌊
F ′H
B ′

⌋

The k′ th wedge with the guard bands and the sampling frequency tolerances

values above and below the practical operating points as

�Fs = 2F
′
L

k′ − 1 −
2F ′H
k′
= �FsL +�FsH

�BL = k
′ − 1
2

�FsH

�BH = k
′

2
�FsL

which shows that symmetric guard bands lead to asymmetric sampling rate tolerance.

Illustration of the
relationship between the
size of guard bands and
allowed sampling frequency
deviations from its nominal
value for the k th wedge. Guard-band widths

Sampling
frequency
tolerance

Practical operating
point

∆B
L

∆FsL

B

∆B
H

B

B

∆FsH
B

Fs
B

2

k´−1
F Ĺ
B

= 

Fs
B

2

k´

F
B́

B
= 

(4.14)

(4.15)

(4.16)

(4.17)

(4.18)

(4.19)

(4.20)

Figure 4.4

Sampling and Reconstruction of Signals

are illustrated in Figure 4.4. The allowable range of sampling rates is divided into

From the shaded orthogonal triangles in Figure 4.4, we obtain

427



If we choose the practical operating point at the vertical midpoint of the wedge,
the sampling rate is

Fs = 12
(

2F ′H
k′
+ 2F

′
L

k′ − 1
)

Since, by construction, �FsL = �FsH = �Fs/2, the guard bands become

�BL = k
′ − 1
4

�Fs

�BH = k
′

4
�Fs

We next provide an example that illustrates the use of this approach.

Suppose we are given a bandpass signal with B = 25 kHz and FL = 10,702.5 kHz. From

kmax = 	FH/B
 = 429

This yields the theoretically minimum sampling frequency

Fs = 2FH
kmax

= 50.0117 kHz

To avoid potential aliasing due to hardware imperfections, we wish to use two guard bands of
�BL = 2.5 kHz and �BH = 2.5 kHz on each side of the signal band. The effective bandwidth
of the signal becomes B ′ = B+�BL+�BH = 30 kHz. In addition, F ′L = FL−�BL = 10,700
kHz and F ′H = FH +�BH

k′max = 	F ′H/B ′
 = 357

Substitution of kmax
frequencies

60.1120 kHz ≤ Fs ≤ 60.1124 kHz

A detailed analysis on how to choose in practice the sampling rate for bandpass
signals is provided by Vaughan et al. (1991) and Qi et al. (1996).

4.2 Interleaved or Nonuniform Second-Order Sampling

Suppose that we sample a continuous-time signal xa(t) with sampling rate Fi = 1/Ti
at time instants t = nTi +�i , where �i is a fixed time offset. Using the sequence

xi(nTi) = xa(nTi +�i), −∞ < n <∞

(4.21)

(4.22)

(4.23)

(4.24)

EXAMPLE 4.1

Sampling and Reconstruction of Signals

(4.10) the maximum wedge index is

= 10,730 kHz. From (4.17), the maximum wedge index is

into the inequality in (4.17) provides the range of acceptable sampling

428



and a reconstruction function g(i)a (t) we generate a continuous-time signal

y(i)a (t) =
∞∑

n=−∞
xi(nTi)g

(i)
a (t − nTi −�i)

The Fourier transform of y(i)a (t) is given by

Y (i)a (F ) =
∞∑

n=−∞
xi(nTi)G

(i)
a (F )e

−j2πF(nTt+�i)

= G(i)a (F )Xi(F )e−j2πF�i

i i i

the Fourier transform of xi(nTi) can be expressed in terms of the Fourier transform
Xa(F )e

j2πF�i of xa(t +�i) as

Xi(F ) = 1
Ti

∞∑
k=−∞

Xa

(
F − k

Ti

)
e
j2π(F− k

Ti
)�i

Y (i)a (F ) = G(i)a (F )
1
Ti

∞∑
k=−∞

Xa

(
F − k

Ti

)
e
−j2π k

Ti
�i

= 1, 2, . . . , p , we obtain p inter-
leaved uniformly sampled sequences xi(nTi), −∞ < n < ∞. The sum of the p
reconstructed signals is given by

ya(t) =
p∑
i=1

y(i)a (t)

a

Ya(F ) =
p∑
i=1

G(i)a (F )V
(i)(F )

where

V (i)(F ) = 1
Ti

∞∑
k=−∞

Xa

(
F − k

Ti

)
e
−j2π k

Ti
�i

We will focus on the most commonly used second-order sampling, defined by

p = 2,�1 = 0,�2 = �, T1 = T2 = 1
B
= T

(4.25)

(4.26)

(4.27)

(4.28)

(4.29)

(4.30)

(4.31)

(4.32)

(4.33)

Sampling and Reconstruction of Signals

where X (F) is the Fourier transform of x (nT ). From the sampling theorem (1.14),

Substitution of (4.28) into (4.27) yields

If we repeat the sampling process (4.24) for i

Using (4.29) and (4.30), the Fourier transform of y (t) can be expressed as

In this case, which is illustrated in Figure 4.5, relations (4.31) and (4.32) yield

429



Ideal A/D

Ideal A/D

Ideal D/A
Ga(1)(F)

Ideal D/A
Ga(2)(F)

t = nT

t = nT+∆

x
a
(t) x

1
(nT)

x
2
(nT)

y
a
(t)y

a
(1)(t)

y
a
(2)(t)

t = nT t = nT+∆

x
1
(nT)

x
2
(nT)

t

x
a
(t)

(a)

(b)

Illustration of second-order bandpass sampling: (a) interleaved
sampled sequences (b) second-order sampling and reconstruction system.

Ya(F ) = BG(1)a (F )
∞∑

k=−∞
Xa(F − kB)+ BG(2)a (F )

∞∑
k=−∞

γ kXa(F − kB)

where
γ = e−j2πB�

a

band and a “negative” band as follows:

X+a (F ) =
{
Xa(F ), F ≥ 0
0, F < 0

, X−a (F ) =
{
Xa(F ), F ≤ 0
0, F > 0

Then, we plot the repeated replicas ofXa(F−kB) and γ kXa(F−kB) as four separate
We note that because each individual

component has bandwidth B and sampling rate Fs = 1/B , its repeated copies fill
the entire frequency axis without overlapping, that is, without aliasing. However,
when we combine them, the negative bands cause aliasing to the positive bands, and
vice versa.

(4.34)

(4.35)

(4.36)

Figure 4.5

Sampling and Reconstruction of Signals

To understand the nature of (4.34) we first split the spectrumX (F) into a “positive”

components, as illustrated in Figure 4.6.

430



Xa(F)

FFL

−FL + mB

mB

mB

FL + B−FL − B −FL 0

1

F0

F0

F0

F0

Xa(F)
+Xa(F)

−

Xa(F − kB)+Σ
k

Xa(F − kB)
−Σ

k

γkXa(F − kB)
+Σ

k

γ−kXa(F − kB)
−Σ

k

k = 0

k = 0

k = 0

k = 0 k = m

k = −m

k = −mk = −m − 1

k = −m − 1

k = m

k = m + 1

k = m + 1

... ...

... ...

... ...

... ...

Illustration of aliasing in second-order bandpass sampling.

We want to determine the interpolation functions G(1)a (F ), G
(2)
a (F ), and the time

a a

G(1)a (F ) = G(2)a (F ) = 0, for |F | < FL and |F | > FL + B
To determine G(1)a (F ) and G

(2)
a (F ) for FL ≤ |F | ≤ FL

that only the components with k = ±m and k = ±(m+ 1), where

m =
⌈

2FL
B

⌉

is the smallest integer greater or equal to 2FL/B , overlap with the original spectrum.

(4.37)

(4.38)

Figure 4.6

Sampling and Reconstruction of Signals

offset � so that Y (F ) = X (F). From Figure 4.6 we see that the first requirement is

+B , we see from Figure 4.6

431



In the region FL ≤ F ≤ −FL

Y+a (F ) =
[
BG(1)a (F )+ BG(2)a (F )

]
X+a (F ) (Signal component)

+
[
BG(1)a (F )+ BγmG(2)a (F )

]
X+a (F −mB) (Aliasing component)

The conditions that assure perfect reconstruction Y+a (F ) = X+a (F ) are given by

BG(1)a (F )+ BG(2)a (F ) = 1
BG(1)a (F )+ BγmG(2)a (F ) = 0

Solving this system of equations yields the solution

G(1)a (F ) =
1
B

1
1− γ−m , G

(2)
a (F ) =

1
B

1
1− γ m

which exists for all � such that γ±m = e∓j2πmB� �= 1.
In the region −FL +mB ≤ F ≤ FL

Y+a (F ) =
[
BG(1)a (F )+ BG(2)a (F )

]
X+a (F )

+
[
BG(1)a (F )+ Bγm+1G(2)a (F )

]
X+a (F − (m+ 1)B)

The conditions that assure perfect reconstruction Y+a (F ) = X+a (F ) are given by

BG(1)a (F )+ BG(2)a (F ) = 1
BG(1)a (F )+ Bγm+1G(2)a (F ) = 0

Solving this system of equations yields the solution

G(1)a (F ) =
1
B

1
1− γ−(m+1) , G

(2)
a (F ) =

1
B

1
1− γ m+1

which exists for all � such that γ±(m+1) = e∓j2π(m+1)B� �= 1.

(4.39)

(4.40)

(4.41)

(4.42)

(4.43)

(4.44)

Sampling and Reconstruction of Signals

+mB , equation (4.34) becomes

+ B , equation (4.34) becomes

432



The reconstruction functions in the frequency range −(FL+B) ≤ F ≤ −FL can

we replace m by −m and m+1 by −(m+1). The function G(1)a (F ) has the bandpass
(2)
a (F ) reveals that

G(2)a (F ) = G(1)a (−F)

which implies that g(2)a (t) = g(1)a (−t). Therefore, for simplicity, we adopt the notation
ga(t) = g(1)a

xa(t) =
∞∑

n=−∞
xa

( n
B

)
ga

(
t − n

B

)
+ xa

( n
B
+�

)
ga

(
−t + n

B
+�

)

ga(t) = a(t)+ b(t)

a(t) = cos[2π(mB − FL)t − πmB�]− cos(2πFLt − πmB�)
2πBt sin(πmB�)

b(t) = cos[2π(FL + B)t − π(m+ 1)B�]− cos[2π(mB − FL)t − π(m+ 1)B�]
2πBt sin[π(m+ 1)B�]

We can see that ga(0) = 1, ga(n/B) = 0 for n �= 0, and ga(n/B ± �) = 0 for
n = 0,±1,±2, . . ., as expected for any interpolation function.

We have shown that a bandpass signal xa(t) with frequencies in the range FL ≤
|F | ≤ FL+B can be perfectly reconstructed from two interleaved uniformly sampled
sequences xa(n/B) and xa(n/B+�), −∞ < n <∞, using the interpolation formula

s = 2B samples/second without any restrictions on the
band location. The time offset � cannot take values that may cause the interpolation
function to take infinite values. This second-order sampling theorem was introduced
by Kohlenberg (1953). The general pth-order sampling case (p > 2) is discussed by
Coulson (1995).

1 − γ m
1

1 − γ −m
1

1 − γ m+1
1

1 − γ −(m+1)
1

A BA

FF
L

+ B−(F
L
 + B) −F

L
+ mBF

L
 − mB −F

L
F

L

B

0

BGa(1)(F)

Frequency domain characterization of the bandpass inter-
polation function for second-order sampling.

(4.45)

(4.46)

(4.47)

(4.48)

(4.49)

Figure 4.7

Sampling and Reconstruction of Signals

be obtained

(t) and express the reconstruction formula (4.30) as follows

Taking the inverse Fourier transform of the function shown in Figure 4.7, we can
show (see Problem 7) that the interpolation function is given by

response shown in Figure 4.7. A similar plot for G

(4.46) with an average rate F

in a similar manner. The formulas are given by (4.41) and (4.44) if

433



Some useful simplifications occur when m = 2FL/B , that is, for integer band
positioning (Linden 1959, Vaughan et al. 1991). In this case, the region A becomes
zero, which implies that a(t) = 0. Therefore, we have ga(t) = b(t). There are two
cases of special interest.

For low pass signals, FL = 0 and m = 0, and the interpolation function becomes

gLP(t) = cos(2πBt − πB�)− cos(πB�)2πBt sin(πB�)
The additional constraint � = 1/2B , which results in uniform sampling rate, yields
the well-known sine interpolation function gLP(t) = sin(2πBt)/2πBt .

For bandpass signals with FL = mB/2 we can choose the time offset � such that
γ±(m+1) = −1. This requirement is satisfied if

� = 2k + 1
2B(m+ 1) =

1
4Fc
+ k

2Fc
, k = 0,±1,±2, . . .

where Fc = FL+B/2 = B(m+1)/2 is the center frequency of the band. In this case,
the interpolation function is specified by GQ(F) = 1/2 in the range mB/2 ≤ |F | ≤
(m + 1)B/2 and GQ(F) = 0 elsewhere. Taking the inverse Fourier transform, we
obtain

gQ(t) = sinπBt
πBt

cos 2πFct

This special case is known as direct
quadrature sampling because the in-phase and quadrature components are obtained

Finally, we note that it is possible to sample a bandpass signal, and then to
reconstruct the discrete-time signal at a band position other than the original. This
spectral relocation or frequency shifting of the bandpass signal is most commonly
done using direct quadrature sampling [Coulson et al. (1994)]. The significance of
this approach is that it can be implemented using digital signal processing.

4.3 Bandpass Signal Representations

The main cause of complications in the sampling of a real bandpass signal xa(t) is the
presence of two separate spectral bands in the frequency regions −(FL + B) ≤ F ≤
−FL and FL ≤ F ≤ FL+B . Since xa(t) is real, the negative and positive frequencies
in its spectrum are related by

Xa(−F) = X∗a(F )

Therefore, the signal can be completely specified by one half of the spectrum. We
next exploit this idea to introduce simplified representations for bandpass signals.
We start with the identity

cos 2πFct = 12e
j2πFct + 1

2
e−j2πFct

(4.50)

(4.51)

(4.52)

(4.53)

(4.54)

Sampling and Reconstruction of Signals

which is a special case of (4.47)–(4.49).

explicitly from the bandpass signal (see Section 4.3).

434



which represents the real signal cos 2πFct by two spectral lines of magnitude 1/2,
one at F = Fc and the other at F = −Fc . Equivalently, we have the identity

cos 2πFct = 2�
{

1
2
ej2πFct

}

which represents the real signal as the real part of a complex signal. In terms of the
spectrum, we now specify the real signal cos 2πFct by the positive part of its spectrum,
that is, the spectral line at F = Fc . The amplitude of the positive frequencies is
doubled to compensate for the omission of the negative frequencies.

The extension to signals with continuous spectra is straightforward. Indeed, the
integral of the inverse Fourier transform of xa(t) can be split into two parts as

xa(t) =
∫ ∞

0
Xa(F )e

j2πF tdF +
∫ 0
−∞

Xa(F )e
j2πF t dF

xa(t) =
∫ ∞

0
Xa(F )e

j2πF t dF +
∫ ∞

0
X∗a(F )e

−j2πF t dF

The last equation can be equivalently written as

xa(t) = �
{∫ ∞

0
2Xa(F )ej2πF t dF

}
= �{ψa(t)}

where the complex signal

ψa(t) =
∫ ∞

0
2Xa(F )ej2πF t dF

is known as the analytic signal or the pre-envelope of xa(t). The spectrum of the
analytic signal can be expressed in terms of the unit step function Va(F ) as follows:


a(F ) = 2Xa(F )Va(F ) =
{

2Xa(F ), F > 0
0, F < 0

In case Xa(0) �= 0, we define 
a(0) = Xa(0). To express the analytic signal ψa(t) in
terms of xa(t), we recall that the inverse Fourier transform of Va(F ) is given by

va(t) = 12δ(t)+
j

2πt

ψa(t) = 2xa(t) ∗ va(t) = xa(t)+ j 1
πt
∗ xa(t)

(4.55)

(4.56)

(4.57)

(4.58)

(4.59)

(4.60)

(4.61)

(4.62)

Sampling and Reconstruction of Signals

Changing the variable in the second integral from F to −F and using (4.53) yields

From (4.60), (4.61), and the frequency-domain convolution theorem, we obtain

435



The signal obtained from the convolution of the impulse response

hQ(t) = 1
πt

and the input signal xa(t) is given by

x̂a(t) = 1
πt
∗ xa(t) = 1

π

∫ ∞
−∞

xa(τ )

t − τ dτ

and it is called the Hilbert transform of xa(t), denoted by x̂a(t). We emphasize that
the Hilbert transform is a convolution and does not change the domain, so both its
input xa(t) and its output x̂a(t) are functions of time.

HQ(F) =
∫ ∞
−∞

hQ(t)e
−j2πF t dt =

{−j, F > 0
j, F < 0

or in terms of magnitude and phase

|HQ(F)| = 1, �HQ(F) =
{−π/2, F > 0
π/2, F < 0

The Hilbert transformer HQ(F) is an allpass quadrature filter that simply shifts the
phase of positive frequency components by −π/2 and the phase of negative fre-
quency components by π/2. Q(t) is noncausal, which
means that the Hilbert transformer is physically unrealizable.

We can now express the analytic signal using the Hilbert transform as

ψa(t) = xa(t)+ j x̂a(t)
We see that the Hilbert transform of xa(t) provides the imaginary part of its analytic
signal representation.

The analytic signal ψa(t) of xa(t) is bandpass in the region FL ≤ F ≤ FL + B .
Therefore, it can be shifted to the baseband region −B/2 ≤ F ≤ B/2 using the
modulation property of the Fourier transform

xLP(t) = e−j2πFctψa(t) F←→ XLP(F ) = 
a(F + Fc)

The complex lowpass signal xLP(t) is known as the complex envelope of xa(t).
The complex envelop can be expressed in rectangular coordinates as

xLP(t) = xI (t)+ jxQ(t)
where xI (t) and xQ(t) are both real-valued lowpass signals in the same frequency
region with xLP(t).
so-called quadrature representation of bandpass signals

xa(t) = xI (t) cos 2πFct − xQ(t) sin 2πFct

(4.63)

(4.64)

(4.65)

(4.66)

(4.68)

(4.69)

(4.70)

Sampling and Reconstruction of Signals

The filter defined by (4.63) has the frequency response function given by

From (4.63) we see that h

From (4.58), (4.68), and (4.69) we can easily deduce the

436



Lowpass
filter

Lowpass
filter

−90o Phase 
shifter

Oscillator
Fc

Oscillator
Fc

xI(t)

xa(t) xa(t)

xQ(t)

−90 o Phase 
shifter

cos 2πFc t cos 2πFct

(a) Generation (b) Reconstruction

(a) Scheme for generating the in-phase and quadrature components
of a bandpass signal. (b) Scheme for reconstructing a bandpass signal from its
in-phase and quadrature components.

We refer to xI (t) as the in-phase component of the bandpass signal and xQ(t) as the
quadrature component because the carriers cos 2πFct and sin 2πFct are in-phase
quadrature with respect to each other. The in-phase and quadrature components
can be obtained from the signal xa(t) using quadrature demodulation as shown

The bandpass signal can be reconstructed using the scheme in

Alternatively, we can express the complex envelope in polar coordinates as

xLP(t) = A(t)ejφ(t)

where A(t) and φ(t) are both real-valued lowpass signals. In terms of this polar
representation, the bandpass signal xa(t) can be written as

xa(t) = A(t) cos[2πFct + φ(t)]

where A(t) is known as the envelope and φ(t) as the phase of the bandpass signal.

angle modulation. We can easily see that xI (t) and xQ(t) are related to A(t) and
φ(t) as follows:

xI (t) = A(t) cos 2πFct, xQ(t) = A(t) sin 2πFct

A(t) =
√
x2I (t)+ x2Q(t), φ(t) = tan−1

[
xQ(t)

xI (t)

]

The phase φ(t) is uniquely defined in terms of xI (t) and xQ(t), modulo 2π .

(4.71)

(4.72)

(4.73)

(4.74)

Figure 4.8

Sampling and Reconstruction of Signals

in Figure 4.8(a).
4.8(b).

Equation (4.72) represents a bandpass signal using a combination of amplitude and

437



4.4 Sampling Using Bandpass Signal Representations

sampling of a bandpass signal at a rate Fs = 2B independently of the band location.
Since the analytic signal ψa(t) has a single band at FL ≤ F ≤ FL + B , it can be

sampled at a rate of B complex samples per second or 2B real samples per second

samples can be obtained by sampling xa(t) and its Hilbert transform x̂a(t) at a rate
of B samples per second. Reconstruction requires a complex bandpass interpolation
function defined by

ga(t) = sin πBt
πBt

ej2πFct
F←→ Ga(F ) =

{
1, FL ≤ F ≤ FL + B
0, otherwise

where Fc = FL+B/2. The major problem with this approach is the design of practical
analog Hilbert transformers.

Similarly, since the in-phase xI (t) and quadrature xQ(t) components of the band-
pass signal xa(t) are lowpass signals with one-sided bandwidth B/2, they can be
uniquely represented by the sequences xI (nT ) and xQ(nT ), where T = 1/B . This
results in a total sampling rate of Fs = 2B real samples per second. The original
bandpass signal can be reconstructed by first reconstructing the in-phase and quadra-
ture components using ideal interpolation and then recombining them using formula

The in-phase and quadrature components can be obtained by directly sampling
the signal xa(t), using second-order sampling with a proper choice of �. This leads
to a major simplification because we can avoid the complex demodulation process
required to generate the in-phase and quadrature signals. To extract directly xI (t)
from xa(t), that is

xa(tn) = xI (tn)
requires sampling at time instants

2πFctn = πn, or tn = n2Fc , n = 0,±1,±2, . . .

Similarly, to obtain xQ(t), we should sample at time instants

2πFctn = π2 (2n+ 1), or tn =
2n+ 1

4Fc
, n = 0,±1,±2, . . .

which yields
xa(tn) = −xQ(tn)

Several variations of this approach are described by Grace and Pitt (1969), Rice and
Wu (1982), Waters and Jarret (1982), and Jackson and Matthewson (1986).

Finally, we note that the quadrature approach to bandpass sampling has been
widely used in radar and communications systems to generate in-phase and quadra-
ture sequences for further processing. However, with the development of faster A/D

(4.75)

(4.76)

(4.77)

(4.78)

(4.79)

Sampling and Reconstruction of Signals

The complex envelope (4.68) and quadrature representations (4.70) allow the

without aliasing (see second graph in Figure 4.6). According to (4.67) these

(4.70).

which is equivalent to the special case second-order sampling defined by (4.51) .

438



converters and digital signal processors it is more convenient and economic to sam-
I

and xQ

5 Sampling of Discrete-Time Signals

In this section we use the techniques developed for the sampling and representation
of continuous-time signals to discuss the sampling and reconstruction of lowpass
and bandpass discrete-time signals. Our approach is to conceptually reconstruct the
underlying continuous-time signal and then resample at the desired sampling rate.
However, the final implementations involve only discrete-time operations.

5.1 Sampling and Interpolation of Discrete-Time Signals

Suppose that a sequence x(n) is sampled periodically by keeping every D th sample
of x(n) and deleting the (D − 1) samples in between. This operation, which is also
known as decimation or down-sampling, yields a new sequence defined by

xd(n) = x(nD), −∞ < n <∞
Without loss of generality we assume that x(n) has been obtained by sampling a
signal xa(t) with spectrum Xa(F ) = 0, |F | > B at a sampling rate Fs = 1/T ≥ 2B ,
that is, x(n) = xa(nT ). Therefore, the spectrum X(F) of x(n) is given by

X(F) = 1
T

∞∑
k=−∞

Xa(F − kFs)

We next sample xa(t) at time instants t = nDT , that is, with a sampling rate Fs/D .
The spectrum of the sequence xd(n) = xa(nDT ) is provided by

Xd(F ) = 1
DT

∞∑
k=−∞

Xa

(
F − kFs

D

)

d

spectrum X(F) as

Xd(F ) = 1
D

D−1∑
k=0

X

(
F − kFs

D

)

To avoid aliasing, the sampling rate should satisfy the condition Fs/D ≥ 2B . If the
sampling frequency Fs is fixed, we can avoid aliasing by reducing the bandwidth of
x(n) to (Fs/2)/D . In terms of the normalized frequency variables, we can avoid
aliasing if the highest frequency fmax or ωmax in x(n) satisfies the conditions

fmax ≤ 12D =
fs

2
or ωmax ≤ π

D
= ωs

2

(5.1)

(5.2)

(5.3)

(5.4)

(5.5)

Sampling and Reconstruction of Signals

ple directly the bandpass signal, as described in Section 4.1, and then obtain x (n)
(n) using the discrete-time approach developed in Section 5.

This process is illustrated in Figure 5.1 forD = 2 andD = 4. We can easily see from
Figure 5.1( c) that the spectrum X (F ) can be expressed in terms of the periodic

439



)(a tx

0 t F

)(a FX

0

F

1

s
T

F
=

sF− sFT

1
T

)(X F

)()( a Tnxnx =

0 t 0

2 sF− 2 sF2T

1
2T

)()( a 2Tnxnx =

)()( a 4Tnxnx =

)(dX F

sF− sF0 0 F

4T 2 sF−3 sF− sF− sF 3 sF2 sF

1
4T

)(dX F

0 t 0 F

Fs  = T
1

Fs = 2T

4T

1

Fs = 
1

(a)

(b)

(c)

(d)

Illustration of discrete-time signal sampling in the frequency domain.

In continuous-time sampling the continuous-time spectrum Xa(F ) is repeated an
infinite number of times to create a periodic spectrum covering the infinite frequency
range. In discrete-time sampling the periodic spectrum X(F) is repeated D times to
cover one period of the periodic frequency domain.

To reconstruct the original sequence x(n) from the sampled sequence xd(n), we
start with the ideal interpolation formula

xa(t) =
∞∑

m=−∞
xd(m)

sin π
DT

(t −mDT )
π
DT

(t −mDT )
which reconstructs xa(t) assuming that Fs/D ≥ 2B . Since x(n) = xa(nT ), substitu-

x(n) =
∞∑

m=−∞
xd(m)

sin π
D
(n−mD)

π
D
(n−mD)

This is not a practical interpolator, since the sin(x)/x function is infinite in extent.
In practice, we use a finite summation from m = −L to m = L. The quality of
this approximation improves with increasing L. The Fourier transform of the ideal

gBL(n) = D sin(π/D)n
πn

F←→ GBL(ω) =
{
D, |ω| ≤ π/D
0, π/D < |ω| ≤ π

Therefore, the ideal discrete-time interpolator has an ideal lowpass frequency char-
acteristic.

(5.6)

(5.7)

(5.8)

Figure 5.1

Sampling and Reconstruction of Signals

tion into (5.6) yields

bandlimited interpolation sequence in (5.7) is

440



Illustration of
continuous-time linear
interpolation.

B
C

x(m)glin(t − mTd )

(m − l)Td        t A    mTd 

x(m − 1)glin(t − mTd + Td )

x(m − l)
xlin(t)

x(m)

To understand the process of discrete-time interpolation, we will analyze the
widely used linear interpolation. For simplicity we use the notation Td = DT for the
sampling period of xd(m) = xa(mTd). The value of xa(t) at a time instant between
mTd and (m + 1)Td is obtained by raising a vertical line from t to the line segment
connecting the samples xd(mTd) and xd(mTd d The
interpolated value is given by

xlin(t) = x(m− 1)+ x(m)− x(m− 1)
Td

[t − (m− 1)Td ], (m− 1)Td ≤ t ≤ mTd

which can be rearranged as follows:

xlin(t) =
[

1− t − (m− 1)Td
Td

]
x(m− 1)+

[
1− mTd − t)

Td

]
x(m)

xlin(t) =
∞∑

m=−∞
x(m)glin(t −mTd)

we note that we always have t − (m−1)Td = |t − (m− 1)Td | and mTd − t = |t −mTd |
because (m−1)Td ≤ t ≤ mTd
if we define

glin(t) =
{

1− |t |
Td
, |t| ≤ Td

0, |t| > Td

d = DT , we obtain

xlin(n) =
∞∑

m=−∞
x(m)glin(n−mD)

where

glin(n) =
{

1− |n|
D
, |n| ≤ D

0, |n| > D
As expected from any interpolation function, glin(0) = 1 and glin(n) = 0 for n =
±D,±2D, . . .. The performance of the linear interpolator can be assessed by com-
paring its Fourier transform

Glin(ω) = 1
D

[
sin(ωD/2)
sin(ω/2)

]2

(5.9)

(5.10)

(5.11)

(5.12)

(5.13)

(5.14)

(5.15)

Figure 5.2

Sampling and Reconstruction of Signals

+ T ), as shown in Figure 5.2.

To put (5.10) in the form of the general reconstruction formula

The discrete-time interpolation formulas are obtained by replacing t by nT in (5.11)
and (5.12). Since T

. Therefore, we can express (5.10) in the form (5.11)

441



Frequency response of ideal
and linear discrete-time
interpolators.

−π
0 π

L
GBL(ω)

Glin(ω)

ω

L = 5

− 4π
5

− 2π
5

− π
5

2π
5

π

5
4π
5

that the linear interpolator has good performance only when the spectrum of the
interpolated signal is negligible for |ω| > π/D , that is, when the original continuous-
time signal has been oversampled.

x(n) =
∞∑

k=−∞
x̃(k)glin(n− k)

at the expense of unnecessary computations involving zero values. A more efficient

5.2 Representation and Sampling of Bandpass Discrete-Time Signals

can be adapted for discrete-time signals with some simple modifications that take
into consideration the periodic nature of discrete-time spectra. Since we cannot
require that the discrete-time Fourier transform is zero for ω < 0 without violating
its periodicity, we define the analytic signal ψ(n) of a bandpass sequence x(n) by


(ω) =
{

2X(ω), 0 ≤ ω < π
0, −π ≤ ω < 0

where X(ω) and 
(ω) are the Fourier transforms of x(n) and ψ(n), respectively.

(5.16)

(5.17)

Figure 5.3

Sampling and Reconstruction of Signals

to that of the ideal interpolator (5.8). This is illustrated in Figure 5.3 which shows

implementation can be obtained using equation (5.13).

The bandpass representations of continuous-time signals, discussed in Section 4.3,

Equations (5.11) and (5.13) resemble a convolution summation; however, they 
are not convolutions. This is illustrated in Figure 5.4 which shows  the compu tation 
of interpolated samples We  note  that  only  a

 subset of  the coefficients  of the linear interpolator is used in each case. Basically, we
 decompose  components  and  we  use  one  at  a time periodically to
 compute the interpolated values. This is essentially the idea behind polyphase filter 
structures.

 
However, if we create a new sequence  by inserting  zero

 samples  between  successive  samples  of we  can compute  using the
 convolution

x(nT ) and x((n+ 1)T ) for D = 5.

glin(n) into D

(D− 1)
xd(m), x(n)

x̃(n)

Sampling and interpolation of a discrete-time signal essentially corresponds to 
a change of its sampling rate by an integer factor. The subject of sampling rate con-
version, which is very important in practical applications. Extensive discussion is 
beyond the scope of this chapter.

442



x(m − 1)

(m − 1)Td mTd t

t

t

nT

nT

nT

(n − 1)T (n + 1)T

x(m)

x
lin

(n)

g
lin

(nT − mTd)

g
lin

(n − k)
x(n)~

Illustration of linear interpolation as a linear filtering process.

The ideal discrete-time Hilbert transformer, defined by

H(ω) =
{−j, 0 < ω < π
j, −π < ω < 0

is a 90-degree phase shifter as in the continuous-time case. We can easily show that


(ω) = X(ω)+ jX̂(ω)
where

X̂(ω) = H(ω)X(ω)
To compute the analytic signal in the time domain, we need the impulse response of
the Hilbert transformer. It is obtained by

h(n) = 1
2π

∫ 0
−π
jejωndω − 1

2π

∫ π
0
jejωndω

(5.18)

(5.19)

(5.20)

(5.21)

Figure 5.4

Sampling and Reconstruction of Signals

443



which yields

h(n) =
{

2
π

sin2(πn/2)
n , n �= 0

0, n = 0
=
{

0, n = even
2
πn, n = odd

The sequence h(n) is nonzero for n < 0 and not absolutely summable; thus, the
ideal Hilbert transformer is noncausal and unstable. The impulse response and the

As in the continuous-time case the Hilbert transform x̂(n) of a sequence x(n)
provides the imaginary part of its analytic signal representation, that is,

ψ(n) = x(n)+ j x̂(n)

The complex envelope, quadrature, and envelope/phase representations are ob-
tained by the corresponding formulas for continuous-time signals by replacing t by
nT in all relevant equations.

Given a bandpass sequence x(n), 0 < ωL ≤ |ω| ≤ ωL + w with normalized
bandwidth w = 2πB/Fs , we can derive equivalent complex envelope or in-phase
and quadrature lowpass representations that can be sampled at a rate fs = 1/D

0 1 2 3 4 5 6 7 8

−1−3−5−5−7

n

h[n]

H(ω)
j

−j

−π π

(a)

(b)

Impulse response (a) and frequency response (b)
of the discrete-time Hilbert transformer.

(5.22)

(5.23)

Figure 5.5

Sampling and Reconstruction of Signals

frequency response of the ideal Hilbert transformer are illustrated in Figure 5.5.

444



compatible with the bandwidth w . If ωL = (k − 1)π/D and w = π/D the sequence

In many radar and communication systems applications it is necessary to process
a bandpass signal xa(t), FL ≤ |F | ≤ FL + B in lowpass form. Conventional tech-
niques employ two quadrature analog channels and two A/D converters following

sample the analog signal and then obtain the quadrature representation using digital
quadrature demodulation, that is, a discrete-time implementation of the first part of

A similar approach can be used to digitally generate single sideband
signals for communications applications (Frerking 1994).

6 Oversampling A/D and D/A Converters

In this section we treat oversampling A/D and D/A converters.

6.1 Oversampling A/D Converters

The basic idea in oversampling A/D converters is to increase the sampling rate of
the signal to the point where a low-resolution quantizer suffices. By oversampling,
we can reduce the dynamic range of the signal values between successive samples
and thus reduce the resolution requirements on the quantizer. As we have observed
in the preceding section, the variance of the quantization error in A/D conversion
is σ 2e = �2/12, where � = R/2b+1 . Since the dynamic range of the signal, which is
proportional to its standard deviation σx , should match the range R of the quantizer,
it follows that � is proportional to σx . Hence for a given number of bits, the power
of the quantization noise is proportional to the variance of the signal to be quantized.
Consequently, for a given fixed SQNR, a reduction in the variance of the signal to
be quantized allows us to reduce the number of bits in the quantizer.

The basic idea for reducing the dynamic range leads us to consider differential
quantization. To illustrate this point, let us evaluate the variance of the difference
between two successive signal samples. Thus we have

d(n) = x(n)− x(n− 1)

The variance of d(n) is

σ 2d = E[d2(n)] = E{[x(n)− x(n− 1)]2}
= E[x2(n)]− 2E[x(n)x(n− 1)]+ E[x2(n− 1)]
= 2σ 2x [1− γxx(1)]

where γxx(1) is the value of the autocorrelation sequence γxx(m) of x(n) evaluated
at m = 1. If γxx(1) > 0.5, we observe that σ 2d < σ 2x . Under this condition, it is
better to quantize the difference d(n) and to recover x(n) from the quantized values
{dq(n)}. To obtain a high correlation between successive samples of the signal, we
require that the sampling rate be significantly higher than the Nyquist rate.

(6.1)

(6.2)

Sampling and Reconstruction of Signals

the two lowpass filters in Figure 4.8. A more up-to-date approach is to uniformly

Figure 4.8.

x(n) can be sampled directly without any aliasing.

445



An even better approach is to quantize the difference

d(n) = x(n)− ax(n− 1)

where a is a parameter selected to minimize the variance in d(n). This leads to the

a = γxx(1)
γxx(0)

= γxx(1)
σ 2x

and
σ 2d = σ 2x [1− a2]

In this case, σ 2d < σ
2
x , since 0 ≤ a ≤ 1. The quantity ax(n− 1) is called a first-order

predictor of x(n).

This system is used in speech encoding and transmission over telephone channels and
is known as differential pulse code modulation (DPCM). The goal of the predictor
is to provide an estimate x̂(n) of x(n) from a linear combination of past values of
x(n), so as to reduce the dynamic range of the difference signal d(n) = x(n)− x̂(n).
Thus a predictor of order p has the form

x̂(n) =
p∑
k=1

akx(n− k)

to avoid the accumulation of quantization errors at the decoder. In this configuration,
the error e(n) = d(n)− dq(n) is

e(n) = d(n)− dq(n) = x(n)− x̂(n)− dq(n) = x(n)− xq(n)

Thus the error in the reconstructed quantized signal xq(n) is equal to the quantization
error for the sample d(n). The decoder for DPCM that reconstructs the signal from

The simplest form of differential predictive quantization is called delta modu-
lation (DM). In DM, the quantizer is a simple 1-bit (two-level) quantizer and the

+

++

−
x(n) x(n)

Q[ ]
xq(n)

xq(n)

dq(n)d(n)x(n)

PR PR

Coder Decoder

^ ^

Encoder and decoder for differential predictive signal
quantizer system.

(6.4)

(6.5)

Figure 6.1

Sampling and Reconstruction of Signals

(6.3)

result (see Problem 16) that the optimum choice of a is

Figure 6.1 shows a more general differential predictive signal quantizer system.

The use of the feedback loop around the quantizer as shown in Fig. 6.1 is necessary

the quantized values is also shown in Fig. 6.1.

446



a staircase approximation of the input signal. At every sampling instant, the sign of
the difference between the input sample x(n) and its most recent staircase approxi-
mation x̂(n) = axq(n− 1) is determined, and then the staircase signal is updated by
a step � in the direction of the difference.

xq(n) = axq(n− 1)+ dq(n)
which is the discrete-time equivalent of an analog integrator. If a = 1, we have an
ideal accumulator (integrator) whereas the choice a < 1 results in a “leaky integra-

+

++

−

z−1 z−1

+

x(n)

x(n)

x(n)

dq(n) xq(n)

xq(n)

d(n)x(n)

x(n)

x(n − 1)

xa(t)

Coder Decoder

a a

−1

+1

x(t)

x(t) x(t)

d(t)x(t)

−1

+1 −1

+1

Time

Slope-overload
distortion Granular noise

Step-size ∆

T = 1
Fs

TT

∫

∫

Integrator

Clock LPF

−

(a)

(b)

(c)

^

^

^

^

^ ^

Delta modulation system and two types of quantization errors.

(6.6)

Figure 6.2

Sampling and Reconstruction of Signals

predictor is a first-order predictor, as shown in Fig. 6.2(a). Basically, DM provides

From Fig. 6.2(a) we observe that

447



tor.”
the practical implementation of a DM system. The analog lowpass filter is necessary
for the rejection of out-of-band components in the frequency range between B and
Fs/2, since Fs >> B due to oversampling.

in DM, slope-overload distortion and granular noise. Since the maximum slope
�/T in x(n) is limited by the step size, slope-overload distortion can be avoided if
max |dx(t)/dt | ≤ �/T . The granular noise occurs when the DM tracks a relatively
flat (slowly changing) input signal. We note that increasing � reduces overload
distortion but increases the granular noise, and vice versa.

One way to reduce these two types of distortion is to use an integrator in front
This has two effects. First, it emphasizes the

low frequencies of x(t) and increases the correlation of the signal into the DM input.
Second, it simplifies the DM decoder because the differentiator (inverse system)
required at the decoder is canceled by the DM integrator. Hence the decoder is

the encoder can be replaced by a single integrator placed before the comparator, as

SDM is an ideal candidate for A/D conversion. Such a converter takes advantage
of the high sampling rate and spreads the quantization noise across the band up to
Fs/2. Since Fs >> B , the noise in the signal-free band B ≤ F ≤ Fs/2 can be

+
−1

+1∫

∫
−

+
−1

+1∫
−

x(t)

Clock

Analog
LPF

Coder Decoder

x(t)

Clock

Analog
LPF

Coder Decoder

(a)

(b)

Sigma-delta modulation system.Figure 6.3

Sampling and Reconstruction of Signals

Figure 6.2(c) shows an analog model that illustrates the basic principle for

The crosshatched areas in Fig. 6.2(b) illustrate two types of quantization error

of the DM, as shown in Fig. 6.3(a).

simply a lowpass filter, as shown in Fig. 6.3(a). Furthermore, the two integrators at

shown in Fig. 6.3(b). This system is known as sigma-delta modulation (SDM).

448



+ + +z−1

−

x(n)

H(z)

dq(n)d(n)

e(n)

Discrete-time model of sigma-delta modulation.

removed by appropriate digital filtering. To illustrate this principle, let us consider

the comparator (1-bit quantizer) is modeled by an additive white noise source with
variance σ 2e = �2/12. The integrator is modeled by the discrete-time system with
system function

H(z) = z
−1

1− z−1
The z-transform of the sequence {dq(n)} is

Dq(z) = H(z)1+H(z)X(z)+
1

1+H(z)E(z)

= Hs(z)X(z)+Hn(z)E(z)
where Hs(z) and Hn(z) are the signal and noise system functions, respectively. A
good SDM system has a flat frequency response Hs(ω) in the signal frequency band
0 ≤ F ≤ B . On the other hand, Hn(z) should have high attenuation in the frequency
band 0 ≤ F ≤ B and low attenuation in the band B ≤ F ≤ Fs/2.

Hs(z) = z−1, Hn(z) = 1− z−1

Thus Hs(z) does not distort the signal. The performance of the SDM system is
therefore determined by the noise system function Hn(z), which has a magnitude
frequency response

|Hn(F )| = 2
∣∣∣∣sin πFFs

∣∣∣∣

σ 2n =
∫ B
−B
|Hn(F )|2Se(F ) dF

where Se(F ) = σ 2e /Fs is the power spectral density of the quantization noise. From
this relationship we note that doubling Fs (increasing the sampling rate by a factor of
2), while keeping B fixed, reduces the power of the quantization noise by 3 dB. This
result is true for any quantizer. However, additional reduction may be possible by
properly choosing the filter H(z).

(6.7)

(6.8)

(6.9)

(6.10)

(6.11)

Figure 6.4

Sampling and Reconstruction of Signals

the discrete-time model of SDM, shown in Fig. 6.4,where we have assumed that

For the first-order SDM system with the integrator specified by (6.7), we have

as shown in Fig. 6.5. The in-band quantization noise variance is given as

449



Sr(F) Hn(F)

 Fs
2−

 Fs
2

−B B F

e/Fsσ2

Frequency (magnitude) response of noise system function.

s >> 2B ,
the in-band quantization noise power is

σ 2n ≈
1
3
π2σ 2e

(
2B
Fs

)3

Note that doubling the sampling frequency reduces the noise power by 9 dB, of
which 3 dB is due to the reduction in Se(F ) and 6 dB is due to the filter characteristic
Hn(F ). An additional 6-dB reduction can be achieved by using a double integrator

In summary, the noise power σ 2n can be reduced by increasing the sampling rate
to spread the quantization noise power over a larger frequency band (−Fs/2, Fs/2),
and then shaping the noise power spectral density by means of an appropriate filter.
Thus, SDM provides a 1-bit quantized signal at a sampling frequency Fs = 2IB,
where the oversampling (interpolation) factor I determines the SNR of the SDM
quantizer.

Next, we explain how to convert this signal into a b-bit quantized signal at the
Nyquist rate. First, we recall that the SDM decoder is an analog lowpass filter with a
cutoff frequency B . The output of this filter is an approximation to the input signal
x(t). Given the 1-bit signal dq(n) at sampling frequency Fs , we can obtain a signal
xq(n) at a lower sampling frequency, say the Nyquist rate of 2B or somewhat faster,
by resampling the output of the lowpass filter at the 2B rate. To avoid aliasing, we
first filter out the out-of-band (B, Fs/2) noise by processing the wideband signal.
The signal is then passed through the lowpass filter and resampled (down-sampled)

For example, if the interpolation factor is I = 256, the A/D converter output
can be obtained by averaging successive nonoverlapping blocks of 128 bits. This
averaging would result in a digital signal with a range of values from zero to 256(b ≈ 8
bits) at the Nyquist rate. The averaging process also provides the required antialiasing
filtering.

Oversampling A/D converters for voiceband (3-kHz) signals are currently fabricated

(6.12)

Figure 6.5

Sampling and Reconstruction of Signals

Figure 6.6 illustrates the basic elements of an oversampling A/D converter.

at the lower rate. The down-sampling process is called decimation.

For the first-order SDM, it can be shown (see Problem 19) that for F

(see Problem 20).

450



x(t)
1-bit
dq(n) xq(n)

Analog section

Analog section

Antialiasing
filter

SDM
Fs FN

FN Fs Fs

b > 1 Digital
LPF

(decimator)

Digital section

Digital section

PCM-to-SDM converter Antialiasing filters

SDM-to-PCM converter

1-bitb-bitb-bit Digital
SDM

Smoothing
filter

Sampled
data
LPF

Digital
LPF

(interpolator)

Basic elements of an oversampling A/D converter.

as integrated circuits. Typically, they operate at a 2-MHz sampling rate, down-sample
to 8 kHz, and provide 16-bit accuracy.

6.2 Oversampling D/A Converters

As we
observe, it is subdivided into a digital front end followed by an analog section. The
digital section consists of an interpolator whose function is to increase the sampling
rate by some factor I , which is followed by an SDM. The interpolator simply increases
the digital sampling rate by inserting I−1 zeros between successive low rate samples.
The resulting signal is then processed by a digital filter with cutoff frequency Fc =
B/Fs in order to reject the images (replicas) of the input signal spectrum. This higher
rate signal is fed to the SDM, which creates a noise-shaped 1-bit sample. Each 1-bit
sample is fed to the 1-bit D/A, which provides the analog interface to the antialiasing
and smoothing filters. The output analog filters have a passband of 0 ≤ F ≤ B hertz
and serve to smooth the signal and to remove the quantization noise in the frequency
band B ≤ F ≤ Fs/2. In effect, the oversampling D/A converter uses SDM with the
roles of the analog and digital sections reversed compared to the A/D converter.

In practice, oversampling D/A (and A/D) converters have many advantages over
the more conventional D/A (and A/D) converters. First, the high sampling rate and

Digital
signal Interpolation

filter

Sigma-
delta

modulator

1-bit
D/A

Analog
smoothing

filter

Analog
output

Analog sectionDigital section

Elements of an oversampling D/A converter.

Figure 6.6

Figure 6.7

Sampling and Reconstruction of Signals

The elements of an oversampling D/A converter are shown in Fig. 6.7.

451



the subsequent digital filtering minimize or remove the need for complex and expen-
sive analog antialiasing filters. Furthermore, any analog noise introduced during the
conversion phase is filtered out. Also, there is no need for S/H circuits. Oversam-
pling SDM A/D and D/A converters are very robust with respect to variations in the
analog circuit parameters, are inherently linear, and have low cost.

This concludes our discussion of signal reconstruction based on simple interpola-
tion techniques. The techniques that we have described are easily incorporated into
the design of practical D/A converters for the reconstruction of analog signals from
digital signals.

7 Summary and References

The major focus of this chapter was on the sampling and reconstruction of signals.
In particular, we treated the sampling of continuous-time signals and the subsequent
operation of A/D conversion. These are necessary operations in the digital process-
ing of analog signals, either on a general-purpose computer or on a custom-designed
digital signal processor. The related issue of D/A conversion was also treated. In
addition to the conventional A/D and D/A conversion techniques, we also described
another type of A/D and D/A conversion, based on the principle of oversampling
and a type of waveform encoding called sigma-delta modulation. Sigma-delta con-
version technology is especially suitable for audio band signals due to their relatively
small bandwidth (less than 20 kHz) and in some applications, the requirements for
high fidelity.

The sampling theorem was introduced by Nyquist (1928) and later popularized
in the classic paper by Shannon (1949). D/A and A/D conversion techniques are
treated in a book by Sheingold (1986). Oversampling A/D and D/A conversion
has been treated in the technical literature. Specifically, we cite the work of Candy
(1986), Candy et al. (1981) and Gray (1990).

Problems

1 Given a continuous-time signal xa(t) with Xa(F ) = 0 for |F | > B determine the
minimum sampling rate Fs for a signal ya(t) defined by (a) dxa(t)/dt (b) x2a (t) (c)
xa(2t) (d) xa(t) cos 6πBt and (e) xa(t) cos 7πBt

2 The sampled sequence xa(nT ) is reconstructed using an ideal D/A with interpolation
function ga(t) = A for |F | < Fc and zero otherwise to produce a continuous-time
signal x̂a(t).

(a) If the spectrum of the original signal xa(t) satisfies Xa(F ) = 0 for |F | > B , find
the maximum value of T , and the values of Fc , and A such that x̂a(t) = xa(t).

(b) If X1(F ) = 0 for |F | > B , X2(F ) = 0 for |F | > 2B , and xa(t) = x1(t)x2(t), find
the maximum value of T , and the values of Fc , and A such that x̂a(t) = xa(t).

(c) Repeat part (b) for xa(t) = x1(t)x2(t/2).

Sampling and Reconstruction of Signals

452



3 A continuous-time periodic signal with Fourier series coefficients ck = (1/2)|k| and
period Tp = 0.1 sec passes through an ideal lowpass filter with cutoff frequency
Fc = 102.5 Hz. The resulting signal ya(t) is sampled periodically with T = 0.005
sec. Determine the spectrum of the sequence y(n) = ya(nT ).

4 a −t ua(t).
5 If Xa(F ) = 0 for |F | > Fs/2, determine the

frequency response H(ω) of the discrete-time system such that ya(t) =
∫ t
−∞ xa(τ )dτ .

6 Consider a signal xa(t) with spectrum Xa(F ) �= 0 for 0 < F1 ≤ |F | ≤ F2 < ∞ and
Xa(F ) = 0 otherwise.
(a) Determine the minimum sampling frequency required to sample xa(t) without

aliasing.

(b) Find the formula needed to reconstruct xa(t) from the samples xa(nT ), −∞ <
n <∞.

7 Prove the nonuniform second-order sampling interpolation formula described by

8 A discrete-time sample-and-hold interpolator, by a factor I, repeats the last input
sample (I − 1) times.
(a) Determine the interpolation function gSH(n).

(b) Determine the Fourier transform GSH(ω) of gSH(n).

(c) Plot the magnitude and phase responses of the ideal interpolator, the linear
interpolator, and the sample-and-hold interpolator for I = 5.

9 Time-domain sampling Consider the continuous-time signal

xa(t) =
{
e−j2πF0t , t ≥ 0
0, t < 0

(a) Compute analytically the spectrum Xa(F ) of xa(t).

(b) Compute analytically the spectrum of the signal x(n) = xa(nT), T = 1/Fs .
(c) Plot the magnitude spectrum |Xa(F )| for F0 = 10 Hz.
(d) Plot the magnitude spectrum |X(F)| for Fs = 10, 20, 40, and 100 Hz.
(e) Explain the results obtained in part (d) in terms of the aliasing effect.

10 Consider the sampling of the bandpass signal whose spectrum is illustrated in Fig.
s to avoid aliasing.

X(F)

F

1

−40−50 0 40 50 60−60
Figure P10

Sampling and Reconstruction of Signals

Repeat Example 1.2 for the signal x (t) = te
Consider the system in Figure 2.1.

equations (4.47)–(4.49).

P10. Determine the minimum sampling rate F

453



11 Consider the sampling of the bandpass signal whose spectrum is illustrated in Fig.
s to avoid aliasing.

X(F)

F−94−100 0−106 10610094

12
(a) Sketch the spectra of the various signals if xa(t) has the Fourier transform shown

s = 2B . How are y1(t) and y2(t) related to xa(t)?
(b) Determine y1(t) and y2(t) if xa(t) = cos 2πF0t , F0 = 20 Hz, and Fs = 50 Hz or

Fs = 30 Hz.

0−B B
F

Xa(F)

xa(t) y1(t)x(n)Ideal
A/D

(.)2
y(n) = x2(n) Ideal

D/A

Fs

xa(t) y2(t)s(n) Ideal
D/A

(.)2
Ideal
A/D

FsFs

sa(t) = x
2(t)a

(a)

(b)

13 A continuous-time signal xa(t) with bandwidth B and its echo xa(t − τ) arrive si-
multaneously at a TV receiver. The received analog signal

sa(t) = xa(t)+ αxa(t − τ), |α| < 1
s and H(z)

so that ya(t) = xa(t) [i.e., remove the “ghost” xa(t − τ) from the received signal]?
sa(t) sa(n)

H(z)
ya(t)y(n)

Fs

Ideal
sampler

Ideal
BL

interpolator

Figure P11

Figure P12

Figure P13

Sampling and Reconstruction of Signals

P11. Determine the minimum sampling rate F

Consider the two systems shown in Fig. P12.

is processed by the system shown in Fig. P13. Is it possible to specify F

in Fig. P12(b) and F

454



14 A bandlimited continuous-time signal xa(t) is sampled at a sampling frequency Fs ≥
2B . Determine the energy Ed of the resulting discrete-time signal x(n) as a function
of the energy of the analog signal, Ea , and the sampling period T = 1/Fs .

15 In a linear interpolator successive sample points are connected by straight-line seg-
ments. Thus the resulting interpolated signal x̂(t) can be repressed as

x̂(t) = x(nT − T )+ x(nT )− x(nT − T )
T

(t − nT ), nT ≤ t ≤ (n+ 1)T

We observe that at t = nT , x̂(nT ) = x(nT−T ) and at t = nT+T , x̂(nT+T ) = x(nT ).
Therefore, x(t) has an inherent delay of T seconds in interpolating the actual signal

(a) Viewed as a linear filter, show that the linear interpolation with a T -second delay
has an impulse response

h(t) =
{
t/T , 0 ≤ t < T
2− t/T , T ≤ t < 2T
0, otherwise

Derive the corresponding frequency response H(F).

(b) Plot |H(F)| and compare this frequency response with that of the ideal recon-
struction filter for a lowpass bandlimited signal.

0
t

x(t)

x(t)

^

Figure P15

Sampling and Reconstruction of Signals

x(t). Figure P15 illustrates this linear interpolation technique.

455



16 Let x(n) be a zero-mean stationary process with variance σ 2x and autocorrelation
γx(l).

(a) Show that the variance σ 2d of the first-order prediction error

d(n) = x(n)− ax(n− 1)

is given by

σ 2d = σ 2x [1+ a2 − 2aρx(1)]

where ρx(1) = γx(1)/γx(0) is the normalized autocorrelation sequence.
(b) Show that σ 2d attains its minimum value

σ 2d = σ 2x [1− ρ2x (1)]

for a = γx(1)/γx(0) = ρx(1).
(c) Under what conditions is σ 2d < σ

2
x ?

(d) Repeat steps (a) to (c) for the second-order prediction error

d(n) = x(n)− a1x(n− 1)− a2x(n− 2)

17 Consider a DM coder with input x(n) = A cos(2πnF/Fs). What is the condition for
avoiding slope overload? Illustrate this condition graphically.

18 Let xa(t) be a bandlimited signal with fixed bandwidth B and variance σ 2x .

(a) Show that the signal-to-quantization noise ratio, SQNR = 10 log10(σ 2x /σ 2e ), in-
s

(b) If we wish to increase the SQNR of a quantizer by doubling its sampling fre-
quency, what is the most efficient way to do it? Should we choose a linear
multibit A/D converter or an oversampling one?

19

(a) Show that the quantization noise power in the signal band (−B,B) is given by

σ 2n =
2σ 2e
π

[
2πB
Fs
− sin

(
2π

B

Fs

)]

(b) Using a two-term Taylor series expansion of the sine function and assuming that
Fs >> B , show that

σ 2n ≈
1
3
π2σ 2e

(
2B
Fs

)3

Sampling and Reconstruction of Signals

creases by 3 dB each time we double the sampling frequency F . Assume that
the quantization noise model discussed in Section 3.3 is valid.

Consider the first-order SDM model shown in Fig. 6.4.

456



20

++ + ++

− −

z−1

z−1z−1

dq(n)

e(n)

x(n)

(a) Determine the signal and noise system functions Hs(z) and Hn(z), respectively.
(b) Plot the magnitude response for the noise system function and compare it with

the one for the first-order SDM. Can you explain the 6-dB difference from these
curves?

(c) Show that the in-band quantization noise power σ 2n is given approximately by

σ 2n ≈
πσ 2e

5

(
2B
Fs

)5
which implies a 15-dB increase for every doubling of the sampling frequency.

21
erator. The samples of one period of the signal

x(n) = cos
(

2π
N
n

)
, n = 0, 1, . . . , N − 1

are stored in memory. A digital sinusoidal signal is generated by stepping through
the table and wrapping around at the end when the angle exceeds 2π . This can be
done by using modulo-N addressing (i.e., using a “circular” buffer). Samples of x(n)
are feeding the ideal D/A converter every T seconds.
(a) Show that by changing Fs we can adjust the frequency F0 of the resulting analog

sinusoid.
(b) Suppose now that Fs = 1/T is fixed. How many distinct analog sinusoids can

be generated using the given lookup table? Explain.

x(0) x(1) … x(N − 1)

Fs =
1

T

DSP Ideal
D/A

xa(t) = cos 2πF0t

Figure P20

Figure P21

Sampling and Reconstruction of Signals

Consider the second-order SDM model shown in Fig. P20.

Figure P21 illustrates the basic idea for a lookup-table-based sinusoidal signal gen-

457



22 Suppose that we represent an analog bandpass filter by the frequency response

H(F) = C(F − Fc)+ C∗(−F − Fc)

where C(f ) is the frequency response of an equivalent lowpass filter, as shown in

(a) Show that the impulse response c(t) of the equivalent lowpass filter is related to
the impulse response h(t) of the bandpass filter as follows:

h(t) = 2�[c(t)ej2πFct ]

(b) Suppose that the bandpass system with frequency response H(F) is excited by
a bandpass signal of the form

x(t) = �[u(t)ej2πFct ]

where u(t) is the equivalent lowpass signal. Show that the filter output may be
expressed as

y(t) = �[v(t)ej2πFct ]
where

v(t) =
∫ ω

c(τ )u(t − τ)dτ

(Hint: Use the frequency domain to prove this result.)

−B 0 B

C(F)

F

23
soidal data

x(n) = cos
(

2A
N
n

)
, 0 ≤ n ≤ N − 1

and the sampling frequency Fs = 1/T are fixed. An engineer wishing to produce a
sinusoid with period 2N suggests that we use either zero-order or first-order (linear)
interpolation to double the number of samples per period in the original sinusoid as

Figure P22

Sampling and Reconstruction of Signals

Fig. P22.

Consider the sinusoidal signal generator in Fig. P23, where both the stored sinu-

illustrated in Fig. P23(a).

458



0 1 2 3 4 5 6 7

0 1 2 3 4 5 6

Interpolated values

Interpolated values

Zero-order interpolation

Linear interpolation
(a)

xt(n) H(z)
y(n)x(n) Insert

zeros

(b)

0

1

3
π

X(ω)

−
3
π

(c)

(a) Determine the signal sequences y(n) generated using zero-order interpolation
and linear interpolation and then compute the total harmonic distortion (THD)
in each case for N = 32, 64, 128.

(b) Repeat part (a) assuming that all sample values are quantized to 8 bits.

(c) Show that the interpolated signal sequences y(n) can be obtained by the sys-

successive samples of x(n). Determine the system H(z) and sketch its magni-
tude response for the zero-order interpolation and for the linear interpolation
cases. Can you explain the difference in performance in terms of the frequency
response functions?

(d) Determine and sketch the spectra of the resulting sinusoids in each case both
analytically [using the results in part (c)] and evaluating the DFT of the resulting
signals.

(e) Sketch the spectra of xi(n) and y(n), if x(n) has the spectrum shown in Fig.

choice for H(z)?

Figure P23

Sampling and Reconstruction of Signals

tem shown in Fig. P23 (b). The first module inserts one zero sample between

P23 (c) for both zero-order and linear interpolation. Can you suggest a better

459



24 Let xa(t) be a time-limited signal; that is, xa(t) = 0 for |t | > τ , with Fourier transform
Xa(F ). The function Xa(F ) is sampled with sampling interval δF = 1/Ts .
(a) Show that the function

xp(t) =
∞∑

n=−∞
xa(t − nTs)

can be expressed as a Fourier series with coefficients

ck = 1
Ts
Xa(kδF )

(b) Show that Xa(F ) can be recovered from the samples Xa(kδF ), −∞ < k <∞ if
Ts ≥ 2τ .

(c) Show that if Ts < 2τ , there is “time-domain aliasing” that prevents exact recon-
struction of Xa(F ).

(d) Show that if Ts ≥ 2τ , perfect reconstruction of Xa(F ) from the samples X(kδF )
is possible using the interpolation formula

Xa(F ) =
∞∑

k=−∞
Xa(kδF )

sin[(π/δF )(F − kδF )]
(π/δF )(F − kδF )

Sampling and Reconstruction of Signals

Answers to Selected Problems

9 (a) Xa(F ) = 1j2π(F+F0)
(b) X(f ) = 1

1−e−j2π(F+F0/Fs )

10
Since

Fc+ B2
B
= 50+1020 = 3 is an integer, then Fs = 2B = 40 Hz

14 ∑∞
n=−∞ x

2(n) = 12π
∫ π
−π |X(w)|2dw = EaT

18
Let Pd denote the power spectral density of the quantization noise. Then

Pn =
∫ B
Fs

− B
Fs

Pddf = 2BFs Pd = σ 2e
SQNR = 10 log 10 σ 2x

σ 2e
= 10 log10 σ

2
x Fs

2BPd
= 10 log10 σ

2
x Fs

2BPd
+ 10 log 10Fs

Thus, SQNR will increase by 3 dB if Fs is doubled.

20
Hs(z) = z−1 ; Hn(z) = (1− z−1)2

460



The Discrete Fourier
Transform: Its Properties
and Applications

Frequency analysis of discrete-time signals is usually and most conveniently per-
formed on a digital signal processor, which may be a general-purpose digital com-
puter or specially designed digital hardware. To perform frequency analysis on a
discrete-time signal {x(n)}, we convert the time-domain sequence to an equivalent
frequency-domain representation. We know that such a representation is given by
the Fourier transform X(ω) of the sequence {x(n)}. However, X(ω) is a continuous
function of frequency and therefore it is not a computationally convenient represen-
tation of the sequence {x(n)}.

In this chapter we consider the representation of a sequence {x(n)} by samples
of its spectrum X(ω). Such a frequency-domain representation leads to the discrete
Fourier transform (DFT), which is a powerful computational tool for performing
frequency analysis of discrete-time signals.

Frequency-Domain Sampling: The Discrete Fourier Transform

Before we introduce the DFT, we consider the sampling of the Fourier transform
of an aperiodic discrete-time sequence. Thus, we establish the relationship between
the sampled Fourier transform and the DFT.

1.1 Frequency-Domain Sampling and Reconstruction
of Discrete-Time Signals

We recall that aperiodic finite-energy signals have continuous spectra. Let us con-
sider such an aperiodic discrete-time signal x(n) with Fourier transform

X(ω) =
∞∑

n=−∞
x(n)e−jωn (1.1)

1

John G. Proakis, Dimitris G. Manolakis. Copyright © 2007 by Pearson Education, Inc. All rights reserved.
From Chapter 7 of Digital Signal Processing: Principles, Algorithms, and Applications, Fourth Edition.

461



Frequency-domain sampling
of the Fourier transform. 0 kδω � δω �

ω
−π π 2π

X(ω)

X(kδω)

Suppose that we sampleX(ω) periodically in frequency at a spacing of δω radians
between successive samples. Since X(ω) is periodic with period 2π , only samples
in the fundamental frequency range are necessary. For convenience, we take N
equidistant samples in the interval 0 ≤ ω < 2π with spacing δω = 2π/N , as shown

First, we consider the selection of N , the number of samples in the
frequency domain.

X

(
2π
N
k

)
=

∞∑
n=−∞

x(n)e−j2πkn/N , k = 0, 1, . . . , N − 1 (1.2)

where each sum contains N terms. Thus

X

(
2π
N
k

)
= · · · +

−1∑
n=−N

x(n)e−j2πkn/N +
N−1∑
n=0

x(n)e−j2πkn/N

+
2N−1∑
n=N

x(n)e−j2πkn/N + · · ·

=
∞∑

l=−∞

lN+N−1∑
n=lN

x(n)e−j2πkn/N

If we change the index in the inner summation from n to n− lN and interchange the
order of the summation, we obtain the result

X

(
2π
N
k

)
=

N−1∑
n=0

[ ∞∑
l=−∞

x(n− lN)
]
e−j2πkn/N

for k = 0, 1, 2, . . . , N − 1.
The signal

xp(n) =
∞∑

l=−∞
x(n− lN)

(1.3)

(1.4)

Figure 1.1

in Fig. 1.1.

If we evaluate (1.1) at ω = 2πk/N , we obtain

The summation in (1.2) can be subdivided into an infinite number of summations,

The Discrete Fourier Transform: Its Properties and Applications

462



obtained by the periodic repetition of x(n) every N samples, is clearly periodic with

xp(n) =
N−1∑
k=0

cke
j2πkn/N , n = 0, 1, . . . , N − 1

with Fourier coefficients

ck = 1
N

N−1∑
n=0

xp(n)e
−j2πkn/N

ck = 1
N
X

(
2π
N
k

)
, k = 0, 1, . . . , N − 1

Therefore,

xp(n) = 1
N

N−1∑
k=0

X

(
2π
N
k

)
ej2πkn/N , n = 0, 1, . . . , N − 1

xp(n) from the samples of the spectrum X(ω). However, it does not imply that we
can recover X(ω) or x(n) from the samples. To accomplish this, we need to consider
the relationship between xp(n) and x(n).

p

x(n) can be recovered from xp(n) if there is no aliasing in the time domain, that is,
if x(n) is time-limited to less than the period N of xp(n). This situation is illustrated

x(n), which is nonzero in the interval 0 ≤ n ≤ L− 1. We observe that when N ≥ L,

x(n) = xp(n), 0 ≤ n ≤ N − 1

so that x(n) can be recovered from xp(n) without ambiguity. On the other hand, if
N < L, it is not possible to recover x(n) from its periodic extension due to time-
domain aliasing. Thus, we conclude that the spectrum of an aperiodic discrete-time
signal with finite duration L can be exactly recovered from its samples at frequencies
ωk = 2πk/N, if N ≥ L. The procedure is to compute xp(n), n = 0, 1, . . . , N − 1

x(n) =
{
xp(n), 0 ≤ n ≤ N − 1
0, elsewhere

                           fundamental period N . Consequently, it can be expanded in a Fourier series as

, k = 0, 1, . . . , N − 1

(1.5)

(1.6)

(1.7)

(1.8)

Upon comparing (1.3) with (1.6), we conclude that

The relationship in (1.8) provides the reconstruction of the periodic signal

Since x (n) is the periodic extension of x(n) as given by (1.4), it is clear that

in Fig. 1.2, where without loss of generality, we consider a finite-duration sequence

and finally, X(ω) can be computed from (1.1).

(1.9)

from (1.8); then

The Discrete Fourier Transform: Its Properties and Applications

463



0 L

0 NL

0 N−N

N > L

N < L

x(n)

xp(n)

xp(n)

n

n

n
……

Aperiodic sequence x(n) of length L and its periodic ex-
tension for N ≥ L (no aliasing) and N < L (aliasing).

As in the case of continuous-time signals, it is possible to express the spectrum
X(ω) directly in terms of its samples X(2πk/N), k = 0, 1, . . . , N − 1. To derive such

Since x(n) = xp(n) for 0 ≤ n ≤ N − 1,

x(n) = 1
N

N−1∑
k=0

X

(
2π
N
k

)
ej2πkn/N , 0 ≤ n ≤ N − 1

X(ω) =
N−1∑
n=0

[
1
N

N−1∑
k=0

X

(
2π
N
k

)
ej2πkn/N

]
e−jωn

=
N−1∑
k=0

X

(
2π
N
k

)[
1
N

N−1∑
n=0

e−j (ω−2πk/N)n
]

polation function shifted by 2πk/N in frequency. Indeed, if we define

P(ω) = 1
N

N−1∑
n=0

e−jωn = 1
N

1− e−jωN
1− e−jω

= sin(ωN/2)
N sin(ω/2)

e−jω(N−1)/2

Figure 1.2

an interpolation formula for X(ω), we assume that N ≥ L and begin with (1.8).

(1.10)

If we use (1.1) and substitute for x(n), we obtain

(1.11)

The inner summation term in the brackets of (1.11) represents the basic inter-

(1.12)

The Discrete Fourier Transform: Its Properties and Applications

464



Plot of the function
[sin(ωN/2)]/[N sin(ω/2)].

−0.2

0.2

0.4

0.6

0.8

1.0

ω

X(ω)

π−π
N = 5

X(ω) =
N−1∑
k=0

X

(
2π
N
k

)
P

(
ω − 2π

N
k

)
, N ≥ L

The interpolation function P(ω) is not the familiar (sin θ)/θ but instead, it is a
periodic counterpart of it, and it is due to the periodic nature ofX(ω). The phase shift

We observe that the function P(ω) has the property

P

(
2π
N
k

)
=
{

1, k = 0
0, k = 1, 2, . . . , N − 1

X(2πk/N) for ω = 2πk/N . At all other frequencies, the formula provides a properly
weighted linear combination of the original spectral samples.

The following example illustrates the frequency-domain sampling of a discrete-
time signal and the time-domain aliasing that results.

Consider the signal
x(n) = anu(n), 0 < a < 1

The spectrum of this signal is sampled at frequencies ωk = 2πk/N , k = 0, 1, . . . , N − 1.
Determine the reconstructed spectra for a = 0.8 when N = 5 and N = 50.

Solution. The Fourier transform of the sequence x(n) is

X(ω) =
∞∑
n=0

ane−jωn = 1
1− ae−jω

Figure 1.3

then (1.11) can be expressed as

(1.13)

in (1.12) reflects the fact that the signal x(n) is a causal, finite-duration sequence
of length N . The function sin(ωN/2)/(N sin(ω/2)) is plotted in Fig. 1.3 for N = 5.

Consequently, the interpolation formula in (1.13) gives exactly the sample values

(1.14)

EXAMPLE 1.1

The Discrete Fourier Transform: Its Properties and Applications

465



Suppose that we sample X(ω) at N equidistant frequencies ωk = 2πk/N , k = 0, 1, . . . , N−1.
Thus we obtain the spectral samples

X(ωk) ≡ X
(

2πk
N

)
= 1

1− ae−j2πk/N , k = 0, 1, . . . , N − 1

The periodic sequence xp(n), corresponding to the frequency samples X(2πk/N), k = 0,

xp(n) =
∞∑

l=−∞
x(n− lN) =

0∑
l=−∞

an−lN

= an
∞∑
l=0

alN = a
n

1− aN , 0 ≤ n ≤ N − 1

where the factor 1/(1 − aN) represents the effect of aliasing. Since 0 < a < 1, the aliasing
error tends toward zero as N →∞.

p = 5 and N = 50 and the corre-

the aliasing effects are negligible for N = 50.
If we define the aliased finite-duration sequence x(n) as

x̂(n) =
{
xp(n), 0 ≤ n ≤ N − 1
0, otherwise

then its Fourier transform is

X̂(ω) =
N−1∑
n=0

x̂(n)e−jωn =
N−1∑
n=0

xp(n)e
−jωN

= 1
1− aN ·

1− aNe−jωn
1− ae−jω

Note that although X̂(ω) �= X(ω), the sample values at ωk = 2πk/N are identical. That is,

X̂

(
2π
N
k

)
= 1

1− aN ·
1− aN

1− ae−j2πkN = X
(

2π
N
k

)

The Discrete Fourier Transform (DFT)

The development in the preceding section is concerned with the frequency-domain
sampling of an aperiodic finite-energy sequence x(n). In general, the equally spaced
frequency samples X(2πk/N), k = 0, 1, . . . , N − 1, do not uniquely represent the
original sequence x(n) when x(n) has infinite duration. Instead, the frequency sam-
ples X(2πk/N), k = 0, 1, . . . , N − 1, correspond to a periodic sequence xp(n) of

1.2

The Discrete Fourier Transform: Its Properties and Applications

1, . . . , N − 1, can be obtained from either (1.4) or (1.8). Hence

For a = 0.8, the sequence x(n) and its spectrum X(ω) are shown in Fig. 1.4(a) and
The aliased sequences x (n) for N1.4(b), respectively.

sponding spectral samples are shown in Fig. 1.4(c) and 1.4(d), respectively. We note that

466



0 50

50 50

0

00

1.0

1.0 1.0

01234

N = 5

N = 5

N = 50

x(n)

n

x(n)

n

n

0 1 2 3 4

|X(ω)|

π 2π

X  
N  

k2π

X   
N  

k2π

k

k

(a) (b)

(c)

(d)

^

x(n)^

ω

—

—

(a) Plot of sequence x(n) = (0.8)nu(n); (b) its Fourier transform (magnitude only); (c)
effect of aliasing with N = 5; (d) reduced effect of aliasing with N = 50.

period N , where xp(n) is an aliased version of x(n), as indicated by the relation in

xp(n) =
∞∑

l=−∞
x(n− lN)

When the sequence x(n) has a finite duration of length L ≤ N , then xp(n) is
simply a periodic repetition of x(n), where xp(n) over a single period is given as

xp(n) =
{
x(n), 0 ≤ n ≤ L− 1
0, L ≤ n ≤ N − 1

Consequently, the frequency samples X(2πk/N), k = 0, 1, . . . , N − 1, uniquely
represent the finite-duration sequence x(n). Since x(n) ≡ xp(n) over a single period

(1.15)

(1.16)

Figure 1.4

The Discrete Fourier Transform: Its Properties and Applications

(1.4), that is,

467



(padded by N −L zeros), the original finite-duration sequence x(n) can be obtained

It is important to note that zero padding does not provide any additional infor-
mation about the spectrum X(ω) of the sequence {x(n)}. The L equidistant samples

However, padding the sequence {x(n)} with N −L zeros and computing an N -point
DFT results in a “better display” of the Fourier transform X(ω).

In summary, a finite-duration sequence x(n) of length L [i.e., x(n) = 0 for n < 0
and n ≥ L] has a Fourier transform

X(ω) =
L−1∑
n=0

x(n)e−jωn, 0 ≤ ω ≤ 2π

where the upper and lower indices in the summation reflect the fact that x(n) = 0
outside the range 0 ≤ n ≤ L−1. When we sampleX(ω) at equally spaced frequencies
ωk = 2πk/N , k = 0, 1, 2, . . . , N − 1, where N ≥ L, the resultant samples are

X(k) ≡ X
(

2πk
N

)
=

L−1∑
n=0

x(n)e−j2πkn/N

X(k) =
N−1∑
n=0

x(n)e−j2πkn/N , k = 0, 1, 2, . . . , N − 1

where for convenience, the upper index in the sum has been increased from L − 1
to N − 1 since x(n) = 0 for n ≥ L.

L ≤ N into a sequence of frequency samples {X(k)} of length N . Since the frequency
samples are obtained by evaluating the Fourier transform X(ω) at a set of N (equally

recover the sequence x(n) from the frequency samples

x(n) = 1
N

N−1∑
k=0

X(k)ej2πkn/N , n = 0, 1, . . . , N − 1

is called the inverse DFT (IDFT). Clearly, when x(n) has length L < N , the N -point
IDFT yields x(n) = 0 for L ≤ n ≤ N − 1. To summarize, the formulas for the DFT
and IDFT are

DFT: X(k) =
N−1∑
n=0

x(n)e−j2πkn/N , k = 0, 1, 2, . . . , N − 1

IDFT: x(n) = 1
N

N−1∑
k=0

X(k)ej2πkn/N , n = 0, 1, 2, . . . , N − 1

(1.17)

(1.18)

(1.19)

(1.20)

(1.21)

The Discrete Fourier Transform: Its Properties and Applications

from the frequency samples {X(2πk/N} by means of the formula (1.8).

of X(ω) are sufficient to reconstruct X(ω) using the reconstruction formula (1.13).

The relation in (1.18) is a formula for transforming a sequence {x(n)} of length

spaced) discrete frequencies, the relation in (1.18) is called the discrete Fourier
transform (DFT) of x(n). In turn, the relation given by (1.10), which allows us to

468



EXAMPLE 1.2

A finite-duration sequence of length L is given as

x(n) =
{

1, 0 ≤ n ≤ L− 1
0, otherwise

Determine the N -point DFT of this sequence for N ≥ L.
Solution. The Fourier transform of this sequence is

X(ω) =
L−1∑
n=0

x(n)e−jωn

=
L−1∑
n=0

e−jωn = 1− e
−jωL

1− e−jω =
sin(ωL/2)
sin(ω/2)

e−jω(L−1)/2

of x(n) is simply X(ω) evaluated at the set of N equally spaced frequencies ωk = 2πk/N ,
k = 0, 1, . . . , N − 1. Hence

X(k) = 1− e
−j2πkL/N

1− e−j2πk/N , k = 0, 1, . . . , N − 1

= sin(πkL/N)
sin(πk/N)

e−jπk(L−1)/N

Figure 1.5
Magnitude and phase
characteristics of the
Fourier transform for signal
in Example 1.2.

10

8

6

4

2

0

|X(ω)|

θ(ω)

π 2π
2
3π

2
π

2
π

2
π

ω

0
π

π

−π

2π
ω

−

The Discrete Fourier Transform: Its Properties and Applications

The magnitude and phase of X(ω) are illustrated in Fig. 1.5 for L = 10. The N -point DFT

469



If N is selected such that N = L, then the DFT becomes

X(k) =
{
L, k = 0
0, k = 1, 2, . . . , L− 1

Thus there is only one nonzero value in the DFT. This is apparent from observation of X(ω),
since X(ω) = 0 at the frequencies ωk = 2πk/L, k �= 0. The reader should verify that x(n) can
be recovered from X(k) by performing an L-point IDFT.

Although the L-point DFT is sufficient to uniquely represent the sequence x(n) in the
frequency domain, it is apparent that it does not provide sufficient detail to yield a good picture
of the spectral characteristics of x(n). If we wish to have a better picture, we must evaluate
(interpolate) X(ω) at more closely spaced frequencies, say ωk = 2πk/N , where N > L. In
effect, we can view this computation as expanding the size of the sequence from L points to
N points by appending N − L zeros to the sequence x(n), that is, zero padding. Then the
N -point DFT provides finer interpolation than the L-point DFT.

N = 50, and N = 100. Now the spectral characteristics of the sequence are more clearly
evident, as one will conclude by comparing these spectra with the continuous spectrum X(ω).

10

8

6

4

2

0 50

0

π

−π

k

50
k

(  )θ    N  2πk

(  )X   N  2πk

N = 50

N = 50

(a)

Figure 1.6 Magnitude and phase of an N -point DFT in
Example 1.2; (a) L = 10, N = 50; (b) L = 10, N = 100.

The Discrete Fourier Transform: Its Properties and Applications

Figure 1.6 provides a plot of the N -point DFT, in magnitude and phase, for L = 10,

470



10

8

6

4

2

0 100

100
0

π

−π

k

k

N = 100

N = 100

(b)

(  )θ    N  2πk

(  )X   N  2πk

Figure 1.6 continued

1.3 The DFT as a Linear Transformation

pressed as

X(k) =
N−1∑
n=0

x(n)WknN , k = 0, 1, . . . , N − 1

x(n) = 1
N

N−1∑
k=0

−kn
N

where, by definition,
WN = e−j2π/N

which is an N th root of unity.

(1.22)

X(k)W , n = 0, 1, . . . , N − 1 (1.23)

(1.24)

The Discrete Fourier Transform: Its Properties and Applications

The formulas for the DFT and IDFT given by (1.18) and (1.19) may be ex-

471



We note that the computation of each point of the DFT can be accomplished
by N complex multiplications and (N − 1) complex additions. Hence the N -point
DFT values can be computed in a total of N2 complex multiplications and N(N − 1)
complex additions.

It is instructive to view the DFT and IDFT as linear transformations on sequences
{x(n)} and {X(k)}, respectively. Let us define an N -point vector xN of the signal
sequence x(n), n = 0, 1, . . . , N − 1, an N -point vector XN of frequency samples,
and an N ×N matrix WN as

xN =




x(0)
x(1)
...

x(N − 1)


 , XN =




X(0)
X(1)
...

X(N − 1)




WN =




1 1 1 · · · 1
1 WN W 2N · · · WN−1N

W 2N W
4
N · · · W 2(N−1)N

...
...

...
...
...
...

...

1 WN−1N W
2(N−1
N · · · W(N−1)(N−1)N




XN =WNxN
where WN is the matrix of the linear transformation. We observe that WN is a
symmetric matrix. If we assume that the inverse of WN
inverted by premultiplying both sides by W−1N . Thus we obtain

xN =W−1N XN
But this is just an expression for the IDFT.

xN = 1
N

W∗NXN

where W∗N denotes the complex conjugate of the matrix WN

W−1N =
1
N

W∗N

which, in turn, implies that
WNW∗N = NIN

where IN is anN×N identity matrix. Therefore, the matrix WN in the transformation
is an orthogonal (unitary) matrix. Furthermore, its inverse exists and is given as
W∗N/N. Of course, the existence of the inverse of WN was established previously
from our derivation of the IDFT.

(1.25)

With these definitions, the N -point DFT may be expressed in matrix form as

(1.26)

(1.27)

(1.28)

. Comparison of (1.27)

(1.29)

(1.30)

The Discrete Fourier Transform: Its Properties and Applications

exists, then (1.26) can be

In fact, the IDFT as given by (1.23) can be expressed in matrix form as

with (1.28) leads us to conclude that

472



EXAMPLE 1.3

Compute the DFT of the four-point sequence

x(n) = ( 0 1 2 3 )

Solution. The first step is to determine the matrix W4 . By exploiting the periodicity property
of W4 and the symmetry property

W
k+N/2
N = −WkN

the matrix W4 may be expressed as

W4 =



W 04 W

0
4 W

0
4 W

0
4

W 04 W
1
4 W

2
4 W

3
4

W 04 W
2
4 W

4
4 W

6
4

W 04 W
3
4 W

6
4 W

9
4


 =




1 1 1 1
1 W 14 W

2
4 W

3
4

1 W 24 W
0
4 W

2
4

1 W 34 W
2
4 W

1
4




=




1 1 1 1
1 −j −1 j
1 −1 1 −1
1 j −1 −j




Then

X4 =W4x4 =




6
−2 + 2j
−2

−2 − 2j




The IDFT of X4 may be determined by conjugating the elements in W4 to obtain W∗4 and

The DFT and IDFT are computational tools that play a very important role
in many digital signal processing applications, such as frequency analysis (spectrum
analysis) of signals, power spectrum estimation, and linear filtering. The importance
of the DFT and IDFT in such practical applications is due to a large extent to the
existence of computationally efficient algorithms, known collectively as fast Fourier
transform (FFT) algorithms, for computing the DFT and IDFT.

1.4 Relationship of the DFT to Other Transforms

In this discussion we have indicated that the DFT is an important computational
tool for performing frequency analysis of signals on digital signal processors. In view
of the other frequency analysis tools and transforms that we have developed, it is
important to establish the relationships of the DFT to these other transforms.

The Discrete Fourier Transform: Its Properties and Applications

then applying the formula (1.28).

473



Relationship to the Fourier series coefficients of a periodic sequence. A periodic
sequence {xp(n)} with fundamental period N can be represented in a Fourier series
of the form

xp(n) =
N−1∑
k=0

cke
j2πnk/N , −∞ < n <∞

where the Fourier series coefficients are given by the expression

ck = 1
N

N−1∑
n=0

xp(n)e
−j2πnk/N , k = 0, 1, . . . , N − 1

formula for the Fourier series coefficients has the form of a DFT. In fact, if we define
a sequence x(n) = xp(n), 0 ≤ n ≤ N − 1, the DFT of this sequence is simply

X(k) = Nck

exact line spectrum of a periodic sequence with fundamental period N .

Relationship to the Fourier transform of an aperiodic sequence. We have already
shown that if x(n) is an aperiodic finite energy sequence with Fourier transformX(ω),
which is sampled at N equally spaced frequencies ωk = 2πk/N , k = 0, 1, . . . , N − 1,
the spectral components

X(k) = X(ω)|ω=2πk/N =
∞∑

n=−∞
x(n)e−j2πnk/N , k = 0, 1, . . . , N − 1

are the DFT coefficients of the periodic sequence of period N, given by

xp(n) =
∞∑

l=−∞
x(n− lN)

Thus xp(n) is determined by aliasing {x(n)} over the interval 0 ≤ n ≤ N − 1. The
finite-duration sequence

x̂(n) =
{
xp(n), 0 ≤ n ≤ N − 1
0, otherwise

bears no resemblance to the original sequence {x(n)}, unless x(n) is of finite duration
and length L ≤ N , in which case

x(n) = x̂(n), 0 ≤ n ≤ N − 1

Only in this case will the IDFT of {X(k)} yield the original sequence {x(n)}.

(1.31)

(1.32)

(1.33)

(1.34)

(1.35)

(1.36)

(1.37)

The Discrete Fourier Transform: Its Properties and Applications

If we compare (1.31) and (1.32) with (1.18) and (1.19), we observe that the

Furthermore, (1.31) has the form of an IDFT. Thus the N -point DFT provides the

474



Relationship to the z-transform. Let us consider a sequence x(n) having the z-trans-
form

X(z) =
∞∑

n=−∞
x(n)z−n

with an ROC that includes the unit circle. If X(z) is sampled at the N equally spaced
points on the unit circle zk = ej2πk/N , 0, 1, 2, . . . , N − 1, we obtain

X(k) ≡ X(z)|z=ej2πnk/N , k = 0, 1, . . . , N − 1

=
∞∑

n=−∞
x(n)e−j2πnk/N

N equally spaced frequencies ωk = 2πk/N , k = 0, 1, . . . , N − 1, which is the topic

If the sequence x(n) has a finite duration of length N or less, the sequence can
be recovered from its N -point DFT. Hence its z-transform is uniquely determined
by its N -point DFT. Consequently, X(z) can be expressed as a function of the DFT
{X(k) } as follows:

X(z) =
N−1∑
n=0

x(n)z−n

X(z) =
N−1∑
n=0

[
1
N

N−1∑
k=0

X(k)ej2πkn/N

]
z−n

X(z) = 1
N

N−1∑
k=0

X(k)

N−1

n=0

(
e z−1

)n

X(z) = 1− z
−N

N

N−1∑
k=0

X(k)

1− ej2πk/Nz−1

duration sequence in terms of its DFT, in the form

X(ω) = 1− e
−jωN

N

N−1∑
k=0

X(k)

1− e−j (ω−2πk/N)

of equally spaced discrete frequencies ωk = 2πk/N , k = 0, 1, . . . , N −1. With some

(1.38)

(1.39)

∑
j2πk/N

(1.40)

(1.41)

The Discrete Fourier Transform: Its Properties and Applications

The expression in (1.38) is identical to the Fourier transform X(ω) evaluated at the

treated in Section 1.1.

When evaluated on the unit circle, (1.40) yields the Fourier transform of the finite-

formula for X(ω) expressed in terms of the values {X(k)} of the polynomial at a set

algebraic manipulations, it is possible to reduce (1.41) to the interpolation formula

This expression for the Fourier transform is a polynomial (Lagrange) interpolation

given previously in (1.13).

475



Relationship to the Fourier series coefficients of a continuous-time signal. Suppose
that xa(t) is a continuous-time periodic signal with fundamental period Tp = 1/F0 .
The signal can be expressed in a Fourier series

xa(t) =
∞∑

k=−∞
cke

j2πktF0

where {ck} are the Fourier coefficients. If we sample xa(t) at a uniform rate Fs =
N/Tp = 1/T , we obtain the discrete-time sequence

x(n) ≡ xa(nT ) =
∞∑

k=−∞
cke

j2πkF0nT =
∞∑

k=−∞
cke

j2πkn/N

=
N−1∑
k=0

[ ∞∑
l=−∞

ck−lN

]
ej2πkn/N

X(k) = N
∞∑

l=−∞
ck−lN ≡ Nc̃k

and

c̃k =
∞∑

l=−∞
ck−lN

Thus the {c̃k} sequence is an aliased version of the sequence {ck}.

2 Properties of the DFT

transform X(ω) for a finite-duration sequence {x(n)} of length L ≤ N . The sampling
ofX(ω) occurs at theN equally spaced frequencies ωk = 2πk/N , k = 0, 1, 2, . . . , N−
1. We demonstrated that the N samples {X(k)} uniquely represent the sequence
{x(n)} in the frequency domain. Recall that the DFT and inverse DFT (IDFT) for
an N -point sequence {x(n)} are given as

DFT: X(k) =
N−1∑
n=0

x(n)WknN , k = 0, 1, . . . , N − 1

IDFT: x(n) = 1
N

N−1∑
k=0

X(k)W−knN , n = 0, 1, . . . , N − 1

where WN is defined as
WN = e−j2π/N

(1.42)

(1.43)

(1.44)

(1.45)

(2.1)

(2.2)

(2.3)

The Discrete Fourier Transform: Its Properties and Applications

It is clear that (1.43) is in the form of an IDFT formula, where

In Section 1.2 we introduced the DFT as a set of N samples {X(k)} of the Fourier

476



In this section we present the important properties of the DFT. In view of the re-

Fourier transforms and z-transforms of discrete-time signals, we expect the prop-
erties of the DFT to resemble the properties of these other transforms and series.

property derived in the following section. A good understanding of these properties
is extremely helpful in the application of the DFT to practical problems.

The notation used below to denote the N -point DFT pair x(n) and X(k) is

x(n)
DFT←→
N

X(k)

2.1 Periodicity, Linearity, and Symmetry Properties

Periodicity. If x(n) and X(k) are an N -point DFT pair, then

x(n+N) = x(n) for all n
X(k +N) = X(k) for all k

We previously illustrated the periodicity property in the sequence x(n) for a
given DFT. However, we had not previously viewed the DFT X(k) as a periodic
sequence. In some applications it is advantageous to do this.

Linearity. If

x1(n)
DFT←→
N

X1(k)

and

x2(n)
DFT←→
N

X2(k)

then for any real-valued or complex-valued constants a1 and a2 ,

a1x1(n)+ a2x2(n) DFT←→
N

a1X1(k)+ a2X2(k)

Circular Symmetries of a Sequence. As we have seen, the N -point DFT of a finite
duration sequence x(n), of length L ≤ N , is equivalent to the N -point DFT of a
periodic sequence xp(n), of period N , which is obtained by periodically extending
x(n), that is,

xp(n) =
∞∑

l=−∞
x(n− lN)

(2.4)

(2.5)

(2.6)

(2.7)

The Discrete Fourier Transform: Its Properties and Applications

However, some important differences exist, one of which is the circular convolution

lationships established in Section 1.4 between the DFT and Fourier series, and

These periodicities in x(n) and X(k) follow immediately from formulas (2.1) and
(2.2) for the DFT and IDFT, respectively.

This property follows immediately from the definition of the DFT given by (2.1).

477



Now suppose that we shift the periodic sequence xp(n) by k units to the right. Thus
we obtain another periodic sequence

x ′p(n) = xp(n− k) =
∞∑

l=−∞
x(n− k − lN)

The finite-duration sequence

x ′(n) =
{
x′p(n), 0 ≤ n ≤ N − 1
0, otherwise

is related to the original sequence x(n) by a circular shift. This relationship is illus-

−1−2−3−4

1
2

3
4

1
2

3
4

1
2

3
4

1
2

3
4

1 2 30

1 2 3 4 5 6 70

−1−2−3−4−5−6

1
2

3
4

1
2

3
4

1
2

3
4

1 2 3 4 50

3
4

1

1 3

4 2

2 4

3 1

2

1 2 30

(a)

(b)

(c)

(d)

(e)

x(n)

xp(n)

xp(n − 2)

x'(n)

x'(n)x(n)

x'(1)x(1)

x'(3)x(3)

x'(0)x(0) x'(2)x(2)

n

n

n

n

Figure 2.1 Circular shift of a sequence.

(2.8)

(2.9)

The Discrete Fourier Transform: Its Properties and Applications

trated in Fig. 2.1 for N = 4.

478



In general, the circular shift of the sequence can be represented as the index
modulo N . Thus we can write

x ′(n) = x(n− k,modulo N)
≡ x((n− k))N

For example, if k = 2 and N = 4, we have

x ′(n) = x((n− 2))4
which implies that

x ′(0) = x((−2))4 = x(2)
x ′(1) = x((−1))4 = x(3)
x ′(2) = x((0))4 = x(0)
x ′(3) = x((1))4 = x(1)

Hence x ′(n) is simply x(n) shifted circularly by two units in time, where the counter-
clockwise direction has been arbitrarily selected as the positive direction. Thus we
conclude that a circular shift of an N -point sequence is equivalent to a linear shift of
its periodic extension, and vice versa.

The inherent periodicity resulting from the arrangement of theN -point sequence
on the circumference of a circle dictates a different definition of even and odd sym-
metry, and time reversal of a sequence.

An N -point sequence is called circularly even if it is symmetric about the point
zero on the circle. This implies that

x(N − n) = x(n), 1 ≤ n ≤ N − 1

An N -point sequence is called circularly odd if it is antisymmetric about the point
zero on the circle. This implies that

x(N − n) = −x(n), 1 ≤ n ≤ N − 1

The time reversal of an N -point sequence is attained by reversing its samples
about the point zero on the circle. Thus the sequence x((−n))N is simply given as

x((−n))N = x(N − n), 0 ≤ n ≤ N − 1

This time reversal is equivalent to plotting x(n) in a clockwise direction on a circle.
An equivalent definition of even and odd sequences for the associated periodic

sequence xp(n) is given as follows

even: xp(n) = xp(−n) = xp(N − n)
odd: xp(n) = −xp(−n) = −xp(N − n)

(2.10)

(2.11)

(2.12)

(2.13)

(2.14)

The Discrete Fourier Transform: Its Properties and Applications

479



If the periodic sequence is complex valued, we have

conjugate even: xp(n) = x∗p(N − n)
conjugate odd: xp(n) = −x∗p(N − n)

These relationships suggest that we decompose the sequence xp(n) as

xp(n) = xpe(n)+ xpo(n)
where

xpe(n) = 12 [xp(n)+ x
∗
p(N − n)]

xpo(n) = 12 [xp(n)− x
∗
p(N − n)]

Symmetry properties of the DFT. The symmetry properties for the DFT can be
obtained by applying the methodology previously used for the Fourier transform.
Let us assume that the N -point sequence x(n) and its DFT are both complex valued.
Then the sequences can be expressed as

x(n) = xR(n)+ jxI (n), 0 ≤ n ≤ N − 1
X(k) = XR(k)+ jXI (k), 0 ≤ k ≤ N − 1

XR(k) =
N−1∑
n=0

[
xR(n) cos

2πkn
N
+ xI (n) sin 2πkn

N

]

XI(k) = −
N−1∑
n=0

[
xR(n) sin

2πkn
N
− xI (n) cos 2πkn

N

]

we obtain

xR(n) = 1
N

N−1∑
k=0

[
XR(k) cos

2πkn
N
−XI(k) sin 2πkn

N

]

xI (n) = 1
N

N−1∑
k=0

[
XR(k) sin

2πkn
N
+XI (k) cos 2πkn

N

]

Real-valued sequences.
that

X(N − k) = X∗(k) = X(−k)
Consequently, |X(N−k)| = |X(k)| and � X(N−k) = −� X(k). Furthermore, xI (n) =

IDFT.

(2.15)

(2.16)

(2.17)

(2.18)

(2.19)

(2.20)

(2.21)

(2.22)

(2.23)

If the sequence x(n) is real, it follows directly from (2.1)

(2.24)

The Discrete Fourier Transform: Its Properties and Applications

By substituting (2.18) into the expression for the DFT given by (2.1), we obtain

Similarly, by substituting (2.19) into the expression for the IDFT given by (2.2),

0 and therefore x(n) can be determined from (2.22), which is another form for the

480



Real and even sequences. If x(n) is real and even, that is,

x(n) = x(N − n), 0 ≤ n ≤ N − 1

I

X(k) =
N−1∑
n=0

x(n) cos
2πkn
N

, 0 ≤ k ≤ N − 1

which is itself real valued and even. Furthermore, since XI(k) = 0, the IDFT re-
duces to

x(n) = 1
N

N−1∑
k=0

X(k) cos
2πkn
N

, 0 ≤ n ≤ N − 1

Real and odd sequences. If x(n) is real and odd, that is,

x(n) = −x(N − n), 0 ≤ n ≤ N − 1

R(k) = 0. Hence

X(k) = −j
N−1∑
n=0

x(n) sin
2πkn
N

, 0 ≤ k ≤ N − 1

which is purely imaginary and odd. Since XR(k) = 0, the IDFT reduces to

x(n) = j 1
N

N−1∑
k=0

X(k) sin
2πkn
N

, 0 ≤ n ≤ N − 1

Purely imaginary sequences. I

XR
∑
n=0

xI (n) sin
2πkn
N

XI (k) =
N−1∑

I

2πkn
N

We observe that XR(k) is odd and XI(k) is even.

(2.25)

(2.26)

(2.27)

(2.28)

(2.29)
N−1

(k) =

n=0
x (n) cos (2.30)

The Discrete Fourier Transform: Its Properties and Applications

then (2.21) yields X (k) = 0. Hence the DFT reduces to

then (2.20) yields X

In this case, x(n) = jx (n). Consequently, (2.20) and
(2.21) reduce to

481



If xI (n) is odd, then XI(k) = 0 and hence X(k) is purely real. On the other
hand, if xI (n) is even, then XR(k) = 0 and hence X(k) is purely imaginary.

The symmetry properties given above may be summarized as follows:

X(k) = Xe(k) + Xo(k) + jXe(k) + jXo(k)R R II

x(n) = xe(n) + xo(n) + jxe(n) + jxo(n)R R I I

For
example, the DFT of the sequence

xpe(n) = 12 [xp(n)+ x∗p(N − n)]

is

XR(k) = XeR(k)+XoR(k)

of these properties for the efficient computation of the DFT of special sequences is
considered in some of the problems at the end of the chapter.

TABLE 1 Symmetry Properties of the DFT

N -Point Sequence x(n),
0 ≤ n ≤ N − 1 N -Point DFT

x(n) X(k)

x∗(n) X∗(N − k)
x∗(N − n) X∗(k)
xR(n) Xce(k) = 12 [X(k)+X∗(N − k)]
jXI (n) Xco(k) = 12 [X(k)−X∗(N − k)]

xce(n) = 12 [x(n)+ x∗(N − n)] XR(k)
xco(n) = 12 [x(n)− x∗(N − n)] jXI (k)

Real Signals

Any real signal X(k) = X∗(N − k)
x(n) XR(k) = XR(N − k)

XI (k) = −XI (N − k)
|X(k)| = |X(N − k)|
� X(k) = −� X(N − k)

xce(n) = 12 [x(n)+ x(N − n)] XR(k)
xco(n) = 12 [x(n)− x(N − n)] jXI (k)

(2.31)

The Discrete Fourier Transform: Its Properties and Applications

All the symmetry properties of the DFT can easily be deduced from (2.31).

The symmetry properties of the DFT are summarized in Table 1. Exploitation

482



2.2 Multiplication of Two DFTs and Circular Convolution

Suppose that we have two finite-duration sequences of length N , x1(n) and x2(n).
Their respective N -point DFTs are

X1(k) =
N−1∑
n=0

x1(n)e
−j2πnk/N , k = 0, 1, . . . , N − 1

X2(k) =
N−1∑
n=0

x2(n)e
−j2πnk/N , k = 0, 1, . . . , N − 1

If we multiply the two DFTs together, the result is a DFT, say X3(k), of a sequence
x3(n) of lengthN . Let us determine the relationship between x3(n) and the sequences
x1(n) and x2(n).

We have
X3(k) = X1(k)X2(k), k = 0, 1, . . . , N − 1

The IDFT of {X3(k)} is

x3(m) = 1
N

N−1∑
k=0

X3(k)e
j2πkm/N

= 1
N

N−1∑
k=0

X1(k)X2(k)e
j2πkm/N

1 2

x3(m) = 1
N

N−1∑
k=0

[
N−1∑
n=0

x1(n)e
−j2πkn/N

][
N−1∑
l=0

x2(l)e
−j2πkl/N

]
ej2πkm/N

= 1
N

N−1∑
n=0

x1(n)

N−1∑
l=0

x2(l)

[
N−1∑
k=0

ej2πk(m−n−l)/N
]

N−1∑
k=0

ak =
{
N, a = 1
1− aN
1− a , a �= 1

where a is defined as
a = ej2π(m−n−l)/N

(2.32)

(2.33)

(2.34)

(2.35)

(2.36)

(2.37)

The Discrete Fourier Transform: Its Properties and Applications

Suppose that we substitute for X (k) and X (k) in (2.35) using the DFTs given in
(2.32) and (2.33). Thus we obtain

The inner sum in the brackets in (2.36) has the form

483



We observe that a = 1 when m−n− l is a multiple of N . On the other hand, aN = 1

N−1∑
k=0

ak =
{
N, l = m− n+ pN = ((m− n))N , p an integer
0, otherwise

for x3(m) in the form

x3(m) =
N−1∑
n=0

x1(n)x2((m− n))N , m = 0, 1, . . . , N − 1

The following example illustrates the operations involved in circular convolution.

EXAMPLE 2.1

Perform the circular convolution of the following two sequences:

x1(n) = {2↑, 1, 2, 1}

x2(n) = {1↑, 2, 3, 4}

Solution. Each sequence consists of four nonzero points. For the purposes of illustrating the
operations involved in circular convolution, it is desirable to graph each sequence as points on

1 2
that the sequences are graphed in a counterclockwise direction on a circle. This establishes
the reference direction in rotating one of the sequences relative to the other.

Now, x3 1 2
Beginning with m = 0 we have

x3(0) =
3∑
n=0

x1(n)x2((−n))N

x2((−n))4 2
In other words, the folded sequence is simply x2(n) graphed in a clockwise direction.

The product sequence is obtained by multiplying x1(n) with x2((−n))4 , point by point.
Finally, we sum the values in the product

sequence to obtain
x3(0) = 14

(2.38)

(2.39)

The Discrete Fourier Transform: Its Properties and Applications

for any value of a �= 0. Consequently, (2.37) reduces to

If we substitute the result in (2.38) into (2.36), we obtain the desired expression

a circle. Thus the sequences x (n) and x (n) are graphed as illustrated in Fig. 2.2(a). We note

(m) is obtained by circularly convolving x (n) with x (n) as specified by (2.39).

This sequence is also illustrated in Fig. 2.2(b).

is simply the sequence x (n) folded and graphed on a circle as illustrated in Fig. 2.2(b).

The expression in (2.39) has the form of a convolution sum. However, it is not the 
of a linear system 

to the input sequence  and the impulse response  Instead, the convolution 
sum in (2.39) involves the index  and is called circular convolution. Thus 
we conclude that multiplication of the DFTs of two sequences is equivalent to the 
circular convolution of the two sequences in the time domain.

y(n)

x(n)
N

h(n).
((m−n))

ordinary linear convolution, which relates the output sequence  

484



(a)

x1(n)

x1(1) = 1

x1(3) = 1

x1(0) = 2x1(2) = 2 x2(n)

x2(1) = 2

x2(3) = 4

x2(0) = 1x2(2) = 3

4

2

 26x2((−n))4 x1(n)x2((−n))4

x2(3) = 4

x2(1) = 2

x2(0) = 1x2(2) = 3

(b)

(c)

(d)

(e)

Folded sequence Product sequence

1

3

 48 x1(n)x2((1 − n))4

Product sequence

2

4

 62 x1(n)x2((2 − n))4

Product sequence

3

1

 84 x1(n)x2((3 − n))4

Product sequence

x2((1 − n))4

x2(0) = 1

x2(2) = 3

x2(1) = 2x2(3) = 4

Folded sequence rotated by one unit in time

x2((2 − n))4

x2(1) = 2

x2(3) = 4

x2(2) = 3x2(0) = 1

Folded sequence rotated by two units in time

x2((3 − n))4

x2(2) = 3

x2(0) = 1

x2(3) = 4x2(1) = 2

Folded sequence rotated by three units in time

Figure 2.2 Circular convolution of two sequences.

The Discrete Fourier Transform: Its Properties and Applications

485



For m = 1 we have
x3(1) =

3∑
n=0

x1(n)x2((1− n))4

It is easily verified that x2((1−n))4 is simply the sequence x2((−n))4 rotated counterclockwise
This rotated sequence multiplies x1(n) to

product sequence to obtain x3(1). Thus

x3(1) = 16
For m = 2 we have

x3(2) =
3∑
n=0

x1(n)x2((2 − n))4

Now x2((2 − n))4
counterclockwise direction.

1 2 4
we obtain

x3(2) = 14
For m = 3 we have

x3(3) =
3∑
n=0

x1(n)x2((3− n))4

The folded sequence x2((−n))4 is now rotated by three units in time to yield x2((3− n))4 and
the resultant sequence is multiplied by x1(n) to yield the product sequence as illustrated in

x3(3) = 16
We observe that if the computation above is continued beyond m = 3, we simply repeat

the sequence of four values obtained above. Therefore, the circular convolution of the two
sequences x1(n) and x2(n) yields the sequence

x3(n) = {1↑4, 16, 14, 16}

The reader can easily show from our previous development that either one of the
two sequences may be folded and rotated without changing the result of the circular
convolution. Thus

x3(m) =
N−1∑
n=0

x2(n)x1((m− n))N , m = 0, 1, . . . , N − 1

The following example serves to illustrate the computation of x3(n) by means of
the DFT and IDFT.

(2.40)

The Discrete Fourier Transform: Its Properties and Applications

by one unit in time as illustrated in Fig. 2.2(c).
yield the product sequence, also illustrated in Fig. 2.2(c). Finally, we sum the values in the

is the folded sequence in Fig. 2.2(b) rotated two units of time in the

the product sequence x (n)x ((2− n)) . By summing the four terms in the product sequence,
The resultant sequence is illustrated in Fig. 2.2(d) along with

Fig. 2.2(e). The sum of the values in the product sequence is

N

From this example, we observe that circular convolution involves basically the same 
four steps as the ordinary linear convolution: folding (time reversing) one sequence, shift-
ing the folded sequence, multiplying the two sequences to obtain a product sequence, 
and finally, summing the values of the product sequence. The basic difference between 
these two types of convolution is that, in circular convolution, the folding and shifting 
(rotating) operations are performed in a circular fashion by computing the index of one 
of the sequences modulo . In linear convolution, there is no modulo  operation.N

486



EXAMPLE 2.2

By means of the DFT and IDFT, determine the sequence x3(n) corresponding to the circular
1 2

Solution. First we compute the DFTs of x1(n) and x2(n). The four-point DFT of x1(n) is

X1(k) =
3∑
n=0

x1(n)e
−j2πnk/4, k = 0, 1, 2, 3

= 2+ e−jπk/2 + 2e−jπk + e−j3πk/2

Thus
X1(0) = 6, X1(1) = 0, X1(2) = 2, X1(3) = 0

The DFT of x2(n) is

X2(k) =
3∑
n=0

x2(n)e
−j2πnk/4, k = 0, 1, 2, 3

= 1+ 2e−jπk/2 + 3e−jπk + 4e−j3πk/2

Thus

X2(0) = 10, X2(1) = −2+ j2, X2(2) = −2, X2(3) = −2− j2

When we multiply the two DFTs, we obtain the product

X3(k) = X1(k)X2(k)

or, equivalently,

X3(0) = 60, X3(1) = 0, X3(2) = −4, X3(3) = 0

Now, the IDFT of X3(k) is

x3(n) =
3∑
k=0

X3(k)e
j2πnk/4, n = 0, 1, 2, 3

= 1
4
(60− 4ejπn)

Thus
x3(0) = 14, x3(1) = 16, x3(2) = 14, x3(3) = 16

We conclude this section by formally stating this important property of the DFT.

The Discrete Fourier Transform: Its Properties and Applications

convolution of the sequences x (n) and x (n) given in Example 2.1.

which is the result obtained in Example 2.1 from circular convolution.

487



Circular convolution. If
x1(n)

DFT←→
N

X1(k)

and

x2(n)
DFT←→
N

X2(k)

then

x1(n)©N x2(n) DFT←→
N

X1(k)X2(k)

where x1(n) ©N x2(n) denotes the circular convolution of the sequence x1(n) and
x2(n).

2.3 Additional DFT Properties

Time reversal of a sequence. If

x(n)
DFT←→
N

X(k)

then

x((−n))N = x(N − n) DFT←→
N

X((−k))N = X(N − k)

Hence reversing the N -point sequence in time is equivalent to reversing the DFT
values. Time reversal of a sequence x(n) is illustrated in Fig. 2.3.

DFT{x(N − n)} =
N−1∑
n=0

x(N − n)e−j2πkn/N

If we change the index from n to m = N − n, then

DFT{x(N − n)} =
N−1∑
m=0

x(m)e−j2πk(N−m)/N

Figure 2.3
Time reversal of a
sequence.

x(n)

x(3)

x(4)

x(5)

x(6)

x(7)

x(0)

x(1)
x(2)

x(−n)

x(5)

x(4)

x(3)

x(2)

x(1)

x(0)

x(7)
x(6)

(2.41)

(2.42)

The Discrete Fourier Transform: Its Properties and Applications

Proof From the definition of the DFT in (2.1) we have

488



=
N−1∑
m=0

x(m)ej2πkm/N

=
N−1∑
m=0

x(m)e−j2πm(N−k)/N = X(N − k)

We note that X(N − k) = X((−k))N , 0 ≤ k ≤ N − 1.
Circular time shift of a sequence. If

x(n)
DFT←→
N

X(k)

then
x((n− l))N DFT←→

N
X(k)e−j2πkl/N

Proof From the definition of the DFT we have

DFT{x((n− l))N } =
N−1∑
n=0

x((n− l))Ne−j2πkn/N

=
l−1∑
n=0

x((n− l))Ne−j2πkn/N

+
N−1∑
n=l

x(n− l)e−jπkn/N

But x((n− l))N = x(N − l + n). Consequently,
l−1∑
n=0

x((n− l))Ne−j2πkn/N =
l−1∑
n=0

x(N − l + n)e−j2πkn/N

=
N−1∑
m=N−l

x(m)e−j2πk(m+l)/N

Furthermore,

N−1∑
n=l

x(n− l)e−j2πkn/N =
N−1−l∑
m=0

x(m)e−j2πk(m+l)/N

Therefore,

DFT{x((n− l))} =
N−1∑
m=0

x(m)e−j2πk(m+l)/N

= X(k)e−j2πkl/N

(2.43)

The Discrete Fourier Transform: Its Properties and Applications

489



Circular frequency shift. If

x(n)
DFT←→
N

X(k)

then

x(n)ej2πln/N
DFT←→
N

X((k − l))N

Hence, the multiplication of the sequence x(n) with the complex exponential se-
quence ej2πkn/N is equivalent to the circular shift of the DFT by l units in frequency.
This is the dual to the circular time-shifting property and its proof is similar to that
of the latter.

Complex-conjugate properties. If

x(n)
DFT←→
N

X(k)

then

x∗(n) DFT←→
N

X∗((−k))N = X∗(N − k)

The proof of this property is left as an exercise for the reader. The IDFT of X∗(k) is

1
N

N−1∑
k=0

X∗(k)ej2πkn/N =
[

1
N

N−1∑
k=0

X(k)ej2πk(N−n)/N
]

Therefore,

x∗((−n))N = x∗(N − n) DFT←→
N

X∗(k)

Circular correlation. In general, for complex-valued sequences x(n) and y(n), if

x(n)
DFT←→
N

X(k)

and

y(n)
DFT←→
N

Y (k)

then

r̃xy(l)
DFT←→
N

R̃xy(k) = X(k)Y ∗(k)

where r̃xy(l) is the (unnormalized) circular crosscorrelation sequence, defined as

r̃xy(l) =
N−1∑
n=0

x(n)y∗((n− l))N

(2.44)

(2.45)

(2.46)

(2.47)

The Discrete Fourier Transform: Its Properties and Applications

490



Proof We can write r̃xy(l) as the circular convolution of x(n) with y∗(−n), that is,

r̃xy(l) = x(l)©N y∗(−l)

r̃xy(l) is

R̃xy(k) = X(k)Y ∗(k)
In the special case where y(n) = x(n), we have the corresponding expression for

the circular autocorrelation of x(n),

r̃xx(l)
DFT←→
N

R̃xx(k) = |X(k)|2

Multiplication of two sequences. If

x1(n)
DFT←→
N

X1(k)

and

x2(n)
DFT←→
N

X2(k)

then

x1(n)x2(n)
DFT←→
N

1
N
X1(k)©N X2(k)

the roles of time and frequency in the expression for the circular convolution of two
sequences.

Parseval’s Theorem. For complex-valued sequences x(n) and y(n), in general, if

x(n)
DFT←→
N

X(k)

and

y(n)
DFT←→
N

Y (k)

then
N−1∑
n=0

x(n)y∗(n) = 1
N

N−1∑
k=0

X(k)Y ∗(k)

Proof The property follows immediately from the circular correlation property in

N−1∑
n=0

x(n)y∗(n) = r̃xy(0)

(2.48)

(2.49)

(2.50)

The Discrete Fourier Transform: Its Properties and Applications

Then, with the aid of the properties in (2.41) and (2.46), the N -point DFT of

This property is the dual of (2.41). Its proof follows simply by interchanging

(2.47). We have

491



TABLE 2 Properties of the DFT

Property Time Domain Frequency Domain

Notation x(n), y(n) X(k), Y (k)

Periodicity x(n) = x(n+N) X(k) = X(k +N)
Linearity a1x1(n)+ a2x2(n) a1X1(k)+ a2X2(k)
Time reversal x(N − n) X(N − k)
Circular time shift x((n− l))N X(k)e−j2πkl/N
Circular frequency shift x(n)ej2πln/N X((k − l))N
Complex conjugate x∗(n) X∗(N − k)
Circular convolution x1(n)©N x2(n) X1(k)X2(k)
Circular correlation x(n)©N y∗(−n) X(k)Y ∗(k)
Multiplication of two sequences x1(n)x2(n)

1
N
X1(k)©N X2(k)

Parseval’s theorem
N−1∑
n=0

x(n)y∗(n)
1
N

N−1∑
k=0

X(k)Y ∗(k)

and

r̃xy(l) = 1
N

N−1∑
k=0

R̃xy(k)e
j2πkl/N

= 1
N

N−1∑
k=0

X(k)Y ∗(k)ej2πkl/N

In the

N−1∑
n=0
|x(n)|2 = 1

N

N−1∑
k=0
|X(k)|2

which expresses the energy in the finite-duration sequence x(n) in terms of the fre-
quency components {X(k)}.

3 Linear Filtering Methods Based on the DFT

Since the DFT provides a discrete frequency representation of a finite-duration se-
quence in the frequency domain, it is interesting to explore its use as a computational
tool for linear system analysis and, especially, for linear filtering. We have already
established that a system with frequency response H(ω), when excited with an input

(2.51)

The Discrete Fourier Transform: Its Properties and Applications

Hence (2.50) follows by evaluating the IDFT at l = 0.
The expression in (2.50) is the general form of Parseval’s theorem.

special case where y(n) = x(n), (2.50) reduces to

The properties of the DFT given above are summarized in Table 2.

492



signal that has a spectrum X(ω), possesses an output spectrum Y (ω) = X(ω)H(ω).
The output sequence y(n) is determined from its spectrum via the inverse Fourier
transform. Computationally, the problem with this frequency-domain approach is
that X(ω), H(ω), and Y (ω) are functions of the continuous variable ω . As a conse-
quence, the computations cannot be done on a digital computer, since the computer
can only store and perform computations on quantities at discrete frequencies.

On the other hand, the DFT does lend itself to computation on a digital computer.
In the discussion that follows, we describe how the DFT can be used to perform
linear filtering in the frequency domain. In particular, we present a computational
procedure that serves as an alternative to time-domain convolution. In fact, the
frequency-domain approach based on the DFT is computationally more efficient than
time-domain convolution due to the existence of efficient algorithms for computing
the DFT. These algorithms are collectively called fast Fourier transform (FFT)
algorithms.

3.1 Use of the DFT in Linear Filtering

In the preceding section it was demonstrated that the product of two DFTs is equiv-
alent to the circular convolution of the corresponding time-domain sequences. Un-
fortunately, circular convolution is of no use to us if our objective is to determine the
output of a linear filter to a given input sequence. In this case we seek a frequency-
domain methodology equivalent to linear convolution.

Suppose that we have a finite-duration sequence x(n) of length L which excites
an FIR filter of length M . Without loss of generality, let

x(n) = 0, n < 0 and n ≥ L
h(n) = 0, n < 0 and n ≥ M

where h(n) is the impulse response of the FIR filter.
The output sequence y(n) of the FIR filter can be expressed in the time domain

as the convolution of x(n) and h(n), that is

y(n) =
M−1∑
k=0

h(k)x(n− k)

Since h(n) and x(n) are finite-duration sequences, their convolution is also finite in
duration. In fact, the duration of y(n) is L+M − 1.

Y (ω) = X(ω)H(ω)

If the sequence y(n) is to be represented uniquely in the frequency domain by samples
of its spectrum Y (ω) at a set of discrete frequencies, the number of distinct samples
must equal or exceed L+M−1. Therefore, a DFT of size N ≥ L+M−1 is required
to represent {y(n)} in the frequency domain.

(3.1)

(3.2)

The Discrete Fourier Transform: Its Properties and Applications

The frequency-domain equivalent to (3.1) is

493



Now if

Y (k) ≡ Y (ω)|ω=2πk/N , k = 0, 1, . . . , N − 1
= X(ω)H(ω)|ω=2πk/N , k = 0, 1, . . . , N − 1

then
Y (k) = X(k)H(k), k = 0, 1, . . . , N − 1

where {X(k)} and {H(k)} are the N -point DFTs of the corresponding sequences x(n)
and h(n), respectively. Since the sequences x(n) and h(n) have a duration less thanN ,
we simply pad these sequences with zeros to increase their length to N . This increase
in the size of the sequences does not alter their spectra X(ω) and H(ω), which are
continuous spectra, since the sequences are aperiodic. However, by sampling their
spectra at N equally spaced points in frequency (computing the N -point DFTs),
we have increased the number of samples that represent these sequences in the
frequency domain beyond the minimum number (L or M , respectively).

Since the (N = L+M − 1)-point DFT of the output sequence y(n) is sufficient
to represent y(n) in the frequency domain, it follows that the multiplication of the

of the N -point IDFT, must yield the sequence {y(n)}. In turn, this implies that the
N -point circular convolution of x(n) with h(n) must be equivalent to the linear con-
volution of x(n) with h(n). In other words, by increasing the length of the sequences
x(n) and h(n) to N points (by appending zeros), and then circularly convolving the
resulting sequences, we obtain the same result as would have been obtained with
linear convolution. Thus with zero padding, the DFT can be used to perform linear
filtering.

The following example illustrates the methodology in the use of the DFT in
linear filtering.

EXAMPLE 3.1

By means of the DFT and IDFT, determine the response of the FIR filter with impulse response

h(n) = {1
↑
, 2, 3}

to the input sequence
x(n) = {1

↑
, 2, 2, 1}

Solution. The input sequence has length L = 4 and the impulse response has length
M = 3. Linear convolution of these two sequences produces a sequence of length N = 6.
Consequently, the size of the DFTs must be at least six.

For simplicity we compute eight-point DFTs. We should also mention that the efficient
computation of the DFT via the fast Fourier transform (FFT) algorithm is usually performed
for a length N that is a power of 2. Hence the eight-point DFT of x(n) is

X(k) =
7∑
n=0

x(n)e−j2πkn/8

= 1+ 2e−jπk/4 + 2e−jπk/2 + e−j3πk/4, k = 0, 1, . . . , 7

(3.3)

The Discrete Fourier Transform: Its Properties and Applications

N -point DFTs X(k) and H(k), according to (3.3), followed by the computation

494



This computation yields

X(0) = 6, X(1) = 2+
√

2
2
− j

(
4+ 3√2

2

)

X(2) = −1− j, X(3) = 2−
√

2
2
+ j

(
4− 3√2

2

)

X(4) = 0, X(5) = 2 −
√

2
2
− j

(
4− 3√2

2

)

X(6) = −1+ j, X(7) = 2+
√

2
2
+ j

(
4+ 3√2

2

)

The eight-point DFT of h(n) is

H(k) =
7∑
n=0

h(n)e−j2πkn/8

= 1+ 2e−jπk/4 + 3e−jπk/2

Hence

H(0) = 6, H(1) = 1+
√

2− j
(

3+
√

2
)
, H(2) = −2− j2

H(3) = 1−
√

2 + j
(

3−
√

2
)
, H(4) = 2

H(5) = 1−
√

2 − j
(

3−
√

2
)
, H(6) = −2 + j2

H(7) = 1+
√

2 + j
(

3+
√

2
)

The product of these two DFTs yields Y (k), which is

Y (0) = 36, Y (1) = −14.07− j17.48, Y (2) = j4, Y (3) = 0.07+ j0.515
Y (4) = 0, Y (5) = 0.07− j0.515, Y (6) = −j4, Y (7) = −14.07+ j17.48

Finally, the eight-point IDFT is

y(n) =
7∑
k=0

Y (k)ej2πkn/8, n = 0, 1, . . . , 7

This computation yields the result

y(n) = {1
↑
, 4, 9, 11, 8, 3, 0, 0}

We observe that the first six values of y(n) constitute the set of desired output values.
The last two values are zero because we used an eight-point DFT and IDFT, when, in fact, the
minimum number of points required is six.

The Discrete Fourier Transform: Its Properties and Applications

495



Although the multiplication of two DFTs corresponds to circular convolution
in the time domain, we have observed that padding the sequences x(n) and h(n)
with a sufficient number of zeros forces the circular convolution to yield the same
output sequence as linear convolution. In the case of the FIR filtering problem

convolution of the sequences

h(n) = {1
↑
, 2, 3, 0, 0, 0}

x(n) = {1
↑
, 2, 2, 1, 0, 0}

results in the output sequence

y(n) = {1
↑
, 4, 9, 11, 8, 3}

which is the same sequence obtained from linear convolution.
It is important for us to understand the aliasing that results in the time domain

when the size of the DFTs is smaller than L+M−1. The following example focuses
on the aliasing problem.

EXAMPLE 3.2

Solution. The four-point DFT of h(n) is

H(k) =
3∑
n=0

h(n)e−j2πkn/4

H(k) = 1+ 2e−jπk/2 + 3e−jkπ , k = 0, 1, 2, 3
Hence

H(0) = 6, H(1) = −2− j2, H(2) = 2, H(3) = −2 + j2
The four-point DFT of x(n) is

X(k) = 1+ 2e−jπk/2 + 2e−jπk + 1e−j3πk/2, k = 0, 1, 2, 3
Hence

X(0) = 6, X(1) = −1− j, X(2) = 0, X(3) = −1+ j
The product of these two four-point DFTs is

Ŷ (0) = 36, Ŷ (1) = j4, Ŷ (2) = 0, Ŷ (3) = −j4
The four-point IDFT yields

ŷ(n) = 1
4

3∑
k=0
Ŷ (k)ej2πkn/4, n = 0, 1, 2, 3

= 1
4
(36+ j4ejπn/2 − j4ej3πn/2)

(3.4)

(3.5)

(3.6)

The Discrete Fourier Transform: Its Properties and Applications

in Example 3.1, it is a simple matter to demonstrate that the six-point circular

Determine the sequence y(n) that results from the use of four-point DFTs in Example 3.1.

496



Therefore,
ŷ(n) = {9

↑
, 7, 9, 11}

The reader can verify that the four-point circular convolution of h(n) with x(n) yields the
same sequence ŷ(n).

If we compare the result ŷ(n), obtained from four-point DFTs, with the sequence
y(n) obtained from the use of eight-point (or six-point) DFTs, the time-domain

In particular, y(4) is
aliased into y(0) to yield

ŷ(0) = y(0)+ y(4) = 9
Similarly, y(5) is aliased into y(1) to yield

ŷ(1) = y(1)+ y(5) = 7

All other aliasing has no effect, since y(n) = 0 for n ≥ 6. Consequently, we have

ŷ(2) = y(2) = 9
ŷ(3) = y(3) = 11

Therefore, only the first two points of ŷ(n) are corrupted by the effect of aliasing [i.e.,
ŷ(0) �= y(0) and ŷ(1) �= y(1)]. This observation has important ramifications in the
discussion of the following section, in which we treat the filtering of long sequences.

3.2 Filtering of Long Data Sequences

In practical applications involving linear filtering of signals, the input sequence x(n) is
often a very long sequence. This is especially true in some real-time signal processing
applications concerned with signal monitoring and analysis.

Since linear filtering performed via the DFT involves operations on a block of
data, which by necessity must be limited in size due to limited memory of a digital
computer, a long input signal sequence must be segmented to fixed-size blocks prior
to processing. Since the filtering is linear, successive blocks can be processed one
at a time via the DFT, and the output blocks are fitted together to form the overall
output signal sequence.

We now describe two methods for linear FIR filtering a long sequence on a
block-by-block basis using the DFT. The input sequence is segmented into blocks
and each block is processed via the DFT and IDFT to produce a block of output
data. The output blocks are fitted together to form an overall output sequence
which is identical to the sequence obtained if the long block had been processed via
time-domain convolution.

The two methods are called the overlap-save method and the overlap-add method.
For both methods we assume that the FIR filter has duration M . The input data se-
quence is segmented into blocks of L points, where, by assumption, L >> M without
loss of generality.

The Discrete Fourier Transform: Its Properties and Applications

aliasing effects derived in Section 2.2 are clearly evident.

497



Overlap-save method. In this method the size of the input data blocks is N = L +
M − 1 and the DFTs and IDFT are of length N . Each data block consists of the last
M−1 data points of the previous data block followed by L new data points to form a
data sequence of length N = L+M−1. An N -point DFT is computed for each data
block. The impulse response of the FIR filter is increased in length by appending
L− 1 zeros and an N -point DFT of the sequence is computed once and stored. The
multiplication of the two N -point DFTs {H(k)} and {Xm(k)} for the mth block of
data yields

Ŷm(k) = H(k)Xm(k), k = 0, 1, . . . , N − 1

Then the N -point IDFT yields the result

Ŷm(n) = {ŷm(0)ŷm(1) · · · ŷm(M − 1)ŷm(M) · · · ŷm(N − 1)}

Since the data record is of length N, the first M − 1 points of ym(n) are corrupted by
aliasing and must be discarded. The last L points of ym(n) are exactly the same as
the result from linear convolution and, as a consequence,

ŷm(n) = ym(n), n = M,M + 1, . . . , N − 1

To avoid loss of data due to aliasing, the last M−1 points of each data record are
saved and these points become the first M − 1 data points of the subsequent record,
as indicated above. To begin the processing, the first M − 1 points of the first record
are set to zero. Thus the blocks of data sequences are

x1(n) = {0, 0, . . . , 0,︸ ︷︷ ︸
M−1 points

x(0), x(1), . . . , x(L− 1)}

x2(n) = {x(L−M + 1), . . . , x(L− 1)︸ ︷︷ ︸
M−1data points from x1(n)

, x(L), . . . , x(2L− 1)︸ ︷︷ ︸
L new data points

}

x3(n) = {x(2L−M + 1), . . . , x(2L− 1)︸ ︷︷ ︸
M−1 data points from x2(n)

, x(2L), . . . , x(3L− 1)︸ ︷︷ ︸
L new data points

}

and so forth.
where the first M−1 points are discarded due to aliasing and the remaining L points
constitute the desired result from linear convolution. This segmentation of the input
data and the fitting of the output data blocks together to form the output sequence
are graphically illustrated in Fig. 3.1.

(3.7)

(3.8)

(3.9)

(3.10)

(3.11)

(3.12)

The Discrete Fourier Transform: Its Properties and Applications

The resulting data sequences from the IDFT are given by (3.8),

498



Figure 3.1
Linear FIR filtering by the
overlap-save method.

Input signal

Output signal

Discard
M − 1
points

Discard
M − 1
points

Discard
M − 1
points

M − 1
zeros M−1

L L

L

L

x1(n)

x2(n)

x3(n)

y1(n)

y2(n)

y3(n)

Overlap-add method. In this method the size of the input data block is L points and
the size of the DFTs and IDFT isN = L+M−1. To each data block we appendM−1
zeros and compute the N -point DFT. Thus the data blocks may be represented as

x1(n) = {x(0), x(1), . . . , x(L− 1), 0, 0, . . . , 0}︸ ︷︷ ︸
M−1 zeros

x2(n) = {x(L), x(L+ 1), . . . , x(2L− 1), 0, 0, . . . , 0}︸ ︷︷ ︸
M−1 zeros

x3(n) = {x(2L), . . . , x(3L− 1), 0, 0, . . . , 0}︸ ︷︷ ︸
M−1 zeros

and so on. The two N -point DFTs are multiplied together to form

Ym(k) = H(k)Xm(k), k = 0, 1, . . . , N − 1

The IDFT yields data blocks of length N that are free of aliasing, since the size of
the DFTs and IDFT is N = L+M − 1 and the sequences are increased to N -points
by appending zeros to each block.

Since each data block is terminated with M − 1 zeros, the last M − 1 points
from each output block must be overlapped and added to the first M − 1 points of

(3.13)

(3.14)

(3.15)

(3.16)

The Discrete Fourier Transform: Its Properties and Applications

499



Figure 3.2
Linear FIR filtering by the
overlap-add method.

Input data

Output data

M − 1
zeros

L L L

x1(n)

y3(n)

y2(n)

y1(n)

x2(n)

x3(n)

M − 1
zeros

M − 1 points
add

together

M − 1 points
add

together

the succeeding block. Hence this method is called the overlap-add method. This
overlapping and adding yields the output sequence

y(n) = {y1(0), y1(1), . . . , y1(L− 1), y1(L)+ y2(0), y1(L+ 1)
+ y2(1), . . . , y1(N − 1)+ y2(M − 1), y2(M), . . .}

The segmentation of the input data into blocks and the fitting of the output data

4 Frequency Analysis of Signals Using the DFT

To compute the spectrum of either a continuous-time or discrete-time signal, the
values of the signal for all time are required. However, in practice, we observe
signals for only a finite duration. Consequently, the spectrum of a signal can only be

(3.17)

The Discrete Fourier Transform: Its Properties and Applications

blocks to form the output sequence are graphically illustrated in Fig. 3.2.
At this point, it may appear to the reader that the use of the DFT in linear FIR 

filtering not only is an indirect method of computing the output of an FIR filter, but also 
may be more expensive computationally, since the input data must first be converted to 
the frequency domain via the DFT, multiplied by the DFT of the FIR filter, and finally, 
converted back to the time domain via the IDFT. On the contrary, however, by using 
the fast Fourier transform algorithm, the DFTs and IDFT require fewer computations 
to compute the output sequence than the direct realization of the FIR filter in the time 
domain. This computational efficiency is the basic advantage of using the DFT to com-
pute the output of an FIR filter.

500



approximated from a finite data record. In this section we examine the implications
of a finite data record in frequency analysis using the DFT.

If the signal to be analyzed is an analog signal, we would first pass it through an
antialiasing filter and then sample it at a rate Fs ≥ 2B , where B is the bandwidth of
the filtered signal. Thus the highest frequency that is contained in the sampled signal
is Fs/2. Finally, for practical purposes, we limit the duration of the signal to the time
interval T0 = LT , where L is the number of samples and T is the sample interval.
As we shall observe in the following discussion, the finite observation interval for
the signal places a limit on the frequency resolution; that is, it limits our ability to
distinguish two frequency components that are separated by less than 1/T0 = 1/LT
in frequency.

Let {x(n)} denote the sequence to be analyzed. Limiting the duration of the
sequence to L samples, in the interval 0 ≤ n ≤ L − 1, is equivalent to multiplying
{x(n)} by a rectangular window w(n) of length L. That is,

x̂(n) = x(n)w(n)

where

w(n) =
{

1, 0 ≤ n ≤ L− 1
0, otherwise

Now suppose that the sequence x(n) consists of a single sinusoid, that is,

x(n) = cosω0n

Then the Fourier transform of the finite-duration sequence x(n) can be expressed as

X̂(ω) = 1
2

[W(ω − ω0)+W(ω + ω0)]

where W(ω) is the Fourier transform of the window sequence, which is (for the
rectangular window)

W(ω) = sin(ωL/2)
sin(ω/2)

e−jω(L−1)/2

To compute X̂(ω) we use the DFT. By padding the sequence x̂(n) with N −L zeros,
we can compute the N -point DFT of the truncated (L points) sequence {x̂(n)}. The
magnitude spectrum |X̂(k)| = |X̂(ωk)| for ωk = 2πk/N , k = 0, 1, . . . , N , is illustratedˆ
is not localized to a single frequency, but instead it is spread out over the whole
frequency range. Thus the power of the original signal sequence {x(n)} that was
concentrated at a single frequency has been spread by the window into the entire
frequency range. We say that the power has “leaked out” into the entire frequency
range. Consequently, this phenomenon, which is a characteristic of windowing the
signal, is called leakage.

(4.1)

(4.2)

(4.3)

(4.4)

(4.5)

The Discrete Fourier Transform: Its Properties and Applications

in Fig. 4.1 for L = 25 and N = 2048. We note that the windowed spectrum X(ω)

501



Figure 4.1
Magnitude spectrum for
L = 25 and N = 2048,
illustrating the occurrence
of leakage.

Windowing not only distorts the spectral estimate due to the leakage effects, it
also reduces spectral resolution. To illustrate this problem, let us consider a signal
sequence consisting of two frequency components,

x(n) = cosω1n+ cosω2n
When this sequence is truncated to L samples in the range 0 ≤ n ≤ L − 1, the
windowed spectrum is

X̂(ω) = 1
2

[W(ω − ω1)+W(ω − ω2)+W(ω + ω1)+W(ω + ω2)]

Figure 4.2 angular
window .

(4.6)

(4.7)

The Discrete Fourier Transform: Its Properties and Applications

Magnitude spectrum for the signal given by (4.8), as observed through a rect

502



The spectrum W(ω) of the rectangular window sequence has its first zero crossing
at ω = 2π/L. Now if |ω1 − ω2| < 2π/L, the two window functions W(ω − ω1)
and W(ω − ω2) overlap and, as a consequence, the two spectral lines in x(n) are
not distinguishable. Only if (ω1 − ω2) ≥ 2π/L will we see two separate lobes in the
spectrum X̂(ω). Thus our ability to resolve spectral lines of different frequencies
is limited by the window main lobe width.
spectrum |X̂(ω)|, computed via the DFT, for the sequence

x(n) = cosω0n+ cosω1n+ cosω2n

where ω0 = 0.2π , ω1 = 0.22π , and ω2 = 0.6π . The window lengths selected are
L = 25, 50, and 100. Note that ω0 and ω1 are not resolvable for L = 25 and 50, but
they are resolvable for L = 100.

w(n) =
{

1
2 (1− cos 2πL−1n), 0 ≤ n ≤ L− 1
0, otherwise

ˆ Its sidelobes
are significantly smaller than those of the rectangular window, but its main lobe is

after it is windowed by the Hanning window, for L = 50, 75, and 100. The reduction
of the sidelobes and the decrease in the resolution, compared with the rectangular
window, is clearly evident.

For a general signal sequence {x(n)}, the frequency-domain relationship be-
tween the windowed sequence x̂(n) and the original sequence x(n) is given by the
convolution formula

X̂(ω) = 1
2π

∫ π
−π
X(θ)W(ω − θ)dθ

Magnitude spectrum of the
Hanning window.

(4.9)

(4.10)

(4.8)

Figure 4.3

The Discrete Fourier Transform: Its Properties and Applications

Figure 4.2 illustrates the magnitude

Figure 4.3 shows |X(ω)| given by (4.4) for the window of (4.9).

approximately twice as wide. Figure 4.4 shows the spectrum of the signal in (4.8),

To reduce leakage, we can select a data window  that has lower sidelobes in the 
frequency domain compared with the rectangular window. However, a reduction of the 
sidelobes in a window  is obtained at the expense of an increase in the width of the 
main lobe of  and hence a loss in resolution. To illustrate this point, let us consider 
the Hanning window, which is specified as

w(n)

W(ω)

W(ω)

503



The DFT of the windowed sequence x̂(n) is the sampled version of the spectrum
X̂(ω). Thus we have

X̂(k) ≡ X̂(ω)|ω=2πk/N

= 1
2π

∫ π
−π
X(θ)W

(
2πk
N
− θ
)
dθ, k = 0, 1, . . . , N − 1

Just as in the case of the sinusoidal sequence, if the spectrum of the window is rel-
atively narrow in width compared to the spectrum X(ω) of the signal, the window
function has only a small (smoothing) effect on the spectrum X(ω). On the other
hand, if the window function has a wide spectrum compared to the width of X(ω),
as would be the case when the number of samples L is small, the window spectrum
masks the signal spectrum and, consequently, the DFT of the data reflects the spectral
characteristics of the window function. Of course, this situation should be avoided.

The exponential signal

xa(t) =
{
e−t , t ≥ 0
0, t < 0

is sampled at the rate Fs = 20 samples per second, and a block of 100 samples is used to

(4.11)

Figure 4.4

EXAMPLE 4.1

The Discrete Fourier Transform: Its Properties and Applications

Magnitude spectrum of the signal in (4.8) as observed through a Hanning window.

504



estimate its spectrum. Determine the spectral characteristics of the signal xa(t) by computing
the DFT of the finite-duration sequence. Compare the spectrum of the truncated discrete-time
signal to the spectrum of the analog signal.

Solution. The spectrum of the analog signal is

Xa(F ) = 11+ j2πF
The exponential analog signal sampled at the rate of 20 samples per second yields the sequence

x(n) = e−nT = e−n/20, n ≥ 0
= (e−1/20)n = (0.95)n, n ≥ 0

Now, let

x(n) =
{
(0.95)n, 0 ≤ n ≤ 99
0, otherwise

The N -point DFT of the L = 100 point sequence is

X̂(k) =
99∑
k=0

x̂(n)e−j2πk/N , k = 0, 1, . . . , N − 1

−10−20−30−40−50

0

1

0 10 20 30 40 50

1 2 3 4 5
0

0.2

0.4

0.6

0.8

1.0

1.2

(a)

(b)

xa(t) =   0,
e −t, t � 0

t � 0

1

1 + (2πF)2
√|Xa(F )| = |F | � 50 

t

F

,

Effect of windowing (truncating) the sampled version of theFigure 4.5
analog signal in Example 4.1

The Discrete Fourier Transform: Its Properties and Applications

505



(c)

(d)

(e)

 n
0

0 20 40 60 80 100 120 140 160 180 200

20 40 60 80 100

k

0 20 40 60 80 100 120 140 160 180 200
k

x(n) � xa(nT) =   0,
e −nT, n = 0, 1, …, 99

otherwise

X(k)

X(k)

 L = 100
N = 200

 L = 20
N = 200

∼

∼

To obtain sufficient detail in the spectrum we choose N = 200. This is equivalent to padding
the sequence x(n) with 100 zeros.

The graph of the analog signal xa(t) and its magnitude spectrum |Xa(F )| are illustrated

In this case the
DFT {X(k)} bears a close resemblance to the spectrum of the analog signal. The effect of the
window function is relatively small.

Figure 4.5 Continued

The Discrete Fourier Transform: Its Properties and Applications

in Fig. 4.5(a) and 4.5(b), respectively. The truncated sequence x(n) and its N = 200 point
DFT (magnitude) are illustrated in Fig. 4.5(c) and 4.5(d), respectively.

506



On the other hand, suppose that a window function of length L = 20 is selected. Then
the truncated sequence x(n) is given as

x̂(n) =
{
(0.95)n, 0 ≤ n ≤ 19
0, otherwise

Its N Now the effect of the wider spectral
window function is clearly evident. First, the main peak is very wide as a result of the wide
spectral window. Second, the sinusoidal envelope variations in the spectrum away from the
main peak are due to the large sidelobes of the rectangular window spectrum. Consequently,
the DFT is no longer a good approximation of the analog signal spectrum.

The Discrete Cosine Transform

The DFT represents an N -point sequence x(n), 0 ≤ n ≤ N − 1, as a linear com-
bination of complex exponentials. As a result, the DFT coefficients are, in general,
complex even if x(n) is real. Suppose that we wish to find an N×N orthogonal trans-
form that expresses a real sequence x(n) as a linear combination of cosine sequences.

is real and even, that is, x(n) = x(N − n), 0 ≤ n ≤ N − 1. The resulting DFT,
X(k), is itself real and even. This observation suggests that we could possibly derive
a discrete cosine transform for any N -point real sequence by taking the 2N -point
DFT of an “even extension” of the sequence. Because there are eight ways to do
this even extension, there are as many definitions of the DCT (Wang 1984, Martucci
1994). We discuss a version known as DCT-II, which is widely used in practice for
speech and image compression applications as part of various standards (Rao and
Huang, 1996). For simplicity, we will use the term DCT to refer to DCT-II.

Forward DCT

Let s(n) be a 2N -point even symmetric extension of x(n) defined by

s(n) =
{
x(n), 0 ≤ n ≤ N − 1
x(2N − n− 1), N ≤ n ≤ 2N − 1

The sequence s(n) has even symmetry about the “half-sample” point n = N +

S(k) =
2N−1∑
n=0

s(n)Wnk2N, 0 ≤ k ≤ 2N − 1

S(k) =
N−1∑
n=0

x(n)Wnk2N +
2N−1∑
n=N

x(2N − n− 1)Wnk2N

(5.1)

(5.2)

(5.3)

5

5.1

The Discrete Fourier Transform: Its Properties and Applications

= 200-point DFT is illustrated in Fig. 4.5(e).

From (2.25) and (2.26), we see that this is possible if the N -point sequence x(n)

(1/2) (see Figure 5.1). The 2N -point DFT of s(n) is given by

Substitution of (5.1) in (5.2) yields

507



Original sequence x(n),
0 ≤ n ≤ N − 1 and its
2N -point even extension
s(n), 0 ≤ n ≤ 2N − 1.

nN − 1

N − 1 2N − 1

0

(a)

x(n)

nN0

(b)

s(n) Center of
symmetry

If we change the second index of summation using n = 2N − 1 − m, we recall that
W 2mN2N = 1 for integer m, and we factor out W−k/22N , we obtain

S(k) = W−k/22N
N−1∑
n=0

x(n)
[
Wnk2NW

k/2
2N +W−nk2N W−k/22N

]
, 0 ≤ k ≤ 2N − 1

The last expression may be written as

S(k) = W−k/22N 2
N−1∑
n=0

x(n) cos
[
π

N

(
n+ 1

2

)
k

]
, 0 ≤ k ≤ 2N − 1

or equivalently

S(k) = W−k/22N 2
[
W
k/2
2N

N−1∑
n=0

x(n)Wkn2N

]
, 0 ≤ k ≤ 2N − 1

If we define the forward DCT by

V (k) = 2
N−1∑
n=0

x(n) cos
[
π

N

(
n+ 1

2

)
k

]
, 0 ≤ k ≤ N − 1

we can easily show that

V (k) = Wk/22N S(k) or S(k) = W−k/22N V (k), 0 ≤ k ≤ N − 1
and

V (k) = 2
[
W
k/2
2N

N−1∑
n=0

x(n)Wkn2N

]
, 0 ≤ k ≤ N − 1

(5.4)

(5.5)

(5.6)

(5.7)

(5.8)

(5.9)

Figure 5.1

The Discrete Fourier Transform: Its Properties and Applications

508



We note that V (k) is real and S(k) is complex. S(k) is complex because the real
sequence s(n) satisfies the symmetry relation s(2N−1−n) = s(n) instead of s(2N−
n) = s(n).

The DCT of x(n) can be computed by taking the 2N -point DFT of s(n), as in
k/2
2N

k/2
2N , and then take twice the real part.

5.2 Inverse DCT

We shall derive the inverse DCT from the inverse DFT of the even extended sequence
s(n). The inverse DFT of S(k) is given by

s(n) = 1
2N

2N−1∑
k=0

S(k)W−nk2N

Since s(n) is real, S(k) is Hermitian symmetric, that is,

S(2N − k) = S∗(k)

S(N) = 0

s(n) = 1
2N

N−1∑
k=0

S(k)W−kn2N +
1

2N

2N−1∑
k=N

S(k)W−kn2N

= 1
2N

N−1∑
k=0

S(k)W−kn2N +
1

2N

N∑
m=1

S(2N −m)W−(2N−m)n2N

= 1
2N

S(0)+ 1
2N

N−1∑
k=1

S(k)W−kn2N +
1

2N

N−1∑
k=1

S∗(k)Wkn2N

or since S(0) is real

s(n) = 1
N

[
S(0)

2
+
N−1∑
k=1

S(k)W−kn2N

]
, 0 ≤ n ≤ 2N − 1

x(n) = 1
N

{
V (0)

2
+
N−1∑
k=1

V (k) cos
[
π

N

(
n+ 1

2

)
k

]}
, 0 ≤ n ≤ N − 1

(5.10)

(5.11)

(5.12)

(5.13)

(5.14)

The Discrete Fourier Transform: Its Properties and Applications

, as in (5.8). Another approach, suggested(5.2), and multiplying the result by W

appended to it, multiply the result by W
by (5.9), is to take the 2N -point DFT of the original sequence x(n) with N zeros

Furthermore, from (5.7), it is easy to show that

With the help of (5.11) and (5.12), (5.10) yields

Substituting (5.8) in (5.13) and using (5.1) yields the desired inverse DCT

509



hence x(n).
An approach to compute the DCT and inverse DCT using an N -point DFT is

discussed in Makhoul (1980). Many special algorithms for the hardware and software
implementation of the DCT are discussed in Rao and Yip (1990).

5.3 DCT as an Orthogonal Transform

However, for reasons to be seen
later, we usually redistribute symmetrically the normalization factors between the
forward and inverse transform. Thus, the DCT of the sequence x(n), 0 ≤ n ≤ N − 1
and its inverse are defined by

C(k) = α(k)
N−1∑
n=0

x(n) cos
[
π(2n+ 1)k

2N

]
, 0 ≤ k ≤ N − 1

x(n) =
N−1∑
k=0

α(k)C(k) cos
[
π(2n+ 1)k

2N

]
, 0 ≤ n ≤ N − 1

where

α(0) =
√

1
N
, α(k) =

√
2
N

for 1 ≤ k ≤ N − 1

pressed in matrix form using the N ×N DCT matrix CN with elements given by

ckn =




1√
N
, k = 0, 0 ≤ n ≤ N − 1√

2
N

cos π(2n+ 1)k2N , 1 ≤ k ≤ N − 1, 0 ≤ n ≤ N − 1

If we define the following signal and coefficient vectors

xN = [ x(0) x(1) . . . x(N − 1) ]T

cN = [C(0) C(1) . . . C(N − 1) ]T

form as

cN = CNxN
xN = CTNcN

N is a real orthogonal matrix, that is, it
satisfies

C−1N = CTN

(5.15)

(5.16)

(5.17)

(5.18)

(5.19)

(5.20)

(5.21)

(5.22)

(5.23)

The Discrete Fourier Transform: Its Properties and Applications

Given V (k), we first compute S(k) by (5.8). In the next step, we take the 2N -point
inverse DFT implied by (5.13). The real part of this inverse DFT yields s(n) and,

Equations (5.7) and (5.14) form a DCT pair.

Like the DFT in Section 1.3, the DCT formulas (5.15) and (5.16) can be ex-

the forward DCT (5.15) and the inverse DCT (5.16) can be written in matrix

It follows from (5.19) and (5.20) that C

510



Orthogonality simplifies the computation of the inverse transform because it replaces
matrix inversion by matrix transposition.

If we denote by cN(k) the columns of CTN , the inverse DCT can be written as

xN =
N−1∑
k=0

C(k)cN(k)

which represents the signal as a linear combination of the DCT cosine basis se-
quences. The value of the coefficient C(k) measures the similarity of the signal with
the kth basis vector.

Consider the discrete-time sinusoidal signal

x(n) = cos(2πk0n/N), 0 ≤ n ≤ N − 1

0 = 5 and N = 32. We note that,
in contrast to the DFT, the DCT, although it shows a distinct peak at 2k0 , also exhibits a
significant amount of ripples at other frequencies. For this reason, the DCT is not useful for

0 5 10 15 20 25 30
−1

−0.5

0

0.5

1

x(
n)

n

0 5 10 15 20 25 30
0

5

10

15

|X
(k

)|

k

0 5 10 15 20 25 30
−1

0

1

2

3

C
(k

)

k

A discrete-time sinusoidal signal and its DFT and DCT representations.

(5.24)

X(k) coefficients, and the N -point DCT coefficients for k

frequency analysis of signals and systems.

EXAMPLE 5.1

Figure 5.2

The Discrete Fourier Transform: Its Properties and Applications

In Figure 5.2 we show plots of the sequence x(n), the absolute values of the N -point DFT

511



N−1∑
k=0
|C(k)|2 = cTNcN = xTNCTNCNxN = xTNxN =

N−1∑
n=0
|x(n)|2 = Ex

Thus, an orthogonal transformation preserves the signal energy, or equivalently, the
length of the vector x in the N -dimensional vector space (generalized Parseval’s
theorem). This means that every orthogonal transformation is simply a rotation of
the vector x in the N -dimensional vector space.

of the signal into a relatively few components of the transform coefficients (energy
compaction property). Since the total energy is preserved, many of the transform
coefficients will contain very little energy. However, as we illustrate in the next
example, different transforms have different energy compaction capabilities.

We will compare the energy compaction capabilities of the DFT and DCT using the ramp

coefficients, respectively. Clearly, the DCT coefficients demonstrate better “energy packing”
than the DFT ones. This implies that we should be able to represent the sequence x(n) using
a smaller number of DCT coefficients.

With the DCT we set the last k0 coefficients to zero and take the inverse DCT to obtain an
approximation xDCT(n) of the original sequence. However, since the DFT of a real sequence
is complex, the information is carried in the first N/2 values. (For simplicity, we assume that
N is an even number.) Therefore, we should remove DFT coefficients in a way that preserves
the complex-conjugate symmetry. This is done by first removing the coefficient X(N/2), then
the coefficients X(N/2 − 1) and X(N/2 + 1), and so forth. Clearly, we can remove only an
odd number of DFT coefficients, that is, k0 = 1, 3, . . . , N − 1. The reconstructed sequence
using the DFT is denoted by xDFT(n).

The reconstruction error for the DCT, which is a function of k0 , is defined by

EDCT(k0) = 1
N

N−1∑
n=0
|x(n)− xDCT(n)|2

the DFT and DCT as a function of the number k0
0 = 5 coefficients. We see that

original signal. In this example, the DFT (due to its inherent periodicity) is trying to model a
sawtooth wave. Therefore, it has to devote many high-frequency coefficients to approximate
the discontinuities at the ends. In contrast, the DCT operates on the “even extension” of
x(n), which is a triangular wave with no discontinuities. As a result, the DCT can approximate
better small blocks of signals that have fairly different values in the first and last samples.

(5.25)

EXAMPLE 5.2

The Discrete Fourier Transform: Its Properties and Applications

Most orthogonal transforms tend to pack a large fraction of the average energy

Using the orthogonality property (5.23), we can easily show that

sequence x(n) = n, 0 ≤ n ≤ N − 1, shown in Figure 5.3(a) for N = 32. Figures 5.3(d)
and 5.3(f) show the absolute values of the DFT coefficients and the values of the DCT

A similar definition is used for the DFT. Figure 5.3(b) shows the reconstruction errors for

we need fewer DCT coefficients than DFT coefficients to get a good approximation of the

of omitted coefficients. Figures 5.3(c) and
5.3(e) show the reconstructed signals when we retain N − k

512



kzero=27

kzero=27

0 10 20 30
0

10

20

30

x(
n)

x D
FT

(n
)

x D
C

T
(n

)

n Number of coefficients set to zero
5 10 15 20 25 30

50
100
150
200
250

M
SE DFT truncation error

DCT truncation error

0 10 20 30
0

200

400

|X
(k

)|

k
0 10 20 30

0

10

20

n

0 10 20 30
−50

0

50
C

(k
)

k
0 10 20 30

0

10

20

n

(a)

(c)

(e)

(b)

(d)

(f)

A discrete time sinusoidal signal and its DFT and DCT representations.

From a statistical viewpoint, the optimum orthogonal transform for signal com-
pression is the Karhunen–Loeve (KL) or Hotelling transform (Jayant and Noll, 1984).
The KL transform has two optimality properties: (a) it minimizes the reconstruction
error for any number of retained coefficients, and (b) it generates a set of uncorrelated
transform coefficients. The KL transform is defined by the eigenvectors of the covari-
ance matrix of the input sequence. The DCT provides a good approximation to the
KL transform for signals that follow the difference equation x(n) = ax(n−1)+w(n),
where w(n) is a white noise sequence, and a (0 < a < 1) is a constant coefficient
with values close to one. Many signals, including natural images, have this character-
istic. More details about orthogonal transforms and their applications can be found
in Jayant and Noll (1984), Clarke (1985), Rao and Yip (1990), and Goyal (2001).

6 Summary and References

The major focus of this chapter was on the discrete Fourier transform, its properties
and its applications. We developed the DFT by sampling the spectrum X(ω) of the
sequence x(n).

Frequency-domain sampling of the spectrum of a discrete-time signal is partic-
ularly important in the processing of digital signals. Of particular significance is
the DFT, which was shown to uniquely represent a finite-duration sequence in the
frequency domain. The existence of computationally efficient algorithms for the

Figure 5.3

The Discrete Fourier Transform: Its Properties and Applications

513



We also described the discrete cosine transform (DCT) in this chapter. An
interesting treatment of the DCT from a linear algebra perspective is given in a
paper by Strang (1999).

Problems

1 The first five points of the eight-point DFT of a real-valued sequence are { 0.25,
0.125− j0.3018, 0, 0.125− j0.0518, 0 }. Determine the remaining three points.

2 Compute the eight-point circular convolution for the following sequences.
(a) x1(n) = {1, 1, 1, 1, 0, 0, 0, 0}

x2(n) = sin 3π8 n, 0 ≤ n ≤ 7

(b) x1(n) = (14 )
n, 0 ≤ n ≤ 7

x2(n) = cos 3π8 n, 0 ≤ n ≤ 7
(c) Compute the DFT of the two circular convolution sequences using the DFTs of

x1(n) and x2(n).
3 Let X(k), 0 ≤ k ≤ N − 1, be the N -point DFT of the sequence x(n), 0 ≤ n ≤ N − 1.

We define

X̂(k) =
{
X(k), 0 ≤ k ≤ kc,N − kc ≤ k ≤ N − 1
0, kc < k < N − kc

and we compute the inverse N -point DFT of X̂(k), 0 ≤ k ≤ N − 1. What is the
effect of this process on the sequence x(n)? Explain.

4 For the sequences

x1(n) = cos 2π
N
n, x2(n) = sin 2π

N
n, 0 ≤ n ≤ N − 1

determine the N -point:
(a) Circular convolution x1(n)©N x2(n)
(b) Circular correlation of x1(n) and x2(n)
(c) Circular autocorrelation of x1(n)
(d) Circular autocorrelation of x2(n)

5 Compute the quantity
N−1∑
n=0

x1(n)x2(n)

for the following pairs of sequences.

The Discrete Fourier Transform: Its Properties and Applications

DFT make it possible to digitally process signals in the frequency domain much faster 
than in the time domain. The processing methods in which the DFT is especially suit-
able include linear filtering as described in this chapter and correlation and spectrum 
analysis. A particularly lucid and concise treatment of the DFT and its application to 
frequency analysis is given in the book by Brigham (1988).

514



(a) x1(n) = x2(n) = cos 2π
N
n, 0 ≤ n ≤ N − 1

(b) x1(n) = cos 2π
N
n, x2(n) = sin 2π

N
n, 0 ≤ n ≤ N − 1

(c) x1(n) = δ(n)+ δ(n− 8), x2(n) = u(n)− u(n−N)
6 Determine the N -point DFT of the Blackman window

w(n) = 0.42 − 0.5 cos 2πn
N − 1 + 0.08 cos

4πn
N − 1 , 0 ≤ n ≤ N − 1

7 If X(k) is the DFT of the sequence x(n), determine the N -point DFTs of the se-
quences

xc(n) = x(n) cos 2πk0n
N

, 0 ≤ n ≤ N − 1

and

xs(n) = x(n) sin 2πk0n
N

, 0 ≤ n ≤ N − 1

in terms of X(k).
8 Determine the circular convolution of the sequences

x1(n) = {1↑, 2, 3, 1}

x2(n) = {4↑, 3, 2, 2}

9 Use the four-point DFT and IDFT to determine the sequence

x3(n) = x1(n)©N x2(n)

1 2

10 Compute the energy of the N -point sequence

x(n) = cos 2πk0n
N

, 0 ≤ n ≤ N − 1

11 Given the eight-point DFT of the sequence

x(n) =
{

1, 0 ≤ n ≤ 3
0, 4 ≤ n ≤ 7

compute the DFT of the sequences

(a) x1(n) =
{ 1, n = 0

0, 1 ≤ n ≤ 4
1, 5 ≤ n ≤ 7

(b) x2(n) =
{ 0, 0 ≤ n ≤ 1

1, 2 ≤ n ≤ 5
0, 6 ≤ n ≤ 7

The Discrete Fourier Transform: Its Properties and Applications

using the time-domain formula in (2.39).

where x (n) and x (n) are the sequence given in Problem 8.

515



12 Consider a finite-duration sequence

x(n) = {0
↑
, 1, 2, 3, 4}

(a) Sketch the sequence s(n) with six-point DFT

S(k) = W ∗2X(k), k = 0, 1, . . . , 6

(b) Determine the sequence y(n) with six-point DFT Y (k) = |X(k)|.
(c) Determine the sequence v(n) with six-point DFT V (k) = �|X(k)|.

13 Let xp(n) be a periodic sequence with fundamental period N . Consider the following
DFTs:

xp(n)
DFT←→
N

X1(k)

xp(n)
DFT←→
3N

X3(k)

(a) What is the relationship between X1(k) and X3(k)?

(b) Verify the result in part (a) using the sequence

xp(n) = {· · · 1, 2, 1, 2↑, 1, 2, 1, 2 · · ·}

14 Consider the sequences

x1(n) = {0↑, 1, 2, 3, 4}, x2(n) = {0↑, 1, 0, 0, 0}, s(n) = {1↑, 0, 0, 0, 0}

and their five-point DFTs.

(a) Determine a sequence y(n) so that Y (k) = X1(k)X2(k).
(b) Is there a sequence x3(n) such that S(k) = X1(k)X3(k)?

15 Consider a causal LTI system with system function

H(z) = 1
1− 0.5z−1

The output y(n) of the system is known for 0 ≤ n ≤ 63. Assuming that H(z) is
available, can you develop a 64-point DFT method to recover the sequence x(n),
0 ≤ n ≤ 63? Can you recover all values of x(n) in this interval?

16 The impulse response of an LTI system is given by h(n) = δ(n) − 14δ(n − k0). To
determine the impulse response g(n) of the inverse system, an engineer computes
the N -point DFT H(k), N = 4k0 , of h(n) and then defines g(n) as the inverse DFT
of G(k) = 1/H(k), k = 0, 1, 2, . . . , N − 1. Determine g(n) and the convolution
h(n)× g(n), and comment on whether the system with impulse response g(n) is the
inverse of the system with impulse response h(n).

The Discrete Fourier Transform: Its Properties and Applications

516



17 Determine the eight-point DFT of the signal

x(n) = {1, 1, 1, 1, 1, 1, 0, 0}

and sketch its magnitude and phase.
18 A linear time-invariant system with frequency response H(ω) is excited with the

periodic input

x(n) =
∞∑

k=−∞
δ(n− kN)

Suppose that we compute the N -point DFT Y (k) of the samples y(n), 0 ≤ n ≤ N−1
of the output sequence. How is Y (k) related to H(ω)?

19 DFT of real sequences with special symmetries

properties), explain how we can compute the DFT of two real symmetric (even)
and two real antisymmetric (odd) sequences simultaneously using an N -point
DFT only.

(b) Suppose now that we are given four real sequences xi(n), i = 1, 2, 3, 4, that are
all symmetric [i.e., xi(n) = xi(N − n), 0 ≤ n ≤ N − 1]. Show that the sequences

si(n) = xi(n+ 1)− xi(n− 1)

are antisymmetric [i.e., si(n) = −si(N − n) and si(0) = 0].
(c) Form a sequence x(n) using x1(n), x2(n), s3(n), and s4(n) and show how to

compute the DFT Xi(k) of xi(n), i = 1, 2, 3, 4 from the N -point DFT X(k) of
x(n).

(d) Are there any frequency samples of Xi(k) that cannot be recovered from X(k)?
Explain.

20 DFT of real sequences with odd harmonics only Let x(n) be an N -point real se-
quence with N -point DFT X(k) (N even). In addition, x(n) satisfies the following
symmetry property:

x

(
n+ N

2

)
= −x(n), n = 0, 1, . . . , N

2
− 1

that is, the upper half of the sequence is the negative of the lower half.

(a) Show that
X(k) = 0, k even

that is, the sequence has a spectrum with odd harmonics.

(b) Show that the values of this odd-harmonic spectrum can be computed by evaluat-
ing the N /2-point DFT of a complex modulated version of the original sequence
x(n).

The Discrete Fourier Transform: Its Properties and Applications

(a) Using the symmetry properties of Section 2 (especially the decomposition

517



21 Let xa(t) be an analog signal with bandwidth B = 3 kHz. We wish to use an N = 2m -
point DFT to compute the spectrum of the signal with a resolution less than or equal
to 50 Hz. Determine (a) the minimum sampling rate, (b) the minimum number of
required samples, and (c) the minimum length of the analog signal record.

22 Consider the periodic sequence

xp(n) = cos 2π10 n, −∞ < n <∞

with frequency f0 = 110 and fundamental period N = 10. Determine the 10-point
DFT of the sequence x(n) = xp(n), 0 ≤ n ≤ N − 1.

23 Compute the N -point DFTs of the signals
(a) x(n) = δ(n)
(b) x(n) = δ(n− n0), 0 < n0 < N
(c) x(n) = an, 0 ≤ n ≤ N − 1
(d) x(n) =

{
1, 0 ≤ n ≤ N/2− 1 (N even)
0, N/2 ≤ n ≤ N − 1

(e) x(n) = ej (2π/N)k0, 0 ≤ n ≤ N − 1
(f) x(n) = cos 2π

N
k0n, 0 ≤ n ≤ N − 1

(g) x(n) = sin 2n
N
k0n, 0 ≤ n ≤ N − 1

(h) x(n) =
{

1, n even
0, n odd, 0 ≤ n ≤ N − 1

24 Consider the finite-duration signal

x(n) = {1, 2, 3, 1}
(a) Compute its four-point DFT by solving explicitly the 4-by-4 system of linear

equations defined by the inverse DFT formula.
(b) Check the answer in part (a) by computing the four-point DFT, using its defini-

tion.
25 (a) Determine the Fourier transform X(ω) of the signal

x(n) = {1, 2, 3
↑
, 2, 1, 0}

(b) Compute the six-point DFT V (k) of the signal

v(n) = {3, 2, 1, 0, 1, 2}
(c) Is there any relation between X(ω) and V (k)? Explain.

26 Prove the identity
∞∑

l=−∞
δ(n+ lN) = 1

N

N−1∑
k=0

ej (2π/N)kn

(Hint: Find the DFT of the periodic signal in the left-hand side.)

The Discrete Fourier Transform: Its Properties and Applications

518



27 Computation of the even and odd harmonics using the DFT. Let x(n) be an N -point
sequence with an N -point DFT X(k) (N even).
(a) Consider the time-aliased sequence

y(n) =




∞∑
l=−∞

x(n+ lM), 0 ≤ n ≤ M − 1

0, elsewhere

What is the relationship between the M -point DFT Y (k) of y(n) and the Fourier
transform X(ω) of x(n)?

(b) Let

y(n) =

 x(n)+ x

(
n+ N

2

)
, 0 ≤ n ≤ N − 1

0, elsewhere

and
y(n)

DFT←→
N/2

Y (k)

Show that X(k) = Y (k/2), k = 2, 4, . . . , N − 2.
(c) Use the results in parts (a) and (b) to develop a procedure that computes the

odd harmonics of X(k) using an N /2-point DFT.
28 Frequency-domain sampling. Consider the following discrete-time signal

x(n) =
{
a|n|, |n| ≤ L
0, |n| > L

where a = 0.95 and L = 10.
(a) Compute and plot the signal x(n).
(b) Show that

X(ω) =
∞∑

n=−∞
x(n)e−jωn = x(0)+ 2

L∑
n−1

x(n) cosωn

Plot X(ω) by computing it at ω = πk/100, k = 0, 1, . . . , 100.
(c) Compute

ck = 1
N
X

(
2π
N
K

)
, k = 0, 1, . . . , N − 1

for N = 30.
(d) Determine and plot the signal

x̃(n) =
N−1∑
k=0

ckej (2π/N)kn

What is the relation between the signals x(n) and x̃(n)? Explain.
(e) Compute and plot the signal x̃1(n) =

∑∞
l=−∞ x(n−lN), −L ≤ n ≤ L for N = 30.

Compare the signals x̃(n) and x̃1(n).
(f) Repeat parts (c) to (e) for N = 15.

The Discrete Fourier Transform: Its Properties and Applications

519



29 Frequency-domain sampling The signal x(n) = a|n| , −1 < a < 1 has a Fourier
transform

X(ω) = 1− a
2

1− 2a cosω + a2

(a) Plot X(ω) for 0 ≤ ω ≤ 2π , a = 0.8. Reconstruct and plot X(ω) from its samples
X(2πk/N), 0 ≤ k ≤ N − 1 for

(b) N = 20
(c) N = 100
(d) Compare the spectra obtained in parts (b) and (c) with the original spectrum

X(ω) and explain the differences.

(e) Illustrate the time-domain aliasing when N = 20.
30 Frequency analysis of amplitude-modulated discrete-time signal The discrete-time

signal

x(n) = cos 2πf1n+ cos 2πf2n

where f1 = 118 and f2 = 5128 , modulates the amplitude of the carrier

xc(n) = cos 2πfcn

where fc = 50128 . The resulting amplitude-modulated signal is

xam(n) = x(n) cos 2πfcn

(a) Sketch the signals x(n), xc(n), and xam(n), 0 ≤ n ≤ 255.
(b) Compute and sketch the 128-point DFT of the signal xam(n), 0 ≤ n ≤ 127.
(c) Compute and sketch the 128-point DFT of the signal xam(n), 0 ≤ n ≤ 99.
(d) Compute and sketch the 256-point DFT of the signal xam(n), 0 ≤ n ≤ 179.
(e) Explain the results obtained in parts (b) through (d), by deriving the spectrum of

the amplitude-modulated signal and comparing it with the experimental results.

31
series as

x(t) = 2
π

(
sinπt − 1

2
sin 2πt + 1

3
sin 3πt − 1

4
sin 4πt · · ·

)

(a) Determine the Fourier series coefficients ck .

(b) Use an N -point subroutine to generate samples of this signal in the time domain
using the first six terms of the expansion for N = 64 and N = 128. Plot the
signal x(t) and the samples generated, and comment on the results.

The Discrete Fourier Transform: Its Properties and Applications

The sawtooth waveform in Fig. P31 can be expressed in the form of a Fourier

520



−1

−1

1

0 1 2 3 4

x(t)

t

32 Recall that the Fourier transform of x(t) = ej�0t is X(j�) = 2πδ(� − �0) and the
Fourier transform of

p(t) =
{

1, 0 ≤ t ≤ T0
0, otherwise

is

P(j�) = T0 sin�T0/2
�T0/2

e−j�T0/2

(a) Determine the Fourier transform Y (j�) of

y(t) = p(t)ej�0t

and roughly sketch |Y (j�)| versus �.
(b) Now consider the exponential sequence

x(n) = ejω0n

where ω0 is some arbitrary frequency in the range 0 < ω0 < π radians. Give
the most general condition that ω0 must satisfy in order for x(n) to be periodic
with period P (P is a positive integer).

(c) Let y(n) be the finite-duration sequence

y(n) = x(n)wN(n) = ejω0nwN(n)
where wN(n) is a finite-duration rectangular sequence of length N and where
x(n) is not necessarily periodic. Determine Y (ω) and roughly sketch |Y (ω)| for
0 ≤ ω ≤ 2π . What effect does N have in |Y (ω)|? Briefly comment on the
similarities and differences between |Y (ω)| and |Y (j�)|.

(d) Suppose that

x(n) = ej (2π/P )n, P a positive integer
and

y(n) = wN(n)x(n)
where N = lP , l a positive integer. Determine and sketch the N -point DFT of
y(n). Relate your answer to the characteristics of |Y (ω)|.

(e) Is the frequency sampling for the DFT in part (d) adequate for obtaining a
rough approximation of |Y (ω)| directly from the magnitude of the DFT sequence
|Y (k)|? If not, explain briefly how the sampling can be increased so that it will be
possible to obtain a rough sketch of |Y (ω)| from an appropriate sequence |Y (k)|.

Figure P31

The Discrete Fourier Transform: Its Properties and Applications

521



33 Develop an algorithm that computes the DCT using the DFT as described in Sec-

34

35 = an cos(2πf0n + φ) with a = 0.8,
f0 = 0.05, and N = 32.

The Discrete Fourier Transform: Its Properties and Applications

tions 5.1 and 5.2.
Use the algorithm developed in Problem 33 to reproduce the results in Exam-
ple 5.2.
Repeat Example 5.2 using the signal x(n)

1 Since x(n) is real, the real part of the DFT is even, imaginary part odd. Thus, the remaining
points are {0.125+ j0.0518, 0, 0.125+ j0.3018}

2 (a) x̃2(l) = sin
(

3π
8

) |l|, |l| ≤ 7
Therefore, x1(n) 8 x2(n) = {1.25, 2.55, 2.55, 1.25, 0.25,−1.06,−1.06, 0.25}

(c) R̃xx(k) = X1(k)X∗1(k) = N
2

4 [δ(k − 1)+ δ(k + 1)]
⇒ r̃xx(n) = N2 cos

(
2π
N
n
)

(d) R̃yy(k) = X2(k)X∗2 (k) = N
2

4 [δ(k − 1) + δ(k + 1)]
⇒ r̃yy(n) = N2 cos

(
2π
N
n
)

5 (a)
∑N−1

n=0 x1(n)x
∗
2 (n) = 14

∑N−1
n=0

(
ej

2π
N
n + e−j 2πN n

)2
= N2

9 X3(k) = {17, 19, 22, 19}
12 (a) s(k) = Wk2X(k)

s(n) = {3, 4, 0, 0, 1, 2}
14 (a) y(n) = x1(n) 5 x2(n) = {4, 0, 1, 2, 3}
21 (a) Fs ≡ FN = 2B = 6000 samples/sec

(b) L = 120 samples
(c) LT = 16000 × 120 = 0.02 seconds

23 (a) X(k) =∑N−1n=0 δ(n)e−j 2πN kn = 1, 0 ≤ k ≤ N − 1
(e) X(k) = Nδ(k − k0)
(h) X(k) = 1

1−e−j
2π
N
k

25 (a) X(w) = 3+ 2 cos(2w)+ 4 cos(4w)
31 (a) ck =

(
2
π
,− 1

π
, 23π ,− 12π · · ·

)
32 (a) Y (j	) = T0 sin c

(
T0(	−	0)

2

)
e−j

T0 (	−	0 )
2

(c) Y (w) = sin N2 (w−w0)
sin

w−w0
2

e−j
N−1

2 (w−w0)

Answers to Selected Problems

522



Efficient Computation of
the DFT: Fast Fourier
Transform Algorithms

The main topic of this chapter is the description of computationally efficient
algorithms for evaluating the DFT. Two different approaches are described. One is
a divide-and-conquer approach in which a DFT of size N , where N is a composite
number, is reduced to the computation of smaller DFTs from which the larger DFT
is computed. In particular, we present important computational algorithms, called
fast Fourier transform (FFT) algorithms, for computing the DFT when the size N is
a power of 2 and when it is a power of 4.

The second approach is based on the formulation of the DFT as a linear filtering
operation on the data. This approach leads to two algorithms, the Goertzel algorithm
and the chirp-z transform algorithm, for computing the DFT via linear filtering of
the data sequence.

1 Efficient Computation of the DFT: FFT Algorithms

In this section we present several methods for computing the DFT efficiently. In view
of the importance of the DFT in various digital signal processing applications, such as
linear filtering, correlation analysis, and spectrum analysis, its efficient computation is
a topic that has received considerable attention by many mathematicians, engineers,
and applied scientists.

Basically, the computational problem for the DFT is to compute the sequence
{X(k)} ofN complex-valued numbers given another sequence of data {x(n)} of length

The discrete fourier transform (DFT) plays an important role in many applications of 
digital signal processing, including linear filtering, correlation analysis, and spectrum 
analysis. A major reason for its importance is the existence of efficient algorithms for 
computing the DFT.

John G. Proakis, Dimitris G. Manolakis. Copyright © 2007 by Pearson Education, Inc. All rights reserved.
From Chapter 8   of Digital Signal Processing: Principles, Algorithms, and Applications, Fourth Edition.

523



N , according to the formula

X(k) =
N−1∑
n=0

x(n)WknN , 0 ≤ k ≤ N − 1

where
WN = e−j2π/N

In general, the data sequence x(n) is also assumed to be complex valued.
Similarly, the IDFT becomes

x(n) = 1
N

N−1∑
k=0

X(k)W−nkN , 0 ≤ n ≤ N − 1

Since the DFT and IDFT involve basically the same type of computations, our discus-
sion of efficient computational algorithms for the DFT applies as well to the efficient
computation of the IDFT.

We observe that for each value of k , direct computation of X(k) involves N
complex multiplications (4N real multiplications) andN−1 complex additions (4N−
2 real additions). Consequently, to compute all N values of the DFT requires N2

complex multiplications and N2 −N complex additions.
Direct computation of the DFT is basically inefficient, primarily because it does

not exploit the symmetry and periodicity properties of the phase factor WN . In
particular, these two properties are:

Symmetry property: Wk+N/2N = −WkN
Periodicity property: Wk+NN = WkN

The computationally efficient algorithms described in this section, known collectively

phase factor.

For a complex-valued sequence x(n) of N points, the DFT may be expressed as

XR(k) =
N−1∑
n=0

[
xR(n) cos

2πkn
N
+ xI (n) sin 2πkn

N

]

XI(k) = −
N−1∑
n=0

[
xR(n) sin

2πkn
N
− xI (n) cos 2πkn

N

]

1. 2N2 evaluations of trigonometric functions.

2. 4N2 real multiplications.
3. 4N(N − 1) real additions.
4. A number of indexing and addressing operations.

(1.1)

(1.2)

(1.3)

(1.4)

(1.5)

(1.6)

(1.7)

Direct Computation of the DFT

as fast Fourier transform (FFT) algorithms, exploit these two basic properties of the

1.1

The direct computation of (1.6) and (1.7) requires:

Efficient Computation of the DFT: Fast Fourier Transform Algorithms

524



These operations are typical of DFT computational algorithms. The operations in
items 2 and 3 result in the DFT valuesXR(k) and XI(k). The indexing and addressing
operations are necessary to fetch the data x(n), 0 ≤ n ≤ N−1, and the phase factors
and to store the results. The variety of DFT algorithms optimize each of these
computational processes in a different way.

1.2 Divide-and-Conquer Approach to Computation of the DFT

The development of computationally efficient algorithms for the DFT is made pos-
sible if we adopt a divide-and-conquer approach. This approach is based on the
decomposition of an N -point DFT into successively smaller DFTs. This basic ap-
proach leads to a family of computationally efficient algorithms known collectively
as FFT algorithms.

To illustrate the basic notions, let us consider the computation of an N -point
DFT, where N can be factored as a product of two integers, that is,

N = LM

The assumption that N is not a prime number is not restrictive, since we can pad any

Now the sequence x(n), 0 ≤ n ≤ N−1, can be stored either in a one-dimensional
array indexed by n or as a two-dimensional array indexed by l and m, where 0 ≤

and m is the column index. Thus, the sequence x(n) can be stored in a rectangular

x(0) x(1) x(2) x(N − 1)

N − 1

0

0 1

1lrow index

column indexm

0

1

2

x(0, 0) x(0, 1)

…

…

…

…

…

x(1, 1)

x(2, 1)

x(1, 0)

x(2, 0)

L − 1

M − 1

(a)

(b)

n

… … … …
…

…

Figure 1.1 Two dimensional data array for storing the sequence
x(n), 0 ≤ n ≤ N − 1.

(1.8)

sequence with zeros to ensure a factorization of the form (1.8).

l ≤ L− 1 and 0 ≤ m ≤ M − 1 as illustrated in Fig. 1.1. Note that l is the row index

Efficient Computation of the DFT: Fast Fourier Transform Algorithms

525



array in a variety of ways, each of which depends on the mapping of index n to the
indexes (l, m).

For example, suppose that we select the mapping

n = Ml +m

This leads to an arrangement in which the first row consists of the first M elements of
x(n), the second row consists of the nextM elements of x(n), and so on, as illustrated

n = l +mL

stores the first L elements of x(n) in the first column, the next L elements in the

0 1 2l

Row-wise
m

0

1

2

x(1) x(2)

x(M + 2)

x(2M + 2)

x(2M − 1)

x(3M − 1)

x(M + 1)

x(2M + 1)

x(0)

x(M)

x(2M)

x((L − 1)M) x((L −1)M +1) x((L −1)M +2) x(LM − 1)

x(M − 1)

L − 1

M − 1

(a)

n = Ml + m

0 1 2l

Column-wise
m

0

1

2

x(L) x(2L)

x(2L + 1)

x(2L + 2)

x((M − 1)L +1)

x((M − 1)L + 2)

x(L + 1)

x(L + 2)

x(0)

x(1)

x(2)

x(L − 1) x(2L − 1) x(3L − 1) x(LM − 1)

x((M − 1)L)

L−1

M − 1

(b)

n = l + mL

…

…

…

…

…

…

…

…

…

…

… … … …

… … … …

Two arrangements for the data arrays.

(1.9)

(1.10)

in Fig. 1.2(a). On the other hand, the mapping

second column, and so on, as illustrated in Fig. 1.2(b).

Figure 1.2

Efficient Computation of the DFT: Fast Fourier Transform Algorithms

526



A similar arrangement can be used to store the computed DFT values. In partic-
ular, the mapping is from the index k to a pair of indices (p, q), where 0 ≤ p ≤ L−1
and 0 ≤ q ≤ M − 1. If we select the mapping

k = Mp + q

the DFT is stored on a row-wise basis, where the first row contains the first M ele-
ments of the DFT X(k), the second row contains the next set of M elements, and so
on. On the other hand, the mapping

k = qL+ p

results in a column-wise storage of X(k), where the first L elements are stored in
the first column, the second set of L elements are stored in the second column, and
so on.

Now suppose that x(n) is mapped into the rectangular array x(l,m) and X(k)
is mapped into a corresponding rectangular array X(p, q). Then the DFT can be
expressed as a double sum over the elements of the rectangular array multiplied by
the corresponding phase factors. To be specific, let us adopt a column-wise mapping

Then

X(p, q) =
M−1∑
m=0

L−1∑
l=0

x(l,m)W
(Mp+q)(mL+l)
N

But
W
(Mp+q)(mL+l)
N = WMLmpN WmLqN WMplN WlqN

However, WNmpN = 1, WmqLN = WmqN/L = WmqM , and WMplN = WplN/M = WplL .

X(p, q) =
L−1∑
l=0

{
W
lq

N

[
M−1∑
m=0

x(l,m)W
mq

M

]}
W
lp

L

L. To elaborate, let us subdivide the computation into three steps:

1. First, we compute the M -point DFTs

F(l, q) ≡
M−1∑
m=0

x(l,m)W
mq

M , 0 ≤ q ≤ M − 1

for each of the rows l = 0, 1, . . . , L− 1.
2. Second, we compute a new rectangular array G(l, q) defined as

G(l, q) = WlqN F(l, q),
0 ≤ l ≤ L− 1
0 ≤ q ≤ M − 1

(1.11)

(1.12)

(1.13)

(1.14)

(1.15)

(1.16)

(1.17)

for x(n) given by (1.10) and the row-wise mapping for the DFT given by (1.11).

With these simplications, (1.13) can be expressed as

The expression in (1.15) involves the computation of DFTs of length M and length

Efficient Computation of the DFT: Fast Fourier Transform Algorithms

527



3. Finally, we compute the L-point DFTs

X(p, q) =
L−1∑
l=0

G(l, q)W
lp
L

for each column q = 0, 1, . . . ,M − 1, of the array G(l, q).
On the surface it may appear that the computational procedure outlined above is
more complex than the direct computation of the DFT. However, let us evaluate

The first step involves the computation
of L DFTs, each of M points. Hence this step requires LM2 complex multiplica-
tions and LM(M − 1) complex additions. The second step requires LM complex
multiplications. Finally, the third step in the computation requires ML2 complex
multiplications and ML(L − 1) complex additions. Therefore, the computational
complexity is

Complex multiplications: N(M + L+ 1)
Complex additions: N(M + L− 2)

where N = ML. Thus the number of multiplications has been reduced from N2 to
N(M + L + 1) and the number of additions has been reduced from N(N − 1) to
N(M + L− 2).

For example, suppose that N = 1000 and we select L = 2 and M = 500. Then,
instead of having to perform 106 complex multiplications via direct computation of
the DFT, this approach leads to 503,000 complex multiplications. This represents a
reduction by approximately a factor of 2. The number of additions is also reduced
by about a factor of 2.

When N is a highly composite number, that is, N can be factored into a product
of prime numbers of the form

N = r1r2 · · · rν
then the decomposition above can be repeated (ν − 1) more times. This proce-
dure results in smaller DFTs, which, in turn, leads to a more efficient computational
algorithm.

In effect, the first segmentation of the sequence x(n) into a rectangular array
of M columns with L elements in each column resulted in DFTs of sizes L and
M . Further decomposition of the data in effect involves the segmentation of each
row (or column) into smaller rectangular arrays which result in smaller DFTs. This
procedure terminates when N is factored into its prime factors.

EXAMPLE 1.1

To illustrate this computational procedure, let us consider the computation of an N = 15 point
DFT. Since N = 5×3 = 15, we select L = 5 and M = 3. In other words, we store the 15-point
sequence x(n) column-wise as follows:

Row 1: x(0, 0) = x(0) x(0, 1) = x(5) x(0, 2) = x(10)
Row 2: x(1, 0) = x(1) x(1, 1) = x(6) x(1, 2) = x(11)
Row 3: x(2, 0) = x(2) x(2, 1) = x(7) x(2, 2) = x(12)
Row 4: x(3, 0) = x(3) x(3, 1) = x(8) x(3, 2) = x(13)
Row 5: x(4, 0) = x(4) x(4, 1) = x(9) x(4, 2) = x(14)

(1.18)

(1.19)

(1.20)

the computational complexity of (1.15).

Efficient Computation of the DFT: Fast Fourier Transform Algorithms

528



Now, we compute the three-point DFTs for each of the five rows. This leads to the
following 5× 3 array:

F(0, 0) F (0, 1) F (0, 2)
F (1, 0) F (1, 1) F (1, 2)
F (2, 0) F (2, 1) F (2, 2)
F (3, 0) F (3, 1) F (3, 2)
F (4, 0) F (4, 1) F (4, 2)

The next step is to multiply each of the terms F(l, q) by the phase factors WlqN = Wlq15 ,
0 ≤ l ≤ 4 and 0 ≤ q ≤ 2. This computation results in the 5× 3 array:

Column 1 Column 2 Column 3
G(0, 0) G(0, 1) G(0, 2)
G(1, 0) G(1, 1) G(1, 2)
G(2, 0) G(2, 1) G(2, 2)
G(3, 0) G(3, 1) G(3, 2)
G(4, 0) G(4, 1) G(4, 2)

The final step is to compute the five-point DFTs for each of the three columns. This
computation yields the desired values of the DFT in the form

X(0, 0) = X(0) X(0, 1) = X(1) X(0, 2) = X(2)
X(1, 0) = X(3) X(1, 1) = X(4) X(1, 2) = X(5)
X(2, 0) = X(6) X(2, 1) = X(7) X(2, 2) = X(8)
X(3, 0) = X(9) X(3, 1) = X(10) X(3, 2) = X(11)
X(4, 0) = X(12) X(4, 1) = X(13) X(4, 2) = X(14)

Figure 1.3 illustrates the steps in the computation.
It is interesting to view the segmented data sequence and the resulting DFT in terms

of one-dimensional arrays. When the input sequence x(n) and the output DFT X(k) in the
two-dimensional arrays are read across from row 1 through row 5, we obtain the following
sequences:

INPUT ARRAY
x(0) x(5) x(10) x(1) x(6) x(11) x(2) x(7) x(12) x(3) x(8) x(13) x(4) x(9) x(14)

OUTPUT ARRAY
X(0) X(1) X(2) X(3) X(4) X(5) X(6) X(7) X(8) X(9) X(10) X(11) X(12) X(13) X(14)

0
5

10 x(10)
x(5)

x(0)

x(1)

x(2)

x(3)

x(4)

X(2)

X(5)

X(8)

X(11)

X(14)

X(1)

X(0)

W

X(3)

X(6)

X(9)

X(12)

1
6

11

2
7

12

3
8

13

4
9

14

lq
15

3-P
oin

t D
FT

5-
Po

in
t D

FT

Figure 1.3 Computation of N = 15-point DFT by means of 3-point and 5-point DFTs.

Efficient Computation of the DFT: Fast Fourier Transform Algorithms

529



We observe that the input data sequence is shuffled from the normal order in the compu-
tation of the DFT. On the other hand, the output sequence occurs in normal order. In this case
the rearrangement of the input data array is due to the segmentation of the one-dimensional
array into a rectangular array and the order in which the DFTs are computed. This shuffling
of either the input data sequence or the output DFT sequence is a characteristic of most FFT
algorithms.

To summarize, the algorithm that we have introduced involves the following
computations:

Algorithm 1

1. Store the signal column-wise.

2. Compute the M -point DFT of each row.

3. Multiply the resulting array by the phase factors WlqN .

4. Compute the L-point DFT of each column

5. Read the resulting array row-wise.

An additional algorithm with a similar computational structure can be obtained if
the input signal is stored row-wise and the resulting transformation is column-wise.
In this case we select

n = Ml +m
k = qL+ p

This choice of indices leads to the formula for the DFT in the form

X(p, q) =
M−1∑
m=0

L−1∑
l=0

x(l,m)W
pm

N W
pl

L W
qm

M

=
M−1∑
m=0

W
mq

M

[
L−1∑
l=0

x(l,m)W
lp

L

]
W
mp

N

Thus we obtain a second algorithm.

Algorithm 2

1. Store the signal row-wise.

2. Compute the L-point DFT at each column.

3. Multiply the resulting array by the factors WpmN .

4. Compute the M -point DFT of each row.

5. Read the resulting array column-wise.

(1.21)

(1.22)

Efficient Computation of the DFT: Fast Fourier Transform Algorithms

530



The two algorithms given above have the same complexity. However, they differ
in the arrangement of the computations. In the following sections we exploit the
divide-and-conquer approach to derive fast algorithms when the size of the DFT is
restricted to be a power of 2 or a power of 4.

1.3 Radix-2 FFT Algorithms

In the preceding section we described four algorithms for efficient computation of
the DFT based on the divide-and-conquer approach. Such an approach is applicable
when the number N of data points is not a prime. In particular, the approach is
very efficient when N is highly composite, that is, when N can be factored as N =
r1r2r3 · · · rν , where the {rj } are prime.

Of particular importance is the case in which r1 = r2 = · · · = rν ≡ r , so that
N = rν . In such a case the DFTs are of size r , so that the computation of the N -point
DFT has a regular pattern. The number r is called the radix of the FFT algorithm.

In this section we describe radix-2 algorithms, which are by far the most widely
used FFT algorithms. Radix-4 algorithms are described in the following section.

Let us consider the computation of the N = 2ν point DFT by the divide-and-
We select M = N/2 and

L = 2. This selection results in a split of the N -point data sequence into two N/2-
point data sequences f1(n) and f2(n), corresponding to the even-numbered and
odd-numbered samples of x(n), respectively, that is,

f1(n) = x(2n)

f2(n) = x(2n+ 1), n = 0, 1, . . . , N2 − 1
Thus f1(n) and f2(n) are obtained by decimating x(n) by a factor of 2, and hence
the resulting FFT algorithm is called a decimation-in-time algorithm.

Now the N -point DFT can be expressed in terms of the DFTs of the decimated
sequences as follows:

X(k) =
N−1∑
n=0

x(n)WknN , k = 0, 1, . . . , N − 1

=
∑
n even

x(n)WknN +
∑
n odd

x(n)WknN

=
(N/2)−1∑
m=0

x(2m)W 2mkN +
(N/2)−1∑
m=0

x(2m+ 1)Wk(2m+1)N

But W 2N = WN/2

X(k) =
(N/2)−1∑
m=0

f1(m)W
km
N/2 +WkN

(N/2)−1∑
m=0

f2(m)W
km
N/2

= F1(k)+WkNF2(k), k = 0, 1, . . . , N − 1

(1.23)

(1.24)

(1.25)

conquer approach specified by (1.16) through (1.18).

. With this substitution, (1.24) can be expressed as

Efficient Computation of the DFT: Fast Fourier Transform Algorithms

531



where F1(k) and F2(k) are the N/2-point DFTs of the sequences f1(m) and f2(m),
respectively.

Since F1(k) and F2(k) are periodic, with period N/2, we have F1(k + N/2) =
F1(k) and F2(k + N/2) = F2(k). In addition, the factor Wk+N/2N = −WkN . Hence

X(k) = F1(k)+WkNF2(k), k = 0, 1, . . . ,
N

2
− 1

X

(
k + N

2

)
= F1(k)−WkNF2(k), k = 0, 1, . . . ,

N

2
− 1

We observe that the direct computation of F1(k) requires (N/2)2 complex mul-
tiplications. The same applies to the computation of F2(k). Furthermore, there are
N/2 additional complex multiplications required to compute WkNF2(k). Hence the
computation of X(k) requires 2(N/2)2 + N/2 = N2/2 + N/2 complex multiplica-
tions. This first step results in a reduction of the number of multiplications from N2

to N2/2+N/2, which is about a factor of 2 for N large.
To be consistent with our previous notation, we may define

G1(k) = F1(k) , k = 0, 1, . . . , N2 − 1

G2(k) = WkNF2(k), k = 0, 1, . . . ,
N

2
− 1

Then the DFT X(k) may be expressed as

X(k) = G1(k)+G2(k), k = 0, 1, . . . , N2 − 1

X(k + N
2
) = G1(k)−G2(k), k = 0, 1, . . . , N2 − 1

Having performed the decimation-in-time once, we can repeat the process for
each of the sequences f1(n) and f2(n). Thus f1(n) would result in the two N /4-point
sequences

v11(n) = f1(2n), n = 0, 1, . . . , N4 − 1

v12(n) = f1(2n+ 1), n = 0, 1, . . . , N4 − 1
and f2(n) would yield

v21(n) = f2(2n), n = 0, 1, . . . , N4 − 1

v22(n) = f2(2n+ 1), n = 0, 1, . . . , N4 − 1

(1.26)

(1.27)

(1.28)

(1.29)

(1.30)

(1.25) can be expressed as

This computation is illustrated in Fig. 1.4.

Efficient Computation of the DFT: Fast Fourier Transform Algorithms

532



x(1)

x(0) x(2) x(4)

F1(0) F1(1)

F2(0) F2(1)

F1(2)

x(3)

x(N − 2)

W
k
N

2
X        −  1 N(   )

2
X        +  1 N(   )2X N( )

X(N  −1)

Phase
factors

N/2-Point

2-Point

DFT

DFT

G2(k)

G1(k)

X(0) X(1)

2
F1        −  1 N(   )

First step in the decimation-in-time algorithm.

By computing N /4-point DFTs, we would obtain the N /2-point DFTs F1(k) and
F2(k) from the relations

F1(k) = V11(k)+WkN/2V12(k), k = 0, 1, . . . ,
N

4
− 1

F1

(
k + N

4

)
= V11(k)−WkN/2V12(k), k = 0, 1, . . . ,

N

4
− 1

F2(k) = V21(k)+WkN/2V22(k), k = 0, 1, . . . ,
N

4
− 1

F2

(
k + N

4

)
= V21(k)−WkN/2V22(k), k = 0, . . . ,

N

4
− 1

where the {Vij (k)} are the N /4-point DFTs of the sequences {vij (n)}.
We observe that the computation of {Vij (k)} requires 4(N/4)2 multiplications

and hence the computation of F1(k) and F2(k) can be accomplished with N2/4+N/2
complex multiplications. An additional N/2 complex multiplications are required
to compute X(k) from F1(k) and F2(k). Consequently, the total number of multipli-
cations is reduced approximately by a factor of 2 again to N2/4+N .

The decimation of the data sequence can be repeated again and again until the
resulting sequences are reduced to one-point sequences. For N = 2ν , this decimation
can be performed ν = log2 N times. Thus the total number of complex multiplica-
tions is reduced to (N/2) log2 N . The number of complex additions is N log2 N .

FFT and in the direct computation of the DFT.

DFT. We observe that the computation is performed in three stages, beginning with
the computations of four two-point DFTs, then two four-point DFTs, and finally, one

(1.31)

(1.32)

Figure 1.4

For illustrative purposes, Fig. 1.5 depicts the computation of an N = 8-point

Efficient Computation of the DFT: Fast Fourier Transform Algorithms

Table 1 presents a comparison of the number of complex multiplications in the

533



TABLE 1 Comparison of Computational Complexity for the Direct Computation of the
DFT Versus the FFT Algorithm
Number of Complex Multiplications Complex Multiplications Speed

Points, in Direct Computation, in FFT Algorithm, Improvement
N N2 (N/2) log2N Factor

4 16 4 4.0
8 64 12 5.3

16 256 32 8.0
32 1,024 80 12.8
64 4,096 192 21.3

128 16,384 448 36.6
256 65,536 1,024 64.0
512 262,144 2,304 113.8

1,024 1,048,576 5,120 204.8

eight-point DFT. The combination of the smaller DFTs to form the larger DFT is

Observe that the basic computation performed at every stage, as illustrated in
r
N, and

then add and subtract the product from a to form two new complex numbers (A,B).

flow graph resembles a butterfly.
In general, each butterfly involves one complex multiplication and two complex

additions. ForN = 2ν , there are N/2 butterflies per stage of the computation process
and log2 N stages. Therefore, as previously indicated, the total number of complex
multiplications is (N/2) log2 N and complex additions is N log2 N .

Once a butterfly operation is performed on a pair of complex numbers (a, b)
to produce (A,B), there is no need to save the input pair (a, b). Hence we can
store the result (A,B) in the same locations as (a, b). Consequently, we require a
fixed amount of storage, namely, 2N storage registers, in order to store the results

x(0)
x(4)

x(2)
x(6)

x(1)
x(5)

x(3)
x(7)

2-point
DFT

2-point
DFT

2-point
DFT

2-point
DFT

Combine
2-point
DFT’s

Combine
2-point
DFT’s

Combine
4-point
DFT’s

X(0)

X(1)

X(2)

X(3)

X(4)

X(5)

X(6)

X(7)

Three stages in the computation of an N = 8-point DFT.

illustrated in Fig. 1.6 for N = 8.

Fig. 1.6, is to take two complex numbers, say the pair (a, b), multiply b by W

This basic computation, which is shown in Fig. 1.7, is called a butterfly because the

Figure 1.5

Efficient Computation of the DFT: Fast Fourier Transform Algorithms

534



x(0)

x(4)

x(2)

x(6)

x(1)

x(5)

x(3)

x(7)

X(0)

X(1)

X(2)

X(3)

X(4)

X(5)

X(6)

X(7)

Stage 1 Stage 2 Stage 3

W 08

W 08

W 08

W 08 W
2
8 W

3
8

W 28

W 18

W 08

W 08

W 28

W 08

−1

−1−1

−1

−1

−1

−1

−1

−1

−1

−1

−1

Eight-point decimation-in-time FFT algorithm.

(N complex numbers) of the computations at each stage. Since the same 2N storage
locations are used throughout the computation of the N -point DFT, we say that the
computations are done in place.

A second important observation is concerned with the order of the input data
sequence after it is decimated (ν − 1) times. For example, if we consider the case
where N = 8, we know that the first decimation yields the sequence x(0), x(2), x(4),
x(6), x(1), x(3), x(5), x(7), and the second decimation results in the sequence x(0),
x(4), x(2), x(6), x(1), x(5), x(3), x(7). This shuffling of the input data sequence
has a well-defined order
illustrates the decimation of the eight-point sequence. By expressing the index n,
in the sequence x(n), in binary form, we note that the order of the decimated data
sequence is easily obtained by reading the binary representation of the index n in
reverse order. Thus the data point x(3) ≡ x(011) is placed in position m = 110 or
m = 6 in the decimated array. Thus we say that the data x(n) after decimation is
stored in bit-reversed order.

Basic butterfly computation
in the decimation-in-time

−1

A = a + WrN   b

B = a − WrN   b

a

b
WrN

Figure 1.6

as can be ascertained from observing Fig. 1.8, which

FFT algorithm.

Figure 1.7

Efficient Computation of the DFT: Fast Fourier Transform Algorithms

535



x(0)0 0 0

0 0 1

0 1 0

0 1 1

1 0 0

1 0 1

1 1 0

1 1 1

x(1)

x(2)

x(3)

x(4)

x(5)

x(6)

x(7)

Memory

Data
decimation 1

(binary)

0

1

2

3

4

5

6

7

Memory address

x(0)

x(2)

x(4)

x(6)

x(1)

x(3)

x(5)

x(7)

x(0)

x(4)

x(2)

x(6)

x(1)

x(5)

x(3)

x(7)

Data
decimation 2

Bit-reversed
order

Natural
order

(decimal)

(a)

(b)

(n2n1n0) → (n1n 0n2) → (n0n1n2)

(0 0 0) → (0 0 0) → (0 0 0)
(0 0 1) → (0 1 0) → (1 0 0)
(0 1 0) → (1 0 0) → (0 1 0)
(0 1 1) → (1 1 0) → (1 1 0)
(1 0 0) → (0 0 1) → (0 0 1)
(1 0 1) → (0 1 1) → (1 0 1)
(1 1 0) → (1 0 1) → (0 1 1)
(1 1 1) → (1 1 1) → (1 1 1)

Shuffling of the data and bit reversal.

With the input data sequence stored in bit-reversed order and the butterfly com-
putations performed in place, the resulting DFT sequence X(k) is obtained in natural
order (i.e., k = 0, 1, . . . , N − 1). On the other hand, we should indicate that it is
possible to arrange the FFT algorithm such that the input is left in natural order
and the resulting output DFT will occur in bit-reversed order. Furthermore, we can
impose the restriction that both the input data x(n) and the output DFT X(k) be in
natural order, and derive an FFT algorithm in which the computations are not done
in place. Hence such an algorithm requires additional storage.

Another important radix-2 FFT algorithm, called the decimation-in-frequency
algorithm, is obtained by using the divide-and-conquer approach described in Sec-

a column-wise storage of the input data sequence. To derive the algorithm, we begin
by splitting the DFT formula into two summations, of which one involves the sum
over the first N /2 data points and the second the sum over the last N /2 data points.

Figure 1.8

tion 1.2 with the choice of M = 2 and L = N/2. This choice of parameters implies

Efficient Computation of the DFT: Fast Fourier Transform Algorithms

536



Thus we obtain

X(k) =
(N/2)−1∑
n=0

x(n)WknN +
N−1∑
n=N/2

x(n)WknN

=
(N/2)−1∑
n=0

x(n)WknN +WNk/2N
(N/2)−1∑
n=0

x

(
n+ N

2

)
WknN

Since WkN/2N
k

X(k) =
(N/2)−1∑
n=0

[
x(n)+ (−1)kx

(
n+ N

2

)]
WknN

Now, let us split (decimate) X(k) into the even- and odd-numbered samples. Thus
we obtain

X(2k) =
(N/2)−1∑
n=0

[
x(n)+ x

(
n+ N

2

)]
WknN/2, k = 0, 1, . . . ,

N

2
− 1

and

X(2k + 1) =
(N/2)−1∑
n=0

{[
x(n)− x

(
n+ N

2

)]
WnN

}
WknN/2, k = 0, 1, . . . ,

N

2
− 1

where we have used the fact that W 2N = WN/2.
If we define the N /2-point sequences g1(n) and g2(n) as

g1(n) = x(n)+ x
(
n+ N

2

)

g2(n) =
[
x(n)− x

(
n+ N

2

)]
WnN, n = 0, 1, 2, . . . ,

N

2
− 1

then

X(2k) =
(N/2)−1∑
n=0

g1(n)W
kn
N/2

X(2k + 1) =
(N/2)−1∑
n=0

g2(n)W
kn
N/2

1 2
subsequent use of these sequences to compute the N /2-point DFTs are depicted in

(1.33)

(1.34)

(1.35)

(1.36)

(1.37)

(1.38)

= (−1) , the expression (1.33) can be rewritten as

The computation of the sequences g (n) and g (n) according to (1.37) and the

Fig. 1.9. We observe that the basic computation in this figure involves the butterfly
operation illustrated in Fig. 1.10.

Efficient Computation of the DFT: Fast Fourier Transform Algorithms

537



First stage of the
decimation-in-frequency
FFT algorithm.

x(0)

x(1)

x(2)

x(3)

x(4)

x(5)

x(6)

x(7)

X(0)

X(2)

X(4)

X(6)

X(1)

X(3)

X(5)

X(7)
W 38

W 28

W 18

W 08

−1

−1

−1

−1

4-point
DFT

4-point
DFT

This computational procedure can be repeated through decimation of the N /2-
point DFTs, X(2k) and X(2k + 1). The entire process involves ν = log2 N stages of

Consequently, the computation of the N -point DFT via the decimation-in-frequency
FFT algorithm requires (N/2) log2 N complex multiplications and N log2 N complex
additions, just as in the decimation-in-time algorithm. For illustrative purposes, the

the output DFT occurs in bit-reversed order. We also note that the computations
are performed in place. However, it is possible to reconfigure the decimation-in-
frequency algorithm so that the input sequence occurs in bit-reversed order while
the output DFT occurs in normal order. Furthermore, if we abandon the requirement
that the computations be done in place, it is also possible to have both the input data
and the output DFT in normal order.

Basic butterfly
computation in the
decimation-in-frequency
FFT algorithm.

A = a + ba

b
−1

B = (a − 2b)WN
rWN

r

Figure 1.9

decimation, where each stage involves N /2 butterflies of the type shown in Fig. 1.10.

eight-point decimation-in-frequency algorithm is given in Fig. 1.11.

We observe from Fig. 1.11 that the input data x(n) occurs in natural order, but

Figure 1.10

Efficient Computation of the DFT: Fast Fourier Transform Algorithms

538



x(0)

x(1)

x(2)

x(3)

x(4)

x(5)

x(6)

x(7)

X(0)

X(4)

X(2)

X(6)

X(1)

X(5)

X(3)

X(7)
W 38 W

2
8

W 28

W 08

W 08

W 28

W 18

W 08

W 08

W 08

W 08

W 08

−1

−1

−1

−1

−1

−1

−1

−1

−1

−1

−1

−1

N = 8-point decimation-in-frequency FFT algorithm.

Radix-4 FFT Algorithms

When the number of data points N in the DFT is a power of 4 (i.e., N = 4ν ), we
can, of course, always use a radix-2 algorithm for the computation. However, for this
case, it is more efficient computationally to employ a radix-4 FFT algorithm.

Let us begin by describing a radix-4 decimation-in-time FFT algorithm, which
is obtained by selecting L = 4 and M = N/4 in the divide-and-conquer approach

q = 0, 1, . . . , N/4− 1; n = 4m+ l ; and k = (N/4)p + q . Thus we split or decimate
the N -point input sequence into four subsequences, x(4n), x(4n + 1), x(4n + 2),
x(4n+ 3), n = 0, 1, . . . , N/4− 1.

X(p, q) =
3∑
l=0

[
W
lq

N F(l, q)
]
W
lp

4 , p = 0, 1, 2, 3

F(l, q) =
(N/4)−1∑
m=0

x(l,m)W
mq

N/4,
l = 0, 1, 2, 3,
q = 0, 1, 2, . . . , N

4
− 1

(1.39)

(1.40)

Figure 1.11

1.4

described in Section 1.2. For this choice of L and M , we have l , p = 0, 1, 2, 3; m,

By applying (1.15) we obtain

where F(l, q) is given by (1.16), that is,

Efficient Computation of the DFT: Fast Fourier Transform Algorithms

539



and

x(l,m) = x(4m+ l)

X(p, q) = X
(
N

4
p + q

)

point DFTs defines a radix-4 decimation-in-time butterfly, which can be expressed
in matrix form as


X(0, q)
X(1, q)
X(2, q)
X(3, q)


 =




1 1 1 1
1 −j −1 j
1 −1 1 −1
1 j −1 −j





W 0NF(0, q)
W
q
NF(1, q)

W
2q
N F (2, q)

W
3q
N F (3, q)




Note that since W 0N = 1, each butterfly involves three complex
multiplications, and 12 complex additions.

the resulting FFT algorithm consists of ν stages, where each stage contains N /4
butterflies. Consequently, the computational burden for the algorithm is 3νN/4 =
(3N/8) log2 N complex multiplications and (3N/2) log2 N complex additions. We
note that the number of multiplications is reduced by 25%, but the number of addi-
tions has increased by 50% from N log2 N to (3N/2) log2 N .

It is interesting to note, however, that by performing the additions in two steps,
it is possible to reduce the number of additions per butterfly from 12 to 8. This can

0

q

2q

3q

−j

j

−1

−1

−1
1

j

−1
−j

W3qN

W2qN

W
q
N

W
0
N

(a) (b)

Figure 1.12 Basic butterfly computation in a radix-4 FFT algorithm.

(1.41)

(1.42)

(1.43)

Thus, the four N /4-point DFTs obtained from (1.40) are combined according to
(1.39) to yield the N -point DFT. The expression in (1.39) for combining the N /4-

Efficient Computation of the DFT: Fast Fourier Transform Algorithms

The radix-4 butterfly is depicted in Fig. 1.12(a) and in a more compact form

This decimation-in-time procedure can be repeated recursively ν times. Hence

in Fig. 1.12(b).

540



a product of two matrices as follows:



X(0, q)
X(1, q)
X(2, q)
X(3, q)


 =




1 0 1 0
0 1 0 −j
1 0 −1 0
0 1 0 j






1 0 1 0
1 0 −1 0
0 1 0 1
0 1 0 −1





W 0NF(0, q)
W
q

NF(1, q)
W

2q
N F (2, q)

W
3q
N F (3, q)




Now each matrix multiplication involves four additions for a total of eight additions.
Thus the total number of complex additions is reduced to N log2 N , which is identical
to the radix-2 FFT algorithm. The computational savings results from the 25%
reduction in the number of complex multiplications.

An illustration of a radix-4 decimation-in-time FFT algorithm is shown in
= 16. Note that in this algorithm, the input sequence is in nor-

mal order while the output DFT is shuffled. In the radix-4 FFT algorithm, where
the decimation is by a factor of 4, the order of the decimated sequence can be de-
termined by reversing the order of the number that represents the index n in a
quaternary number system (i.e., the number system based on the digits 0, 1, 2, 3).

A radix-4 decimation-in-frequency FFT algorithm can be obtained by selecting
L = N/4, M = 4; l , p = 0, 1, . . . , N/4 − 1; m, q = 0, 1, 2, 3; n = (N/4)m+ l ; and
x(0)

x(1)

x(2)

x(3)

x(4)

x(5)

x(6)

x(7)

x(8)

x(9)

x(10)

x(11)

x(12)

x(13)

x(14)

x(15)

X(0)

X(4)

X(8)

X(12)

X(1)

X(5)

X(9)

X(13)

X(2)

X(6)

X(10)

X(14)

X(3)

X(7)

X(11)

X(15)

0

0

0

0

0

0

1

2

0

6

2

4

0

9

3

6

0

0

0
0

0
0

0

0

0

0

0

0

Sixteen-point radix-4 decimation-in-time algorithm with input in
normal order and output in digit-reversed order. The integer multipliers shown
on the graph represent the exponents on W16 .

(1.44)

Efficient Computation of the DFT: Fast Fourier Transform Algorithms

be accomplished by expressing the matrix of the linear transformation in (1.43) as

Fig. 1.13 for N

Figure 1.13

541



can be expressed as

X(p, q) =
(N/4)−1∑
l=0

G(l, q)W
lp

N/4

where

G(l, q) = WlqN F(l, q),
q = 0, 1, 2, 3
l = 0, 1, . . . , N

4
− 1

and

F(l, q) =
3∑

m=0
x(l,m)W

mq

4 ,
q = 0, 1, 2, 3
l = 0, 1, 2, 3, . . . , N

4
− 1

We note that X(p, q) = X(4p + q), q = 0, 1, 2, 3. Consequently, the N -
point DFT is decimated into four N/4-point DFTs and hence we have a decimation-
in-frequency FFT algorithm.
basic radix-4 butterfly for the decimation-in-frequency algorithm. Note that the
multiplications by the factors WlqN occur after the combination of the data points
x(l,m), just as in the case of the radix-2 decimation-in-frequency algorithm.

Its input is in normal order and its output is in digit-reversed order. It has exactly the
same computational complexity as the decimation-in-time radix-4 FFT algorithm.

For illustrative purposes, let us rederive the radix-4 decimation-in-frequency
algorithm by breaking the N -point DFT formula into four smaller DFTs. We have

X(k) =
N−1∑
n=0

x(n)WknN

=
N/4−1∑
n=0

x(n)WknN +
N/2−1∑
n=N/4

x(n)WknN +
3N/4−1∑
n=N/2

x(n)WknN +
N−1∑

n=3N/4
x(n)WknN

=
N/4−1∑
n=0

x(n)WknN +WNk/4N
N/4−1∑
n=0

x

(
n+ N

4

)
WknN

+WkN/2N
N/4−1∑
n=0

x

(
n+ N

2

)
WnkN +W 3kN/4N

N/4−1∑
n=0

x

(
n+ 3N

4

)
WknN

From the definition of the phase factors, we have

W
kN/4
N = (−j)k, WNk/2N = (−1)k, W 3Nk/4N = (j)k

(1.45)

(1.46)

(1.47)

(1.48)

(1.49)

Efficient Computation of the DFT: Fast Fourier Transform Algorithms

k = 4p + q . With this choice of parameters, the general equation given by (1.15)

The computations in (1.46) and (1.47) define the

A 16-point radix-4 decimation-in-frequency FFT algorithm is shown in Fig. 1.14.

542



x(0)

x(1)

x(2)

x(3)

x(4)

x(5)

x(6)

x(7)

x(8)

x(9)

x(10)

x(11)

x(12)

x(13)

x(14)

x(15)

X(0)

X(4)

X(8)

X(12)

X(1)

X(5)

X(9)

X(13)

X(2)

X(6)

X(10)

X(14)

X(3)

X(7)

X(11)

X(15)

00

0

0

0

0

2

4

6

3
0

6

9

0

0

0
0

1

2

3

0

0

0

0

0

0

0

0

0

0

0

0

Sixteen-point, radix-4 decimation-in-frequency algorithm with input
in normal order and output in digit-reversed order.

X(k) =
N/4−1∑
n=0

[
x(n)+ (−j)kx

(
n+ N

4

)

+ (−1)kx
(
n+ N

4

)
+ (j)kx

(
n+ 3N

4

)]
WnkN

pends on N and not on N/4. To convert it into an N/4-point DFT, we subdivide the
DFT sequence into four N/4-point subsequences, X(4k), X(4k+ 1), X(4k+ 2), and
X(4k+3), k = 0, 1, . . . , N/4−1. Thus we obtain the radix-4 decimation-in-frequency
DFT as

X(4k) =
N/4−1∑
n=0

[
x(n)+ x

(
n+ N

4

)

+ x
(
n+ N

2

)
+ x

(
n+ 3N

4

)]
W 0NW

kn
N/4

(1.50)

(1.51)

Efficient Computation of the DFT: Fast Fourier Transform Algorithms

Figure 1.14

After substitution of (1.49) into (1.48), we obtain

The relation in (1.50) is not an N/4-point DFT because the phase factor de-

543



X(4k + 1) =
N/4−1∑
n=0

[
x(n)− jx

(
n+ N

4

)

− x
(
n+ N

2

)
+ jx

(
n+ 3N

4

)]
WnNW

kn
N/4

X(4k + 2) =
N/4−1∑
n=0

[
x(n)− x

(
n+ N

4

)

+ x
(
n+ N

2

)
− x

(
n+ 3N

4

)]
W 2nN W

kn
N/4

X(4k + 3) =
N/4−1∑
n=0

[
x(n)+ jx

(
n+ N

4

)

− x
(
n+ N

2

)
− jx

(
n+ 3N

4

)]
W 3nN W

kn
N/4

where we have used the property W 4knN = WknN/4 . Note that the input to each N/4-
point DFT is a linear combination of four signal samples scaled by a phase factor.
This procedure is repeated ν times, where ν = log4N .

1.5 Split-Radix FFT Algorithms

of the odd-numbered points. This suggests the possibility of using different computa-
tional methods for independent parts of the algorithm with the objective of reducing
the number of computations. The split-radix FFT (SRFFT) algorithms exploit this
idea by using both a radix-2 and a radix-4 decomposition in the same FFT algorithm.

due to Duhamel (1986). First, we recall that in the radix-2 decimation-in-frequency
FFT algorithm, the even-numbered samples of the N -point DFT are given as

X(2k) =
N/2−1∑
n=0

[
x(n)+ x

(
n+ N

2

)]
WnkN/2, k = 0, 1, . . . ,

N

2
− 1

Note that these DFT points can be obtained from an N/2-point DFT without any
additional multiplications. Consequently, a radix-2 suffices for this computation.

The odd-numbered samples {X(2k + 1)} of the DFT require the premultiplica-
tion of the input sequence with the phase factors WnN . For these samples a radix-4
decomposition produces some computational efficiency because the four-point DFT
has the largest multiplication-free butterfly. Indeed, it can be shown that using a radix
greater than 4 does not result in a significant reduction in computational complexity.

(1.52)

(1.53)

(1.54)

(1.55)

Efficient Computation of the DFT: Fast Fourier Transform Algorithms

We illustrate this approach with a decimation-in-frequency SRFFT algorithm

indicates that the even-numbered points of the DFT can be computed independently
An inspection of the radix-2 decimation-in-frequency flowgraph shown in Fig. 1.11

544



If we use a radix-4 decimation-in-frequency FFT algorithm for the odd-numbered
samples of the N -point DFT, we obtain the following N/4-point DFTs:

X(4k + 1) =
N/4−1∑
n=0
{[x(n)− x(n+N/2)]

− j [x(n+N/4)− x(n+ 3N/4)]}WnNWknN/4

X(4k + 3) =
N/4−1∑
n=0
{[x(n)− x(n+N/2)]

+ j [x(n+N/4)− x(n+ 3N/4)]}W 3nN WknN/4

Thus the N -point DFT is decomposed into one N/2-point DFT without additional
phase factors and two N/4-point DFTs with phase factors. The N -point DFT is
obtained by successive use of these decompositions up to the last stage. Thus we
obtain a decimation-in-frequency SRFFT algorithm.

uency SRFFT algorithm.
points constitute the sequence

g0(n) = x(n)+ x(n+N/2), 0 ≤ n ≤ 15

This is the sequence required for the computation of X(2k). The next 8 points
constitute the sequence

g1(n) = x(n)− x(n+N/2), 0 ≤ n ≤ 7

The bottom eight points constitute the sequence jg2(n), where

g2(n) = x(n+N/4)− x(n+ 3N/4), 0 ≤ n ≤ 7

The sequences g1(n) and g2(n) are used in the computation ofX(4k+1) andX(4k+3).
Thus, at stage A we have completed the first decimation for the radix-2 component
of the algorithm. At stage B, the bottom eight points constitute the computation of
[g1(n) + jg2(n)]W 3n32 , 0 ≤ n ≤ 7, which is used to compute X(4k + 3), 0 ≤ k ≤ 7.
The next eight points from the bottom constitute the computation of [g1(n)−jg2(n)]
Wn32 , 0 ≤ n ≤ 7, which is used to compute X(4k+ 1), 0 ≤ k ≤ 7. Thus at stage B, we
have completed the first decimation for the radix-4 algorithm, which results in two
8-point sequences. Hence the basic butterfly computation for the SRFFT algorithm

Now we repeat the steps in the computation above. Beginning with the top 16
points at stage A, we repeat the decomposition for the 16-point DFT. In other words,

(1.56)

(1.57)

(1.58)

(1.59)

(1.60)

Efficient Computation of the DFT: Fast Fourier Transform Algorithms

= 32, the top 16At stage A of the computation for N
Figure 1.15 shows the flow graph for an in-place 32-point decimation-in-freq-

has the “L-shaped” form illustrated in Fig. 1.16.

545



0

1
2

3
4
5

6
7
8
9

10
11
12
13
14
15
16
17

18

19

20

21
22

23
24

25
26

27
28

29
30

31

j
j

j
j

j
j

j
j

j

j

j

j

j

j
j
j
j
j

j
j

j

j

j

W
1

W
2

W
3

W
4

W
5

W
6

W
7

W 3
W 6
W 9
W 12
W 15

W 12

W 12

W 12

W
4

W 18
W 21

W
2

W
4

W
6

W 6W 12W 18

j

W
4

W
4

0

16
8
24

4

20
12
28

2

18
10
26
6

22
14
30

1

17
9
25

5

21
13
29

3

19
11
27
7

23
15
31

A B

Length 32 split-radix FFT algorithms from paper by Duhamel (1986); reprinted with
permission from the IEEE.

we decompose the computation into an eight-point, radix-2 DFT and two four-point,
radix-4 DFTs. Thus at stage B, the top eight points constitute the sequence (with
N = 16)

g′0(n) = g0(n)+ g0(n+N/2), 0 ≤ n ≤ 7

and the next eight points constitute the two four-point sequences g′1(n) and jg
′
2(n),

where
g′1(n) = g0(n)− g0(n+N/2), 0 ≤ n ≤ 3

g′2(n) = g0(n+N/4)− g0(n+ 3N/4), 0 ≤ n ≤ 3

(1.61)

(1.62)

Efficient Computation of the DFT: Fast Fourier Transform Algorithms

Figure 1.15

546



x(n)

Use for
X(4k + 1)

Use for
X(4k + 3)

Use for X(2k)

−1

−1

− j

j

n
NW

3n
NW

x  n +     N4(     )

x  n +     N2(     )

x  n +    3N2(     )

Butterfly for SRFFT algorithm.

The bottom 16 points of stage B are in the form of two eight-point DFTs. Hence
each eight-point DFT is decomposed into a four-point, radix-2 DFT and a four-point,
radix-4 DFT. In the final stage, the computations involve the combination of two-
point sequences.

and additions required to perform an N -point DFT with complex-valued data, using
a radix-2, radix-4, radix-8, and a split-radix FFT. Note that the SRFFT algorithm
requires the lowest number of multiplication and additions. For this reason, it is
preferable in many practical applications.

Another type of SRFFT algorithm has been developed by Price (1990). Its
relation to Duhamel’s algorithm described previously can be seen by noting that the
radix-4 DFT terms X(4k + 1) and X(4k + 3) involve the N/4-point DFTs of the
sequences [g1(n) − jg2(n)]WnN and [g1(n) + jg2(n)]W 3nN , respectively. In effect, the
sequences g1(n) and g2(n) are multiplied by the factor (vector) (1,−j) = (1,W 832)

Number of Nontrivial Real Multiplications and Additions to
Compute an N -point Complex DFT

Real Multiplications Real Additions

Radix Radix Radix Split Radix Radix Radix Split
N 2 4 8 Radix 2 4 8 Radix

16 24 20 20 152 148 148

32 88 68 408 388

64 264 208 204 196 1,032 976 972 964

128 712 516 2,504 2,308

256 1,800 1,392 1,284 5,896 5,488 5,380

512 4,360 3,204 3,076 13,566 12,420 12,292

1,024 10,248 7,856 7,172 30,728 28,336 27,652

Source: Extracted from Duhamel (1986).

Efficient Computation of the DFT: Fast Fourier Transform Algorithms

TABLE 2

Figure 1.16

Table 2 presents a comparison of the number of nontrivial real multiplications

547



and by WnN for the computation of X(4k + 1), while the computation of X(4k + 3)
involves the factor (1, j) = (1,W−832 ) and W 3nN . Instead, one can rearrange the
computation so that the factor for X(4k + 3) is (−j,−1) = −(W−832 , 1). As a result
of this phase rotation, the phase factors in the computation of X(4k + 3) become
exactly the same as those for X(4k + 1), except that they occur in mirror image
order. 21, W 18, . . . ,W 3

1 , W 2, . . . ,W 7 , respectively. This mirror-image symmetry occurs
at every subsequent stage of the algorithm. As a consequence, the number of phase
factors that must be computed and stored is reduced by a factor of 2 in comparison
to Duhamel’s algorithm. The resulting algorithm is called the “mirror” FFT (MFFT)
algorithm.

An additional factor-of-2 savings in storage of phase factors can be obtained by
introducing a 90◦ phase offset at the midpoint of each factor array, which can be
removed if necessary at the output of the SRFFT computation. The incorporation of
this improvement into the SRFFT (or the MFFT) results in another algorithm, also
due to Price (1990), called the “phase” FFT (PFFT) algorithm.

Implementation of FFT Algorithms

Now that we have described the basic radix-2 and radix-4 FFT algorithms, let us
consider some of the implementation issues. Our remarks apply directly to radix-2
algorithms, although similar comments may be made about radix-4 and higher-radix
algorithms.

Basically, the radix-2 FFT algorithm consists of taking two data points at a time
from memory, performing the butterfly computations and returning the resulting
numbers to memory. This procedure is repeated many times ((N log2 N)/2 times) in
the computation of an N -point DFT.

The butterfly computations require the phase factors {WkN } at various stages in
either natural or bit-reversed order. In an efficient implementation of the algorithm,
the phase factors are computed once and stored in a table, either in normal order or
in bit-reversed order, depending on the specific implementation of the algorithm.

Memory requirement is another factor that must be considered. If the computa-
tions are performed in place, the number of memory locations required is 2N since
the numbers are complex. However, we can instead double the memory to 4N, thus
simplifying the indexing and control operations in the FFT algorithms. In this case
we simply alternate in the use of the two sets of memory locations from one stage of
the FFT algorithm to the other. Doubling of the memory also allows us to have both
the input sequence and the output sequence in normal order.

There are a number of other implementation issues regarding indexing, bit rever-
sal, and the degree of parallelism in the computations. To a large extent, these issues
are a function of the specific algorithm and the type of implementation, namely, a
hardware or software implementation. In implementations based on a fixed-point
arithmetic, or floating-point arithmetic on small machines, there is also the issue of

Efficient Computation of the DFT: Fast Fourier Transform Algorithms

are replaced by W
For example, at stage B of Fig. 1.15, the phase factors W

1.6

round-off errors in the computation. This topic is considered in Section 4.

548



Although the FFT algorithms described previously were presented in the context
of computing the DFT efficiently, they can also be used to compute the IDFT, which is

x(n) = 1
N

N−1∑
k=0

X(k)W−nkN

The only difference between the two transforms is the normalization factor 1/N and
the sign of the phase factor WN . Consequently, an FFT algorithm for computing the
DFT can be converted to an FFT algorithm for computing the IDFT by changing the
sign on all the phase factors and dividing the final output of the algorithm by N .

In fact, if we take the decimation-in-time algorithm that we described in Sec-

factors, interchange the output and input, and finally, divide the output by N , we
obtain a decimation-in-frequency FFT algorithm for computing the IDFT. On the
other hand, if we begin with the decimation-in-frequency FFT algorithm described

time FFT algorithm for computing the IDFT. Thus it is a simple matter to devise FFT
algorithms for computing the IDFT.

Finally, we note that the emphasis in our discussion of FFT algorithms was on
radix-2, radix-4, and split-radix algorithms. These are by far the most widely used
in practice. When the number of data points is not a power of 2 or 4, it is a simple
matter to pad the sequence x(n) with zeros such that N = 2ν or N = 4ν .

The measure of complexity for FFT algorithms that we have emphasized is the
required number of arithmetic operations (multiplications and additions). Although
this is a very important benchmark for computational complexity, there are other
issues to be considered in practical implementation of FFT algorithms. These include
the architecture of the processor, the available instruction set, the data structures for
storing phase factors, and other considerations.

For general-purpose computers, where the cost of the numerical operations dom-
inates, radix-2, radix-4, and split-radix FFT algorithms are good candidates. How-
ever, in the case of special-purpose digital signal processors, featuring single-cycle
multiply-and-accumulate operation, bit-reversed addressing, and a high degree of in-
struction parallelism, the structural regularity of the algorithm is equally as important
as arithmetic complexity. Hence for DSP processors, radix-2 or radix-4 decimation-
in-frequency FFT algorithms are preferable in terms of speed and accuracy. The
irregular structure of the SRFFT may render it less suitable for implementation on
digital signal processors. Structural regularity is also important in the implementation
of FFT algorithms on vector processors, multiprocessors, and in VLSI. Interprocessor
communication is an important consideration in such implementations on parallel
processors.

In conclusion, we have presented several important considerations in the imple-
mentation of FFT algorithms. Advances in digital signal processing technology, in
hardware and software, will continue to influence the choice among FFT algorithms
for various practical applications.

(1.63)

Efficient Computation of the DFT: Fast Fourier Transform Algorithms

tion 1.3, reverse the direction of the flow graph, change the sign on the phase

in Section 1.3 and repeat the changes described above, we obtain a decimation-in-

549



2 Applications of FFT Algorithms

The FFT algorithms described in the preceding section find application in a variety
of areas, including linear filtering, correlation, and spectrum analysis. Basically, the
FFT algorithm is used as an efficient means to compute the DFT and the IDFT.

2.1 Efficient Computation of the DFT of Two Real Sequences

The FFT algorithm is designed to perform complex multiplications and additions,
even though the input data may be real valued. The basic reason for this situation is
that the phase factors are complex and hence, after the first stage of the algorithm,
all variables are basically complex valued.

In view of the fact that the algorithm can handle complex-valued input sequences,
we can exploit this capability in the computation of the DFT of two real-valued
sequences.

Suppose that x1(n) and x2(n) are two real-valued sequences of length N , and let
x(n) be a complex-valued sequence defined as

x(n) = x1(n)+ jx2(n), 0 ≤ n ≤ N − 1
The DFT operation is linear and hence the DFT of x(n) can be expressed as

X(k) = X1(k)+ jX2(k)
The sequences x1(n) and x2(n) can be expressed in terms of x(n) as follows:

x1(n) = x(n)+ x
∗(n)

2

x2(n) = x(n)− x
∗(n)

2j

Hence the DFTs of x1(n) and x2(n) are

X1(k) = 12 {DFT[x(n)]+DFT[x
∗(n)]}

X2(k) = 12j {DFT[x(n)]−DFT[x
∗(n)]}

Recall that the DFT of x∗(n) is X∗(N − k). Therefore,

X1(k) = 12[X(k)+X
∗(N − k)]

X2(k) = 1
j2

[X(k)−X∗(N − k)]

(2.1)

(2.2)

(2.3)

(2.4)

(2.5)

(2.6)

(2.7)

(2.8)

In this section we consider the use of the FFT algorithm in linear filtering and in 
the computation of the crosscorrelation of two sequences. In addition we illustrate how 
to enhance the efficiency of the FFT algorithm by forming complex-valued sequences 
from real-valued sequences prior to the computation of the DFT.

Efficient Computation of the DFT: Fast Fourier Transform Algorithms

550



Thus, by performing a single DFT on the complex-valued sequence x(n), we have
obtained the DFT of the two real sequences with only a small amount of additional
computation that is involved in computing X1(k) and X2(k) from X(k) by use of

2.2 Efficient Computation of the DFT of a 2N -Point Real Sequence

Suppose that g(n) is a real-valued sequence of 2N points. We now demonstrate
how to obtain the 2N -point DFT of g(n) from computation of one N -point DFT
involving complex-valued data. First, we define

x1(n) = g(2n)
x2(n) = g(2n+ 1)

Thus we have subdivided the 2N -point real sequence into two N -point real se-
quences. Now we can apply the method described in the preceding section.

Let x(n) be the N -point complex-valued sequence

x(n) = x1(n)+ jx2(n)

From the results of the preceding section, we have

X1(k) = 12[X(k)+X
∗(N − k)]

X2(k) = 12j [X(k)−X
∗(N − k)]

Finally, we must express the 2N -point DFT in terms of the two N -point DFTs,
X1(k) and X2(k). To accomplish this, we proceed as in the decimation-in-time FFT
algorithm, namely,

G(k) =
N−1∑
n=0

g(2n)W 2nk2N +
N−1∑
n=0

g(2n+ 1)W(2n+1)k2N

=
N−1∑
n=0

x1(n)W
nk
N +Wk2N

N−1∑
n=0

x2(n)W
nk
N

Consequently,

G(k) = X1(k)+Wk2NX2(k), k = 0, 1, . . . , N − 1
G(k +N) = X1(k)−Wk2NX2(k), k = 0, 1, . . . , N − 1

Thus we have computed the DFT of a 2N -point real sequence from one N -point

(2.9)

(2.10)

(2.11)

(2.12)

Efficient Computation of the DFT: Fast Fourier Transform Algorithms

(2.7) and (2.8).

DFT and some additional computation as indicated by (2.11) and (2.12).

551



2.3 Use of the FFT Algorithm in Linear Filtering and Correlation

Let h(n), 0 ≤ n ≤ M − 1, be the unit sample response of the FIR filter and
let x(n) denote the input data sequence. The block size of the FFT algorithm is N ,
where N = L+M − 1 and L is the number of new data samples being processed by
the filter. We assume that for any given value of M , the number L of data samples is
selected so that N is a power of 2. For purposes of this discussion, we consider only
radix-2 FFT algorithms.

The N -point DFT of h(n), which is padded by L− 1 zeros, is denoted as H(k).
This computation is performed once via the FFT and the resulting N complex num-
bers are stored. To be specific we assume that the decimation-in-frequency FFT
algorithm is used to compute H(k). This yields H(k) in bit-reversed order, which is
the way it is stored in memory.

In the overlap-save method, the first M − 1 data points of each data block are
the last M − 1 data points of the previous data block. Each data block contains
L new data points, such that N = L + M − 1. The N -point DFT of each data
block is performed by the FFT algorithm. If the decimation-in-frequency algorithm
is employed, the input data block requires no shuffling and the values of the DFT
occur in bit-reversed order. Since this is exactly the order of H(k), we can multiply
the DFT of the data, say Xm(k), with H(k), and thus the result

Ym(k) = H(k)Xm(k)

is also in bit-reversed order.
The inverse DFT (IDFT) can be computed by use of an FFT algorithm that takes

the input in bit-reversed order and produces an output in normal order. Thus there
is no need to shuffle any block of data in computing either the DFT or the IDFT.

If the overlap-add method is used to perform the linear filtering, the computa-
tional method using the FFT algorithm is basically the same. The only difference
is that the N -point data blocks consist of L new data points and M − 1 additional
zeros. After the IDFT is computed for each data block, the N -point filtered blocks

Let us assess the computational complexity of the FFT method for linear fil-
tering. For this purpose, the one-time computation of H(k) is insignificant and can
be ignored. Each FFT requires (N/2) log2 N complex multiplications and N log2 N
additions. Since the FFT is performed twice, once for the DFT and once for the
IDFT, the computational burden is N log2 N complex multiplications and 2N log2 N
additions. There are also N complex multiplications and N−1 additions required to
compute Ym(k). Therefore, we have (N log2 2N)/L complex multiplications per out-
put data point and approximately (2N log2 2N)/L additions per output data point.

An important application of the FFT algorithm is in FIR linear filtering of long data 
sequences. The two methods are the overlap-add and the overlapsave methods for filter-
ing a long data sequence with an FIR filter, based on the use of the DFT. In this section 
we consider the use of these two methods in conjunction with the FFT algorithm for 
computing the DFT and the IDFT.

are overlapped, and the overlapping data points between successive output re-
cords are added together.

M − 1

Efficient Computation of the DFT: Fast Fourier Transform Algorithms

552



The overlap-add method requires an incremental increase of (M − 1)/L in the num-
ber of additions.

By way of comparison, a direct-form realization of the FIR filter involves M real
multiplications per output point if the filter is not linear phase, and M/2 if it is linear
phase (symmetric). Also, the number of additions is M − 1 per output point.

It is interesting to compare the efficiency of the FFT algorithm with the direct
form realization of the FIR filter. Let us focus on the number of multiplications,
which are more time consuming than additions. Suppose that M = 128 = 27 and
N = 2ν . Then the number of complex multiplications per output point for an FFT
size of N = 2ν is

c(ν) = N log2 2N
L

= 2
ν(ν + 1)

N −M + 1

≈ 2
ν(ν + 1)
2ν − 27

there is an optimum value of ν which minimizes c(ν). For the FIR filter of size
M = 128, the optimum occurs at ν = 10.

We should emphasize that c(ν) represents the number of complex multiplica-
tions for the FFT-based method. The number of real multiplications is four times this
number. However, even if the FIR filter has linear phase, the number of computa
tions per output point is still less with the FFT-based method. Further- more, the
efficiency of the FFT method can be improved by computing the DFT of two succ
essive data blocks simultaneously, according to the method just described. Conse
quently, the FFT-based method is indeed superior from a computationalpoint of view
when the filter length is relatively large.

The computation of the cross correlation between two sequences by means of
the FFT algorithm is similar to the linear FIR filtering problem just described. In
practical applications involving crosscorrelation, at least one of the sequences has
finite duration and is akin to the impulse response of the FIR filter. The second
sequence may be a long sequence which contains the desired sequence corrupted
by additive noise. Hence the second sequence is akin to the input to the FIR filter.

Computational Complexity

Size of FFT c(ν) Number of Complex
ν = log2 N Multiplications per Output Point

9 13.3

10 12.6

11 12.8

12 13.4

14 15.1

Efficient Computation of the DFT: Fast Fourier Transform Algorithms

-

-
-

The values of c(ν) for different values of ν are given in Table 3. We observe that

TABLE 3

553



By time reversing the first sequence and computing its DFT, we have reduced the
cross correlation to an equivalent convolution problem (i.e., a linear FIR filtering
problem). Therefore, the methodology we developed for linear FIR filtering by use
of the FFT applies directly.

3 A Linear Filtering Approach to Computation of the DFT

The FFT algorithm takes N points of input data and produces an output sequence of
N points corresponding to the DFT of the input data. As we have shown, the radix-2
FFT algorithm performs the computation of the DFT in (N/2) log2 N multiplications
and N log2 N additions for an N -point sequence.

There are some applications where only a selected number of values of the DFT
are desired, but the entire DFT is not required. In such a case, the FFT algorithm
may no longer be more efficient than a direct computation of the desired values of
the DFT. In fact, when the desired number of values of the DFT is less than log2N ,
a direct computation of the desired values is more efficient.

The direct computation of the DFT can be formulated as a linear filtering op-
eration on the input data sequence. As we will demonstrate, the linear filter takes
the form of a parallel bank of resonators where each resonator selects one of the
frequencies ωk = 2πk/N , k = 0, 1, . . . , N − 1, corresponding to the N frequencies
in the DFT.

There are other applications in which we require the evaluation of the z-transform
of a finite-duration sequence at points other than the unit circle. If the set of desired
points in the z-plane possesses some regularity, it is possible to also express the com-
putation of the z-transform as a linear filtering operation. In this connection, we
introduce another algorithm, called the chirp-z transform algorithm, which is suit-
able for evaluating the z-transform of a set of data on a variety of contours in the
z-plane. This algorithm is also formulated as a linear filtering of a set of input data.
As a consequence, the FFT algorithm can be used to compute the chirp-z transform
and thus to evaluate the z-transform at various contours in the z-plane, including
the unit circle.

3.1 The Goertzel Algorithm

The Goertzel algorithm exploits the periodicity of the phase factors {WkN } and allows
us to express the computation of the DFT as a linear filtering operation. Since
W−kNN = 1, we can multiply the DFT by this factor. Thus

X(k) = W−kNN
N−1∑
m=0

x(m)WkmN =
N−1∑
m=0

x(m)W
−k(N−m)
N

yk(n) as

yk(n) =
N−1∑
m=0

x(m)W
−k(n−m)
N

(3.1)

(3.2)

Efficient Computation of the DFT: Fast Fourier Transform Algorithms

We note that (3.1) is in the form of a convolution. Indeed, if we define the sequence

554



then it is clear that yk(n) is the convolution of the finite-duration input sequence x(n)
of length N with a filter that has an impulse response

hk(n) = W−knN u(n)

The output of this filter at n = N yields the value of the DFT at the frequency
ωk = 2πk/N . That is,

X(k) = yk(n)|n=N

The filter with impulse response hk(n) has the system function

Hk(z) = 1
1−W−kN z−1

This filter has a pole on the unit circle at the frequency ωk = 2πk/N . Thus, the entire
DFT can be computed by passing the block of input data into a parallel bank of
N single-pole filters (resonators), where each filter has a pole at the corresponding
frequency of the DFT.

compute yk(n) recursively. Thus we have

yk(n) = W−kN yk(n− 1)+ x(n), yk(−1) = 0

The desired output is X(k) = yk(N), for k = 0, 1, . . . , N − 1. To perform this
computation, we can compute once and store the phase factors W−kN .

combining the pairs of resonators possessing complex-conjugate poles. This leads to
two-pole filters with system functions of the form

Hk(z) = 1−W
k
Nz
−1

1− 2 cos(2πk/N)z−1 + z−2

the difference equations

vk(n) = 2 cos 2πk
N
vk(n− 1)− vk(n− 2)+ x(n)

yk(n) = vk(n)−WkNvk(n− 1)

with initial conditions vk(−1) = vk(−2) = 0.

(3.3)

(3.4)

(3.5)

(3.6)

(3.7)

(3.8)

(3.9)

Efficient Computation of the DFT: Fast Fourier Transform Algorithms

as can be verified by comparing (3.1) with (3.2).

Instead of performing the computation of the DFT as in (3.2), via convolution,
we can use the difference equation corresponding to the filter given by (3.5) to

The complex multiplications and additions inherent in (3.6) can be avoided by

The direct form II realization of the system illustrated in Fig. 3.1 is described by

555



Direct form II realization
of two-pole resonator for
computing the DFT.

+

+

x(n) vk(n) yk(n)

z −1

z −1

+

W2πk
N

2 cos

−1

k
n

Each iteration requires one real

symmetry, the value of X(N − k).
The Goertzel algorithm is particularly attractive when the DFT is to be computed

2 N . Otherwise, the FFT
algorithm is a more efficient method.

The Chirp-z Transform Algorithm

The DFT of an N -point data sequence x(n) has been viewed as the z-transform of
x(n) evaluated at N equally spaced points on the unit circle. It has also been viewed
as N equally spaced samples of the Fourier transform of the data sequence x(n).
In this section we consider the evaluation of X(z) on other contours in the z-plane,
including the unit circle.

Suppose that we wish to compute the values of the z-transform of x(n) at a set
of points {zk}. Then,

X(zk) =
N−1∑
n=0

x(n)z−nk , k = 0, 1, . . . , L− 1

For example, if the contour is a circle of radius r and the zk are N equally spaced
points, then

zk = rej2πkn/N , k = 0, 1, 2, . . . , N − 1

X(zk) =
N−1∑
n=0

[x(n)r−n]e−j2πkn/N , k = 0, 1, 2, . . . , N − 1

In this case the FFT algorithm can be applied on the modified sequence x(n)r−n .
More generally, suppose that the points zk in the z-plane fall on an arc which

begins at some point
z0 = r0ejθ0

(3.10)

(3.11)

Efficient Computation of the DFT: Fast Fourier Transform Algorithms

Figure 3.1

The recursive relation in (3.8) is iterated for n = 0, 1, . . . , N , but the equation

multiplication and two additions. Consequently, for a real input sequence x(n), this

at a relatively small number M of values, where M ≤ log

algorithm requires N + 1 real multiplications to yield not only X(k) but also, due to

in (3.9) is computed only once at time n = N .

3.2

556



and spirals either in toward the origin or out away from the origin such that the points
{zk} are defined as

zk = r0ejθ0(R0ejφ0)k, k = 0, 1, . . . , L− 1

Note that if R0 < 1, the points fall on a contour that spirals toward the origin, and
if R0 > 1, the contour spirals away from the origin. If R0 = 1, the contour is a
circular arc of radius r0 . If r0 = 1 and R0 = 1, the contour is an arc of the unit circle.
The latter contour would allow us to compute the frequency content of the sequence
x(n) at a dense set of L frequencies in the range covered by the arc without having
to compute a large DFT, that is, a DFT of the sequence x(n) padded with many
zeros to obtain the desired resolution in frequency. Finally, if r0 = R0 = 1, θ0 = 0,
φ0 = 2π/N , and L = N , the contour is the entire unit circle and the frequencies are

Im(z) Im(z)

Im(z) Im(z)

Re(z) Re(z)

Unit
circle

R0 = r0 = 1
φ0 = θ0 = 0

R0 = 1
r0 < 1
φ0 = θ0 = 0 

R0 < 1 R0 > 1

Unit
circle

Unit
circle

Unit
circle

Re(z) Re(z)

r0
θ0

r0

Figure 3.2 Some examples of contours on which we may evaluate the z-
transform.

(3.12)

Efficient Computation of the DFT: Fast Fourier Transform Algorithms

those of the DFT. The various contours are illustrated in Fig. 3.2.

557



k

form, we obtain

X(zk) =
N−1∑
n=0

x(n)z−nk

=
N−1∑
n=0

x(n)(r0e
jθ0)−nV −nk

where, by definition,
V = R0ejφ0

nk = 12 [n2 + k2 − (k − n)2]

X(zk) = V −k2/2
N−1∑
n=0

[x(n)(r0ejθ0)−nV −n
2/2]V (k−n)

2/2

Let us define a new sequence g(n) as

g(n) = x(n)(r0ejθ0)−nV −n2/2

X(zk) = V −k2/2
N−1∑
n=0

g(n)V (k−n)
2/2

g(n) with the impulse response h(n) of a filter, where

h(n) = V n2/2

X(zk) = V −k2/2y(k)

= y(k)
h(k)

, k = 0, 1, . . . , L− 1

where y(k) is the output of the filter

y(k) =
N−1∑
n=0

g(n)h(k − n), k = 0, 1, . . . , L− 1

(3.13)

(3.14)

(3.15)

(3.16)

(3.17)

(3.18)

(3.19)

(3.20)

(3.21)

Efficient Computation of the DFT: Fast Fourier Transform Algorithms

When points {z } in (3.12) are substituted into the expression for the z-trans-

We can express (3.13) in the form of a convolution, by noting that

Substitution of (3.15) into (3.13) yields

Then (3.16) can be expressed as

The summation in (3.18) can be interpreted as the convolution of the sequence

Consequently, (3.18) may be expressed as

558



We observe that both h(n) and g(n) are complex-valued sequences.
The sequence h(n) with R0 = 1 has the form of a complex exponential with

argument ωn = n2φ0/2 = (nφ0/2)n. The quantity nφ0/2 represents the frequency of
the complex exponential signal, which increases linearly with time. Such signals are
used in radar systems and are called chirp signals. Hence the z-transform evaluated

algorithm. The sequence g(n) is of length N . However, h(n) has infinite duration.
Fortunately, only a portion h(n) is required to compute the L values of X(z).

circular convolution of the N -point sequence g(n) with an M -point section of h(n),
where M > N . In such a case, we know that the first N − 1 points contain aliasing
and that the remaining M − N + 1 points are identical to the result that would be
obtained from a linear convolution of h(n) with g(n). In view of this, we should
select a DFT of size

M = L+N − 1
which would yield L valid points and N − 1 points corrupted by aliasing.

The section of h(n) that is needed for this computation corresponds to the values
of h(n) for −(N − 1) ≤ n ≤ (L− 1), which is of length M = L+N − 1, as observed

1

h1(n) = h(n−N + 1), n = 0, 1, . . . ,M − 1

and compute its M -point DFT via the FFT algorithm to obtain H1(k). From x(n)

M -point DFT to yield G(k). The IDFT of the product Y1(k) = G(k)H1(k) yields the
M -point sequence y1(n), n = 0, 1, . . . ,M − 1. The first N − 1 points of y1(n) are
corrupted by aliasing and are discarded. The desired values are y1(n) for N − 1 ≤

y(n) = y1(n+N − 1), n = 0, 1, . . . , L− 1

Alternatively, we can define a sequence h2(n) as

h2(n) =
{
h(n), 0 ≤ n ≤ L− 1
h(n−N − L+ 1), L ≤ n ≤ M − 1

The M -point DFT of h2(n) yields H2(k), which when multiplied by G(k) yields
Y2(k) = G(k)H2(k). The IDFT of Y2(k) yields the sequence y2(n) for 0 ≤ n ≤ M−1.
Now the desired values of y2(n) are in the range 0 ≤ n ≤ L− 1, that is,

y(n) = y2(n), n = 0, 1, . . . , L− 1

Finally, the complex values X(zk) are computed by dividing y(k) by h(k), k = 0,

(3.22)

(3.23)

(3.24)

(3.25)

Efficient Computation of the DFT: Fast Fourier Transform Algorithms

as in (3.18) is called the chirp-z transform.
The linear convolution in (3.21) is most efficiently done by use of the FFT

Since we will compute the convolution in (3.21) via the FFT, let us consider the

from (3.21). Let us define the sequence h (n) of length M as

we compute g(n) as specified by (3.17), pad g(n) with L− 1 zeros, and compute its

n ≤M − 1, which correspond to the range 0 ≤ n ≤ L− 1 in (3.21), that is,

1, . . . , L− 1, as specified by (3.20).

559



In general, the computational complexity of the chirp-z transform algorithm
described above is of the order of M log2 M complex multiplications, where M =
N + L − 1. This number should be compared with the product, N · L, the number
of computations required by direct evaluation of the z-transform. Clearly, if L is
small, direct computation is more efficient. However, if L is large, then the chirp-z
transform algorithm is more efficient.

The chirp-z transform method has been implemented in hardware to compute
the DFT of signals. For the computation of the DFT, we select r0 = R0 = 1, θ0 = 0,
φ0 = 2π/N, and L = N. In this case

V −n
2/2 = e−jπn2/N

= cos πn
2

N
− j sin πn

2

N

The chirp filter with impulse response

h(n) = V n2/2

= cos πn
2

N
+ j sin πn

2

N

= hr(n)+ jhi(n)
has been implemented as a pair of FIR filters with coefficients hr(n) and hi(n), re-
spectively. Both surface acoustic wave (SAW) devices and charge coupled devices

x

x

+
+

+

+

ROM

cos πn
2

N

x(n)

− sin πn
2

N

ROM

FIR
filter

Chirp Filters

hr(n) = cos
πn2
N

FIR
filter

hi(n) = sin
πn2
N

FIR
filter

hi(n) = sin
πn2
N

FIR
filter

hr(n) = cos
πn2
N −

(  ) 2

(  ) 2
+

+

√
y(n)

Figure 3.3 Block diagram illustrating the implementation of the chirp-z transform
for computing the DFT (magnitude only).

(3.26)

(3.27)

Efficient Computation of the DFT: Fast Fourier Transform Algorithms

560



(CCD) have been used in practice for the FIR filters. The cosine and sine sequences

ally stored in a read-only memory (ROM). Furthermore, we note that if only the
magnitude of the DFT is desired, the postmultiplications are unnecessary. In this
case,

|X(zk)| = |y(k)|, k = 0, 1, . . . , N − 1
Thus the linear FIR filtering approach using the chirp-z

transform has been implemented for the computation of the DFT.

4 Quantization Effects in the Computation of the DFT1

As we have observed in our previous discussions, the DFT plays an important role in
many digital signal processing applications, including FIR filtering, the computation
of the correlation between signals, and spectral analysis. For this reason it is important
for us to know the effect of quantization errors in its computation. In particular, we
shall consider the effect of round-off errors due to the multiplications performed in
the DFT with fixed-point arithmetic.

Of particular interest is the analysis of round-off errors in the computation of
the DFT via the FFT algorithm. However, we shall first establish a benchmark by
determining the round-off errors in the direct computation of the DFT.

4.1 Quantization Errors in the Direct Computation of the DFT

Given a finite-duration sequence {x(n)}, 0 ≤ n ≤ N − 1, the DFT of {x(n)} is
defined as

X(k) =
N−1∑
n=0

x(n)WknN , k = 0, 1, . . . , N − 1

where WN = e−j2π/N . We assume that in general, {x(n)} is a complex-valued se-
quence. We also assume that the real and imaginary components of {x(n)} and
{WknN } are represented by b bits. Consequently, the computation of the product
x(n)WknN requires four real multiplications. Each real multiplication is rounded from
2b bits to b bits, and hence there are four quantization errors for each complex-valued
multiplication.

In the direct computation of the DFT, there are N complex-valued multiplica-
tions for each point in the DFT. Therefore, the total number of real multiplications
in the computation of a single point in the DFT is 4N . Consequently, there are 4N
quantization errors.

1

(3.28)

(4.1)

The model that we shall adopt for characterizing round-off errors in multiplication 
is the additive white noise model thatweuse in the statistical analysis of round-off errors 
in IIR and FIR filters. Although the statistical analysis is performed for rounding, the 
analysis can be easily modified to apply to truncation in two’s-complement arithmetic.

It is recommended that the reader review quantization of filter coefficients prior to reading this section.

Efficient Computation of the DFT: Fast Fourier Transform Algorithms

given in (3.26) needed for the premultiplications and postmultiplications are usu-

as illustrated in Fig. 3.3.

561



Let us evaluate the variance of the quantization errors in a fixed-point compu-
tation of the DFT. First, we make the following assumptions about the statistical
properties of the quantization errors.

1. The quantization errors due to rounding are uniformly distributed random vari-
ables in the range (−�/2,�/2) where � = 2−b .

2. The 4N quantization errors are mutually uncorrelated.

3. The 4N quantization errors are uncorrelated with the sequence {x(n)}.
Since each of the quantization errors has a variance

σ 2e =
�2

12
= 2

−2b

12

the variance of the quantization errors from the 4N multiplications is

σ 2q = 4Nσ 2e

= N
3
· 2−2b

Hence the variance of the quantization error is proportional to the size of DFT. Note
that when N is a power of 2 (i.e., N = 2ν ), the variance can be expressed as

σ 2q =
2−2(b−ν/2)

3

This expression implies that every fourfold increase in the size N of the DFT requires
an additional bit in computational precision to offset the additional quantization
errors.

To prevent overflow, the input sequence to the DFT requires scaling. Clearly, an
upper bound on |X(k)| is

|X(k)| ≤
N−1∑
n=0
|x(n)|

If the dynamic range in addition is (−1, 1), then |X(k)| < 1 requires that
N−1∑
n=0
|x(n)| < 1

If |x(n)| is initially scaled such that |x(n)| < 1 for all n, then each point in the

is uniformly distributed in the range (−1/N, 1/N). Then the variance of the signal
sequence is

σ 2x =
(2/N)2

12
= 1

3N2

(4.2)

(4.3)

(4.4)

(4.5)

(4.6)

(4.7)

Efficient Computation of the DFT: Fast Fourier Transform Algorithms

sequence can be divided by N to ensure that (4.6) is satisfied.

signal sequence {x(n)} is white and, after scaling, each value |x(n)| of the sequence
The scaling implied by (4.6) is extremely severe. For example, suppose that the

562



and the variance of the output DFT coefficients |X(k)| is
σ 2X = Nσ 2x

= 1
3N

Thus the signal-to-noise power ratio is

σ 2X

σ 2q
= 2

2b

N2

We observe that the scaling is responsible for reducing the SNR by N and the
combination of scaling and quantization errors results in a total reduction that is
proportional to N2

a severe penalty on the signal-to-noise ratio in the DFT.

sequence with an SNR of 30 dB.

Solution. The size of the sequence is N = 210 . Hence the SNR is

10 log10
σ 2X

σ 2q
= 10 log10 22b−20

For an SNR of 30 dB, we have

3(2b − 20) = 30
b = 15 bits

Note that the 15 bits is the precision for both multiplication and addition.

Instead of scaling the input sequence {x(n)}, suppose we simply require that
|x(n)| < 1. Then we must provide a sufficiently large dynamic range for addition
such that |X(k)| < N . In such a case, the variance of the sequence {|x(n)|} is σ 2x = 13 ,
and hence the variance of |X(k)| is

σ 2X = Nσ 2x =
N

3

Consequently, the SNR is
σ 2X

σ 2q
= 22b

required to achieve an SNR of 30 dB is b = 5 bits. However, we need an additional
10 bits for the accumulator (the adder) to accommodate the increase in the dynamic
range for addition. Although we did not achieve any reduction in the dynamic range
for addition, we have managed to reduce the precision in multiplication from 15 bits
to 5 bits, which is highly significant.

(4.8)

(4.9)

(4.10)

(4.11)

Efficient Computation of the DFT: Fast Fourier Transform Algorithms

. Hence scaling the input sequence {x(n)} to satisfy (4.6) imposes

EXAMPLE 4.1

Use (4.9) to determine the number of bits required to compute the DFT of a 1024-point

If we repeat the computation in Example 4.1, we find that the number of bits

563



4.2 Quantization Errors in FFT Algorithms

As we have shown, the FFT algorithms require significantly fewer multiplications
than the direct computation of the DFT. In view of this we might conclude that the
computation of the DFT via an FFT algorithm will result in smaller quantization
errors. Unfortunately, that is not the case, as we will demonstrate.

Let us consider the use of fixed-point arithmetic in the computation of a radix-2
FFT algorithm. To be specific, we select the radix-2, decimation-in-time algorithm

obtain for this radix-2 FFT algorithm are typical of the results obtained with other
radix-2 and higher radix algorithms.

We observe that each butterfly computation involves one complex-valued mul-
tiplication or, equivalently, four real multiplications. We ignore the fact that some
butterflies contain a trivial multiplication by ±1. If we consider the butterflies that
affect the computation of any one value of the DFT, we find that, in general, there
are N/2 in the first stage of the FFT, N/4 in the second stage, N/8 in the third state,
and so on, until the last stage, where there is only one. Consequently, the number of

x(0)

x(4)

x(2)

x(6)

x(1)

x(5)

x(3)

x(7)

X(0)

X(1)

X(2)

X(3)

X(4)

X(5)

X(6)

X(7)

Stage 1 Stage 2 Stage 3

W 08

W 08

W 08

W 08 W
2
8 W

3
8

W 28

W 18

W 08

W 08

W 28

W 08

−1

−1−1

−1

−1

−1

−1

−1

−1

−1

−1

−1

Decimation-in-time FFT algorithm.

Efficient Computation of the DFT: Fast Fourier Transform Algorithms

illustrated in Fig. 4.1 for the case N = 8. The results on quantization errors that we

Figure 4.1

564



butterflies per output point is

2ν−1 + 2ν−2 + · · · + 2+ 1 = 2ν−1
[

1+
(

1
2

)
+ · · · +

(
1
2

)ν−1]

= 2ν[1−
(

1
2

)ν
] = N − 1

For example, the butterflies that affect the computation of X(3) in the eight-point

The quantization errors introduced in each butterfly propagate to the output.
Note that the quantization errors introduced in the first stage propagate through
(ν−1) stages, those introduced in the second stage propagate through (ν−2) stages,
and so on. As these quantization errors propagate through a number of subsequent
stages, they are phase shifted (phase rotated) by the phase factors WknN . These phase
rotations do not change the statistical properties of the quantization errors and, in
particular, the variance of each quantization error remains invariant.

If we assume that the quantization errors in each butterfly are uncorrelated with
the errors in other butterflies, then there are 4(N − 1) errors that affect the output
of each point of the FFT. Consequently, the variance of the total quantization error
at the output is

σ 2q = 4(N − 1)
�2

12
≈ N�

2

3

−1

x(0)

x(4)

x(2)

x(6)

x(1)

x(5)

x(3)

x(7)

−1 −1

−1

−1 −1
W 08

W 08

W 08

W 08

X(3)

W 38W
2
8

W 28

Figure 4.2 Butterflies that affect the computation of X(3).

(4.12)

(4.13)

Efficient Computation of the DFT: Fast Fourier Transform Algorithms

FFT algorithm of Fig. 4.1 are illustrated in Fig. 4.2.

565



where � = 2−b . Hence
σ 2q =

N

3
· 2−2b

This is exactly the same result that we obtained for the direct computation of the
DFT.

not reduce the number of multiplications required to compute a single point of the
DFT. It does, however, exploit the periodicities in WknN and thus reduces the number
of multiplications in the computation of the entire block of N points in the DFT.

As in the case of the direct computation of the DFT, we must scale the input
sequence to prevent overflow. Recall that if |x(n)| < 1/N , 0 ≤ n ≤ N − 1, then
|X(k)| < 1 for 0 ≤ k ≤ N − 1. Thus overflow is avoided. With this scaling, the

of the DFT, apply to the FFT algorithm as well. Consequently, the same SNR is
obtained for the FFT.

Since the FFT algorithm consists of a sequence of stages, where each stage con-
tains butterflies that involve pairs of points, it is possible to devise a different scaling
strategy that is not as severe as dividing each input point byN . This alternative scaling
strategy is motivated by the observation that the intermediate values |Xn(k)| in the

max[|Xn+1(k)|, |Xn+1(l)|] ≥ max[|Xn(k)|, |Xn(l)|]
n+1(k)|, |Xn+1(l)|] ≤ 2max[|Xn(k)|, |Xn(l)|]

In view of these relations, we can distribute the total scaling of 1/N into each of
the stages of the FFT algorithm. In particular, if |x(n)| < 1, we apply a scale factor
of 12 in the first stage so that |x(n)| < 12 . Then the output of each subsequent stage in
the FFT algorithm is scaled by 12 , so that after ν stages we have achieved an overall
scale factor of ( 12 )

ν = 1/N . Thus overflow in the computation of the DFT is avoided.
This scaling procedure does not affect the signal level at the output of the FFT

algorithm, but it significantly reduces the variance of the quantization errors at the
output. Specifically, each factor of 12 reduces the variance of a quantization error
term by a factor of 14 . Thus the 4(N/2) quantization errors introduced in the first
stage are reduced in variance by ( 14 )

ν−1 , the 4(N/4) quantization errors introduced
in the second stage are reduced in variance by ( 14 )

ν−2 , and so on. Consequently, the
total variance of the quantization errors at the output of the FFT algorithm is

σ 2q =
�2

12

{
4
(
N

2

)(
1
4

)ν−1
+ 4

(
N

4

)(
1
4

)ν−2
+ 4

(
N

8

)(
1
4

)ν−3
+ · · · + 4

}

= �
2

3

{(
1
2

)ν−1
+
(

1
2

)ν−2
+ · · · + 1

2
+ 1

}

= 2�
2

3

[
1−

(
1
2

)ν]
≈ 2

3
· 2−2b

where the factor ( 12 )
ν is negligible.

(4.14)

(4.15)

(4.16)

Efficient Computation of the DFT: Fast Fourier Transform Algorithms

The result in (4.14) should not be surprising. In fact, the FFT algorithm does

relations in (4.7), (4.8), and (4.9), obtained previously for the direct computation

max[|X

n = 1, 2, . . . , ν stages of the FFT algorithm satisfy the conditions (see Problem 35)

566



the signal has the variance σ 2X

σ 2X

σ 2q
= 1

2N
· 22b

= 22b−ν−1

Thus, by distributing the scaling of 1/N uniformly throughout the FFT algorithm,
we have achieved an SNR that is inversely proportional to N instead of N2 .

Determine the number of bits required to compute an FFT of 1024 points with an SNR of
30 dB when the scaling is distributed as described above.

Solution. The size of the FFT is N = 210

10 log10 2
2b−ν−1 = 30

3(2b − 11) = 30

b = 21
2
(11 bits)

This can be compared with the 15 bits required if all the scaling is performed in the first stage
of the FFT algorithm.

5 Summary and References

The focus of this chapter was on the efficient computation of the DFT. We demon-
strated that by taking advantage of the symmetry and periodicity properties of the ex-
ponential factors WknN , we can reduce the number of complex multiplications needed
to compute the DFT from N2 to N log2 N when N is a power of 2. As we indicated,
any sequence can be augmented with zeros, such that N = 2ν .

For decades, FFT-type algorithms were of interest to mathematicians who were
concerned with computing values of Fourier series by hand. However, it was not
until Cooley and Tukey (1965) published their well-known paper that the impact
and significance of the efficient computation of the DFT was recognized. Since then
the Cooley–Tukey FFT algorithm and its various forms, for example, the algorithms
of Singleton (1967, 1969), have had a tremendous influence on the use of the DFT in
convolution, correlation, and spectrum analysis. For a historical perspective on the
FFT algorithm, the reader is referred to the paper by Cooley et al. (1967).

hamel and Hollmann (1984, 1986). The “mirror” FFT (MFFT) and “phase” FFT
(PFFT) algorithms were described to the authors by R. Price. The exploitation of
symmetry properties in the data to reduce the computation time is described in a
paper by Swarztrauber (1986).

(4.17)

Efficient Computation of the DFT: Fast Fourier Transform Algorithms

We now observe that (4.16) is no longer proportional to N . On the other hand,
= 1/3N , as given in (4.8). Hence the SNR is

EXAMPLE 4.2

The split-radix FFT (SRFFT) algorithm described in Section 1.5 is due to Du-

. Hence the SNR according to (4.17) is

567



Over the years, a number of tutorial papers have been published on FFT algo-
rithms. We cite the early papers by Brigham and Morrow (1967), Cochran et al.
(1967), Bergland (1969), and Cooley et al. (1967, 1969).

The recognition that the DFT can be arranged and computed as a linear con-
volution is also highly significant. Goertzel (1968) indicated that the DFT can be
computed via linear filtering, although the computational savings of this approach
is rather modest, as we have observed. More significant is the work of Bluestein
(1970), who demonstrated that the computation of the DFT can be formulated as
a chirp linear filtering operation. This work led to the development of the chirp-z
transform algorithm by Rabiner et al. (1969).

In addition to the FFT algorithms described in this chapter, there are other
efficient algorithms for computing the DFT, some of which further reduce the number
of multiplications, but usually require more additions. Of particular importance is
an algorithm due to Rader and Brenner (1976), the class of prime factor algorithms,
such as the Good algorithm (1971), and the Winograd algorithm (1976, 1978). For
a description of these and related algorithms, the reader may refer to the text by
Blahut (1985).

Problems

1 Show that each of the numbers

ej (2π/N)k, 0 ≤ k ≤ N − 1

corresponds to an N th root of unity. Plot these numbers as phasors in the complex
plane and illustrate, by means of this figure, the orthogonality property

N−1∑
n=0

ej (2π/N)kne−j (2π/N)ln =
{
N,

0,

2 (a) Show that the phase factors can be computed recursively by

W
ql
N = WqNWq(l−1)N

(b) Perform this computation once using single-precision floating-point arithmetic
and once using only four significant digits. Note the deterioration due to the
accumulation of round-off errors in the latter case.

(c) Show how the results in part (b) can be improved by resetting the result to the
correct value −j , each time ql = N/4.

3 Let x(n) be a real-valuedN -point (N = 2ν) sequence. Develop a method to compute
an N -point DFT X′(k), which contains only the odd harmonics [i.e., X′(k) = 0 if k
is even] by using only a real N/2-point DFT.

4 A designer has available a number of eight-point FFT chips. Show explicitly how he
should interconnect three such chips in order to compute a 24-point DFT.

if mod (k − l, N ) 0
otherwise

Efficient Computation of the DFT: Fast Fourier Transform Algorithms

=

568



5 The z-transform of the sequence x(n) = u(n)− u(n− 7) is sampled at five points on
the unit circle as follows:

x(k) = X(z)|z = ej2πk/5, k = 0, 1, 2, 3, 4
Determine the inverse DFT x′(n) of X(k). Compare it with x(n) and explain the
results.

6 Consider a finite-duration sequence x(n), 0 ≤ n ≤ 7, with z-transform X(z). We
wish to compute X(z) at the following set of values:

zk = 0.8ej[(2πk/8)+(π/8)], 0 ≤ k ≤ 7
(a) Sketch the points {zk} in the complex plane.
(b) Determine a sequence s(n) such that its DFT provides the desired samples of

X(z).
7

8

x(n) =
{

1, 0 ≤ n ≤ 7
0, otherwise

9 Derive the signal flow graph for the N = 16-point, radix-4 decimation-in-time FFT

done in place.
10

FFT algorithm in which the input sequence is in digit-reversed order and the output
DFT is in normal order.

11 Compute the eight-point DFT of the sequence

x(n) =
{

1
2
,

1
2
,

1
2
,

1
2
, 0, 0, 0, 0

}

using the in-place radix-2 decimation-in-time and radix-2 decimation-in-frequency
algorithms. Follow exactly the corresponding signal flow graphs and keep track of
all the intermediate quantities by putting them on the diagrams.

12 Compute the 16-point DFT of the sequence

x(n) = cos π
2
n, 0 ≤ n ≤ 15

using the radix-4 decimation-in-time algorithm.
13

(a) What is the gain of the “signal path” that goes from x(7) to X(2)?
(b) How many paths lead from the input to a given output sample? Is this true for

every output sample?
(c) Compute X(3) using the operations dictated by this flow graph.

Efficient Computation of the DFT: Fast Fourier Transform Algorithms

algorithm in which the input sequence is in normal order and the computations are

Compute the eight-point DFT of the sequence

as a special case of the more general algorithmic procedure given by (1.16) through

by using the decimation-in-frequency FFT algorithm described in the text.

Derive the signal flow graph for the N = 16-point, radix-4 decimation-in-frequency

Derive the radix-2 decimation-in-time FFT algorithm given by (1.26) and (1.27)

(1.18).

Consider the eight-point decimation-in-time (DIT) flow graph in Fig. 1.6.

569



14 Draw the flow graph for the decimation-in-frequency (DIF) SRFFT algorithm for
N = 16. What is the number of nontrivial multiplications?

15 Derive the algorithm and draw the N = 8 flow graph for the DIT SRFFT algorithm.

16 Show that the product of two complex numbers (a+jb) and (c+jd) can be performed
with three real multiplications and five additions using the algorithm

xR = (a − b)d + (c − d)a
xI = (a − b)d + (c + d)b

where
x = xR + jxI = (a + jb)(c + jd)

17 Explain how the DFT can be used to compute N equispaced samples of the z-
transform of an N -point sequence, on a circle of radius r .

18 A real-valued N -point sequence x(n) is called DFT bandlimited if its DFT X(k) = 0
for k0 ≤ k ≤ N − k0 . We insert (L− 1)N zeros in the middle of X(k) to obtain the
following LN -point DFT:

X′(k) =
{
X(k), 0 ≤ k ≤ k0 − 1
0, k0 ≤ k ≤ LN − k0
X(k +N − LN), LN − k0 + 1 ≤ k ≤ LN − 1

Show that
Lx′(Ln) = x(n), 0 ≤ n ≤ N − 1

where

x ′(n) DFT←→
LN

X′(k)

Explain the meaning of this type of processing by working out an example with
N = 4, L = 1, and X(k) = {1, 0, 0, 1}.

19 Let X(k) be the N -point DFT of the sequence x(n), 0 ≤ n ≤ N − 1. What is the
N -point DFT of the sequence s(n) = X(n), 0 ≤ n ≤ N − 1?

20 Let X(k) be the N -point DFT of the sequence x(n), 0 ≤ n ≤ N − 1. We define a
2N -point sequence y(n) as

y(n) =
{
x
(n

2

)
, n even

0, n odd

Express the 2N -point DFT of y(n) in terms of X(k).
21 (a) Determine the z-transform W(z) of the Hanning window

w(n) = (1− cos 2πn
N−1

)
/2.

(b) Determine a formula to compute the N -point DFT Xw(k) of the signal xw(n) =
w(n)x(n), 0 ≤ n ≤ N − 1, from the N -point DFT X(k) of the signal x(n).

Efficient Computation of the DFT: Fast Fourier Transform Algorithms

Compare your flow graph with the DIF radix-2 FFT flow graph shown in Fig. 1.11.

570



22 Create a DFT coefficient table that uses only N/4 memory locations to store the first
quadrant of the sine sequence (assume N even).

23
it with the computational burden required in the 2N -point DFT of g(n). Assume
that the FFT algorithm is a radix-2 algorithm.

24 Consider an IIR system described by the difference equation

y(n) = −
N∑
k=1

aky(n− k)+
M∑
k=0

bkx(n− k)

Describe a procedure that computes the frequency responseH
(

2π
N
k

)
, k = 0, 1, . . . ,

N − 1 using the FFT algorithm (N = 2ν).
25 Develop a radix-3 decimation-in-time FFT algorithm for N = 3ν and draw the

corresponding flow graph for N = 9. What is the number of required complex
multiplications? Can the operations be performed in place?

26
27 FFT input and output pruning In many applications we wish to compute only a

few points M of the N -point DFT of a finite-duration sequence of length L (i.e.,
M << N and L << N ).

(a) Draw the flow graph of the radix-2 DIF FFT algorithm for N = 16 and eliminate
[i.e., prune] all signal paths that originate from zero inputs assuming that only
x(0) and x(1) are nonzero.

(b) Repeat part (a) for the radix-2 DIT algorithm.

(c) Which algorithm is better if we wish to compute all points of the DFT? What
happens if we want to compute only the points X(0), X(1), X(2), and X(3)?
Establish a rule to choose between DIT and DIF pruning depending on the
values of M and L.

(d) Give an estimate of saving in computations in terms of M , L, and N .

28 Parallel computation of the DFT Suppose that we wish to compute an N = 2p2ν -
point DFT using 2p digital signal processors (DSPs). For simplicity we assume that
p = ν = 2. In this case each DSP carries out all the computations that are necessary
to compute 2ν DFT points.

(a) Using the radix-2 DIF flow graph, show that to avoid data shuffling, the entire
sequence x(n) should be loaded to the memory of each DSP.

(b) Identify and redraw the portion of the flow graph that is executed by the DSP
that computes the DFT samples X(2), X(10), X(6), and X(14).

(c) Show that, if we use M = 2p DSPs, the computation speed-up S is given by

S = M log2 N
log2 N − log2 M + 2(M − 1)

Efficient Computation of the DFT: Fast Fourier Transform Algorithms

Determine the computational burden of the algorithm given by (2.12) and compare

Repeat Problem 25 for the DIF case.

571



29 Develop an inverse radix-2 DIT FFT algorithm starting with the definition. Draw the
flow graph for computation and compare with the corresponding flow graph for the
direct FFT. Can the IFFT flow graph be obtained from the one for the direct FFT?

30
31 Show that an FFT on data with Hermitian symmetry can be derived by reversing the

flow graph of an FFT for real data.

32 Determine the system function H(z) and the difference equation for the system that
uses the Goertzel algorithm to compute the DFT value X(N − k).

33 (a) Suppose that x(n) is a finite-duration sequence of N = 1024 points. It is desired
to evaluate the z-transform X(z) of the sequence at the points

zk = ej (2π/1024)k, k = 0, 100, 200, . . . , 1000

by using the most efficient method or algorithm possible. Describe an algorithm
for performing this computation efficiently. Explain how you arrived at your
answer by giving the various options or algorithms that can be used.

(b) Repeat part (a) if X(z) is to be evaluated at

zk = 2(0.9)kej [(2π/5000)k+π/2], k = 0, 1, 2, . . . , 999

34 Repeat the analysis for the variance of the quantization error, carried out in Sec-

35 The basic butterfly in the radix-2 decimation-in-time FFT algorithm is

Xn+1(k) = Xn(k)+WmNXn(l)
Xn+1(l) = Xn(k)−WmNXn(l)

(a) If we require that |Xn(k)| < 12 and |Xn(l)| < 12 , show that

|Re[X[Xn+1(k)]| < 1, |Re[Xn+1(l)]| < 1
|Im[X[Xn+1(k)]| < 1, |Im[Xn+1(l)]| < 1

Thus overflow does not occur.

(b) Prove that

max[|Xn+1(k)|, |Xn+1(l)|] ≥ max[|Xn(k)|, |Xn(l)|]
max[|Xn+1(k)|, |Xn+1(l)|] ≤ 2 max[|Xn(k)|, |Xn(l)|]

Efficient Computation of the DFT: Fast Fourier Transform Algorithms

tion 4.2, for the decimation-in-frequency radix-2 FFT algorithm.

Repeat Problem 29 for the DIF case.

572



36 Computation of the DFT Use an FFT subroutine to compute the following DFTs
and plot the magnitudes |X(k)| of the DFTs.
(a) The 64-point DFT of the sequence

x(n) =
{

1, n = 0, 1, . . . , 15 (N1 = 16)
0, otherwise

(b) The 64-point DFT of the sequence

x(n) =
{

1, n = 0, 1, . . . , 7 (N1 = 8)
0, otherwise

(c) The 128-point DFT of the sequence in part (a).

(d) The 64-point DFT of the sequence

x(n) =
{

10ej (π/8)n, n = 0, 1, . . . , 63 (N1 = 64)
0, otherwise

Answer the following questions.

1. What is the frequency interval between successive samples for the plots in
parts (a), (b), (c), and (d)?

2. What is the value of the spectrum at zero frequency (dc value) obtained from
the plots in parts (a), (b), (c), (d)?

From the formula

X(k) =
N−1∑
n=0

x(n)e−j (2π/N)nk

compute the theoretical values for the dc value and check these with the
computer results.

3. In plots (a), (b), and (c), what is the frequency interval between successive
nulls in the spectrum? What is the relationship between N1 of the sequence
x(n) and the frequency interval between successive nulls?

4. Explain the difference between the plots obtained from parts (a) and (c).

37 Identification of pole positions in a system Consider the system described by the
difference equation

y(n) = −r2y(n− 2)+ x(n)
(a) Let r = 0.9 and x(n) = δ(n). Generate the output sequence y(n) for 0 ≤ n ≤

127. Compute the N = 128-point DFT {Y (k)} and plot {|Y (k)|}.

Efficient Computation of the DFT: Fast Fourier Transform Algorithms

573



(b) Compute the N = 128-point DFT of the sequence

w(n) = (0.92)−ny(n)

where y(n) is the sequence generated in part (a). Plot the DFT values |W(k)|.
What can you conclude from the plots in parts (a) and (b)?

(c) Let r = 0.5 and repeat part (a).
(d) Repeat part (b) for the sequence

w(n) = (0.55)−ny(n)

where y(n) is the sequence generated in part (c). What can you conclude from
the plots in parts (c) and (d)?

(e) Now let the sequence generated in part (c) be corrupted by a sequence of “mea-
surement” noise which is Gaussian with zero mean and variance σ 2 = 0.1.
Repeat parts (c) and (d) for the noise-corrupted signal.

5 X(k) = 2+ 2e−j 2π5 + e−j 4π5 + · · · + e−j 8π5
x ′(n) = {2, 2, 1, 1, 1}
x ′(n) =∑m x(n+ 7m), n = 0, 1, · · · 4

8 W8 = 1√2 (1− j)

multiplications do not change this sequence. The next stage produces {4, 4, 0, 0, 0, 0, 0.0} which
again remains unchanged by the phase factors. The last state produces {8, 0, 0, 0, 0, 0, 0, 0} . The
bit reversal to permute the sequence into proper order unscrambles only zeros so the result
remains {8, 0, 0, 0, 0, 0, 0, 0} .

13 (a) “gain” = W 08W 08 (−1)W 28 = −W 28 = j
(b) Given a certain output sample, there is one path from every input leading to it. This is true

for every output.

(c) X(3) = x(0)+W 38 x(1)−W 28 x(2)+W 28W 38 x(3)−W 08 x(4)−W 08W 38 x(5)+
W 08W

2
8 x(6)+W 08W 28W 38 x(7)

16 x = xR + jxI = (a + jb)(c + jd)
e = (a − b)d 1 add 1 mult
xR = e + (c − d)a 2 adds 1 mult
xI = e + (c + d)b 2 adds 1 mult

Total 5 adds 3 mult

19 X(k) =
N−1∑
n=0

x(n)WknN

Let F(t), t = 0, 1, · · · , N − 1 be the DFT of the sequence on k X(k).

F(t) =
N−1∑
k=0

X(k)W tkN = {x(N − 1), x(N − 2), · · · , x(1), x(0)}

Efficient Computation of the DFT: Fast Fourier Transform Algorithms

Refer to Fig. 1.9. The first stage of butterflies produces {2, 2, 2, 2, 0, 0, 0, 0} . The phase factor

Answers to Selected Problems

574



21 (a) W(z) = 1
2

1− z−N
1− z−1 −

1
4

1−
(
z−1ej

2π
N−1

)N
1− z−1ej 2πN−1

− 1
4

1−
(
z−1e−j

2π
N−1

)N
1−z−1 e−j

2π
N−1

(b) xw(n) = w(n)x(n) ; Xw(k) = W(k)NX(k)
32 X(k) =∑N−1m−0 x(m)W−k(N−m)N

This can be viewed as the convolution of the N -length sequence x(n) with the impulse response
of a linear filter.
hk(n)WknN u(n) , evaluated at time N
Hk(z) =

∑∞
n−0 W

kn
N z
−n = 1

1−Wk
N
z−1 =

Yu(z)

X(z)

Then, yk(n) = WkNyk(n− 1)+ x(n), yk(−1) = 0 and yk(N) = X(k)
34 In the DIF case, the number of butterflies affecting a given output is N2 in the first stage,

N

4 in the
second,· · ·. The total number is N − 1.
Every butterfly requires 4 real multiplies, and the error variance is δ

2

12 . Under the assumption
that the errors are uncorrelated, the variance of the total output quantization error is

σ 2q = 4(N − 1)
δ2

12
= Nδ

2

3

Efficient Computation of the DFT: Fast Fourier Transform Algorithms

575



This page intentionally left blank 



Implementation of
Discrete-Time Systems

Of particular importance are the cascade, parallel, and lattice structures, which
exhibit robustness in finite-word-length implementations. Also described in this
chapter is the frequency-sampling realization for an FIR system, which often has
the advantage of being computationally efficient when compared with alternative
FIR realizations. Other filter structures are obtained by employing a state-space
formulation for linear time-invariant systems. Due to space limitations, state-space
structures are not covered.

In addition to describing the various structures for the realization of discrete-time
systems, we also treat problems associated with quantization effects in the imple-
mentation of digital filters using finite-precision arithmetic. This treatment includes
the effects on the filter frequency response characteristics resulting from coefficient
quantization and the round-off noise effects inherent in the digital implementation
of discrete-time systems.

1 Structures for the Realization of Discrete-Time Systems

Let us consider the important class of linear time-invariant discrete-time systems
characterized by the general linear constant-coefficient difference equation

The focus of this chapter is on the realization of linear time-invariant discrete-time 
systems either in software or hardware. There are various configurations or struc-
tures for the realization of any FIR (finite-duration impulse response) and IIR 
(infinite-duration impulse response) discrete-time system. The simplest of these 
structures is the direct-form realizations. However, there are other more practical 
structures that offer some distinct advantages, especially when quantization effects 
are taken into consideration.

John G. Proakis, Dimitris G. Manolakis. Copyright © 2007 by Pearson Education, Inc. All rights reserved.
From Chapter    of Digital Signal Processing: Principles, Algorithms, and Applications, Fourth Edition.9

577



y(n) = −
N∑
k=1

aky(n− k)+
M∑
k=0

bkx(n− k)

As we have shown by means of the z-transform, linear time-invariant discrete-time
systems of this class are also characterized by the rational system function

H(z) =

M∑
k=0

bkz
−k

1+
N∑
k=1

akz
−k

which is a ratio of two polynomials in z−1 . From the latter characterization, we
obtain the zeros and poles of the system function, which depend on the choice of
the system parameters {bk} and {ak} and which determine the frequency response
characteristics of the system.

on the form in which these two characterizations are arranged.

for determining the output sequence y(n) of the system from the input sequence
x(n).
equivalent sets of difference equations. Each set of equations defines a computational
procedure or an algorithm for implementing the system. From each set of equations

If the system is to be implemented in software, the block diagram or, equivalently,

a program that runs on a digital computer. Alternatively, the structure in block
diagram form implies a hardware configuration for implementing the system.

These are the important questions which are answered in this chapter. At this
point in our development, we simply state that the major factors that influence our
choice of a specific realization are computational complexity, memory requirements,
and finite-word-length effects in the computations.

Computational complexity refers to the number of arithmetic operations (multi-
plications, divisions, and additions) required to compute an output value y(n) for the
system. In the past, these were the only items used to measure computational com-
plexity. However, with recent developments in the design and fabrication of rather

(1.1)

(1.2)

we can construct a block diagram consisting of an interconnection of delay elements, 
multipliers, and adders. We refer to such a block diagram as a realization of the system 
or, equivalently, as a structure for realizing the system.

Implementation of Discrete-Time Systems

Our focus in this chapter is on the various methods of implementing (1.1) or
(1.2) either in hardware, or in software on a programmable digital computer. We
shall show that (1.1) or (1.2) can be implemented in a variety of ways depending

In general, we can view (1.1) as a computational procedure (an algorithm)

However, in various ways, the computations in (1.1) can be arranged into

the set of equations that are obtained by rearranging (1.1), can be converted into

Perhaps, the one issue that may not be clear to the reader at this point is 
why we are considering any rearrangements of (1.1) or (1.2). Why not just imple-
ment (1.1) or (1.2) directly without any rearrangement? If either (1.1) or (1.2) is 
rearranged in some manner, what are the benefits gained in the corresponding 
implementation? 

578



sophisticated programmable digital signal processing chips, other factors, such as the
number of times a fetch from memory is performed or the number of times a compar-
ison between two numbers is performed per output sample, have become important
in assessing the computational complexity of a given realization of a system.

Memory requirements refers to the number of memory locations required to store
the system parameters, past inputs, past outputs, and any intermediate computed
values.

Finite-word-length effects or finite-precision effects refer to the quantization ef-
fects that are inherent in any digital implementation of the system, either in hardware
or in software. The parameters of the system must necessarily be represented with
finite precision. The computations that are performed in the process of computing
an output from the system must be rounded off or truncated to fit within the limited
precision constraints of the computer or the hardware used in the implementation.
Whether the computations are performed in fixed-point or floating-point arithmetic
is another consideration. All these problems are usually called finite-word-length
effects and are extremely important in influencing our choice of a system realization.
We shall see that different structures of a system, which are equivalent for infinite
precision, exhibit different behavior when finite-precision arithmetic is used in the
implementation. Therefore, it is very important in practice to select a realization
that is not very sensitive to finite-word-length effects.

Although these three factors are the major ones in influencing our choice of the

such as whether the structure or the realization lends itself to parallel processing,
or whether the computations can be pipelined, may play a role in our selection of
the specific implementation. These additional factors are usually important in the
realization of more complex digital signal processing algorithms.

In our discussion of alternative realizations, we concentrate on the three major
factors just outlined. Occasionally, we will include some additional factors that may
be important in some implementations.

2 Structures for FIR Systems

In general, an FIR system is described by the difference equation

y(n) =
M−1∑
k=0

bkx(n− k)

or, equivalently, by the system function

H(z) =
M−1∑
k=0

bkz
−k

Furthermore, the unit sample response of the FIR system is identical to the coeffi-
cients {bk}, that is,

h(n) =
{
bn, 0 ≤ n ≤ M − 1
0, otherwise

(2.1)

(2.2)

(2.3)

Implementation of Discrete-Time Systems

realization of a system of the type described by either (1.1) or (1.2), other factors,

579



The length of the FIR filter is selected as M to conform with the established notation
in the technical literature.

We shall present several methods for implementing an FIR system, beginning
with the simplest structure, called the direct form. A second structure is the cascade-
form realization. The third structure that we shall describe is the frequency-sampling
realization. Finally, we present a lattice realization of an FIR system. In this discus-
sion we follow the convention often used in the technical literature, which is to use
{h(n)} for the parameters of an FIR system.

2.1 Direct-Form Structure

The direct-form realization follows immediately from the nonrecursive difference

y(n) =
M−1∑
k=0

h(k)x(n− k)

We observe that this structure requires M − 1 memory locations for storing the
M−1 previous inputs, and has a complexity ofM multiplications andM−1 additions
per output point. Since the output consists of a weighted linear combination of M−1
past values of the input and the weighted current value of the input, the structure in

direct-form realization is often called a transversal or tapped-delay-line filter.

h(n) = ±h(M − 1− n)
x(n)

h(0) h(1) h(2) h(3) h(M − 2) h(M − 1)

y(n)
+++++

z−1 z−1 z−1 z−1•  •  •

•  •  •

Direct-form realization of FIR system.

(2.4)

(2.5)

Implementation of Discrete-Time Systems

equation given by (2.1) or, equivalently, by the convolution summation

The structure is illustrated in Fig. 2.1.

Fig. 2.1 resembles a tapped delay line or a transversal system. Consequently, the

Figure 2.1

In addition to the four realizations indicated above, an FIR system can be 
realized by means of the DFT. From one point of view, the DFT can be consid-
ered as a computational procedure rather than a structure for an FIR system. 
However, when the computational procedure is implemented in hardware, there 
is a corresponding structure for the FIR system. In practice, hardware imple-
mentations of the DFT are based on the use of the fast Fourier transform (FFT) 

When the FIR system has linear phase, the unit sample response of the  
system satisfies either the symmetry or asymmetry condition

algorithms.

580



x(n)

y(n)

Input

Output

+

•  •  •

•  •  •

h(0)

+

h(1)

+

h(2) h

+

+

+++ •  •  •

z −1

z −1 z −1 z −1 z −1 z −1

z −1 z −1 z −1 z −1

M  −  3
2( ( h M  −  12( (

Direct-form realization of linear-phase FIR system (M odd).

For such a system the number of multiplications is reduced from M to M/2 for M
even and to (M − 1)/2 for M odd. For example, the structure that takes advantage

2.2 Cascade-Form Structures

It is a simple matter to factor H(z) into second-order FIR systems so that

H(z) =
K∏
k=1

Hk(z)

where

Hk(z) = bk0 + bk1z−1 + bk2z−2, k = 1, 2, . . . , K

and K is the integer part of (M + 1)/2. The filter parameter b0 may be equally
distributed among the K filter sections, such that b0 = b10b20 · · · bK0 or it may be
assigned to a single filter section. The zeros of H(z) are grouped in pairs to produce

of complex-conjugate roots so that the coefficients {bki
On the other hand, real-valued roots can be paired in any arbitrary manner. The
cascade-form realization along with the basic second-order section are shown in

The cascade realization follows naturally from the system function given by (2.2).

(2.6)

(2.7)

In the case of linear-phase FIR filters, the symmetry in  implies that the 
zeros of  also exhibit a form of symmetry. In particular, if and ∗  are a 
pair of complex-conjugate zeros then  and are also a pair of complex-
conjugate zeros. Consequently, we gain some simplification by forming fourth-

h(n)

H(z) zk z
∗
k

1/zk 1/z∗k

Implementation of Discrete-Time Systems

Figure 2.2

of this symmetry is illustrated in Fig. 2.2 for the case in which M is odd.

the second-order FIR systems of the form (2.7). It is always desirable to form pairs
} in (2.7) are real valued.

Fig. 2.3.

581



xk(n)

bk0 bk1 bk2

+ +

z −1 z −1

x(n) = x1(n) y1(n) =

x2(n)

y2(n) =

x3(n)

(a)

(b)

yK − 1(n) = yK(n) = y(n)

yk(n) = xk + 1(n)

xK(n)
H1(z) H2(z) HK(z)•  •  •

order sections of the FIR system as follows:

Hk(z) = ck0(1− zkz−1)(1− z∗kz−1)(1− z−1/zk)(1− z−1/z∗k)

= ck0 + ck1z−1 + ck2z−2 + ck1z−3 + ck0z−4

where the coefficients {ck1} and {ck2} are functions of zk . Thus, by combining the
two pairs of poles to form a fourth-order filter section, we have reduced the number

the basic fourth-order FIR filter structure.

Fourth-order section in a
cascade realization of an
FIR system.

xk(n)

yk(n)

+

ck0 ck1 ck2

+

++

z −1

z −1 z −1

z −1

(2.8)

Implementation of Discrete-Time Systems

Figure 2.3

Figure 2.4

Cascade realization of an FIR system.

of multiplications from six to three (i.e., by a factor of 50%). Figure 2.4 illustrates

582



2.3 Frequency-Sampling Structures

The frequency-sampling realization is an alternative structure for an FIR filter in
which the parameters that characterize the filter are the values of the desired fre-
quency response instead of the impulse response h(n). To derive the frequency-
sampling structure, we specify the desired frequency response at a set of equally
spaced frequencies, namely

ωk = 2π
M
(k + α), k = 0, 1, . . . , M − 1

2
, M odd

k = 0, 1, . . . , M
2
− 1, M even

α = 0 or 12
and solve for the unit sample response h(n) from these equally spaced frequency
specifications. Thus we can write the frequency response as

H(ω) =
M−1∑
n=0

h(n)e−jωn

and the values of H(ω) at frequencies ωk = (2π/M)(k + α) are simply

H(k + α) = H
(

2π
M
(k + α)

)

=
M−1∑
n=0

h(n)e−j2π(k+α)n/M, k = 0, 1, . . . ,M − 1

The set of values {H(k + α)} are called the frequency samples of H(ω). In the case
where α = 0, {H(k)} corresponds to the M -point DFT of {h(n)}.

samples. The result is

h(n) = 1
M

M−1∑
k=0

H(k + α)ej2π(k+α)n/M, n = 0, 1, . . . ,M − 1

When α =
substitute for h(n) in the z-transform H(z), we have

H(z) =
M−1∑
n=0

h(n)z−n

=
M−1∑
n=0

[
1
M

M−1∑
k=0

H(k + α)ej2π(k+α)n/M
]
z−n

(2.9)

(2.10)

(2.11)

Implementation of Discrete-Time Systems

It is a simple matter to invert (2.9) and express h(n) in terms of the frequency

0, (2.10) is simply the IDFT of {H(k)}. Now if we use (2.10) to

583



the summation over the index n we obtain

H(z) =
M−1∑
k=0

H(k + α)
[

1
M

M−1∑
n=0

(ej2π(k+α)/Mz−1)n
]

= 1− z
−Mej2πα

M

M−1∑
k=0

H(k + α)
1− ej2π(k+α)/Mz−1

Thus the system function H(z) is characterized by the set of frequency samples
{H(k + α)} instead of {h(n)}.

We view this FIR filter realization as a cascade of two filters [i.e., H(z) =
H1(z)H2(z)]. One is an all-zero filter, or a comb filter, with system function

H1(z) = 1
M
(1− z−Mej2πα)

Its zeros are located at equally spaced points on the unit circle at

zk = ej2π(k+α)/M, k = 0, 1, . . . ,M − 1

The second filter with system function

H2(z) =
M−1∑
k=0

H(k + α)
1− ej2π(k+α)/Mz−1

consists of a parallel bank of single-pole filters with resonant frequencies

pk = ej2π(k+α)/M, k = 0, 1, . . . ,M − 1

Note that the pole locations are identical to the zero locations and that both
occur at ωk = 2π(k+α)/M , which are the frequencies at which the desired frequency
response is specified. The gains of the parallel bank of resonant filters are simply
the complex-valued parameters {H(k+ α)}. This cascade realization is illustrated in

When the desired frequency response characteristic of the FIR filter is narrow-
band, most of the gain parameters {H(k + α)} are zero. Consequently, the corre-
sponding resonant filters can be eliminated and only the filters with nonzero gains
need be retained. The net result is a filter that requires fewer computations (mul-

(2.12)

(2.13)

(2.14)

Implementation of Discrete-Time Systems

By interchanging the order of the two summations in (2.11) and performing

Fig. 2.5.

584



+

+ +

+

z −1
e j2πα/M

z −1

H(α)

H(1 + α)

H(2 + α)

e j2π(1 +α)/M

e j2π(2 +α)/M

+ +

z −1

−e j2πα 

α  = 0 or

z −M

H(M − 1 + α)

y(n)

x(n)

e j2π(M − 1 +α)/M

+ +

z −1

• 
 •

  •

• 
 •

  •

1
2

1
M

Frequency-sampling realization of FIR filter.

tiplications and additions) than the corresponding direct-form realization. Thus we
obtain a more efficient realization.

The frequency-sampling filter structure can be simplified further by exploiting
the symmetry in H(k + α), namely, H(k) = H ∗(M − k) for α = 0 and

H
(
k + 12

)
= H

(
M − k − 12

)
, for α = 12

of single-pole filters can be combined to form a single two-pole filter with real-valued

Implementation of Discrete-Time Systems

Figure 2.5

These relations are easily deduced from (2.9). As a result of this symmetry, a pair

585



parameters. Thus for α = 0 the system function H2(z) reduces to

H2(z) = H(0)1− z−1 +
(M−1)/2∑
k=1

A(k)+ B(k)z−1
1− 2 cos(2πk/M)z−1 + z−2 , M odd

H2(z) = H(0)1− z−1 +
H(M/2)
1+ z−1 +

(M/2)−1∑
k=1

A(k)+ B(k)z−1
1− 2 cos(2πk/M)z−1 + z−2 , M even

where, by definition,

A(k) = H(k)+H(M − k)
B(k) = H(k)e−j2πk/M +H(M − k)ej2πk/M

Similar expressions can be obtained for α = 12 .

Sketch the block diagram for the direct-form realization and the frequency-sampling realiza-
tion of the M = 32, α = 0, linear-phase (symmetric) FIR filter which has frequency samples

H

(
2πk
32

)
=




1, k = 0, 1, 2
1
2
, k = 3

0, k = 4, 5, . . . , 15
Compare the computational complexity of these two structures.

Solution. Since the filter is symmetric, we exploit this symmetry and thus reduce the number
of multiplications per output point by a factor of 2, from 32 to 16 in the direct-form realization.
The number of additions per output point is 31. The block diagram of the direct realization is

x(n)

y(n)

h(15)

+++++

++++

h(14)h(2)h(1)h(0)

z −1z −1z −1

z −1z −1z −1

z −1

z −1

•  •  •

•  •  •

•  •  •

Direct-form realization of M = 32 FIR filter.

(2.15)

(2.16)

Implementation of Discrete-Time Systems

EXAMPLE 2.1

illustrated in Fig. 2.6.

Figure 2.6

586



+

+

+

z −1

A(1)

2 cos

z −32

+

z −1

z −1

B(1)

A(2)

B(2)

A(3)

B(3)

+ +

y(n)

x(n)

16
π

8
π

1
32

+

2 cos

16
3π

2 cos

−1

−1

−1

+

z −1

z −1

+ +

+ +

+

z −1

z −1

+

−

+

all terms that have zero-gain coefficients {H(k)} . The nonzero coefficients are H(k) and the
corresponding pairs are H(M − k), for k = 0, 1, 2, 3. The block diagram of the resulting

Implementation of Discrete-Time Systems

Figure 2.7 Frequency-sampling realization for the FIR filter in Example 2.1.

We use the form in (2.13) and (2.15) for the frequency-sampling realization and drop

realization is shown in Fig. 2.7. Since H(0) = 1, the single-pole filter requires no multipli-

587



cation. The three double-pole filter sections require three multiplications each for a total of
nine multiplications. The total number of additions is 13. Therefore, the frequency-sampling
realization of this FIR filter is computationally more efficient than the direct-form realization.

Lattice Structure

In this section we introduce another FIR filter structure, called the lattice filter or
lattice realization. Lattice filters are used extensively in digital speech processing
and in the implementation of adaptive filters.

Let us begin the development by considering a sequence of FIR filters with
system functions

Hm(z) = Am(z), m = 0, 1, 2, . . . ,M − 1

where, by definition, Am(z) is the polynomial

Am(z) = 1+
m∑
k=1

αm(k)z
−k, m ≥ 1

and A0(z) = 1. The unit sample response of the mth filter is hm(0) = 1 and hm(k) =
αm(k), k = 1, 2, . . . , m. The subscript m on the polynomial Am(z) denotes the degree
of the polynomial. For mathematical convenience, we define αm(0) = 1.

If {x(n)} is the input sequence to the filter Am(z) and {y(n)} is the output se-
quence, we have

y(n) = x(n)+
m∑
k=1

αm(k)x(n− k)

x(n)

y(n)

•  •  •

•  •  •+ +

z −1 z −1

αm(1) αm(2) αm(3) αm(m)αm(m − 1)1

+ + +

z −1 z −1

x(n) y(n)

 −
•  •  •

•  •  •+ +

z −1 z −1z −1

−αm(1) −αm(2) −αm(3) −αm(4) −αm(m)

+ +

z −1 z −1

+

Direct-form realization of the FIR prediction filter.

(2.17)

(2.18)

(2.19)

Implementation of Discrete-Time Systems

2.4

Two direct-form structures of the FIR filter are illustrated in Fig. 2.8.

Figure 2.8

588



x(n)

K1

f0(n)

g0(n)
g1(n)

f1(n) = y(n)

f0(n) = g0(n) = x(n)

f1(n) = f0(n) + K1g0(n − 1) = x(n) + K1x(n − 1)
g1(n) = K1 f0(n) + g0(n − 1) = K1x(n) + x(n − 1)

g0(n − 1)

K1

+

+

z −1

Single-stage lattice filter.

x̂(n) = −
m∑
k=1

αm(k)x(n− k)

is the one-step forward predicted value of x(n), based on m past inputs, and y(n) =
In this

Now suppose that we have a filter of order m = 1. The output of such a filter is

y(n) = x(n)+ α1(1)x(n− 1)

This output can also be obtained from a first-order or single-stage lattice filter, illus-

from the top branch. 1 = α1(1).
The parameter K1

Next, let us consider an FIR filter for which m = 2. In this case the output from
a direct-form structure is

y(n) = x(n)+ α2(1)x(n− 1)+ α2(2)x(n− 2)

f1(n) = x(n)+K1x(n− 1)
g1(n) = K1x(n)+ x(n− 1)

The output from the second stage is

f2(n) = f1(n)+K2g1(n− 1)
g2(n) = K2f1(n)+ g1(n− 1)

(2.20)

(2.21)

(2.22)

(2.23)

(2.24)

The FIR structures shown in Fig. 2.8 are intimately related with the topic of 
linear prediction, where

Implementation of Discrete-Time Systems

Figure 2.9

x(n) − x̂(n), given by (2.19), represents the prediction error sequence.
context, the top filter structure in Fig. 2.8 is called a prediction error filter.

in the lattice is called a reflection coefficient.

trated in Fig. 2.9, by exciting both of the inputs by x(n) and selecting the output
Thus the output is exactly (2.21), if we select K

By cascading two lattice stages as shown in Fig. 2.10, it is possible to obtain the
same output as (2.22). Indeed, the output from the first stage is

589



x(n)

K1

f0(n) f1(n)

g0(n) g1(n) g2(n)

f2(n) = y(n)

K1

z −1
K2

K2

+ +

++ z −1

Two-stage lattice filter.

2 1 1

f2(n) = x(n)+K1x(n− 1)+K2[K1x(n− 1)+ x(n− 2)]
= x(n)+K1(1+K2)x(n− 1)+K2x(n− 2)

if we equate the coefficients, that is,

α2(2) = K2, α2(1) = K1(1+K2)

or, equivalently,

K2 = α2(2), K1 = α2(1)1+ α2(2)
Thus the reflection coefficients K1 and K2 of the lattice filter can be obtained from
the coefficients {αm(k)} of the direct-form realization.

By continuing this process, one can easily demonstrate, by induction, the equiva-
lence between an mth-order direct-form FIR filter and an m-order or m-stage lattice
filter. The lattice filter is generally described by the following set of order-recursive
equations:

f0(n) = g0(n) = x(n)
fm(n) = fm−1(n)+Kmgm−1(n− 1), m = 1, 2, . . . ,M − 1
gm(n) = Kmfm−1(n)+ gm−1(n− 1), m = 1, 2, . . . ,M − 1

Then the output of the (M − 1)-stage filter corresponds to the output of an (M − 1)-
order FIR filter, that is,

y(n) = fM−1(n)

If we focus our attention on f (n) and substitute for f (n) and g (n−1) from (2.23)

(2.25)

(2.26)

(2.27)

(2.28)

(2.29)

(2.30)

Implementation of Discrete-Time Systems

Figure 2.10

into (2.24), we obtain

Figure 2.11 illustrates an (M − 1)-stage lattice filter in block diagram form along
with a typical stage that shows the computations specified by (2.29) and (2.30).

Now (2.25) is identical to the output of the direct-form FIR filter as given by (2.22),

590



•  •  •

•  •  •

Km

fm − 1(n)

gm − 1(n)

f0(n)

g0(n)

f1(n)

g1(n)

f2(n)

g2(n)

fM − 2(n)

gM − 2(n)

fM − 1(n) = y(n)

gM − 1(n)
x(n)

fm(n)

gm(n)

(b)

(a)

Km

z −1

+

+

(M − 1)st
stage

Second
stage

First
stage

Figure 2.11 (M − 1)-stage lattice filter.

As a consequence of the equivalence between an FIR filter and a lattice filter,
the output fm(n) of an m-stage lattice filter can be expressed as

fm(n) =
m∑
k=0

αm(k)x(n− k), αm(0) = 1

Fm(z) = Am(z)X(z)
or, equivalently,

Am(z) = Fm(z)
X(z)

= Fm(z)
F0(z)

The other output component from the lattice, namely, gm(n), can also be ex-

coefficients, say {βm(k)}. That this in fact is the case becomes apparent from obser-

the lattice filter that produces f1(n) are {1,K1} = {1, α1(1)} while the coefficients for
the filter with output g1(n) are {K1, 1} = {α1(1), 1}. We note that these two sets of
coefficients are in reverse order. If we consider the two-stage lattice filter, with the

2

g2(n) = K2f1(n)+ g1(n− 1)
= K2[x(n)+K1x(n− 1)]+K1x(n− 1)+ x(n− 2)
= K2x(n)+K1(1+K2)x(n− 1)+ x(n− 2)
= α2(2)x(n)+ α2(1)x(n− 1)+ x(n− 2)

Consequently, the filter coefficients are {α2(2), α2(1), 1}, whereas the coefficients for
the filter that produces the output f2(n) are {1, α2(1), α2(2)}. Here, again, the two
sets of filter coefficients are in reverse order.

(2.31)

(2.32)

Implementation of Discrete-Time Systems

Since (2.31) is a convolution sum, it follows that the z-transform relationship is

pressed in the form of a convolution sum as in (2.31), by using another set of

vation of (2.23) and (2.24). From (2.23) we note that the filter coefficients for

output given by (2.24), we find that g (n) can be expressed in the form

591



From this development it follows that the output gm(n) from an m-stage lattice
filter can be expressed by the convolution sum of the form

gm(n) =
m∑
k=0

βm(k)x(n− k)

where the filter coefficients {βm(k)} are associated with a filter that produces fm(n) =
y(n) but operates in reverse order. Consequently,

βm(k) = αm(m− k), k = 0, 1, . . . , m
with βm(m) = 1.

In the context of linear prediction, suppose that the data x(n), x(n − 1), . . . ,
x(n −m+ 1) is used to linearly predict the signal value x(n −m) by use of a linear
filter with coefficients {−βm(k)}. Thus the predicted value is

x̂(n−m) = −
m−1∑
k=0

βm(k)x(n− k)

Since the data are run in reverse order through the predictor, the prediction per-
In contrast, the FIR filter with

system function Am(z) is called a forward predictor.

Gm(z) = Bm(z)X(z)
or, equivalently,

Bm(z) = Gm(z)
X(z)

where Bm(z) represents the system function of the FIR filter with coefficients {βm(k)},
that is,

Bm(z) =
m∑
k=0

βm(k)z
−k

Since βm(k) = αm

Bm(z) =
m∑
k=0

αm(m− k)z−k

=
m∑
l=0

αm(l)z
l−m

= z−m
m∑
l=0

αm(l)z
l

= z−mAm(z−1)

(2.33)

(2.34)

(2.35)

(2.36)

(2.37)

(2.38)

(2.39)

Implementation of Discrete-Time Systems

formed in (2.35) is called backward prediction.

In the z-transform domain, (2.33) becomes

(m− k), (2.38) may be expressed as

592



Bm(z) are simply the reciprocals of the zeros of Am(z). Hence Bm(z) is called the
reciprocal or reverse polynomial of Am(z).

Now that we have established these interesting relationships between the direct-
form FIR filter and the lattice structure, let us return to the recursive lattice equations

F0(z) = G0(z) = X(z)
Fm(z) = Fm−1(z)+Kmz−1Gm−1(z), m = 1, 2, . . . ,M − 1
Gm(z) = KmFm−1(z)+ z−1Gm−1(z), m = 1, 2, . . . ,M − 1

If we divide each equation by X(z), we obtain the desired results in the form

A0(z) = B0(z) = 1
Am(z) = Am−1(z)+Kmz−1Bm−1(z), m = 1, 2, . . . ,M − 1
Bm(z) = KmAm−1(z)+ z−1Bm−1(z), m = 1, 2, . . . ,M − 1

Thus a lattice stage is described in the z-domain by the matrix equation

[
Am(z)

Bm(z)

]
=
[

1 Km
Km 1

] [
Am−1(z)

z−1Bm−1(z)

]

Before concluding this discussion, it is desirable to develop the relationships
for converting the lattice parameters {Ki}, that is, the reflection coefficients, to the
direct-form filter coefficients {αm(k)}, and vice versa.
Conversion of lattice coefficients to direct-form filter coefficients. The direct-form
FIR filter coefficients {αm(k)} can be obtained from the lattice coefficients {Ki} by
using the following relations:

A0(z) = B0(z) = 1
Am(z) = Am−1(z)+Kmz−1Bm−1(z), m = 1, 2, . . . ,M − 1
Bm(z) = z−mAm(z−1), m = 1, 2, . . . ,M − 1

The solution is obtained recursively, beginning with m = 1. Thus we obtain a se-
quence of (M − 1) FIR filters, one for each value of m. The procedure is best
illustrated by means of an example.

Given a three-stage lattice filter with coefficients K1 = 14 , K2 = 14 , K3 = 13 , determine the

(2.40)

(2.41)

(2.42)

(2.43)

(2.44)

(2.45)

(2.46)

(2.47)

(2.48)

(2.49)

Implementation of Discrete-Time Systems

The relationship in (2.39) implies that the zeros of the FIR filter with system function

in (2.28) through (2.30) and transfer them to the z-domain. Thus we have

FIR filter coefficients for the direct-form structure.

EXAMPLE 2.2

593



Solution. Thus we
have

A1(z) = A0(z)+K1z−1B0(z)
= 1+K1z−1 = 1+ 14z−1

Hence the coefficients of an FIR filter corresponding to the single-stage lattice are α1(0) = 1,
α1(1) = K1 = 14 . Since Bm(z) is the reverse polynomial of Am(z), we have

B1(z) = 14 + z−1

A2(z) = A1(z)+K2z−1B1(z)
= 1+ 38 z−1 + 12z−2

Hence the FIR filter parameters corresponding to the two-stage lattice are α2(0) = 1, α2(1) =
3
8 , α2(2) = 12 . Also,

B2(z) = 12 + 38 z−1 + z−2
Finally, the addition of the third stage to the lattice results in the polynomial

A3(z) = A2(z)+K3z−1B2(z)
= 1+ 1324 z−1 + 58z−2 + 13z−3

Consequently, the desired direct-form FIR filter is characterized by the coefficients

α3(0) = 1, α3(1) = 1324 , α3(2) = 58 , α3(3) = 13

As this example illustrates, the lattice structure with parameters K1 , K2, . . . ,
Km , corresponds to a class of m direct-form FIR filters with system functions A1(z),
A2(z), . . . , Am(z). It is interesting to note that a characterization of this class ofm FIR
filters in direct form requires m(m + 1)/2 filter coefficients. In contrast, the lattice-
form characterization requires only the m reflection coefficients {Ki}. The reason
that the lattice provides a more compact representation for the class of m FIR filters
is simply that the addition of stages to the lattice does not alter the parameters of
the previous stages. On the other hand, the addition of the mth stage to a lattice
with (m − 1) stages results in an FIR filter with system function Am(z) that has
coefficients totally different from the coefficients of the lower-order FIR filter with
system function Am−1(z).

A formula for determining the filter coefficients {αm(k)} recursively can be eas-
From the

Am(z) = Am−1(z)+Kmz−1Bm−1(z)
m∑
k=0

αm(k)z
−k =

m−1∑
k=0

αm−1(k)z−k +Km
m−1∑
k=0

αm−1(m− 1− k)z−(k+1)
(2.50)

Implementation of Discrete-Time Systems

We solve the problem recursively, beginning with (2.48) for m = 1.

Next we add the second stage to the lattice. For m = 2, (2.48) yields

ily derived from polynomial relationships in (2.47) through (2.49).
relationship in (2.48) we have

594



By equating the coefficients of equal powers of z−1 and recalling that αm(0) = 1
for m = 1, 2, . . . ,M − 1, we obtain the desired recursive equation for the FIR filter
coefficients in the form

αm(0) = 1
αm(m) = Km
αm(k) = αm−1(k)+Kmαm−1(m− k)

= αm−1(k)+ αm(m)αm−1(m− k), 1 ≤ k ≤ m− 1
m = 1, 2, . . . ,M − 1

Conversion of direct-form FIR filter coefficients to lattice coefficients. Suppose that
we are given the FIR coefficients for the direct-form realization or, equivalently,
the polynomial Am(z), and we wish to determine the corresponding lattice filter
parameters {Ki}. For the m-stage lattice we immediately obtain the parameterKm =
αm(m). To obtain Km−1 we need the polynomials Am−1(z) since, in general, Km is
obtained from the polynomial Am(z) for m = M − 1, M − 2, . . . , 1. Consequently,
we need to compute the polynomials Am(z) starting from m = M − 1 and “stepping
down” successively to m = 1.

The desired recursive relation for the polynomials is easily determined from

Am(z) = Am−1(z)+Kmz−1Bm−1(z)
= Am−1(z)+Km[Bm(z)−KmAm−1(z)]

If we solve for Am−1(z), we obtain

Am−1(z) = Am(z)−KmBm(z)1−K2m
, m = M − 1,M − 2, . . . , 1

Thus we compute all lower-degree polynomials Am(z) beginning with AM−1(z) and
obtain the desired lattice coefficients from the relation Km = αm(m). We observe
that the procedure works as long as |Km| �= 1 for m = 1, 2, . . . ,M − 1.

H(z) = A3(z) = 1+ 1324 z−1 + 58z−2 + 13z−3

Solution. First we note that K3 = α3(3) = 13 . Furthermore,

B3(z) = 13 + 58 z−1 + 1324z−2 + z−3

(2.51)

(2.52)

(2.53)

(2.54)

equations.

Implementation of Discrete-Time Systems

We note that (2.51) through (2.53) are simply the Levinson–Durbin recursive

(2.44) and (2.45). We have

Determine the lattice coefficients corresponding to the FIR filter with system function

EXAMPLE 2.3

595



A2(z) = A3(z)−K3B3(z)
1−K23

= 1+ 38z−1 + 12 z−2

Hence K2 = α2(2) = 12 and B2(z) = 12 + 38z−1 + z−1 . By repeating the step-down recursion in

A1(z) = A2(z)−K2B2(z)
1−K22

= 1+ 14z−1

Hence K1 = α1(1) = 14 .

a formula for recursively computing Km , beginning with m = M − 1 and stepping
down to m = 1. For m = M − 1, M − 2, . . . , 1 we have

Km = αm(m), αm−1(0) = 1

αm−1(k) = αm(k)−Kmβm(k)1−K2m

= αm(k)− αm(m)αm(m− k)
1− α2m(m)

, 1 ≤ k ≤ m− 1

parameters |Km| = 1. If this occurs, it is indicative of the fact that the polynomial
Am−1(z) has a root on the unit circle. Such a root can be factored out from Am−1(z)

3 Structures for IIR Systems

In this section we consider different IIR systems structures described by the differ-

the case of FIR systems, there are several types of structures or realizations, including
direct-form structures, cascade-form structures, lattice structures, and lattice-ladder
structures. In addition, IIR systems lend themselves to a parallel-form realization.
We begin by describing two direct-form realizations.

3.1 Direct-Form Structures

be viewed as two systems in cascade, that is,

H(z) = H1(z)H2(z)

(2.55)

(2.56)

(3.1)

Implementation of Discrete-Time Systems

The step-down relationship in (2.54) with m = 3 yields

(2.51), we obtain

From the step-down recursive equation in (2.54), it is relatively easy to obtain

As indicated above, the recursive equation in (2.56) breaks down if any lattice

and the iterative process in (2.56) is carried out for the reduced-order system.

ence equation in (1.1) or, equivalently, by the system function in (1.2). Just as in

The rational system function as given by (1.2) that characterizes an IIR system can

596



where H1(z) consists of the zeros of H(z), and H2(z) consists of the poles of H(z),

H1(z) =
M∑
k=0

bkz
−k

and

H2(z) = 1

1+
N∑
k=1

akz
−k

x(n) b0 y(n)

z −1 z −1

z −1z −1

z −1 z −1

z −1 z −1

b1

b2

b3

bM − 1

bM

−a1

−a2

−a3

−aN − 1

−aN

+ +

+ +

+ +

+ +

+ +

• 
 •

  •

• 
 •

  •

• 
 •

  •

• 
 •

  •

All-zero system All-pole system

Figure 3.1 Direct form I realization.

(3.2)

(3.3)

Consider two different direct-form realizations, characterized by whether 
H1(z) precedes H2(z), or vice versa. Since H1(z) is an FIR system, its direct-form  
realization was illustrated in Fig. 2.1. By attaching the all-pole system in cascade with 
H1(z), we obtain the direct form I realization depicted in Fig. 3.1. This realization 
requires M+N +1 multiplications, M+N additions, and M+N +1 memory locations.

Implementation of Discrete-Time Systems

597



w(n) = −
N∑
k=1

akw(n− k)+ x(n)

Since w(n) is the input to the all-zero system, its output is

y(n) =
M∑
k=0

bkw(n− k)

Consequently, only a single delay line or a single set of memory locations is required

This
{
M,N} memory locations. Since the direct form II realization minimizes the number

of memory locations, it is said to be canonic. However, we should indicate that
other IIR structures also possess this property, so that this terminology is perhaps
unjustified.

because they are obtained directly from the system function H(z) without any re-
arrangement of H(z). Unfortunately, both structures are extremely sensitive to
parameter quantization, in general, and are not recommended in practical applica-

N is large, a small change in a filter coefficient due to parameter quantization results
in a large change in the location of the poles and zeros of the system.

Direct form II realization
(N = M).

x(n) b0 y(n)

b1

b2

bN − 1

bN

−a1

−a2

−aN − 1

−aN

z −1

z −1

z −1

• 
 •

  •

• 
 •

  •
• 

 •
  •

• 
 •

  •

+

+ +

+ +

++

+

(3.4)

(3.5)

Implementation of Discrete-Time Systems

for storing the past values of {w(n)}. The resulting structure that implements (3.4)

structure requires M+N+1 multiplications, M+N additions, and the maximum of

We note that both (3.4) and (3.5) involve delayed versions of the sequence {w(n) }.

and (3.5) is called a direct form II realization and is depicted in Fig. 3.2.

The structures in Figs. 3.1 and 3.2 are both called “direct-form” realizations

Figure 3.2

tions. This topic is discussed in detail in Section 6, where we demonstrate that when

If the all-pole filter H2(z) is placed before the  all-zero filter H1(z),  a more 
compact structure is obtained. The  difference equation for the all-pole filter is

598



Signal Flow Graphs and Transposed Structures

A signal flow graph provides an alternative, but equivalent, graphical representation
to a block diagram structure that we have been using to illustrate various system
realizations. The basic elements of a flow graph are branches and nodes. A signal
flow graph is basically a set of directed branches that connect at nodes. By definition,
the signal out of a branch is equal to the branch gain (system function) times the
signal into the branch. Furthermore, the signal at a node of a flow graph is equal to
the sum of the signals from all branches connecting to the node.

To illustrate these basic notions, let us consider the two-pole and two-zero IIR
The system block diagram

We note that the
flow graph contains five nodes labeled 1 through 5. Two of the nodes (1, 3) are
summing nodes (i.e., they contain adders), while the other three nodes represent
branching points. Branch transmittances are indicated for the branches in the flow
graph. Note that a delay is indicated by the branch transmittance z−1 . When the
branch transmittance is unity, it is left unlabeled. The input to the system originates
at a source node and the output signal is extracted at a sink node.

We observe that the signal flow graph contains the same basic information as the
block diagram realization of the system. The only apparent difference is that both

x(n) b0

b0

b1

b2
−a2

−a1
z −1

z −1

y(n)

b1

b2

−a1

−a2

z −1

z −1

+

+ +

+

(a)

(b)

Source node
x(n) 1 2

4

5

3
Sink node
y(n)

Figure 3.3 Second-order filter structure (a) and its signal flow
graph (b).

Implementation of Discrete-Time Systems

3.2

system depicted in block diagram form in Fig. 3.3(a).
can be converted to the signal flow graph shown in Fig. 3.3(b).

599



branch points and adders in the block diagram are represented by nodes in the signal
flow graph.

The subject of linear signal flow graphs is an important one in the treatment
of networks, and many interesting results are available. One basic notion involves
the transformation of one flow graph into another without changing the basic input–
output relationship. Specifically, one technique that is useful in deriving new system
structures for FIR and IIR systems stems from the transposition or flow-graph rever-
sal theorem. This theorem simply states that if we reverse the directions of all branch
transmittances and interchange the input and output in the flow graph, the system
function remains unchanged. The resulting structure is called a transposed structure
or a transposed form.

original flow graph resulted in branching nodes becoming adder nodes, and vice
versa.

Let us apply the transposition theorem to the direct form II structure. First,

adders and adders into nodes, and finally, we interchange the input and the output.

Signal flow graph of
transposed structure (a) and
its realization (b).

y(n) b0

b0

b1

b2−a2

−a1 z 
−1

z −1

x(n)

b1−a1

z −1

z −1

+

+

(b)

(a)

x(n)1 2

4

5

3y(n)

b2−a2

+

Implementation of Discrete-Time Systems

For example, the transposition of the signal flow graph in Fig. 3.3(b) is illus-
trated in Fig. 3.4(a). The corresponding block diagram realization of the transposed
form is depicted in Fig. 3.4(b). It is interesting to note that the transposition of the

we reverse all the signal flow directions in Fig. 3.2. Second, we change nodes into

Figure 3.4

600



Transposed direct form II
structure.

y(n) b0 x(n)

b1−a1

+

b2−a2

bN − 1−aN − 1

−aN bN

+

z −1

+

• 
 •

  •

• 
 •

  •

• 
 •

  •

z −1

+

+

z −1

the output on the right.

by the set of difference equations

y(n) = w1(n− 1)+ b0x(n)
wk(n) = wk+1(n− 1)− aky(n)+ bkx(n), k = 1, 2, . . . , N − 1

Transposed direct form II
structure.

x(n) b0 y(n)

b1
w1(n)

w2(n)

wN(n)

−a1

−aN

bN − 1 −aN − 1

bN

z −1

+

• 
 •

  •

• 
 •

  •

• 
 •

  •

z −1

+

z −1

+

+

(3.6)

(3.7)

Implementation of Discrete-Time Systems

Figure 3.5

These operations result in the transposed direct form II structure shown in Fig. 3.5.

The transposed direct form II realization that we have obtained can be described

This structure can be redrawn as in Fig. 3.6, which shows the input on the left and

Figure 3.6

601



wN(n) = bNx(n)− aNy(n)
Without loss of generality, we have assumed that M = N in writing equations. It

equivalent to the single difference equation

y(n) = −
N∑
k=1

aky(n− k)+
M∑
k=0

bkx(n− k)

Finally, we observe that the transposed direct form II structure requires the same
number of multiplications, additions, and memory locations as the original direct-
form II structure.

Although our discussion of transposed structures has been concerned with the

k = 0, k = 1, 2, . . . , N, also has a transposed direct form

ak
set of difference equations

wM(n) = bMx(n)
wk(n) = wk+1(n− 1)+ bkx(n), k = M − 1,M − 2, . . . , 1
y(n) = w1(n− 1)+ b0x(n)

difference equations for a basic two-pole and two-zero IIR system with system func-
tion

H(z) = b0 + b1z
−1 + b2z−2

1+ a1z−1 + a2z−2
This is the basic building block in the cascade realization of high-order IIR systems,
as described in the following section. Of the three direct-form structures given in

memory locations required in their implementation.
Finally, we note that in the z-domain, the set of difference equations describing

a linear signal flow graph constitute a linear set of equations. Any rearrangement of
such a set of equations is equivalent to a rearrangement of the signal flow graph to
obtain a new structure, and vice versa.

x(n)

bM bM−1 b2 b1 b0

wM(n) w2(n)
++++

•  •  •

•  •  •z −1
w1(n)

y(n)
z −1 z −1

Transposed FIR structure.

(3.8)

(3.9)

(3.10)

(3.11)

(3.12)

(3.13)

Implementation of Discrete-Time Systems

is also clear from observation of Fig. 3.6 that this set of difference equations is

from (3.9) by setting the a

= 0, k = 1, 2, . . . , N . This transposed-form realization may be described by the

general form of an IIR system, it is interesting to note that an FIR system, obtained

as illustrated in Fig. 3.7. This structure is simply obtained from Fig. 3.6 by setting

Figure 3.7

In summary, Table 1 illustrates the direct-form structures and the corresponding

Table 1, the direct form II structures are preferable due to the smaller number of

602



TABLE 1 Some Second-Order Modules for Discrete-Time Systems

Structure Implementation Equations System Function

x(n) b0

b1

b2

y(n)

−a1

−a2
z −1

z −1

z −1

z −1

+

+ +

+

D
ir

ec
t F

or
m

 I y(n) = b0x(n)+ b1x(n− 1)
+ b2x(n− 2)
− a1y(n− 1)− a2y(n− 2)

H(z) = b0 + b1z−1 + b2z−2
1+ a1z−1 + a2z−2

x(n) b0 y(n)

w(n − 1)

w(n − 2)

b1

b2

−a1

−a2

+ +

++

z −1

z −1

R
eg

ul
ar

 D
ir

ec
t F

or
m

 I
I

w(n) = −a1w(n− 1)− a2w(n− 2)
+x(n)

y(n) = b0w(n)+ b1w(n− 1)
+b2w(n− 2)

H(z) = b0 + b1z−1 + b2z−2
1+ a1z−1 + a2z−2

x(n) b0 y(n)

w1(n)

w2(n)

b1

b2

−a1

−a2
+

z −1

z −1

+

+

T
ra

ns
po

se
d 

D
ir

ec
t F

or
m

 I
I

y(n) = b0x(n)+ w1(n− 1)
w1(n) = b1x(n)− a1y(n)

+w2(n− 1)
w2(n) = b2x(n)− a2y(n)

H(z) = b0 + b1z−1 + b2z−2
1+ a1z−1 + a2z−2

Cascade-Form Structures

out loss of generality we assume that N ≥ M . The system can be factored into a
cascade of second-order subsystems, such that H(z) can be expressed as

H(z) =
K∏
k=1

Hk(z)

where K is the integer part of (N + 1)/2. Hk(z) has the general form

Hk(z) = bk0 + bk1z
−1 + bk2z−2

1+ ak1z−1 + ak2z−2
As in the case of FIR systems based on a cascade-form realization, the parameter b0
can be distributed equally among the K filter sections so that b0 = b10b20 . . . bK0 .

The coefficients {aki} and {bki}in the second-order subsystems are real. This

(3.14)

(3.15)

Implementation of Discrete-Time Systems

3.3

Let us consider a high-order IIR system with system function given by (1.2). With-

implies that in forming the second-order subsystems or quadratic factors in (3.15),

603



we should group together a pair of complex-conjugate poles and we should group
together a pair of complex-conjugate zeros. However, the pairing of two complex-
conjugate poles with a pair of complex-conjugate zeros or real-valued zeros to form

Furthermore,
any two real-valued zeros can be paired together to form a quadratic factor and,
likewise, any two real-valued poles can be paired together to form a quadratic factor.

a pair of real roots or a pair of complex-conjugate roots. The same statement applies

If N > M , some of the second-order subsystems have numerator coefficients
that are zero, that is, either bk2 = 0 or bk1 = 0 or both bk2 = bk1 = 0 for some k .
Furthermore, if N is odd, one of the subsystems, say Hk(z), must have ak2 = 0, so
that the subsystem is of first order. To preserve the modularity in the implementation
of H(z), it is often preferable to use the basic second-order subsystems in the cascade
structure and have some zero-valued coefficients in some of the subsystems.

can be realized in either direct form I, or direct form II, or transposed direct form
II. Since there are many ways to pair the poles and zeros of H(z) into a cascade
of second-order sections, and several ways to order the resulting subsystems, it is
possible to obtain a variety of cascade realizations. Although all cascade realizations
are equivalent for infinite-precision arithmetic, the various realizations may differ
significantly when implemented with finite-precision arithmetic.

If we use
the direct form II structure for each of the subsystems, the computational algorithm
for realizing the IIR system with system function H(z) is described by the following
set of equations.

xk(n) bk01 yk(n) = xk + 1(n)

x(n) = x1(n) x2(n)

y1(n)

xK(n)

y2(n) y(n)

bk2

bk1−ak1

−ak2

z −1

z −1

(b)

(a)

H1(z) •  •  •H2(z) HK(z)

+ +

+ +

Cascade structure of second-order systems and a realization of each
second-order section.

Implementation of Discrete-Time Systems

a subsystem of the type given by (3.15) can be done arbitrarily.

Consequently, the quadratic factor in the numerator of (3.15) may consist of either

to the denominator of (3.15).

Each of the second-order subsystems with system function of the form (3.15)

The general form of the cascade structure is illustrated in Fig. 3.8.

Figure 3.8

604



y0(n) = x(n)
wk(n) = −ak1wk(n− 1)− ak2wk(n− 2)+ yk−1(n), k = 1, 2, . . . , K
yk(n) = bk0wk(n)+ bk1wk(n− 1)+ bk2wk(n− 2), k = 1, 2, . . . , K
y(n) = yK(n)

Thus this set of equations provides a complete description of the cascade structure
based on direct form II sections.

3.4 Parallel-Form Structures

A parallel-form realization of an IIR system can be obtained by performing a partial-
fraction expansion of H(z). Without loss of generality, we again assume that N ≥ M
and that the poles are distinct. Then, by performing a partial-fraction expansion of
H(z), we obtain the result

H(z) = C +
N∑
k=1

Ak

1− pkz−1

where {pk} are the poles, {Ak} are the coefficients (residues) in the partial-fraction
expansion, and the constant C is defined as C = bN/aN . The structure implied by

In such a case,
the corresponding coefficients Ak are also complex valued. To avoid multiplications
by complex numbers, we can combine pairs of complex-conjugate poles to form
two-pole subsystems. In addition, we can combine, in an arbitrary manner, pairs of
real-valued poles to form two-pole subsystems. Each of these subsystems has the
form

Hk(z) = bk0 + bk1z
−1

1+ ak1z−1 + ak2z−2

Parallel structure of IIR
system.

x(n)

• 
 •

  •

• 
 •

  •

C

H2(z)

HK(z)
y(n)

+

+H1(z)

+

(3.16)

(3.17)

(3.18)

(3.19)

(3.20)

(3.21)

Implementation of Discrete-Time Systems

(3.20) is shown in Fig. 3.9. It consists of a parallel bank of single-pole filters.

Figure 3.9

In general, some of the poles of H(z) may be complex valued.

605



x(n) yk(n)bk0

bk1−ak1

−ak2

z −1

z −1

+

+

+

Figure 3.10 Structure of second-order section in a parallel IIR
system realization.

where the coefficients {bki} and {aki] are real-valued system parameters. The overall
function can now be expressed as

H(z) = C +
K∑
k=1

Hk(z)

where K is the integer part of (N + 1)/2. When N is odd, one of the Hk(z) is really
a single-pole system (i.e., bk1 = ak2 = 0).

The individual second-order sections which are the basic building blocks for
H(z) can be implemented in either of the direct forms or in a transposed direct form.

building block, the parallel-form realization of the FIR system is described by the
following set of equations:

wk(n) = −ak1wk(n− 1)− ak2wk(n− 2)+ x(n), k = 1, 2, . . . , K
yk(n) = bk0wk(n)+ bk1wk(n− 1), k = 1, 2, . . . , K

y(n) = Cx(n)+
K∑
k=1

yk(n)

Determine the cascade and parallel realizations for the system described by the system function

H(z) = 10(1−
1
2 z
−1)(1− 23z−1)(1+ 2z−1)

(1− 34 z−1)(1− 18 z−1)[1− ( 12 + j 12 )z−1][1− ( 12 − j 12 )z−1]
Solution. The cascade realization is easily obtained from this form. One possible pairing of
poles and zeros is

H1(z) =
1− 23 z−1

1− 78z−1 + 332 z−2

H2(z) =
1+ 32z−1 − z−2
1− z−1 + 12 z−2

(3.22)

(3.23)

(3.24)

(3.25)

Implementation of Discrete-Time Systems

The direct form II structure is illustrated in Fig. 3.10. With this structure as a basic

EXAMPLE 3.1

606



and hence

H(z) = 10H1(z)H2(z)

To obtain the parallel-form realization, H(z) must be expanded in partial fractions. Thus
we have

H(z) = A1
1− 34z−1

+ A2
1− 18z−1

+ A3
1− ( 12 + j 12 )z−1

+ A
∗
3

1− ( 12 − j 12 )z−1

y(n)

y(n)

1

−1

+ +

++

3
2

1
2

−

z −1

z −1

10x(n)

+

++

2
3

7
8

3
32

−

−
z −1

z −1

(a)

x(n)
+

++
−14.75

24.5

−12.9
7
8

3
32

−

z −1

z −1

+

+

26.82

1

1
2

−

z −1

z −1

10

(b)

+

+

Figure 3.11

Implementation of Discrete-Time Systems

The cascade realization is depicted in Fig. 3.11(a).

Cascade and parallel realizations for the system in Example 3.1.

607



where A1 , A2 , A3 , and A∗3 are to be determined. After some arithmetic we find that

A1 = 2.93, A2 = −17.68, A3 = 12.25− j14.57, A∗3 = 12.25+ j14.57

Upon recombining pairs of poles, we obtain

H(z) = −14.75− 12.90z
−1

1− 78z−1 + 332 z−2
+ 24.50+ 26.82z

−1

1− z−1 + 12z−2

3.5 Lattice and Lattice-Ladder Structures for IIR Systems

system. In this section we extend the development to IIR systems.
Let us begin with an all-pole system with system function

H(z) = 1

1+
N∑
k=1

aN(k)z
−k
= 1
AN(z)

equation for this IIR system is

y(n) = −
N∑
k=1

aN(k)y(n− k)+ x(n)

It is interesting to note that if we interchange the roles of input and output [i.e.,

x(n) = −
N∑
k=1

aN(k)x(n− k)+ y(n)

or, equivalently,

y(n) = x(n)+
N∑
k=1

aN(k)x(n− k)

x(n)

−aN(N − 1) −aN(1)−aN(2)−aN(N)

•  •  •

•  •  •

z −1 z −1 z −1

y(n)
+ + ++

Figure 3.12 Direct-form realization of an all-pole system.

(3.26)

(3.27)

(3.28)

Implementation of Discrete-Time Systems

The parallel-form realization is illustrated in Fig. 3.11(b).

In Section 2.4 we developed a lattice filter structure that is equivalent to an FIR

The direct-form realization of this system is illustrated in Fig. 3.12. The difference

interchange x(n) with y(n) in (3.27)], we obtain

608



function H(z) = AN(z), while the system described by the difference equation in
N(z). One system

can be obtained from the other simply by interchanging the roles of the input and
output.

Based on this observation, we shall use the all-zero (FIR) lattice described in

the roles of the input and output. First, we take the all-zero lattice filter illustrated

x(n) = fN(n)

and the output as

y(n) = f0(n)

These are exactly the opposite of the definitions for the all-zero lattice filter. These
definitions dictate that the quantities {fm(n)} be computed in descending order [i.e.,
fN(n), f (n), . . .]. This computation can be accomplished by rearranging the

m−1(n) in terms of fm(n), that is,

fm−1(n) = fm(n)−Kmgm−1(n− 1), m = N,N − 1, . . . , 1

m(n) remains unchanged.
The result of these changes is the set of equations

fN(n) = x(n)
fm−1(n) = fm(n)−Kmgm−1(n− 1), m = N,N − 1, . . . , 1
gm(n) = Kmfm−1(n)+ gm−1(n− 1), m = N,N − 1, . . . , 1
y(n) = f0(n) = g0(n)

x(n) f2(n) f1(n) f0(n) = y(n)

KN
fN(n)

fN − 1(n)

gN(n) g2(n) g1(n) g0(n)

KN

z −1 z −1

+

+

K2 

K2

+

K1

K1  

+

+

Output
−−−

Input

•  •  •

•  •  • + z −1

Lattice structure for an all-pole IIR system.

(3.29)

(3.30)

(3.31)

(3.32)

(3.33)

(3.34)

Implementation of Discrete-Time Systems

We note that the equation in (3.28) describes an FIR system having the system

(3.27) represents an IIR system with system function H(z) = 1/A

Section 2.4 to obtain a lattice structure for an all-pole IIR system by interchanging

in Fig. 2.11 and then redefine the input as

The equation (2.30) for g

N−1
recursive equation in (2.29) and thus solving for f

which correspond to the structure shown in Fig. 3.13.

Figure 3.13

609



all-pole IIR system, let us consider the case where N = 1. The equations reduce to

x(n) = f1(n)
f0(n) = f1(n)−K1g0(n− 1)
g1(n) = K1f0(n)+ g0(n− 1)
y(n) = f0(n)
= x(n)−K1y(n− 1)

Furthermore, the equation for g1(n) can be expressed as

g1(n) = K1y(n)+ y(n− 1)

represents a first-order FIR system. The pole is a result of the feedback introduced
by the solution of the {fm(n)} in descending order. This feedback is depicted in

x(n) y(n)f0(n)f1(n)

f2(n)

g2(n) g1(n) g0(n)

K1

K1

K2

K2

+

Forward

Reverse

+

+ z −1z −1+

(b)

(a)

y(n)f0(n)x(n)

f1(n)

g1(n) g0(n)

K1

K1

Forward

Reverse

Feedback

+

+ z −1

−

− −

Figure 3.14 Single-pole and two-pole lattice system.

(3.35)

(3.36)

Implementation of Discrete-Time Systems

To demonstrate that the set of equations (3.31) through (3.34) represent an

We observe that (3.35) represents a first-order all-pole IIR system while (3.36)

Fig. 3.14(a).

610



Next, let us consider the case N = 2, which corresponds to the structure in

f2(n) = x(n)
f1(n) = f2(n)−K2g1(n− 1)
g2(n) = K2f1(n)+ g1(n− 1)
f0(n) = f1(n)−K1g0(n− 1)
g1(n) = K1f0(n)+ g0(n− 1)
y(n) = f0(n) = g0(n)

After some simple substitutions and manipulations we obtain

y(n) = −K1(1+K2)y(n− 1)−K2y(n− 2)+ x(n)
g2(n) = K2y(n)+K1(1+K2)y(n− 1)+ y(n− 2)

Note
that the coefficients for the FIR system are identical to those in the IIR system except
that they occur in reverse order.

In general, these conclusions hold for any N . Indeed, with the definition of
Am

Ha(z) = Y (z)
X(z)

= F0(z)
Fm(z)

= 1
Am(z)

Similarly, the system function of the all-zero (FIR) system is

Hb(z) = Gm(z)
Y (z)

= Gm(z)
G0(z)

= Bm(z) = z−mAm(z−1)

Thus the coefficients in the FIR system Hb(z) are identical to the coefficients in
Am(z), except that they occur in reverse order.

It is interesting to note that the all-pole lattice structure has an all-zero path with
input g0(n) and output gN(n), which is identical to its counterpart all-zero path in
the all-zero lattice structure. The polynomial Bm(z), which represents the system
function of the all-zero path common to both lattice structures, is usually called the
backward system function, because it provides a backward path in the all-pole lattice
structure.

From this discussion the reader should observe that the all-zero and all-pole
lattice structures are characterized by the same set of lattice parameters, namely,
K1 , K2, . . . , KN . The two lattice structures differ only in the interconnections of
their signal flow graphs. Consequently, the algorithms for converting between the

(3.37)

(3.38)

(3.39)

(3.40)

(3.41)

Implementation of Discrete-Time Systems

Fig. 3.14(b). The equations corresponding to this structure are

Clearly, the difference equation in (3.38) represents a two-pole IIR system, and the
relation in (3.39) is the input–output equation for a two-zero FIR system.

(z) given in (2.32), the system function for the all-pole IIR system is

where we used the previously established relationships in (2.36) through (2.42).

611



system parameters {αm(k)} in the direct-form realization of an FIR system, and the
parameters of its lattice counterpart apply as well to the all-pole structure.

We recall that the roots of the polynomial AN(z) lie inside the unit circle if and
only if the lattice parameters |Km| < 1 for all m = 1, 2, . . . , N . Therefore, the
all-pole lattice structure is a stable system if and only if its parameters |Km| < 1 for
all m.

In practical applications the all-pole lattice structure has been used to model
the human vocal tract and a stratified earth. In such cases the lattice parameters,
{Km}, have the physical significance of being identical to reflection coefficients in
the physical medium. This is the reason that the lattice parameters are often called
reflection coefficients. In such applications, a stable model of the medium requires
that the reflection coefficients, obtained by performing measurements on output
signals from the medium, be less than unity.

The all-pole lattice provides the basic building block for lattice-type structures
that implement IIR systems that contain both poles and zeros. To develop the ap-
propriate structure, let us consider an IIR system with system function

H(z) =

M∑
k=0

cM(k)z
−k

1+
N∑
k=1

aN(k)z
−k
= CM(z)
AN(z)

where the notation for the numerator polynomial has been changed to avoid con-
fusion with our previous development. Without loss of generality, we assume that
N ≥ M .

equations

w(n) = −
N∑
k=1

aN(k)w(n− k)+ x(n)

y(n) =
M∑
k=0

cM(k)w(n− k)

input–output of an all-zero system. Furthermore, we observe that the output of the
all-zero system is simply a linear combination of delayed outputs from the all-pole
system. This is easily seen by observing the direct form II structure redrawn as in

Since zeros result from forming a linear combination of previous outputs, we
can carry over this observation to construct a pole–zero IIR system using the all-pole
lattice structure as the basic building block. We have already observed that gm(n) is
a linear combination of present and past outputs. In fact, the system

Hb(z) = Gm(z)
Y (z)

= Bm(z)

(3.42)

(3.43)

(3.44)

Implementation of Discrete-Time Systems

In the direct form II structure, the system in (3.42) is described by the difference

Note that (3.43) is the input–output of an all-pole IIR system and that (3.44) is the

Fig. 3.15.

612



x(n)

w(n)

cM(0)

y(n)

cM(1) cM(2) cM(M − 1) cM(M)

++ +

z −1

a1 a2 aM − 1 aM

z −1 z −1

+

w(n − 1) w(n − 2) w(n − M + 1) w(n − M)
z −1

+ + + +

•  •  •

•  •  •

•  •  •

Figure 3.15 Direct form II realization of IIR system.

is an all-zero system. Therefore, any linear combination of {gm(n)} is also an all-zero
system.

Thus we begin with an all-pole lattice structure with parameters Km , 1 ≤ m ≤ N ,
and we add a ladder part by taking as the output a weighted linear combination of
{gm(n)}. The result is a pole–zero IIR system which has the lattice-ladder structure

y(n) =
M∑
m=0

vmgm(n)

x(n)

y(n)

f2(n) f1(n) f0(n)

KN
fN(n)

fN − 1(n)

gN(n) g2(n)gN − 1(n) g1(n)
g0(n)

KN

vN

z −1

vN − 1

+

+

K2

K2

K1

K1

−−−

•  •  •

•  •  •

•  •  •

+ z −1

v2

+ +

v1 v0

+ +

+ z −1

+ +

Figure 3.16 Lattice-ladder structure for the realization of a pole–zero system.

(3.45)

Implementation of Discrete-Time Systems

shown in Fig. 3.16 for M = N . Its output is

613



where {vm} are the parameters that determine the zeros of the system. The system

H(z) = Y (z)
X(z)

=
M∑
m=0

vm
Gm(z)

X(z)

Since X(z) = FN 0 0

H(z) =
M∑
m=0

vm
Gm(z)

G0(z)

F0(z)

FN(z)

=
M∑
m=0

vm
Bm(z)

AN(z)

=

M∑
m=0

vmBm(z)

AN(z)

CM(z) =
M∑
m=0

vmBm(z)

This is the desired relationship that can be used to determine the weighting coef-
ficients {vm}.
polynomial CM(z) determine the ladder parameters {vm}, whereas the coefficients
in the denominator polynomial AN(z) determine the lattice parameters {Km}.

Given the polynomials CM(z) and AN(z), where N ≥ M , the parameters of
the all-pole lattice are determined first, as described previously, by the conversion

we obtain the lattice parameters {Km} and the polynomials Bm(z), m = 1, 2, . . . , N .

Cm(z) =
m−1∑
k=0

vkBk(z)+ vmBm(z)

or, equivalently, as
Cm(z) = Cm−1(z)+ vmBm(z)

(3.46)

(3.47)

(3.48)

(3.49)

(3.50)

Implementation of Discrete-Time Systems

function corresponding to (3.45) is

(z) and F (z) = G (z), (3.46) can be written as

Thus, we have demonstrated that the coefficients of the numerator

If we compare (3.41) with (3.47), we conclude that

algorithm given in Section 2.4, which converts the direct-form coefficients into
lattice parameters. By means of the step-down recursive relations given by (2.54),

The ladder parameters are determined from (3.48), which can be expressed as

614



Thus Cm(z) can be computed recursively from the reverse polynomials Bm(z), m = 1,
2, . . . ,M . Since βm(m) = 1 for all m, the parameters vm , m = 0, 1, . . . ,M can be
determined by first noting that

vm = cm(m), m = 0, 1, . . . ,M

Cm−1(z) = Cm(z)− vmBm(z)

and running this recursive relation backward in m (i.e., m = M , M − 1, . . . , 2), we
obtain cm

The lattice-ladder filter structures that we have presented require the minimum
amount of memory but not the minimum number of multiplications. Although lattice
structures with only one multiplier per lattice stage exist, the two multiplier-per-stage
lattice that we have described is by far the most widely used in practical applications.
In conclusion, the modularity, the built-in stability characteristics embodied in the
coefficients {Km}, and its robustness to finite-word-length effects make the lattice
structure very attractive in many practical applications, including speech processing
systems, adaptive filtering, and geophysical signal processing.

4 Representation of Numbers

Up to this point we have considered the implementation of discrete-time systems
without being concerned about the finite-word-length effects that are inherent in
any digital realization, whether it be in hardware or in software. In fact, we have
analyzed systems that are modeled as linear when, in fact, digital realizations of such
systems are inherently nonlinear.

In this and the following two sections, we consider the various forms of quanti-
zation effects that arise in digital signal processing. Although we describe floating-
point arithmetic operations briefly, our major concern is with fixed-point realizations
of digital filters.

In this section we consider the representation of numbers for digital compu-
tations. The main characteristic of digital arithmetic is the limited (usually fixed)
number of digits used to represent numbers. This constraint leads to finite numerical
precision in computations, which leads to round-off errors and nonlinear effects in
the performance of digital filters. We now provide a brief introduction to digital
arithmetic.

4.1 Fixed-Point Representation of Numbers

The representation of numbers in a fixed-point format is a generalization of the
familiar decimal representation of a number as a string of digits with a decimal point.
In this notation, the digits to the left of the decimal point represent the integer part

(3.51)

(3.52)

Implementation of Discrete-Time Systems

Then, by rewriting (3.50) as

(m) and therefore the ladder parameters according to (3.51).

615



of the number, and the digits to the right of the decimal point represent the fractional
part of the number. Thus a real number X can be represented as

X = (b−A, . . . , b−1, b0, b1, . . . , bB)r

=
B∑

i=−A
bir
−i , 0 ≤ bi ≤ (r − 1)

where bi represents the digit, r is the radix or base, A is the number of integer
digits, and B is the number of fractional digits. As an example, the decimal number
(123.45)10 and the binary number (101.0)2 represent the following sums:

(123.45)10 = 1× 102 + 2× 101 + 3× 100 + 4× 10−1 + 5× 10−2

(101.01)2 = 1× 22 + 0× 21 + 1× 20 + 0× 2−1 + 1× 2−2

Let us focus our attention on the binary representation since it is the most im-
portant for digital signal processing. In this case r = 2 and the digits {bi} are called
binary digits or bits and take the values {0, 1}. The binary digit b−A is called the
most significant bit (MSB) of the number, and the binary digit bB is called the least
significant bit (LSB). The “binary point” between the digits b0 and b1 does not exist
physically in the computer. Simply, the logic circuits of the computer are designed
so that the computations result in numbers that correspond to the assumed location
of this point.

By using an n-bit integer format (A = n− 1, B = 0), we can represent unsigned
integers with magnitude in the range 0 to 2n− 1. Usually, we use the fraction format
(A = 0, B = n − 1), with a binary point between b0 and b1 , that permits numbers
in the range from 0 to 1 − 2−n . Note that any integer or mixed number can be
represented in a fraction format by factoring out the term rA In the
sequel we focus our attention on the binary fraction format because mixed numbers
are difficult to multiply and the number of bits representing an integer cannot be
reduced by truncation or rounding.

There are three ways to represent negative numbers. This leads to three formats
for the representation of signed binary fractions. The format for positive fractions is
the same in all three representations, namely,

X = 0.b1b2 · · · bB =
B∑
i=1

bi · 2−i , X ≥ 0

Note that the MSB b0 is set to zero to represent the positive sign. Consider now the
negative fraction

X = −0.b1b2 · · · bB = −
B∑
i=1

bi · 2−i

This number can be represented using one of the following three formats.

(4.1)

(4.2)

(4.3)

Implementation of Discrete-Time Systems

in (4.1).

616



Sign-magnitude format. In this format, the MSB is set to 1 to represent the nega-
tive sign,

XSM = 1.b1b2 · · · bB, for X ≤ 0
One’s-complement format. In this format the negative numbers are represented as

X1C = 1.b1b2 · · · bB, X ≤ 0
where bi = 1 − bi is the one’s complement of bi . Thus if X is a positive number,
the corresponding negative number is determined by complementing (changing 1’s
to 0’s and 0’s to 1’s) all the bits. An alternative definition for X1C can be obtained
by noting that

X1C = 1× 20 +
B∑
i=1
(1− bi) · 2−i = 2− 2−B |X|

Two’s-complement format. In this format a negative number is represented by
forming the two’s complement of the corresponding positive number. In other words,
the negative number is obtained by subtracting the positive number from 2.0. More
simply, the two’s complement is formed by complementing the positive number and
adding one LSB. Thus

X2C = 1.b1b2 · · · bB + 00 · · · 01, X < 0
where + represents modulo-2 addition that ignores any carry generated from the
sign bit. For example, the number − 38 is simply obtained by complementing 0011
( 38) to obtain 1100 and then adding 0001. This yields 1101, which represents − 38 in
two’s complement.

X2C = X1C + 2−B = 2− |X|

1 =
B∑
i=1

2−i + 2−B

X2C = −
B∑
i=1

bi · 2−i + 1− 1

= −1+
B∑
i=1
(1− bi)2−i + 2−B

= −1+
B∑
i=1

bi · 2−1 + 2−B

(4.4)

(4.5)

(4.6)

(4.7)

(4.8)

(4.9)

Implementation of Discrete-Time Systems

From (4.6) and (4.7) it can easily be seen that

To demonstrate that (4.7) truly represents a negative number, we use the identity

The negative number X in (4.3) can be expressed as

617



In summary, the value of a binary string b0b1 · · · bB depends on the format used.
For positive numbers, b0 For negative
numbers, we use these corresponding formulas for the three formats.

Express the fraction 78 and − 78 in sign-magnitude, two’s-complement, and one’s-complement
format.

Solution. X = 78 is represented as 2−1 + 2−2 + 2−3 , so that X = 0.111. In sign-magnitude
format, X = − 78 is represented as 1.111. In one’s complement, we have

X1C = 1.000

In two’s complement, the result is

X2C = 1.000+ 0.001 = 1.001

The basic arithmetic operations of addition and multiplication depend on the
format used. For one’s-complement and two’s-complement formats, addition is car-
ried out by adding the numbers bit by bit. The formats differ only in the way in which
a carry bit affects the MSB. For example, 48 − 38 = 18 . In two’s complement, we have

0100⊕ 1101 = 0001

where ⊕ indicates modulo-2 addition. Note that the carry bit, if present in the MSB,
is dropped. On the other hand, in one’s-complement arithmetic, the carry in the
MSB, if present, is carried around to the LSB. Thus the computation 48 − 38 = 18
becomes

0100⊕ 1100 = 0000⊕ 0001 = 0001
Addition in the sign-magnitude format is more complex and can involve sign checks,
complementing, and the generation of a carry. On the other hand, direct multiplica-
tion of two sign-magnitude numbers is relatively straightforward, whereas a special
algorithm is usually employed for one’s-complement and two’s-complement multi-
plication.

Most fixed-point digital signal processors use two’s-complement arithmetic.
Hence, the range for (B + 1)-bit numbers is from −1 to 1 − 2−B . These numbers

arithmetic is basically arithmetic modulo 2B+1 [i.e., any number that falls outside the
range (overflow or underflow) is reduced to this range by subtracting an appropriate
multiple of 2B+1 ]. This type of arithmetic can be viewed as counting using the wheel

final sum of a string of numbers X1 , X2, . . . , XN is within the range, it will be com-
puted correctly, even if individual partial sums result in overflows. This and other

Implementation of Discrete-Time Systems

which is exactly the two’s-complement representation of (4.7).

= 0, and the number is given by (4.2).

EXAMPLE 4.1

can be viewed in a wheel format as shown in Fig. 4.1 for B = 2. Two’s-complement

of Fig. 4.1. A very important property of two’s-complement addition is that if the

characteristics of two’s-complement arithmetic are considered in Problem 29.

618



0

000

111 001

101 011

100

010110

−4

−2 2

1−1

−3 3

0.0

000

111 001

101 011

100

010110

−01.0

−0.5 0.5

0.25−0.25

−0.75 0.75

(a) (b)

Figure 4.1 Counting wheel for 3-bit two’s-complement numbers:
(a) integers and (b) fractions.

In general, the multiplication of two fixed-point numbers each b bits in length
results in a product of 2b bits in length. In fixed-point arithmetic, the product is either
truncated or rounded back to b bits. As a result we have a truncation or round-off
error in the b least significant bits. The characterization of such errors is treated
below.

4.2 Binary Floating-Point Representation of Numbers

A fixed-point representation of numbers allows us to cover a range of numbers, say,
xmax − xmin with a resolution

� = xmax − xmin
m− 1

where m = 2b is the number of levels and b is the number of bits. A basic character-
istic of the fixed-point representation is that the resolution is fixed. Furthermore, �
increases in direct proportion to an increase in the dynamic range.

A floating-point representation can be employed as a means for covering a larger
dynamic range. The binary floating-point representation commonly used in practice
consists of a mantissa M , which is the fractional part of the number and falls in the
range 12 ≤ M < 1, multiplied by the exponential factor 2E , where the exponent E is
either a positive or negative integer. Hence a number X is represented as

X = M · 2E

The mantissa requires a sign bit for representing positive and negative numbers, and
the exponent requires an additional sign bit. Since the mantissa is a signed fraction,
we can use any of the four fixed-point representations just described.

For example, the number X1 = 5 is represented by the following mantissa and
exponent:

M1 = 0.101000
E1 = 011

Implementation of Discrete-Time Systems

619



while the number X2 = 38 is represented by the following mantissa and exponent

M2 = 0.110000
E2 = 101

where the leftmost bit in the exponent represents the sign bit.
If the two numbers are to be multiplied, the mantissas are multiplied and the

exponents are added. Thus the product of these two numbers is

X1X2 = M1M2 · 2E1+E2

= (0.011110) · 2010

= (0.111100) · 2001

On the other hand, the addition of the two floating-point numbers requires that the
exponents be equal. This can be accomplished by shifting the mantissa of the smaller
number to the right and compensating by increasing the corresponding exponent.
Thus the number X2 can be expressed as

M2 = 0.000011
E2 = 011

With E2 = E1 , we can add the two numbers X1 and X2 . The result is

X1 +X2 = (0.101011) · 2011

It should be observed that the shifting operation required to equalize the expo-
nent of X2 with that for X1 results in loss of precision, in general. In this example
the six-bit mantissa was sufficiently long to accommodate a shift of four bits to the
right for M2 without dropping any of the ones. However, a shift of five bits would
have caused the loss of a single bit and a shift of six bits to the right would have
resulted in a mantissa of M2 = 0.000000, unless we round upward after shifting so
that M2 = 0.000001.

Overflow occurs in the multiplication of two floating-point numbers when the
sum of the exponents exceeds the dynamic range of the fixed-point representation
of the exponent.

In comparing a fixed-point representation with a floating-point representation,
each with the same number of total bits, it is apparent that the floating-point repre-
sentation allows us to cover a larger dynamic range by varying the resolution across
the range. The resolution decreases with an increase in the size of successive num-
bers. In other words, the distance between two successive floating-point numbers
increases as the numbers increase in size. It is this variable resolution that results in
a larger dynamic range. Alternatively, if we wish to cover the same dynamic range
with both fixed-point and floating-point representations, the floating-point represen-
tation provides finer resolution for small numbers but coarser resolution for the larger

Implementation of Discrete-Time Systems

620



numbers. In contrast, the fixed-point representation provides a uniform resolution
throughout the range of numbers.

For example, if we have a computer with a word size of 32 bits, it is possible to
represent 232 numbers. If we wish to represent the positive integers beginning with
zero, the largest possible integer that can be accommodated is

232 − 1 = 4,294,967,295

The distance between successive numbers (the resolution) is 1. Alternatively, we
can designate the leftmost bit as the sign bit and use the remaining 31 bits for the
magnitude. In such a case a fixed-point representation allows us to cover the range

−(231 − 1) = −2,147,483,647 to (231 − 1) = 2,147,483,647

again with a resolution of 1.
On the other hand, suppose that we increase the resolution by allocating 10 bits

for a fractional part, 21 bits for the integer part, and 1 bit for the sign. Then this
representation allows us to cover the dynamic range

−(231 − 1) · 2−10 = −(221 − 2−10) to (231 − 1) · 2−10 = 221 − 2−10

or, equivalently,
−2,097,151.999 to 2,097,151.999

In this case, the resolution is 2−10 . Thus, the dynamic range has been decreased by a
factor of approximately 1000 (actually 210 ), while the resolution has been increased
by the same factor.

For comparison, suppose that the 32-bit word is used to represent floating- point
numbers. In particular, let the mantissa be represented by 23 bits plus a sign bit and
let the exponent be represented by 7 bits plus a sign bit. Now, the smallest number
in magnitude will have the representation,

sign
0.

23 bits
100 · · · 0

sign
1

7 bits
1111111 = 12 × 2−127 ≈ 0.3× 10−38

At the other extreme, the largest number that can be represented with this floating-
point representation is

sign
0

23 bits
111 · · · 1

sign
0

7 bits
1111111 = (1− 2−23)× 2127 ≈ 1.7× 1038

Thus, we have achieved a dynamic range of approximately 1076 , but with varying
resolution. In particular, we have fine resolution for small numbers and coarse
resolution for larger numbers.

Implementation of Discrete-Time Systems

621



The representation of zero poses some special problems. In general, only the
mantissa has to be zero, but not the exponent. The choice of M and E , the repre-
sentation of zero, the handling of overflows, and other related issues have resulted
in various floating-point representations on different digital computers. In an effort
to define a common floating-point format, the Institute of Electrical and Electronic
Engineers (IEEE) introduced the IEEE 754 standard, which is widely used in prac-
tice. For a 32-bit machine, the IEEE 754 standard single-precision, floating-point
number is represented as X = (−1)s · 2E−127(M), where

0 1 8 9 31

S E M

This number has the following interpretations:

If E = 255 and M �= 0, then X is not a number
If E = 255 and M = 0, then X = (−1)S · ∞
If 0 < E < 255, then X = (−1)S · 2E−127(1.M)
If E = 0 and M �= 0, then X = (−1)S · 2−126(0.M)
If E = 0 and M = 0, then X = (−1)S · 0

where 0.M is a fraction and 1.M is a mixed number with one integer bit and 23
fractional bits. For example, the number

1 0 0 0 0 0 1 0 1 0 1 0 000

S E M

…

has the value X = −10×2130−127×1.1010 . . . 0 = 23× 138 = 13. The magnitude range
of the 32-bit IEEE 754 floating-point numbers is from 2−126×2−23 to (2−2−23)×2127
(i.e., from 1.18×10−38 to 3.40×1038 ). Computations with numbers outside this range
result in either underflow or overflow.

4.3 Errors Resulting from Rounding and Truncation

Inperformingcomputationssuch as multiplications with either fixed-point or floating-
point arithmetic, we are usually faced with the problem of quantizing a number via
truncation or rounding, from a given level of precision to a level of lower precision.
The effect of rounding and truncation is to introduce an error whose value depends
on the number of bits in the original number relative to the number of bits after
quantization. The characteristics of the errors introduced through either truncation
or rounding depend on the particular form of number representation.

To be specific, let us consider a fixed-point representation in which a number x
is quantized from bu bits to b bits. Thus the number

x =
bu︷ ︸︸ ︷

0.1011 · · · 01

Implementation of Discrete-Time Systems

622



consisting of bu bits prior to quantization is represented as

x =
b︷ ︸︸ ︷

0.101 · · · 1

after quantization, where b < bu . For example, if x represents the sample of an
analog signal, then bu may be taken as infinite. In any case if the quantizer truncates
the value of x , the truncation error is defined as

Et = Qt(x)− x

First, we consider the range of values of the error for sign-magnitude and two’s-
complement representation. In both of these representations, the positive numbers
have identical representations. For positive numbers, truncation results in a number
that is smaller than the unquantized number. Consequently, the truncation error
resulting from a reduction of the number of significant bits from bu to b is

−(2−b − 2−bu) ≤ Et ≤ 0

where the largest error arises from discarding bu − b bits, all of which are ones.
In the case of negative fixed-point numbers based on the sign-magnitude rep-

resentation, the truncation error is positive, since truncation basically reduces the
magnitude of the numbers. Consequently, for negative numbers, we have

0 ≤ Et ≤ (2−b − 2−bu)

In the two’s-complement representation, the negative of a number is obtained by
subtracting the corresponding positive number from 2. As a consequence, the effect
of truncation on a negative number is to increase the magnitude of the negative
number. Consequently, x > Qt(x) and hence

−(2−b − 2−bu) ≤ Et ≤ 0

Hence we conclude that the truncation error for the sign-magnitude representation is
symmetric about zero and falls in the range

−(2−b − 2−bu) ≤ Et ≤ (2−b − 2−bu)

On the other hand, for two’s-complement representation, the truncation error is al-
ways negative and falls in the range

−(2−b − 2−bu) ≤ Et ≤ 0

Next, let us consider the quantization errors due to rounding of a number. A
number x , represented by bu bits before quantization and b bits after quantization,
incurs a quantization error

Er = Qr(x)− x

(4.10)

(4.11)

(4.12)

(4.13)

(4.14)

(4.15)

(4.16)

Implementation of Discrete-Time Systems

623



Basically, rounding involves only the magnitude of the number and, consequently,
the round-off error is independent of the type of fixed-point representation. The
maximum error that can be introduced through rounding is (2−b − 2−bu)/2 and this
can be either positive or negative, depending on the value of x . Therefore, the
round-off error is symmetric about zero and falls in the range

− 12 (2−b − 2−bu) ≤ Er ≤ 12 (2−b − 2−bu)

plitude (bu = ∞).

2 − b

Qr(x)

Er = Qr(x) − x

x

(a)

2 − b

2

2 − b

Qt(x)

Et = Qt(x) − x

x

(b)

2 − b 2 − b

2 − b 0  ≤  ≤ Et

2 − b

Qt(x)

Et = Qt(x) − x

x

(c)

− 2 − b 2 − b ≤  ≤ Et

1
2

− 2 − b 1
2

2 − b ≤  ≤Er.

Figure 4.2 Quantization errors in rounding and truncation: (a) rounding; (b) trun-
cation in two’s complement; (c) truncation in sign-magnitude.

(4.17)

Implementation of Discrete-Time Systems

These relationships are summarized in Fig. 4.2 when x is a continuous signal am-

624



In a floating-point representation, the mantissa is either rounded or truncated.
Due to the nonuniform resolution, the corresponding error in a floating-point rep-
resentation is proportional to the number being quantized. An appropriate repre-
sentation for the quantized value is

Q(x) = x + ex
where e is called the relative error. Now

Q(x)− x = ex
In the case of truncation based on two’s-complement representation of the man-

tissa, we have
−2E2−b < etx < 0

for positive numbers. Since 2E−1 ≤ x < 2E , it follows that
−2−b+1 < et ≤ 0, x > 0

On the other hand, for a negative number in two’s-complement representation, the
error is

0 ≤ etx < 2E2−b

and hence
0 ≤ et < 2−b+1, x < 0

In the case where the mantissa is rounded, the resulting error is symmetric rela-
tive to zero and has a maximum value of ±2−b/2. Consequently, the round-off error
becomes

−2E · 2−b/2 < erx ≤ 2E · 2−b/2
Again, since x falls in the range 2E−1 ≤ x < 2E , we divide through by 2E−1 so that

−2−b < er ≤ 2−b

In arithmetic computations involving quantization via truncation and rounding,
it is convenient to adopt a statistical approach to the characterization of such errors.
The quantizer can be modeled as introducing an additive noise to the unquantized
value x . Thus we can write

Q(x) = x + �
where � = Er for rounding and � = Et for truncation. This model is illustrated in

Since x can be any number that falls within any of the levels of the quantizer,
the quantization error is usually modeled as a random variable that falls within the
limits specified. This random variable is assumed to be uniformly distributed within
the ranges specified for the fixed-point representations. Furthermore, in practice,
bu >> b , so that we can neglect the factor of 2−bu in the formulas given below. Under
these conditions, the probability density functions for the round-off and truncation

(4.18)

(4.19)

(4.20)

(4.21)

(4.22)

(4.23)

(4.24)

Implementation of Discrete-Time Systems

Fig. 4.3.

errors in the two fixed-point representations are illustrated in Fig. 4.4. We note that

625



Additive noise model for
the nonlinear quantization
process: (a) actual system;
(b) model for quantization.

Quantizer
Q(x)

x

(a)

(b)

x + ε

x x + ε 

ε

+

in the case of truncation of the two’s-complement representation of the number,
the average value of the error has a bias of 2−b/2, whereas in all other cases just
illustrated, the error has an average value of zero.

We shall use this statistical characterization of the quantization errors in our
treatment of such errors in digital filtering and in the computation of the DFT for
fixed-point implementation.

Statistical characterization
of quantization errors:
(a) round-off error;
(b) truncation error
for sign-magnitude;
(c) truncation error for
two’s complement.

0
2

Er

p(Er)

∆

∆ = 2−b

2
∆

1
∆

∆

1
∆

−

(a)

0
Et

p(Et)

∆ = 2−b

−∆

1
2∆

(b)

0
Et

p(Et)

∆ = 2−b

−∆ ∆

(c)

Implementation of Discrete-Time Systems

Figure 4.3

Figure 4.4

626



Quantization of Filter Coefficients

In the realization of FIR and IIR filters in hardware or in software on a general-
purpose computer, the accuracy with which filter coefficients can be specified is lim-
ited by the word length of the computer or the length of the register provided to
store the coefficients. Since the coefficients used in implementing a given filter are
not exact, the poles and zeros of the system function will, in general, be different from
the desired poles and zeros. Consequently, we obtain a filter having a frequency re-
sponse that is different from the frequency response of the filter with unquantized
coefficients.

sponse characteristics to quantization of the filter coefficients is minimized by re-
alizing a filter having a large number of poles and zeros as an interconnection of
second-order filter sections. This leads us to the parallel-form and cascade-form
realizations in which the basic building blocks are second-order filter sections.

Analysis of Sensitivity to Quantization of Filter Coefficients

To illustrate the effect of quantization of the filter coefficients in a direct-form real-
ization of an IIR filter, let us consider a general IIR filter with system function

H(z) =

M∑
k=0

bkz
−k

1+
N∑
k=1

akz
−k

The direct-form realization of the IIR filter with quantized coefficients has the system
function

H(z) =

M∑
k=0

bkz
−k

1+
N∑
k=1

akz
−k

where the quantized coefficients {bk}and {ak} can be related to the unquantized
coefficients {bk} and {ak} by the relations

ak = ak +�ak, k = 1, 2, . . . , N
bk = bk +�bk, k = 0, 1, . . . ,M

and {�ak} and {�bk} represent the quantization errors.
The denominator of H(z) may be expressed in the form

D(z) = 1+
N∑
k=0

akz
−k =

N∏
k=1
(1− pkz−1)

(5.1)

(5.2)

(5.3)

(5.4)

Implementation of Discrete-Time Systems

5

In Section 5.1, we demonstrate that the sensitivity of the filter frequency re-

5.1

627



where {pk} are the poles of H(z). Similarly, we can express the denominator of H(z)
as

D(z) =
N∏
k=1
(1− pkz−1)

where pk = pk+�pk , k = 1, 2, . . . , N , and �pk is the error or perturbation resulting
from the quantization of the filter coefficients.

We shall now relate the perturbation �pk to the quantization errors in the {ak}.
The perturbation error �pi can be expressed as

�pi =
N∑
k=1

∂pi

∂ak
�ak

where ∂pi/∂ak , the partial derivative of pi with respect to ak , represents the incre-
mental change in the pole pi due to a change in the coefficient ak . Thus the total
error �pi is expressed as a sum of the incremental errors due to changes in each of
the coefficients {ak}.

The partial derivatives ∂pi/∂ak , k = 1, 2, . . . , N, can be obtained by differenti-
ating D(z) with respect to each of the {ak}. First we have(

∂D(z)

∂ak

)
z=pi
=
(
∂D(z)

∂z

)
z=pi

(
∂pi

∂ak

)

Then
∂pi

∂ak
= (∂D(z)/∂ak)z=pi
(∂D(z)/∂z)z=pi

(
∂D(z)

∂ak

)
z=pi
= −z−k|z=pi = −p−ki

(
∂D(z)

∂z

)
z=pi
=
{
∂

∂z

[
N∏
l=1
(1− plz−1)

]}
z=pi

=




N∑
k=1

pk

z2

N∏
l=1
l �=i

(1− plz−1)



z=pi

= 1
pNi

N∏
l=1
l �=i

(pi − pl)

(5.5)

(5.6)

(5.7)

(5.8)

(5.9)

(5.10)

Implementation of Discrete-Time Systems

The numerator of (5.8) is

The denominator of (5.8) is

628



∂pi

∂ak
= −p

N−k
i

N∏
l=1
l �=i

(pi − pl)

�pi in the form

�pi = −
N∑
k=1

pN−ki
N∏
l=1
l �=i

(pi − pl)
�ak

This expression provides a measure of the sensitivity of the i th pole to changes in
the coefficients {ak}. An analogous result can be obtained for the sensitivity of the
zeros to errors in the parameters {bk}.

The terms (pi l
plane from the poles {pl} to the pole pi . If the poles are tightly clustered as they are

i − pl | are small for
the poles in the vicinity of pi . These small lengths will contribute to large errors and
hence a large perturbation error �pi results.

The error �pi can be minimized by maximizing the lengths |pi − pl |. This can
be accomplished by realizing the high-order filter with either single-pole or double-
pole filter sections. In general, however, single-pole (and single-zero) filter sections
have complex-valued poles and require complex-valued arithmetic operations for
their realization. This problem can be avoided by combining complex-valued poles
(and zeros) to form second-order filter sections. Since the complex-valued poles are

Pole positions for a
bandpass IIR filter.

Im(z)

Re(z)

Unit circle

(5.11)

(5.12)

Implementation of Discrete-Time Systems

Therefore, (5.8) can be expressed as

Substitution of the result in (5.11) into (5.6) yields the total perturbation error

− p ) in the denominator of (5.12) represent vectors in the z-

in a narrowband filter, as illustrated in Fig. 5.1, the lengths |p

Figure 5.1

629



Realization of a two-pole
IIR filter.

+

+

x(n) y(n)

2r cos θ

− r2

z −1

z −1

usually sufficiently far apart, the perturbation errors {�pi} are minimized. As a con-
sequence, the resulting filter with quantized coefficients more closely approximates
the frequency response characteristics of the filter with unquantized coefficients.

It is interesting to note that even in the case of a two-pole filter section, the
structure used to realize the filter section plays an important role in the errors caused
by coefficient quantization. To be specific, let us consider a two-pole filter with system
function

H(z) = 1
1− (2r cos θ)z−1 + r2z−2

This filter has poles at z = re±jθ .
two coefficients, a1 = 2r cos θ and a2 = −r2 . With infinite precision it is possible
to achieve an infinite number of pole positions. Clearly, with finite precision (i.e.,
quantized coefficients a1 and a2 ), the possible pole positions are also finite. In fact,
when b bits are used to represent the magnitudes of a1 and a2 , there are at most
(2b−1)2 possible positions for the poles in each quandrant, excluding the case a1 = 0
and a2 = 0.

For example, suppose that b = 4. Then there are 15 possible nonzero values
for a1 . There are also 15 possible values for r2 . We illustrate these possible values

positions in this case. The nonuniformity in their positions is due to the fact that we
are quantizing r2 , whereas the pole positions lie on a circular arc of radius r . Of
particular significance is the sparse set of poles for values of θ near zero and, due to
symmetry, near θ = π . This situation would be highly unfavorable for lowpass filters
and highpass filters which normally have poles clustered near θ = 0 and θ = π .

An alternative realization of the two-pole filter is the coupled-form realization

y1(n) = x(n)+ r cos θ y1(n− 1)− r sin θ y(n− 1)
y(n) = r sin θ y1(n− 1)+ r cos θ y(n− 1)

By transforming these two equations into the z-domain, it is a simple matter to show
that

Y (z)

X(z)
= H(z) = (r sin θ)z

−1

1− (2r cos θ)z−1 + r2z−2

(5.13)

(5.14)

(5.15)

Implementation of Discrete-Time Systems

Figure 5.2

When realized as shown in Fig. 5.2, it has

in Fig. 5.3 for the first quandrant of the z-plane only. There are 169 possible pole

illustrated in Fig. 5.4. The two coupled equations are

630



Possible pole positions
for two-pole IIR filter

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶✶

In the coupled form we observe that there are also two coefficients, α1 = r sin θ
and α2 = r cos θ . Since they are both linear in r , the possible pole positions are now

the pole positions are now uniformly distributed inside the unit circle, which is a
more desirable situation than the previous realization, especially for lowpass filters.
(There are 198 possible pole positions in this case.) However, the price that we pay
for this uniform distribution of pole positions is an increase in computations. The
coupled-form realization requires four multiplications per output point, whereas the

Coupled-form realization of
a two-pole IIR filter.

+ +

+

x(n)

y(n)

y1(n − 1)

y1(n)

y(n − 1)

r cos θ

− r sin θ
r sin θ

r cos θ

z −1

z −1

Implementation of Discrete-Time Systems

Figure 5.3

realization in Fig. 5.2.

equally spaced points on a rectangular grid, as shown in Fig. 5.5. As a consequence,

realization in Fig. 5.2 requires only two multiplications per output point.

Figure 5.4

631



Possible pole positions for
the coupled-form two-pole

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

✶

Since there are various ways in which one can realize a second-order filter section,
there are obviously many possibilities for different pole locations with quantized
coefficients. Ideally, we should select a structure that provides us with a dense set of
points in the regions where the poles lie. Unfortunately, however, there is no simple
and systematic method for determining the filter realization that yields this desired
result.

Given that a higher-order IIR filter should be implemented as a combination of
second-order sections, we still must decide whether to employ a parallel configuration
or a cascade configuration. In other words, we must decide between the realization

H(z) =
K∏
k=1

bk0 + bk1z−1 + bk2z−2
1+ ak1z−1 + ak2z−2

and the realization

H(z) =
K∑
k=1

ck0 + ck1z−1
1+ ak1z−1 + ak2z−2

If the IIR filter has zeros on the unit circle, as is generally the case with elliptic
and Chebyshev type II filters, each second-order section in the cascade configuration

k

determine the location of these zeros. If the {bk} are quantized, the sensitivity of
the system response to the quantization errors is easily and directly controlled by
allocating a sufficiently large number of bits to the representation of the {bki}. In
fact, we can easily evaluate the perturbation effect resulting from quantizing the

(5.16)

(5.17)

Implementation of Discrete-Time Systems

Figure 5.5

filter in Fig. 5.4.

of (5.16) contains a pair of complex-conjugate zeros. The coefficients {b } directly

632



coefficients {bki} to some specified precision. Thus we have direct control of both
the poles and the zeros that result from the quantization process.

On the other hand, the parallel realization of H(z) provides direct control of the
poles of the system only. The numerator coefficients {ck0} and {ck1} do not specify the
location of the zeros directly. In fact, the {ck0} and {ck1} are obtained by performing a
partial-fraction expansion of H(z). Hence they do not directly influence the location
of the zeros, but only indirectly through a combination of all the factors of H(z). As
a consequence, it is more difficult to determine the effect of quantization errors in
the coefficients {cki} on the location of the zeros of the system.

It is apparent that quantization of the parameters {cki} is likely to produce a
significant perturbation of the zero positions and usually, it is sufficiently large in
fixed-point implementations to move the zeros off the unit circle. This is a highly
undesirable situation, which can be easily remedied by use of a floating-point repre-
sentation. In any case the cascade form is more robust in the presence of coefficient
quantization and should be the preferred choice in practical applications, especially
where a fixed-point representation is employed.

Determine the effect of parameter quantization on the frequency response of the seventh-

Solution.

Effect of coefficient
quantization of the
magnitude and phase
response of an N = 7
elliptic filter realized in
cascade form.

−100
0 .1 .2 .3 .4 .5

f
−90
−80
−70
−60
−50
−40
−30
−20
−10

0
10

Unquantized
Quantized to 3 and 4 digits

Relative frequency

G
ai

n 
(d

B
)

−180
0 .1 .2 .3 .4 .5

f

−120

− 60

0

60

120

180

Ph
as

e 
(d

eg
re

e)

The coefficients for an elliptic filter are specified for the cascade form to 
six significant digits. We quantized these coefficients to four and then three signifi-
cant digits (by rounding) and plotted the magnitude (in decibels) and the phase of

Implementation of Discrete-Time Systems

EXAMPLE 5.1

Figure 5.6

order elliptic filter when it is realized as a cascade of second-order sections.

633



the filter with unquantized (six significant digits) coefficients. We observe that there is an
insignificant degradation due to coefficient quantization for the cascade realization.

Solution.

H(z) = 0.2781304+ 0.0054373108z
−1

1− 0.790103z−1

+ −0.3867805+ 0.3322229z
−1

1− 1.517223z−1 + 0.714088z−2

+ 0.1277036− 0.1558696z
−1

1− 1.421773z−1 + 0.861895z−2

+ −0.015824186+ 0.38377356z
−1

1− 1.387447z−1 + 0.962242z−2
The frequency response of this filter with coefficients quantized to four digits is shown

observe that the zeros in the parallel realization have been perturbed sufficiently so that the
nulls in the magnitude response are now at −80, −85, and −92 dB. The phase response has
also been perturbed by a small amount.

When the coefficients are quantized to three significant digits, the frequency response
characteristic deteriorates significantly, in both magnitude and phase, as illustrated in Fig.

It is apparent from the magnitude response that the zeros are no longer on the
unit circle as a result of the quantization of the coefficients. This result clearly illustrates the
sensitivity of the zeros to quantization of the coefficients in the parallel form.

form is definitely more robust to parameter quantization than the parallel form.

5.2 Quantization of Coefficients in FIR Filters

As indicated in the preceding section, the sensitivity analysis performed on the poles
of a system also applies directly to the zeros of the IIR filters. Consequently, an

In
effect, we should generally realize FIR filters with a large number of zeros as a
cascade of second-order and first-order filter sections to minimize the sensitivity to
coefficient quantization.

Of particular interest in practice is the realization of linear-phase FIR filters.

property even when the coefficients are quantized. This follows easily from the
observation that the system function of a linear-phase FIR filter satisfies the property

H(z) = ±z−(M−1)H(z−1)

isThe system function for a 7-order elliptic filter

Implementation of Discrete-Time Systems

the frequency response. The results are shown in Fig. 5.6 along the frequency response of

Repeat the computation of the frequency response for the elliptic filter considered in Exam-

EXAMPLE 5.2

ple 5.1 when it is realized in the parallel form with second-order sections.

in Fig. 5.7(a). When this result is compared with the frequency response in Fig. 5.6, we

5.7(b).

When compared with the results of Example 5.1, it is also apparent that the cascade

expression analogous to (5.12) can be obtained for the zeros of an FIR filter.

The direct-form realizations shown in Figs. 2.1 and 2.2 maintain the linear-phase

634



Effect of coefficient
quantization of the
magnitude and phase
response of an N = 7
elliptic filter realized
in parallel form:
(a) quantization to four
digits; (b) quantization to
three digits.

−100
0 .1 .2 .3 .4 .5

f
−90
−80
−70
−60
−50
−40
−30
−20
−10

0
10

G
ai

n 
(d

B
)

−180
0 .1 .2 .3 .4 .5

f

−120

−60

0

60

120

180

Ph
as

e 
(d

eg
re

e)

Relative frequency
(a) Quantization to 4 digits

−100
0 .1 .2 .3 .4 .5

f
−90
−80
−70
−60
−50
−40
−30
−20
−10

0
10

G
ai

n 
(d

B
)

−180
0 .1 .2 .3 .4 .5

f

−120

−60

0

60

120

180

Ph
as

e 
(d

eg
re

e)

Relative frequency
(b) Quantization to 3 digits

Implementation of Discrete-Time Systems

Figure 5.7

635



Determine the effect of parameter quantization on the frequency response of an M = 32
linear-phase FIR bandpass filter. The filter is realized in the direct form.

Solution. The frequency response of a linear-phase FIR bandpass filter with unquantized

digits, the effect on the frequency response is insignificant. However, when the coefficients are
quantized to three significant digits, the sidelobes increase by several decibels, as illustrated

coefficients of this FIR filter and, preferably, 12 to 14 bits, if possible.

From this example we learn that a minimum of 10 bits is required to represent
the coefficients in a direct-form realization of an FIR filter of moderate length. As
the filter length increases, the number of bits per coefficient must be increased to
maintain the same error in the frequency response characteristic of the filter.

For example, suppose that each filter coefficient is rounded to (b+ 1) bits. Then
the maximum error in a coefficient value is bounded as

−2−(b+1) < eh(n) < 2−(b+1)

Effect of coefficient
quantization of the
magnitude of an M = 32
linear-phase FIR filter
realized in direct form:
(a) no quantization;
(b) quantization to three
digits.

−100
0 .1 .2 .3 .4 .5

f
−90
−80
−70
−60
−50
−40
−30
−20
−10

0
10

G
ai

n 
(d

B
)

Relative frequency

(a)  No quantization

−100
0 .1 .2 .3 .4 .5

f
−90
−80
−70
−60
−50
−40
−30
−20
−10

0
10

G
ai

n 
(d

B
)

Relative frequency

(b)  Quantization to 3 digits

independent of whether the coefficients are quantized or unquantized. Consequent-
ly, coefficient quantization does not affect the phase characteristic of the FIR filter, 
but affects only the magnitude. As a result, coefficient quantization effects are not as 
severe on a linear-phase FIR filter, since the only effect is in the magnitude.

Implementation of Discrete-Time Systems

EXAMPLE 5.3

coefficients is illustrated in Fig. 5.8(a). When the coefficients are quantized to four significant

in Fig. 5.8(b). This result indicates that we should use a minimum of 10 bits to represent the

Figure 5.8

636



Since the quantized values may be represented as h(n) = h(n) + eh(n), the error in
the frequency response is

EM(ω) =
M−1∑
n=0

eh(n)e
−jωn

Since eh(n) is zero mean, it follows that EM(ω) is also zero mean. Assuming that
the coefficient error sequence eh(n), 0 ≤ n ≤ M − 1, is uncorrelated, the variance of
the error EM(ω) in the frequency response is just the sum of the variances of the M
terms. Thus we have

σ 2E =
2−2(b+1)

12
M = 2

−2(b+2)

3
M

Here we note that the variance of the error inH(ω) increases linearly with M . Hence
the standard deviation of the error in H(ω) is

σE = 2
−(b+2)
√

3

√
M

Consequently, for every factor-of-4 increase in M , the precision in the filter coef-
ficients must be increased by one additional bit to maintain the standard deviation

frequency error remains tolerable for filter lengths up to 256, provided that filter
coefficients are represented by 12 to 13 bits. If the word length of the digital signal
processor is less than 12 bits or if the filter length exceeds 256, the filter should be
implemented as a cascade of smaller length filters to reduce the precision require-
ments.

In a cascade realization of the form

H(z) = G
K∏
k=1

Hk(z)

where the second-order sections are given as

Hk(z) = 1+ bk1z−1 + bk2z−2

the coefficients of complex-valued zeros are expressed as bk1 = −2rk cos θk and
bk2 = r2k . Quantization of bk1 and bk2
except that the grid extends to points outside the unit circle.

A problem may arise, in this case, in maintaining the linear-phase property,
because the quantized pair of zeros at z = (1/rk)e±jθk may not be the mirror image
of the quantized zeros at z = rke±jθk . This problem can be avoided by rearranging
the factors corresponding to the mirror-image zero. That is, we can write the mirror-
image factor as(

1− 2
rk

cos θkz−1 + 1
r2k
z−2

)
= 1
r2k
(r2k − 2rk cos θkz−1 + z−2)

(5.18)

(5.19)

(5.20)

Implementation of Discrete-Time Systems

fixed. This result, taken together with the results of Example 5.3, implies that the

results in zero locations as shown in Fig. 5.3,

637



The factors {1/r2k } can be combined with the overall gain factor G, or they can be

the same parameters as the factor (1− 2rk cos θkz−1 + r2k z−2), and consequently, the
zeros now occur in mirror-image pairs even when the parameters are quantized.

In this brief treatment we have given the reader an introduction to the problems
of coefficient quantization in IIR and FIR filters. We have demonstrated that a high-
order filter should be reduced to a cascade (for FIR or IIR filters) or a parallel (for IIR
filters) realization to minimize the effects of quantization errors in the coefficients.
This is especially important in fixed-point realizations in which the coefficients are
represented by a relatively small number of bits.

Round-Off Effects in Digital Filters

erations performed in a digital filter. The presence of one or more quantizers in the
realization of a digital filter results in a nonlinear device with characteristics that may
be significantly different from the ideal linear filter. For example, a recursive digital
filter may exhibit undesirable oscillations in its output, as shown in the following
section, even in the absence of an input signal.

As a result of the finite-precision arithmetic operations performed in the digital
filter, some registers may overflow if the input signal level becomes large. Overflow
represents another form of undesirable nonlinear distortion on the desired signal at
the output of the filter. Consequently, special care must be exercised to scale the
input signal properly, either to prevent overflow completely or, at least, to minimize
its rate of occurrence.

The nonlinear effects due to finite-precision arithmetic make it extremely difficult
to precisely analyze the performance of a digital filter. To perform an analysis of
quantization effects, we adopt a statistical characterization of quantization errors
which, in effect, results in a linear model for the filter. Thus we are able to quantify the
effects of quantization errors in the implementation of digital filters. Our treatment
is limited to fixed-point realizations where quantization effects are very important.

Limit-Cycle Oscillations in Recursive Systems

In the realization of a digital filter, either in digital hardware or in software on a digital
computer, the quantization inherent in the finite-precision arithmetic operations
renders the system nonlinear. In recursive systems, the nonlinearities due to the
finite-precision arithmetic operations often cause periodic oscillations to occur in the
output, even when the input sequence is zero or some nonzero constant value. Such
oscillations in recursive systems are called limit cycles and are directly attributable
to round-off errors in multiplication and overflow errors in addition.

To illustrate the characteristics of a limit-cycle oscillation, let us consider a single-
pole system described by the linear difference equation

y(n) = ay(n− 1)+ x(n) (6.1)

Implementation of Discrete-Time Systems

distributed in each of the second-order filters. The factor in (5.20) contains exactly

6.1

6

In Section 4 we characterized the quantization errors that occur in arithmetic op-

638



Ideal single-pole recursive
system.

+
x(n)

y(n)

z −1

a

the other hand, the actual system, which is described by the nonlinear difference
equation

v(n) = Q[av(n− 1)]+ x(n)

arithmetic based on four bits for the magnitude plus a sign bit. The quantization that
takes place after multiplication is assumed to round the resulting product upward.

of the pole z = a , and an input x(n) = βδ(n), where β = 15/16, which has the binary
representation 0.1111. Ideally, the response of the system should decay toward zero
exponentially [i.e., y(n) = an → 0 as n → ∞]. In the actual system, however,
the response v(n) reaches a steady-state periodic output sequence with a period
that depends on the value of the pole. When the pole is positive, the oscillations
occur with a period Np = 1, so that the output reaches a constant value of 116 for
a = 12 and 18 for a = 34 . On the other hand, when the pole is negative, the output
sequence oscillates between positive and negative values (± 116 for a = − 12 and ± 18
for a = − 34 ). Hence the period is Np = 2.

These limit cycles occur as a result of the quantization effects in multiplications.
When the input sequence x(n) to the filter becomes zero, the output of the filter
then, after a number of iterations, enters into the limit cycle. The output remains in
the limit cycle until another input of sufficient size is applied that drives the system
out of the limit cycle. Similarly, zero-input limit cycles occur from nonzero initial
conditions with the input x(n) = 0. The amplitudes of the output during a limit cycle
are confined to a range of values that is called the dead band of the filter.

It is interesting to note that when the response of the single-pole filter is in the
limit cycle, the actual nonlinear system operates as an equivalent linear system with

Actual nonlinear system.

+
x(n)

ν(n)

a

z −1Q[ ]

(6.2)

Implementation of Discrete-Time Systems

Figure 6.1

where the pole is at z = a . The ideal system is realized as shown in Fig. 6.1. On

is realized as shown in Fig. 6.2.
Suppose that the actual system in Fig. 6.2 is implemented with fixed-point

In Table 2 we list the response of the actual system for four different locations

Figure 6.2

639



Limit Cycles for Lowpass Single-Pole Filter

n a = 0.1000 = 12 a = 1.1000 = − 12 a = 0.1100 = 34 a = 1.1100 = − 34
0 0.1111

( 15
16

)
0.1111

( 15
16

)
0.1011

( 11
16

)
0.1011

( 11
16

)
1 0.1000

( 8
16

)
1.1000

(− 816 ) 0.1000 ( 816 ) 1.1000 (− 816 )
2 0.0100

( 4
16

)
0.0100

( 4
16

)
0.0110

( 6
16

)
0.0110

( 6
16

)
3 0.0010

( 2
16

)
1.0010

(− 216 ) 0.0101 ( 516 ) 1.0101 (− 516 )
4 0.0001

( 1
16

)
0.0001

( 1
16

)
0.0100

( 4
16

)
0.0100

( 4
16

)
5 0.0001

( 1
16

)
1.0001

(− 116 ) 0.0011 ( 316 ) 1.0011 (− 316 )
6 0.0001

( 1
16

)
0.0001

( 1
16

)
0.0010

( 2
16

)
0.0010

( 2
16

)
7 0.0001

( 1
16

)
1.0001

(− 116 ) 0.0010 ( 216 ) 1.0010 (− 216 )
8 0.0001

( 1
16

)
0.0001

( 1
16

)
0.0010

( 2
16

)
0.0010

( 2
16

)

a pole at z = 1 when the pole is positive and z = −1 when the pole is negative.
That is,

Qr [av(n− 1)] =
{
v(n− 1), a > 0
−v(n− 1), a < 0

Since the quantized product av(n − 1) is obtained by rounding, it follows that the
quantization error is bounded as

|Qr [av(n− 1)]− av(n− 1)| ≤ 12 · 2−b

where b is the number of bits (exclusive of sign) used in the representation of the

|v(n− 1)| − |av(n− 1)| ≤ 12 · 2−b

and hence

|v(n− 1)| ≤
1
2 · 2−b
1− |a|

For
example, when b = 4 and |a| = 12 , we have a dead band with a range of amplitudes
(− 116 , 116 ). When b = 4 and |a| = 34 , the dead band increases to (− 18 , 18 ).

The limit-cycle behavior in a two-pole filter is much more complex and a larger

by the linear difference equation,

y(n) = a1y(n− 1)+ a2y(n− 2)+ x(n)
whereas the actual system is described by the nonlinear difference equation

v(n) = Qr [a1v(n− 1)]+Qr [a2v(n− 2)]+ x(n)

(6.3)

(6.4)

(6.5)

variety of oscillations can occur. In this case the ideal two-pole system is described

(6.6)

(6.7)

Implementation of Discrete-Time Systems

TABLE 2

pole a and v(n). Consequently, (6.4) and (6.3) lead to

The expression in (6.5) defines the dead band for a single-pole filter.

640



When the filter coefficients satisfy the condition a21 < −4a2 , the poles of the
system occur at

z = re±jθ

where a2 = −r2 and a1 = 2r cos θ . As in the case of the single-pole filter, when the
system is in a zero-input or zero-state limit cycle,

Qr [a2v(n− 2)] = −v(n− 2)

the unit circle (i.e., a2 = −r2 2

|Qr [a2v(n− 2)]− a2v(n− 2)| ≤ 12 · 2−b

|v(n− 2)| − |a2v(n− 2)| ≤ 12 · 2−b

or equivalently,

|v(n− 2)| ≤
1
2 · 2−b

1− |a2|

congugate poles. We observe that the dead-band limits depend only on |a2|. The
parameter a1 = 2r cos θ determines the frequency of oscillation.

Another possible limit-cycle mode with zero input, which occurs as a result of
rounding the multiplications, corresponds to an equivalent second-order system with
poles at z = ±1. In this case it was shown by Jackson (1969) that the two-pole
filter exhibits oscillations with an amplitude that falls in the dead band bounded by
2−b/(1− |a1| − a2).

It is interesting to note that these limit cycles result from rounding the product
of the filter coefficients with the previous outputs, v(n− 1) and v(n− 2). Instead of
rounding, we may choose to truncate the products to b bits. With truncation, we can
eliminate many, although not all, of the limit cycles as shown by Claasen et al. (1973).
However, recall that truncation results in a biased error unless the sign-magnitude
representation is used, in which case the truncation error is symmetric about zero.
In general, this bias is undesirable in digital filter implementation.

In a parallel realization of a high-order IIR system, each second-order filter
section exhibits its own limit-cycle behavior, with no interaction among the second-
order filter sections. Consequently, the output is the sum of the zero-input limit cycles
from the individual sections. In the case of a cascade realization for a high-order IIR
system, the limit cycles are much more difficult to analyze. In particular, when the
first filter section exhibits a zero-input limit cycle, the output limit cycle is filtered
by the succeeding sections. If the frequency of the limit cycle falls near a resonance
frequency in a succeeding filter section, the amplitude of the sequence is enhanced by
the resonance characteristic. In general, we must be careful to avoid such situations.

(6.8)

(6.9)

(6.10)

Implementation of Discrete-Time Systems

= −1). Rounding the product a v(n− 2) implies that
In other words, the system behaves as an oscillator with complex-conjugate poles on

Upon substitution of (6.8) into (6.9), we obtain the result

The expression in (6.10) defines the dead band of the two-pole filter with complex-

641



Two-pole filter realization.

+

+

x(n)
y(n)

a1

z −1

z −1

a2

In addition to limit cycles caused by rounding the result of multiplications, there
are limit cycles caused by overflows in addition. An overflow in addition of two
or more binary numbers occurs when the sum exceeds the word size available in
the digital implementation of the system. For example, let us consider the second-

two’s-complement arithmetic. Thus we can write the output y(n) as

y(n) = g[a1y(n− 1)+ a2y(n− 2)+ x(n)]

where the function g[·] represents the two’s-complement addition. It is easily verified

Recall that the range of values of the parameters (a1, a2) for a stable filter is
sufficient

to prevent overflow oscillation with two’s-complement arithmetic. In fact, it can easily
be shown that a necessary and sufficient condition for ensuring that no zero-input
overflow limit cycles occur is

|a1| + |a2| < 1

which is extremely restrictive and hence an unreasonable constraint to impose on
any second-order section.

Characteristic functional
relationship for
two’s-complement addition
of two or more numbers.

ν

g[ν]

−1

−1

1

1 3 5

(6.11)

(6.12)

Implementation of Discrete-Time Systems

Figure 6.3

Figure 6.4

that the function g(v) versus v is described by the graph in Fig. 6.4.

order filter section illustrated in Fig. 6.3, in which the addition is performed in

given by the stability triangle. However, these conditions are no longer

642



Characteristic functional
relationship for addition
with clipping at ±1.

ν

g(ν)

−1

−1

1

1

An effective remedy for curing the problem of overflow oscillations is to modify

arithmetic. Thus when an overflow (or underflow) is sensed, the output of the adder
will be the full-scale value of ±1. The distortion caused by this nonlinearity in the
adder is usually small, provided that saturation occurs infrequently. The use of such
a nonlinearity does not preclude the need for scaling of the signals and the system
parameters, as described in the following section.

6.2 Scaling to Prevent Overflow

Saturation arithmetic as just described eliminates limit cycles due to overflow, on the
one hand, but on the other hand, it causes undesirable signal distortion due to the
nonlinearity of the clipper. In order to limit the amount of nonlinear distortion, it is
important to scale the input signal and the unit sample response, between the input
and any internal summing node in the system, such that overflow becomes a rare
event.

For fixed-point arithmetic, let us first consider the extreme condition that over-
flow is not permitted at any node of the system. Let yk(n) denote the response of the
system at the kth node when the input sequence is x(n) and the unit sample response
between the node and the input is hk(n). Then

|yk(n)| =
∣∣∣∣∣
∞∑

m=−∞
hk(m)x(n−m)

∣∣∣∣∣ ≤
∞∑

m=−∞
|hk(m)||x(n−m)|

Suppose that x(n) is upper bounded by Ax . Then

|yk(n)| ≤ Ax
∞∑

m=−∞
|hk(m)|, for all n

Now, if the dynamic range of the computer is limited to (−1, 1), the condition

|yk(n)| < 1

(6.13)

Implementation of Discrete-Time Systems

Figure 6.5

the adder characteristic, as illustrated in Fig. 6.5, so that it performs saturation

643



can be satisfied by requiring that the input x(n) be scaled such that

Ax <
1

∞∑
m=−∞

|hk(m)|

sufficient to prevent overflow.

input signal may be scaled too much. In such a case, much of the precision used
to represent x(n) is lost. This is especially true for narrowband sequences, such as

signals we can use the frequency response characteristics of the system in determining
the appropriate scaling. Since |H(ω)| represents the gain of the system at frequency
ω , a less severe and reasonably adequate scaling is to require that

Ax <
1

max
0≤ω≤π

|Hk(ω)|

where Hk(ω) is the Fourier transform of {hk(n)}.

Ax <
1

M−1∑
m=0
|hk(m)|

which is now a sum over the M nonzero terms of the filter unit sample response.
Another approach to scaling is to scale the input so that

∞∑
n=−∞

|yk(n)|2 ≤ C2
∞∑

n=−∞
|x(n)|2 = C2Ex

From Parseval’s theorem we have

∞∑
n=−∞

|yk(n)|2 = 12π
∫ π
−π
|H(ω)X(ω)|2dω

≤ Ex 12π
∫ π
−π
|H(ω)|2dω

C2 ≤ 1∞∑
n=−∞

|hk(n)|2
= 1
(1/2π)

∫ π
−π
|H(ω)|2dω

(6.14)

(6.15)

(6.16)

(6.17)

(6.18)

(6.19)

Implementation of Discrete-Time Systems

for all possible nodes in the system. The condition in (6.14) is both necessary and

The condition in (6.14) is overly conservative, however, to the point where the

sinusoids, where the scaling implied by (6.14) is extremely severe. For narrowband

In the case of an FIR filter, the condition in (6.14) reduces to

By combining (6.17) with (6.18), we obtain

644



If we compare the different scaling factors given above, we find that

[ ∞∑
n=−∞

|hk(n)|2
]1/2
≤ max

ω
|Hk(ω)| ≤

∞∑
n=−∞

|hk(n)|

In the following section we observe the ramifications of this scaling on the out-
put signal-to-noise (power) ratio (SNR) from a first-order and a second-order filter
section.

6.3 Statistical Characterization of Quantization Effects
in Fixed-Point Realizations of Digital Filters

It is apparent from our treatment in the previous section that an analysis of quantiza-
tion errors in digital filtering, based on deterministic models of quantization effects, is
not a very fruitful approach. The basic problem is that the nonlinear effects in quan-
tizing the products of two numbers and in clipping the sum of two numbers to prevent
overflow are not easily modeled in large systems that contain many multipliers and
many summing nodes.

To obtain more general results on the quantization effects in digital filters, we
shall model the quantization errors in multiplication as an additive noise sequence
e(n), just as we did in characterizing the quantization errors in A/D conversion of
an analog signal. For addition, we consider the effect of scaling the input signal to
prevent overflow.

Let us begin our treatment with the characterization of the round-off noise in a
single-pole filter which is implemented in fixed-point arithmetic and is described by
the nonlinear difference equation

v(n) = Qr [av(n− 1)]+ x(n)

The effect of rounding the product av(n − 1) is modeled as a noise sequence e(n)
added to the actual product av(n− 1), that is,

Qr [av(n− 1)] = av(n− 1)+ e(n)

With this model for the quantization error, the system under consideration is de-
scribed by the linear difference equation

v(n) = av(n− 1)+ x(n)+ e(n)

(6.20)

(6.21)

(6.22)

(6.23)

Implementation of Discrete-Time Systems

Clearly, (6.14) is the most pessimistic constraint.

The corresponding system is illustrated in block diagram form in Fig. 6.6.

645



Additive noise model for
the quantization error in a
single-pole filter.

+

+

x(n)
ν(n)

e(n)

a

z −1

separated into two components. One is the response of the system to the input
sequence x(n). The second is the response of the system to the additive quantization
noise e(n). In fact, we can express the output sequence v(n) as a sum of these two
components, that is,

v(n) = y(n)+ q(n)
where y(n) represents the response of the system to x(n), and q(n) represents the

y(n)+ q(n) = ay(n− 1)+ aq(n− 1)+ x(n)+ e(n)

To simplify the analysis, we make the following assumptions about the error
sequence e(n).

1. For any n, the error sequence {e(n)} is uniformly distributed over the range
(− 12 · 2−b , 12 · 2−b ). This implies that the mean value of e(n) is zero and its
variance is

σ 2e =
2−2b

12

2. The error {e(n)} is a stationary white noise sequence. In other words, the error
e(n) and the error e(m) are uncorrelated for n �= m.

3. The error sequence {e(n)} is uncorrelated with the signal sequence {x(n)}.

uncoupled difference equations, namely,

y(n) = ay(n− 1)+ x(n)
q(n) = aq(n− 1)+ e(n)

quantization error at the output of the system.

(6.24)

(6.25)

(6.26)

(6.27)

(6.28)

Implementation of Discrete-Time Systems

Figure 6.6

It is apparent from (6.23) that the output sequence v(n) of the filter can be

for v(n) into (6.23), we obtain
response of the system to the quantization error e(n). Upon substitution from (6.24)

The difference equation in (6.27) represents the input–output relation for the de-
sired system and the difference equation in (6.28) represents the relation for the

The last assumption allows us to separate the difference equation in (6.25) into two

646



mq = me
∞∑

n=−∞
h(n)

or, equivalently,
mq = meH(0)

where H(0) is the value of the frequency response H(ω) of the filter evaluated at
ω = 0.

The second important relationship is the expression for the autocorrelation se-
quence of the output q(n) of the filter with impulse response h(n) when the input
random sequence e(n) has an autocorrelation γee(n). This result is

γqq(n) =
∞∑

k=−∞

∞∑
l=−∞

h(k)h(l)γee(k − l + n)

the autocorrelation γee(n) is a unit sample sequence scaled by the variance σ 2e , that is,

γee(n) = σ 2e δ(n)

relation sequence at the output of a filter excited by white noise, namely,

γqq(n) = σ 2e
∞∑

k=−∞
h(k)h(k + n)

The variance σ 2q of the output noise is simply obtained by evaluating γqq(n) at n = 0.
Thus

σ 2q = σ 2e
∞∑

k=−∞
h2(k)

and with the aid of Parseval’s theorem, we have the alternative expression

σ 2q =
σ 2e

2π

∫ π
−π
|H(ω)|2dω

In the case of the single-pole filter under consideration, the unit sample re-
sponse is

h(n) = anu(n)

(6.29)

(6.30)

(6.31)

In the important special case where the random sequence is white (spectrally flat),

(6.32)

(6.33)

(6.34)

(6.35)

(6.36)

To complete the analysis, we make use of two important relationships. The first 
is the relationship for the mean value of the output  of a linear shift-invariant 
filter with impulse response  when excited by a random sequence  having a 
mean value  . The result is

q(n)

h(n) e(n)

me

Implementation of Discrete-Time Systems

Upon substituting (6.32) into (6.31), we obtain the desired result for the autocor-

647



Since the quantization error due to rounding has zero mean, the mean value of the
error at the output of the filter is mq = 0. The variance of the error at the output of
the filter is

σ 2q = σ 2e
∞∑
k=0

a2k

= σ
2
e

1− a2
We observe that the noise power σ 2q at the output of the filter is enhanced relative

to the input noise power σ 2e by the factor 1/(1−a2). This factor increases as the pole
is moved closer to the unit circle.

To obtain a clearer picture of the effect of the quantization error, we should
also consider the effect of scaling the input. Let us assume that the input sequence
{x(n)} is a white noise sequence (wideband signal), whose amplitude has been scaled

Ax < 1− |a|
If we assume that x(n) is uniformly distributed in the range (−Ax,Ax), then, accord-

σ 2y = σ 2x
∞∑
k=0

a2k

= σ
2
x

1− a2

where σ 2x = (1 − |a|)2/3 is the variance of the input signal. The ratio of the signal
power σ 2y to the quantization error power σ

2
q , which is called the signal-to-noise ratio

(SNR), is simply
σ 2y

σ 2q
= σ

2
x

σ 2e

= (1− |a|)2 · 22(b+1)

This expression for the output SNR clearly illustrates the severe penalty paid as
a consequence of the scaling of the input, especially when the pole is near the unit
circle. By comparison, if the input is not scaled and the adder has a sufficient number
of bits to avoid overflow, then the signal amplitude may be confined to the range
(−1, 1). In this case, σ 2x = 13 , which is independent of the pole position. Then

σ 2y

σ 2q
= 22(b+1)

need to use more bits in addition than in multiplication. The number of additional

(6.37)

(6.38)

(6.39)

(6.40)

Implementation of Discrete-Time Systems

according to (6.14) to prevent overflows in addition. Then

ing to (6.31) and (6.34), the signal power at the output of the filter is

The difference between the SNRs in (6.40) and (6.39) clearly demonstrates the

648



Two-pole digital filter with
rounding quantizers.

Qr[ ]

Qr[ ]

+

+

x(n)
ν(n)

a1

z −1

z −1

a2

bits depends on the position of the pole and should be increased as the pole is moved
closer to the unit circle.

Next, let us consider a two-pole filter with infinite precision which is described
by the linear difference equation

y(n) = a1y(n− 1)+ a2y(n− 2)+ x(n)

where a1 = 2r cos θ and a2 = −r2 . When the two products are rounded, we have a
system which is described by the nonlinear difference equation

v(n) = Qr [a1v(n− 1)]+Qr [a2v(n− 2)]+ x(n)

Now there are two multiplications, and hence two quantization errors are pro-
duced for each output. Consequently, we should introduce two noise sequences e1(n)
and e2(n), which correspond to the quantizer outputs

Qr [a1v(n− 1)] = a1v(n− 1)+ e1(n)
Qr [a2v(n− 2)] = a2v(n− 2)+ e2(n)

error sequences e1(n) and e2(n) can be moved directly to the input of the filter.
As in the case of the first-order filter, the output of the second-order filter can be

separated into two components, the desired signal component and the quantization
error component. The former is described by the difference equation

y(n) = a1y(n− 1)+ a2y(n− 2)+ x(n)

while the latter satisfies the difference equation

q(n) = a1q(n− 1)+ a2q(n− 2)+ e1(n)+ e2(n)

It is reasonable to assume that the two sequences e1(n) and e2(n) are uncorrelated.

Implementation of Discrete-Time Systems

Figure 6.7

(6.41)

(6.42)

(6.43)

This system is illustrated in block diagram form in Fig. 6.7.

A block diagram for the corresponding model is shown in Fig. 6.8. Note that the

(6.44)

(6.45)

649



Additive noise model for
the quantization errors in a
two-pole filter realization.

+

+ +

+

x(n)
ν(n)

a1

z−1

z−1

a2

e2(n)

e1(n)

Now the second-order filter has a unit sample response

h(n) = r
n

sin θ
sin(n+ 1)θu(n)

Hence
∞∑
n=0

h2(n) = 1+ r
2

1− r2
1

r4 + 1− 2r2 cos 2θ

of the filter in the form

σ 2q = σ 2e
(

1+ r2
1− r2

1
r4 + 1− 2r2 cos 2θ

)

overflow, the power in the output signal is

σ 2y = σ 2x
∞∑
n=0

h2(n)

where the power in the input signal x(n) is given by the variance

σ 2x =
1

3

[ ∞∑
n=0
|h(n)|

]2

Consequently, the SNR at the output of the two-pole filter is

σ 2y

σ 2q
= σ

2
x

σ 2e
= 2

2(b+1)[ ∞∑
n=0
|h(n)|

]2

Implementation of Discrete-Time Systems

Figure 6.8

(6.46)

(6.47)

By applying (6.34), we obtain the variance of the quantization errors at the output

(6.48)

In the case of the signal component, if we scale the input as in (6.14) to avoid

(6.49)

(6.50)

(6.51)

650



Although it is difficult to determine the exact value of the denominator term in

bounded as

|h(n)| ≤ 1
sin θ

rn, n ≥ 0

so that
∞∑
n=0
|h(n)| ≤ 1

sin θ

∞∑
n=0

rn = 1
(1− r) sin θ

The lower bound may be obtained by noting that

|H(ω)| =
∣∣∣∣∣
∞∑
n=0

h(n)e−jωn
∣∣∣∣∣ ≤

∞∑
n=0
|h(n)|

But

H(ω) = 1
(1− rejθ e−jω)(1− re−jθ e−jω)

At ω = θ , which is the resonant frequency of the filter, we obtain the largest value
of |H(ω)|. Hence

∞∑
n=0
|h(n)| ≥ |H(θ)| = 1

(1− r)√1+ r2 − 2r cos 2θ

Therefore, the SNR is bounded from above and below according to the relation

22(b+1)(1− r)2 sin2 θ ≤ σ
2
y

σ 2q
≤ 22(b+1)(1− r)2(1+ r2 − 2r cos 2θ)

22(b+1)(1− r)2 ≤ σ
2
y

σ 2q
≤ 22(b+1)(1− r)2(1+ r)2

The dominant term in this bound is (1 − r)2 which acts to reduce the SNR
dramatically as the poles move toward the unit circle. Hence the effect of scaling
in the second-order filter is more severe than in the single-pole filter. Note that if
d = 1 − r
reduced by d2 , whereas in the single-pole filter the reduction is proportional to d .
These results serve to reinforce the earlier statement regarding the use of more bits
in addition than in multiplication as a mechanism for avoiding the severe penalty
due to scaling.

The analysis of the quantization effects in a second-order filter can be applied
directly to higher-order filters based on a parallel realization. In this case each second-
order filter section is independent of all the other sections, and therefore the total

Implementation of Discrete-Time Systems

(6.51), it is easy to obtain an upper and a lower bound. In particular, |h(n)| is upper

(6.52)

(6.53)

(6.54)

(6.55)

For example, when θ = π/2, the expression in (6.55) reduces to

(6.56)

is the distance of the pole from the unit circle, the SNR in (6.56) is

651



quantization noise power at the output of the parallel bank is simply the linear sum of
the quantization noise powers of each of the individual sections. On the other hand,
the cascade realization is more difficult to analyze. For the cascade interconnection,
the noise generated in any second-order filter section is filtered by the succeeding
sections. As a consequence, there is the issue of how to pair together real-valued
poles to form second-order sections and how to arrange the resulting second-order
filters to minimize the total noise power at the output of the high-order filter. This
general topic was investigated by Jackson (1970a, b), who showed that poles close
to the unit circle should be paired with nearby zeros to reduce the gain of each
second-order section. In ordering the second-order sections in cascade, a reasonable
strategy is to place the sections in the order of decreasing maximum frequency gain.
In this case the noise power generated in the early high-gain section is not boosted
significantly by the latter sections.

The following example illustrates the point that proper ordering of sections in a
cascade realization is important in controlling the round-off noise at the output of
the overall filter.

Determine the variance of the round-off noise at the output of the two cascade realizations of
the filter with system function

H(z) = H1(z)H2(z)
where

H1(z) = 1
1− 12 z−1

H2(z) = 1
1− 14 z−1

Solution. Let h(n), h1(n), and h2(n) represent the unit sample responses corresponding to
the system functions H(z), H1(z), and H2(z), respectively. It follows that

h1(n) = ( 12 )nu(n), h2(n) = ( 14 )nu(n)

h(n) = [2( 12 )n − ( 14 )n]u(n)

In the first cascade realization, the variance of the output is

σ 2q1 = σ 2e
[ ∞∑
n=0

h2(n)+
∞∑
n=0

h22(n)

]

In the second cascade realization, the variance of the output noise is

σ 2q2 = σ 2e
[ ∞∑
n=0

h2(n)+
∞∑
n=0

h21(n)

]

Implementation of Discrete-Time Systems

EXAMPLE 6.1

The two cascade realizations are shown in Fig. 6.9.

652



x(n)

e1(n)

y(n)

z −1

+

+

+

1
2

e2(n)

z −1

+
1
4

(a)  Cascade realization I

x(n)

e1(n)

y(n)

z −1

+

+

+

1
4

e2(n)

z −1

+
1
2

(b)  Cascade realization II

realization I; (b) cascade realization II.

Now

∞∑
n=0

h21(n) =
1

1− 14
= 4

3

∞∑
n=0

h22(n) =
1

1− 116
= 16

15

∞∑
m=0

h2(n) = 4
1− 14

− 4
1− 18

+ 1
1− 116

= 1.83

Therefore,

σ 2q1 = 2.90σ 2e
σ 2q2 = 3.16σ 2e

and the ratio of noise variances is
σ 2q2

σ 2
q1

= 1.09

Consequently, the noise power in the second cascade realization is 9% larger than in the first
realization.

Implementation of Discrete-Time Systems

Figure 6.9 Two cascade realizations in Example 6.1: (a) cascade

653



7 Summary and References

From the treatment in this chapter we have seen that there are various realizations of
discrete-time systems. FIR systems can be realized in a direct form, a cascade form,
a frequency sampling form, and a lattice form. IIR systems can also be realized in a
direct form, a cascade form, a lattice or a lattice-ladder form, and a parallel form.

For any given system described by a linear constant-coefficient difference equa-
tion, these realizations are equivalent in that they represent the same system and
produce the same output for any given input, provided that the internal computa-
tions are performed with infinite precision. However, the various structures are not
equivalent when they are realized with finite-precision arithmetic.

Additional FIR and IIR filter structures can be obtained by adopting a state-
space formulation that provides an internal description of a system. Such state-space

from this edition due to space limitations. The use of state-space filter structures in
the realization of IIR systems has been proposed by Mullis and Roberts (1976a,b),
and further developed by Hwang (1977), Jackson et al. (1979), Jackson (1979), Mills
et al. (1981), and Bomar (1985).

Three important factors are presented for choosing among the various FIR and
IIR system realizations. These factors are computational complexity, memory re-
quirements, and finite-word-length effects. Depending on either the time-domain or
the frequency-domain characteristics of a system, some structures may require less
computation and/or less memory than others. Hence our selection must consider
these two important factors.

cepts and operations on signal flow graphs. Signal flow graphs are treated in depth
in the books by Mason and Zimmerman (1960) and Chow and Cassignol (1962).

Another important structure for IIR systems, a wave digital filter, has been inves-
tigated by Fettweis (1971) and further developed by Sedlmeyer and Fettweis (1973).
A treatment of this filter structure can also be found in the book by Antoniou (1979).

Finite-word-length effects are an important factor in the implementation of dig-
ital signal processing systems. In this chapter we described the effects of a finite
word length in digital filtering. In particular, we considered the following problems
dealing with finite-word length effects:

1. Parameter quantization in digital filters

2. Round-off noise in multiplication

3. Overflow in addition

4. Limit cycles

Implementation of Discrete-Time Systems

In deriving the transposed structures in Section 3, we introduced several con-

realizations were treated in previous editions of this text, but have been dropped

654



These four effects are internal to the filter and influence the method by which the
system will be implemented. In particular, we demonstrated that high-order systems,
especially IIR systems, should be realized by using second-order sections as building
blocks. We advocated the use of the direct form II realization, either the conventional
or the transposed form.

Effects of round-off errors in fixed-point implementations of FIR and IIR filter
structures have been investigated by many researchers. We cite the papers by Gold
and Rader (1966), Rader and Gold (1967b), Jackson (1970a,b), Liu (1971), Chan and
Rabiner (1973a,b,c), and Oppenheim and Weinstein (1972).

Limit-cycle oscillations occur in IIR filters as a result of quantization effects
in fixed-point multiplication and rounding. Investigation of limit cycles in digital
filtering and their characteristic behavior is treated in the papers by Parker and Hess
(1971), Brubaker and Gowdy (1972), Sandberg and Kaiser (1972), and Jackson (1969,
1979). The latter paper deals with limit cycles in state-space structures. Methods
have also been devised to eliminate limit cycles caused by round-off errors. For
example, the papers by Barnes and Fam (1977), Fam and Barnes (1979), Chang
(1981), Butterweck et al. (1984), and Auer (1987) discuss this problem. Overflow
oscillations have been treated in the paper by Ebert et al. (1969).

The effects of parameter quantization have been treated in a number of papers.
We cite for reference the work of Rader and Gold (1967b), Knowles and Olcayto
(1968), Avenhaus and Schuessler (1970), Herrmann and Schuessler (1970b), Chan
and Rabiner (1973c), and Jackson (1976).

Finally, we mention that the lattice and lattice-ladder filter structures are known
to be robust in fixed-point implementations. For a treatment of these types of filters,
the reader is referred to the papers of Gray and Markel (1973), Makhoul (1978), and
Morf et al. (1977) and to the book by Markel and Gray (1976).

Problems

Determine a direct-form realization for the following linear phase filters.

(a) h(n) = {1
↑
, 2, 3, 4, 3, 2, 1}

(b) h(n) = {1
↑
, 2, 3, 3, 2, 1}

2 Consider an FIR filter with system function

H(z) = 1+ 2.88z−1 + 3.4048z−2 + 1.74z−3 + 0.4z−4

Sketch the direct-form and lattice realizations of the filter and determine in detail
the corresponding input–output equations. Is the system minimum phase?

Implementation of Discrete-Time Systems

1

655



3 Determine the system function and the impulse response of the system shown in

x(n) y(n)
+ + +

z −1

1 2

2

3

2

4 Determine the system function and the impulse response of the system shown in

x(n) y(n)
+

+ +

+

z −1

z −1

1

3

5

3

1 2
2

−

5
the original and the transposed system have the same system function.

Implementation of Discrete-Time Systems

Fig. P3.

Figure P3

Fig. P4.

Figure P4

Determine the transposed structure of the system in Fig. P4 and verify that both

656



6 Determine a1 , a2 and c1, and c0 in terms of b1 and b2 so that the two systems in

x(n) y(n)

+

+

+

z −1

z −1
b2

a1 a2

b1

x(n) y(n)

z −1

+ + +

c1

c0

z −1

7

x(n)

z −1

b1b2

a1a2

y(n)

b0

+ z −1+ +

(a) Determine its system function.
(b) Sketch the pole–zero plot and check for stability if

1. b0 = b2 = 1, b1 = 2, a1 = 1.5, a2 = −0.9
2. b0 = b2 = 1, b1 = 2, a1 = 1, a2 = −2

(c) Determine the response to x(n) = cos(πn/3) if b0 = 1, b1 = b2 = 0, a1 = 1,
and a2 = −0.99.

Implementation of Discrete-Time Systems

Fig. P6 are equivalent.

Figure P6

Figure P7

Consider the filter shown in Fig. P7.

657



8 Consider an LTI system, initially at rest, described by the difference equation

y(n) = 14y(n− 2)+ x(n)
(a) Determine the impulse response, h(n), of the system.
(b) What is the response of the system to the input signal

x(n) = [( 12 )n + (− 12 )n]u(n)
(c) Determine the direct form II, parallel-form, and cascade-form realizations for

this system.
(d) Sketch roughly the magnitude response |H(ω)| of this system.

9 Obtain the direct form I, direct form II, cascade, and parallel structures for the
following systems.
(a) y(n) = 34y(n− 1)− 18y(n− 2)+ x(n)+ 13x(n− 1)
(b) y(n) = −0.1y(n− 1)+ 0.72y(n− 2)+ 0.7x(n)− 0.252x(n− 2)
(c) y(n) = −0.1y(n− 1)+ 0.2y(n− 2)+ 3x(n)+ 3.6x(n− 1)+ 0.6x(n− 2)

(d) H(z) = 2(1− z
−1)(1+√2z−1 + z−2)

(1+ 0.5z−1)(1− 0.9z−1 + 0.81z−2)
(e) y(n) = 12y(n− 1)+ 14y(n− 2)+ x(n)+ x(n− 1)
(f) y(n) = y(n− 1)− 12y(n− 2)+ x(n)− x(n− 1)+ x(n− 2)
Which of the systems above are stable?

10

x(n)

x(n)

y(n)

y(n)

++

+

+

+

z −1

z −1

z −1

z −1

r sin ω0

−r sin ω0

−r2

r cos ω0

2r cos ω0

r cos ω0

Implementation of Discrete-Time Systems

Show that the systems in Fig. P10 are equivalent.

Figure P10

658



11 Determine all the FIR filters which are specified by the lattice parameters K1 = 12 ,
K2 = 0.6, K3 = −0.7, and K4 = 13 .

12 Determine the set of difference equations for describing a realization of an IIR
system based on the use of the transposed direct form II structure for the second-
order subsystems.

∗ Write a program that implements a parallel-form realization based on transposed
direct form II second-order modules.

∗ Write a program that implements a cascade-form realization based on regular direct
form II second-order modules.

15 Determine the parameters {Km} of the lattice filter corresponding to the FIR filter
described by the system function

H(z) = A2(z) = 1+ 2z−1 + z−2

16 (a) Determine the zeros and sketch the zero pattern for the FIR lattice filter with
parameters

K1 = 12 , K2 = − 13 , K3 = 1

(b) The same as in part (a) but with K3 = −1.
(c) You should have found that all the zeros lie exactly on the unit circle. Can this

result be generalized? How?

(d) Sketch the phase response of the filters in parts (a) and (b). What did you notice?
Can this result be generalized? How?

17 Consider an FIR lattice filter with coefficientsK1 = 0.65, K2 = −0.34, and K3 = 0.8.
(a) Find its impulse response by tracing a unit impulse input through the lattice

structure.

(b) Draw the equivalent direct-form structure.

18 Consider a causal IIR system with system function

H(z) = 1+ 2z
−1 + 3z−2 + 2z−3

1+ 0.9z−1 − 0.8z−2 + 0.5z−3

(a) Determine the equivalent lattice-ladder structure.

(b) Check if the system is stable.

Implementation of Discrete-Time Systems

13

14

659



19 Determine the input–output relationship, the system function, and plot the pole–zero

x(n)

r cos θ

y(n)

r cos θ

r sin θ

r sin θ

−r sin θ
−r cos θ

+

+ z −1

z −1

20 Determine the lattice realization for the digital resonator

H(z) = 1
1− (2r cosω0)z−1 + r2z−2

21
(a) Determine the impulse response of an FIR lattice filter with parameters K1 =

0.6, K2 = 0.3, K3 = 0.5, and K4 = 0.9.
(b) Sketch the direct-form and lattice all-zero and all-pole filters specified by the

K -parameters given in part (a).
22 (a) Determine the lattice-ladder realization for the resonator

H(z) = 1− z
−1

1− (2r cosω0)z−1 + r2z−2
(b) What happens if r = 1?

23 Sketch the lattice-ladder structure for the system

H(z) = 1− 0.8z
−1 + 0.15z−2

1+ 0.1z−1 − 0.72z−2
24 Consider a pole–zero system with system function

H(z) = (1− 0.5e
jπ/4z−1)(1− 0.5e−jπ/4z−1)

(1− 0.8ejπ/3z−1)(1− 0.8e−jπ/3z−1)
Sketch the regular and transpose direct form II realizations of the system.

25 Determine a parallel and a cascade realization of the system

H(z) = 1+ z
−1

(1− z−1)(1− 0.8ejπ/4z−1)(1− 0.8e−jπ/4z−1)

Implementation of Discrete-Time Systems

pattern for the discrete-time system shown in Fig. P19.

Figure P19

660



26 The generic floating-point format for a DSP microprocessor is the following:

E S M

sign bit

exponent mantissa

The value of the number X is given by

X =



01.M × 2E if S = 0
10.M × 2E if S = 1
0 if E is the most negative two’s-complement value

Determine the range of positive and negative numbers for the following two formats:

(a)
E S M

15 12 11 10 0

short format

(b)
E S M

31 24 23 22 0

single-precision format

27 F (n), hR(n), and h(n)
denote the impulse responses of the FIR section, the recursive section, and the overall
filter, respectively.

x(n)

y(n)
+++++

z −1 z −1 z −1

z −1

−a2 −a1

z −1

•  •  •

•  •  •

(a) Find all the causal and stable recursive second-order sections with integer coeffi-
cients (a1, a2) and determine and sketch their impulse responses and frequency
responses. These filters do not require complicated multiplications or quantiza-
tion after multiplications.

(b) Show that three of the sections obtained in part (a) can be obtained by intercon-
nection of other sections.

(c) Find a difference equation that describes the impulse response h(n) of the filter
and determine the conditions for the overall filter to be FIR.

(d) Rederive the results in parts (a) to (c) using z-domain considerations.

Implementation of Discrete-Time Systems

Consider the IIR recursive filter shown in Fig. P27 and let h

Figure P26

Figure P27

661



28 This problem illustrates the development of digital filter structures using Horner’s
rule for polynomial evaluation. To this end consider the polynomial

p(x) = αpxp + ap−1xp−1 + · · · + a1x + a0

which computes p(x) with the minimum cost of p multiplications and p additions.

(a) Draw the structures corresponding to the factorizations

H1(z) = b0(1+ b1z−1(1+ b2z−1(1+ b3z−1)))
H(z) = b0(z−3 + (b1z−2 + (b2z−1 + b3)))

and determine the system function, number of delay elements, and arithmetic
operations for each structure

(b) Draw the Horner structure for the following linear-phase system:

H(z) = z−1
[
α0 +

3∑
k=1
(z−k + zk)αk

]

29 Let x1 and x2 be (b+ 1)-bit binary numbers with magnitude less than 1. To compute
the sum of x1 and x2 using two’s-complement representation we treat them as (b+1)-
bit unsigned numbers, perform addition modulo-2 and ignore any carry after the sign
bit.

(a) Show that if the sum of two numbers with the same sign has the opposite sign,
this corresponds to overflow.

(b) Show that when we compute the sum of several numbers using two’s-complement
representation, the result will be correct, even if there are overflows, if the correct
sum is less than 1 in magnitude. Illustrate this argument by constructing a simple
example with three numbers.

30 Consider the system described by the difference equation

y(n) = ay(n− 1)− ax(n)+ x(n− 1)

(a) Show that it is allpass.

(b) Obtain the direct form II realization of the system

(c) If you quantize the coefficients of the system in part (b), is it still allpass?

(d) Obtain a realization by rewriting the difference equation as

y(n) = a[y(n− 1)− x(n)]+ x(n− 1)

(e) If you quantize the coefficients of the system in part (d), is it still allpass?

Implementation of Discrete-Time Systems

662



31 Consider the system

y(n) = 12y(n− 1)+ x(n)

(a) Compute its response to the input x(n) = ( 14 )nu(n) assuming infinite-precision
arithmetic.

(b) Compute the response of the system y(n), 0 ≤ n ≤ 5 to the same input, assuming
finite-precision sign-and-magnitude fractional arithmetic with five bits (i.e., the
sign bit plus four fractional bits). The quantization is performed by truncation.

(c) Compare the results obtained in parts (a) and (b).

32 The input to the system

y(n) = 0.999y(n− 1)+ x(n)

is quantized to b = 8 bits. What is the power produced by the quantization noise at
the output of the filter?

33 Consider the system

y(n) = 0.875y(n− 1)− 0.125y(n− 2)+ x(n)

(a) Compute its poles and design the cascade realization of the system.

(b) Quantize the coefficients of the system using truncation, maintaining a sign bit
plus three other bits. Determine the poles of the resulting system.

(c) Repeat part (b) for the same precision using rounding.

(d) Compare the poles obtained in parts (b) and (c) with those in part (a). Which
realization is better? Sketch the frequency responses of the systems in parts (a),
(b), and (c).

34 Consider the system

H(z) = 1−
1
2z
−1

(1− 14z−1)(1+ 14z−1)

(a) Draw all possible realizations of the system.

(b) Suppose that we implement the filter with fixed-point sign-and-magnitude frac-
tional arithmetic using (b + 1) bits (one bit is used for the sign). Each resulting
product is rounded into b bits. Determine the variance of the round-off noise
created by the multipliers at the output of each one of the realizations in part
(a).

Implementation of Discrete-Time Systems

663



35
fixed-point two’s-complement fractional arithmetic. Products are rounded to four-bit
representation. Using the input x(n) = 0.10δ(n), determine:

+
x(n)

y1(n)

z −1

α

(a) The first five outputs if α = 0.5. Does the filter go into a limit cycle?
(b) The first five outputs if α = 0.75. Does the filter go into a limit cycle?

36
two’s-complement A/D converter with rounding, and the filter H(z) is implemented
using eight-bit (including sign) fixed-point two’s-complement fractional arithmetic
with rounding. The input x(t) is a zero-mean uniformly distributed random process
having autocorrelation γxx(τ ) = 3δ(τ ). Assume that the A/D converter can handle
input values up to ±1.0 without overflow.
(a) What value of attenuation should be applied prior to the A/D converter to assure

that it does not overflow?

(b)

(c) The six-bit A/D samples can be left justified, right justified, or centered in the
eight-bit word used as the input to the digital filter. What is the correct strategy
to use for maximum SNR at the filter output without overflow?

(d) What is the SNR at the output of the filter due to all quantization noise sources?

+
x(n)

x(t)

y(n)

z−1

A/D
converter

6 bits

Filter H(z)
(8 bits)

Attenuator

0.75

Implementation of Discrete-Time Systems

Figure P36

The digital system shown in Fig. P36 uses a six-bit (including sign) fixed-point

The first-order filter shown in Fig. P35 is implemented in four-bit (including sign)

Figure P35

With the attenuation above, what is the signal-to-quantization-noise ratio  
(SQNR) at the A/D converter output?

664



37
poles at x = re±jθ . There are four real multiplications per output point. Let ei(n),
i = 1, 2, 3, 4 represent the round-off noise in a fixed-point implementation of the
filter. Assume that the noise sources are zero-mean mutually uncorrelated stationary
white noise sequences. For each n the probability density function p(e) is uniform
in the range −�/2 ≤ e ≤ �/2, where � = 2−b .

+ +

+

x(n)

y(n)

ν(n − 1)

ν(n)

y(n − 1)
r cos θ

r cos θ

r sin θ

−r sin θ

+
e1(n)

+e3(n)

+e2(n)

+e4(n)

z −1

z −1

(a) Write the two coupled difference equations for y(n) and v(n), including the
noise sources and the input sequence x(n).

(b) From these two difference equations, show that the filter system functions H1(z)
and H2(z) between the input noise terms e1(n)+ e2(n) and e3(n)+ e4(n) and the
output y(n) are:

H1(z) = r sin θz
−1

1− 2r cos θz−1 + r2z−2

H2(z) = 1− r cos θz
−1

1− 2r cos θz−1 + r2z−2
We know that

H(z) = 1
1− 2r cos θz−1 + r2z−2 ⇒ h(n) =

1
sin θ

rn sin(n+ 1)θu(n)

Determine h1(n) and h2(n).
(c) Determine a closed-form expression for the variance of the total noise from

ei(n), i = 1, 2, 3, 4 at the output of the filter.

Implementation of Discrete-Time Systems

Shown in Fig. P37 is the coupled-form implementation of a two-pole filter with

Figure P37

665



38 Determine the variance of the round-off noise at the output of the two cascade

H(z) = H1(z)H2(z)

where

H1(z) = 1
1− 12z−1

H2(z) = 1
1− 13z−1

x(n)

e1(n)

y(n)

z −1

+

+

+

1
2

e2(n)

z −1

+
1
3

(a)  Cascade realization I

x(n)

e1(n)

y(n)

z −1

+

+

+

1
3

e2(n)

z −1

+
1
2

(b)  Cascade realization II

39 Quantization effects in direct-form FIR filters Consider a direct-form realization of
an FIR filter of length M . Suppose that the multiplication of each coefficient with
the corresponding signal sample is performed in fixed-point arithmetic with b bits
and each product is rounded to b bits. Determine the variance of the quantization
noise at the output of the filter by using a statistical characterization of the round-off

Implementation of Discrete-Time Systems

realizations of the filter shown in Fig. P38, with system function

noise as in Section 6.3.

Figure P38

666



40 Consider the system specified by the system function

H(z) = B(z)
A(z)

=
[
G1
(1− 0.8ejπ/4z−1)(1− 0.8e−jπ/4z−1)

(1− 12z−1)(1+ 13z−1)

][
G2

(1+ 14z−1)(1− 58z−1)
(1− 0.8ejπ/3z−1)(1− 0.8e−jπ/3z−1)

]

(a) Choose G1 and G2 so that the gain of each second-order section at ω = 0 is
equal to 1.

(b) Sketch the direct form 1, direct form 2, and cascade realizations of the system.

(c) Write a program that implements the direct form 1 and direct form 2, and com-
pute the first 100 samples of the impulse response and the step response of the
system.

(d) Plot the results in part (c) to illustrate the proper functioning of the programs.

41 1 = G2 = 1.
(a) Determine a lattice realization for the system

H(z) = B(z)
(b) Determine a lattice realization for the system

H(z) = 1
A(z)

(c) Determine a lattice-ladder realization for the system H(z) = B(z)/A(z).
(d) Write a program for the implementation of the lattice-ladder structure in part

(c).

(e) Determine and sketch the first 100 samples of the impulse responses of the
systems in parts (a) through (c) by working with the lattice structures.

(f) Compute and sketch the first 100 samples of the convolution of impulse responses
in parts (a) and (b). What did you find? Explain your results.

42
and write a program for its implementation.

3 H(z) = 8−z−1
1−0.5z−1

h(n) = 8(0.5)nu(n)− (0.5)n−1u(n− 1)
6 c0 = 1; c1 = −(b1 + b2); d1 = b1 ; c2 = b2
8 (a) h(n) = 12

[(
1
2

)n + (− 12 )]n u(n)
11 H(z) = C (1− 53150 z−1 + 0.52z−2 − 0.74z−3 + 13 z−4), where C is a constant
15 k2 = 13 ; k1 = 32
17 h(n) = δ(n)+ 0.157δ(n− 1)+ 0.0032δ(n− 2)+ 0.8δ(n− 3)
21 H(z) = 1+ 1.38z−1 + 1.311z−2 + 1.337z−3 + 0.9z−4

30 (a) |H(ejw)|2 = a2−2a cosw+1
1−2a cosw+a2 = 1∀w

Implementation of Discrete-Time Systems

Consider the system given in Problem 40. Determine the parallel-form structure

Consider the system given in Problem 40 with G

Answers to Selected Problems

667



32 y(n) = 0.999y(n− 1)+ e(n)
where e(n) is white noise, uniformly distributed in the interval

[
− 1

29
, 1

29

]
. Then E{y2(n)} =

0.9992E{y2(n− 1)} + E{e2(n)} = 6.361x10−4

35 (a) y(n) = Q[0.1δ(n)] + Q[0.5y(n − 1)]; y(0) = Q[0.1] = 18 ; y(1) = Q
[

1
16

] = 0 and y(2) =
y(3) = y(4) = 0; no limit cycle

37 Define ρc = r cos θ , ρs = r sin θ for convenience, (a)

−ρsy(n− 1)+ e1(n)+ x(n)+ ρcυ(n− 1)+ e2(n) = υ(n)

ρsυ(n− 1)+ e3(n)+ ρcy(n− 1) + e4(n) = y(n)

38 (a) h(n) =
[
2
(

1
2

)n − ( 14 )n] u(n)
σ 2q = 6435σ 2e1 + 1615σ 2e2

40 (a) G1

(
1−0.8ej

π
4

)(
1−0.8e−j

π
4

)
(1−0.5)

(
1+ 13

) = 1; G1 = 1.1381

G2
(1+0.25)

(
1− 58

)
(

1−0.8ej
π
3

)(
1−0.8e−j

π
3

) = 1; G2 = 1.7920

41 (a) k1 = − 49 ; k2 = 532
(b) k2 = − 16 ; k1 = − 15

Implementation of Discrete-Time Systems

668



John G. Proakis, Dimitris G. Manolakis. Copyright © 2007 by Pearson Education, Inc. All rights reserved.
From Chapter 10 of Digital Signal Processing: Principles, Algorithms, and Applications, Fourth Edition.

Design of Digital Filters

669



Design of Digital Filters

In the design of frequency-selective filters, the desired filter characteristics are
specified in the frequency domain in terms of the desired magnitude and phase re-
sponse of the filter. In the filter design process, we determine the coefficients of a
causal FIR or IIR filter that closely approximates the desired frequency response
specifications. The issue of which type of filter to design, FIR or IIR, depends on the
nature of the problem and on the specifications of the desired frequency response.

In practice, FIR filters are employed in filtering problems where there is a re-
quirement for a linear-phase characteristic within the passband of the filter. If there
is no requirement for a linear-phase characteristic, either an IIR or an FIR filter may
be employed. However, as a general rule, an IIR filter has lower sidelobes in the
stopband than an FIR filter having the same number of parameters. For this reason,
if some phase distortion is either tolerable or unimportant, an IIR filter is prefer-
able, primarily because its implementation involves fewer parameters, requires less
memory and has lower computational complexity.

In conjunction with our discussion of digital filter design, we describe frequency
transformations in both the analog and digital domains for transforming a lowpass
prototype filter into either another lowpass, bandpass, bandstop, or highpass filter.

Today, FIR and IIR digital filter design is greatly facilitated by the availability of
numerous computer software programs. In describing the various digital filter design
methods in this chapter, our primary objective is to give the reader the background
necessary to select the filter that best matches the application and satisfies the design
requirements.

In this chapter, we shall describe several methods for designing FIR (finite-duration 
impulse response) and IIR (infinite-duration impulse response) digital filters.

670



1 General Considerations

1.1 Causality and Its Implications

Let us consider the issue of causality in more detail by examining the impulse response
h(n) of an ideal lowpass filter with frequency response characteristic

H(ω) =
{

1, |ω| ≤ ωc
0, ωc < ω ≤ π

The impulse response of this filter is

h(n) =



ωc

π
, n = 0

ωc

π

sinωcn
ωcn

, n �= 0

A plot of h(n) for ωc It is clear that the ideal
lowpass filter is noncausal and hence it cannot be realized in practice.

One possible solution is to introduce a large delay n0 in h(n) and arbitrarily
to set h(n) = 0 for n < n0 . However, the resulting system no longer has an ideal
frequency response characteristic. Indeed, if we set h(n) = 0 for n < n0 , the Fourier
series expansion of H(ω) results in the Gibbs phenomenon, as will be described in

Although this discussion is limited to the realization of a lowpass filter, our
conclusions hold, in general, for all the other ideal filter characteristics. In brief,

h(n)

0.25

20 −20
n

Figure 1.1 Unit sample response of an ideal lowpass filter.

(1.1)

(1.2)

Ideal filters are not causal and therefore are not physically realizable. In this sec-
tion, the issue of causality and its implications is considered in detail. Following this 
discussion, we present the frequency response characteristics of causal FIR and IIR 
digital filters.

Design of Digital Filters

= π/4 is illustrated in Fig. 1.1.

Section 2.

671



A question that naturally arises at this point is the following: What are the
necessary and sufficient conditions that a frequency response characteristic H(ω)
must satisfy in order for the resulting filter to be causal? The answer to this question
is given by the Paley–Wiener theorem, which can be stated as follows:

Paley–Wiener Theorem. If h(n) has finite energy and h(n) = 0 for n < 0, then [for
a reference, see Wiener and Paley (1934)]

∫ π
−π
|ln|H(ω)||dω <∞

we can associate with |H(ω)| a phase response �(ω), so that the resulting filter with
frequency response

H(ω) = |H(ω)|ej�(ω)

is causal.
One important conclusion that we draw from the Paley–Wiener theorem is that

the magnitude function |H(ω)| can be zero at some frequencies, but it cannot be
zero over any finite band of frequencies, since the integral then becomes infinite.
Consequently, any ideal filter is noncausal.

Apparently, causality imposes some tight constraints on a linear time-invariant
system. In addition to the Paley–Wiener condition, causality also implies a strong
relationship between HR(ω) and HI(ω), the real and imaginary components of the
frequency response H(ω). To illustrate this dependence, we decompose h(n) into
an even and an odd sequence, that is,

h(n) = he(n)+ ho(n)

where

he(n) = 12 [h(n)+ h(−n)]

and

ho(n) = 12[h(n)− h(−n)]

Now, if h(n) is causal, it is possible to recover h(n) from its even part he(n) for
0 ≤ n ≤ ∞ or from its odd component ho(n) for 1 ≤ n ≤ ∞.

Indeed, it can be easily seen that

h(n) = 2he(n)u(n)− he(0)δ(n), n ≥ 0

and
h(n) = 2ho(n)u(n)+ h(0)δ(n), n ≥ 1

(1.3)

(1.4)

(1.5)

(1.6)

(1.7)

(1.8)

none of the ideal filter characteristics are causal, hence all are physically unrealiz-
able.

Design of Digital Filters

Conversely, if |H(ω)| is square integrable and if the integral in (1.3) is finite, then

672



Since ho(n) = 0 for n = 0, we cannot recover h(0) from ho(n) and hence we also
must know h(0). In any case, it is apparent that ho(n) = he(n) for n ≥ 1, so there is
a strong relationship between ho(n) and he(n).

If h(n) is absolutely summable (i.e., BIBO stable), the frequency response H(ω)
exists, and

H(ω) = HR(ω)+ jHI (ω)
In addition, if h(n) is real valued and causal, the symmetry properties of the Fourier
transform imply that

he(n)
F←→ HR(ω)

ho(n)
F←→ HI (ω)

Since h(n) is completely specified by he(n), it follows that H(ω) is completely
determined if we know HR(ω). Alternatively, H(ω) is completely determined from
HI(ω) and h(0). In short, HR(ω) and HI(ω) are interdependent and cannot be
specified independently if the system is causal. Equivalently, the magnitude and
phase responses of a causal filter are interdependent and hence cannot be specified
independently.

Given HR(ω) for a corresponding real, even, and absolutely summable sequence
he(n), we can determine H(ω). The following example illustrates the procedure.

Consider a stable LTI system with real and even impulse response h(n). Determine H(ω) if

HR(ω) = 1− a cosω1− 2a cosω + a2 , |a| < 1

Solution. The first step is to determine he(n). This can be done by noting that

HR(ω) = HR(z)|z=ejω
where

HR(z) = 1− a(z + z
−1)/2

1− a(z+ z−1)+ a2 =
z− a(z2 + 1)/2
(z − a)(1− az)

The ROC has to be restricted by the poles at p1 = a and p2 = 1/a and should include the unit
circle. Hence the ROC is |a| < |z| < 1/|a|. Consequently, he(n) is a two-sided sequence, with
the pole at z = a contributing to the causal part and p2 = 1/a contributing to the anticausal
part. By using a partial-fraction expansion, we obtain

he(n) = 12a|n| + 12 δ(n)

h(n) = anu(n)
Finally, we obtain the Fourier transform of h(n) as

H(ω) = 1
1− ae−jω

(1.9)

(1.10)

(1.11)

Design of Digital Filters

EXAMPLE 1.1

By substituting (1.11) into (1.7), we obtain h(n) as

673



The relationship between the real and imaginary components of the Fourier
transform of an absolutely summable, causal, and real sequence can be easily estab-

H(ω) = HR(ω)+ jHI (ω) = 1
π

∫ π
−π
HR(λ)U(ω − λ)dλ− he(0)

where U(ω) is the Fourier transform of the unit step sequence u(n). Although the

U(ω) = πδ(ω)+ 1
1− e−jω

= πδ(ω)+ 1
2
− j 1

2
cot

ω

2
, −π ≤ ω ≤ π

the relation between HR(ω) and HI(ω) as

HI(ω) = − 12π
∫ π
−π
HR(λ) cot

ω − λ
2

dλ

Thus HI (ω) is uniquely determined from HR(ω) through this integral relationship.
The integral is called a discrete Hilbert transform. It is left as an exercise to the reader
to establish the relationship for HR(ω) in terms of the discrete Hilbert transform of
HI(ω).

To summarize, causality has very important implications in the design of frequency-
selective filters. These are: (a) the frequency response H(ω) cannot be zero, except
at a finite set of points in frequency; (b) the magnitude |H(ω)| cannot be constant in
any finite range of frequencies and the transition from passband to stopband cannot
be infinitely sharp [this is a consequence of the Gibbs phenomenon, which results
from the truncation of h(n) to achieve causality]; and (c) the real and imaginary parts
of H(ω) are interdependent and are related by the discrete Hilbert transform. As
a consequence, the magnitude |H(ω)| and phase �(ω) of H(ω) cannot be chosen
arbitrarily.

Now that we know the restrictions that causality imposes on the frequency re-
sponse characteristic and the fact that ideal filters are not achievable in practice,
we limit our attention to the class of linear time-invariant systems specified by the
difference equation

y(n) = −
N∑
k=1

aky(n− k)+
M−1∑
k=0

bkx(n− k)

(1.12)

(1.13)

(1.14)

unit step sequence is not absolutely summable, it has a Fourier transform.

Design of Digital Filters

lished from (1.7). The Fourier transform relationship for (1.7) is

By substituting (1.13) into (1.12) and carrying out the integration, we obtain

674



which are causal and physically realizable. As we have demonstrated, such systems
have a frequency response

H(ω) =

M−1∑
k=0

bke
−jωk

1+
N∑
k=1

ake
−jωk

The basic digital filter design problem is to approximate any of the ideal frequency

by properly selecting the coefficients {ak} and {bk}. The approximation problem is

filter design.

1.2 Characteristics of Practical Frequency-Selective Filters

As we observed from our discussion of the preceding section, ideal filters are non-
causal and hence physically unrealizable for real-time signal processing applications.
Causality implies that the frequency response characteristic H(ω) of the filter cannot
be zero, except at a finite set of points in the frequency range. In addition, H(ω) can-
not have an infinitely sharp cutoff from passband to stopband, that is, H(ω) cannot
drop from unity to zero abruptly.

Although the frequency response characteristics possessed by ideal filters may
be desirable, they are not absolutely necessary in most practical applications. If we
relax these conditions, it is possible to realize causal filters that approximate the
ideal filters as closely as we desire. In particular, it is not necessary to insist that the
magnitude |H(ω)| be constant in the entire passband of the filter. A small amount

Passband ripple

Transition
band

Stopband

|H(ω)|

1 + δ1
1 − δ1

δ2

0 ωp ωs π
ω

δ1  ~ Passband ripple
δ2  ~ Stopband ripple
ωp ~ Passband edge frequency
ωs  ~ Stopband edge frequency

Figure 1.2 Magnitude characteristics of physically realizable filters.

(1.15)

Design of Digital Filters

response characteristics with a system that has the frequency response (1.15),

of ripple in the passband, as illustrated in Fig. 1.2, is usually tolerable. Similarly,

treated in detail in Sections 2 and 3, where we discuss techniques for digital

675



it is not necessary for the filter response |H(ω) to be zero in the stopband. A small,
nonzero value or a small amount of ripple in the stopband is also tolerable.

The transition of the frequency response from passband to stopband defines the

edge frequency ωp defines the edge of the passband, while the frequency ωs denotes
the beginning of the stopband. Thus the width of the transition band is ωs −ωp . The
width of the passband is usually called the bandwidth of the filter. For example, if
the filter is lowpass with a passband edge frequency ωp , its bandwidth is ωp .

If there is ripple in the passband of the filter, its value is denoted as δ1 , and the
magnitude |H(ω)| varies between the limits 1± δ1 . The ripple in the stopband of the
filter is denoted as δ2 .

To accommodate a large dynamic range in the graph of the frequency response
of any filter, it is common practice to use a logarithmic scale for the magnitude
|H(ω)|. Consequently, the ripple in the passband is 20 log10 δ1 decibels, and that in
the stopband is 20 log10 δ2 .

In any filter design problem we can specify (1) the maximum tolerable passband
ripple, (2) the maximum tolerable stopband ripple, (3) the passband edge frequency
ωp , and (4) the stopband edge frequency ωs . Based on these specifications, we can
select the parameters {ak} and {bk} in the frequency response characteristic, given

H(ω) approximates the specifications depends in part on the criterion used in the
selection of the filter coefficients {ak} and {bk} as well as on the numbers (M,N) of
coefficients.

In the following section we present a method for designing linear-phase FIR
filters.

2 Design of FIR Filters

In this section we describe several methods for designing FIR filters. Our treatment
is focused on the important class of linear-phase FIR filters.

2.1 Symmetric and Antisymmetric FIR Filters

An FIR filter of length M with input x(n) and output y(n) is described by the differ-
ence equation

y(n) = b0x(n)+ b1x(n− 1)+ · · · + bM−1x(n−M + 1)

=
M−1∑
k=0

bkx(n− k)

where {bk} is the set of filter coefficients. Alternatively, we can express the output
sequence as the convolution of the unit sample response h(n) of the system with the
input signal. Thus we have

y(n) =
M−1∑
k=0

h(k)x(n− k)

(2.1)

(2.2)

Design of Digital Filters

transition band or transition region of the filter, as illustrated in Fig. 1.2. The band-

by (1.15), which best approximate the desired specification. The degree to which

676



where the lower and upper limits on the convolution sum reflect the causality and

in form and hence it follows that bk = h(k), k = 0, 1, . . . ,M − 1.
The filter can also be characterized by its system function

H(z) =
M−1∑
k=0

h(k)z−k

which we view as a polynomial of degree M− 1 in the variable z−1 . The roots of this
polynomial constitute the zeros of the filter.

An FIR filter has linear phase if its unit sample response satisfies the condition

h(n) = ±h(M − 1− n), n = 0, 1, . . . ,M − 1

−1 −2 + · · · + h(M − 2)z−(M−2) + h(M − 1)z−(M−1)

= z−(M−1)/2

h

(
M − 1

2

)
+
(M−3)/2∑
n=0

h(n)
[
z(M−1−2k)/2 ± z−(M−1−2k)/2

]
 , M odd

= z−(M−1)/2
(M/2)−1∑
n=0

h(n)[z(M−1−2k)/2 ± z−(M−1−2k)/2], M even

Now, if we substitute z−1
equation by z−(M−1) , we obtain

z−(M−1)H(z−1) = ±H(z)

This result implies that the roots of the polynomial H(z) are identical to the roots
of the polynomial H(z−1). Consequently, the roots of H(z) must occur in reciprocal
pairs. In other words, if z1 is a root or a zero of H(z), then 1/z1 is also a root.
Furthermore, if the unit sample response h(n) of the filter is real, complex-valued
roots must occur in complex-conjugate pairs. Hence, if z1 is a complex-valued root, z∗1∗

1
illustrates the symmetry that exists in the location of the zeros of a linear-phase
FIR filter.

(2.3)

(2.4)

(2.5)

(2.6)

Design of Digital Filters

finite-duration characteristics of the filter. Clearly, (2.1) and (2.2) are identical

When the symmetry and antisymmetry conditions in (2.4) are incorporated into

+ h(2)zH(z) = h(0)+ h(1)z

(2.3), we have

for z in (2.3) and multiply both sides of the resulting

is also a root. As a consequence of (2.6), H(z) also has a zero at 1/z . Figure 2.1

677



Figure 2.1
Symmetry of zero locations
for a linear-phase FIR filter.

z3

z2

z1

z*

z*
1

z3
1

z1
1

z2
1

z*
1

Unit
circle

3

1

z*3

1

The frequency response characteristics of linear-phase FIR filters are obtained

H(ω).
When h(n) = h(M − 1− n), H(ω) can be expressed as

H(ω) = Hr(ω)e−jω(M−1)/2

where Hr(ω) is a real function of ω and can be expressed as

Hr(ω) = h
(
M − 1

2

)
+ 2

(M−3)/2∑
n=0

h(n) cosω
(
M − 1

2
− n

)
, M odd

Hr(ω) = 2
(M/2)−1∑
n=0

h(n) cosω
(
M − 1

2
− n

)
, M even

The phase characteristic of the filter for both M odd and M even is

�(ω) =



−ω

(
M − 1

2

)
, if Hr(ω) > 0

−ω
(
M − 1

2

)
+ π, if Hr(ω) < 0

When
h(n) = −h(M − 1− n)

the unit sample response is antisymmetric. For M odd, the center point of the anti-
symmetric h(n) is n = (M − 1)/2. Consequently,

h

(
M − 1

2

)
= 0

However, if M is even, each term in h(n) has a matching term of opposite sign.

(2.7)

(2.8)

(2.9)

(2.10)

Design of Digital Filters

by evaluating (2.5) on the unit circle. This substitution yields the expression for

678



It is straightforward to show that the frequency response of an FIR filter with an
antisymmetric unit sample response can be expressed as

H(ω) = Hr(ω)ej [−ω(M−1)/2+π/2]

where

Hr(ω) = 2
(M−3)/2∑
n=0

h(n) sinω
(
M − 1

2
− n

)
, M odd

Hr(ω) = 2
(M/2)−1∑
n=0

h(n) sinω
(
M − 1

2
− n

)
, M even

The phase characteristic of the filter for both M odd and M even is

�(ω) =



π

2
− ω

(
M − 1

2

)
, if Hr(ω) > 0

3π
2
− ω

(
M − 1

2

)
, if Hr(ω) < 0

These general frequency response formulas can be used to design linear-phase
FIR filters with symmetric and antisymmetric unit sample responses. We note that,
for a symmetric h(n), the number of filter coefficients that specify the frequency
response is (M + 1)/2 when M is odd or M/2 when M is even. On the other hand,
if the unit sample response is antisymmetric,

h

(
M − 1

2

)
= 0

so that there are (M − 1)/2 filter coefficients when M is odd and M/2 coefficients
when M is even to be specified.

The choice of a symmetric or antisymmetric unit sample response depends on
the application. As we shall see later, a symmetric unit sample response is suitable
for some applications, while an antisymmetric unit sample response is more suitable

implies that Hr(0) = 0 and Hr(π) = 0.
either a lowpass filter or a highpass filter. Similarly, the antisymmetric unit sample
response with M even also results in Hr(0) =

of a lowpass linear-phase FIR filter. On the other hand, the symmetry condition
h(n) = h(M − 1 − n) yields a linear-phase FIR filter with a nonzero response at
ω = 0, if desired, that is,

Hr(0) = h
(
M − 1

2

)
+ 2

(M−3)/2∑
n=0

h(n), M odd

Hr(0) = 2
(M/2)−1∑
n=0

h(n), M even

(2.11)

(2.12)

(2.13)

(2.14)

(2.15)

(2.16)

Design of Digital Filters

for other applications. For example, if h(n) = −h(M−1−n) and M is odd, (2.12)

0, as can be easily verified from

Consequently, (2.12) is not suitable as

(2.13). Consequently, we would not use the antisymmetric condition in the design

679



In summary, the problem of FIR filter design is simply to determine the M
coefficients h(n), n = 0, 1, . . . ,M − 1, from a specification of the desired frequency
response Hd(ω) of the FIR filter. The important parameters in the specification of
d

In the following subsections we describe design methods based on specification
of Hd(ω).

Design of Linear-Phase FIR Filters Using Windows

In this method we begin with the desired frequency response specificationHd(ω) and
determine the corresponding unit sample response hd(n). Indeed, hd(n) is related
to Hd(ω) by the Fourier transform relation

Hd(ω) =
∞∑
n=0

hd(n)e
−jωn

where

hd(n) = 12π
∫ π
−π
Hd(ω)e

jωndω

Thus, given Hd(ω), we can determine the unit sample response hd(n) by evaluating

d

duration and must be truncated at some point, say at n = M − 1, to yield an FIR
filter of length M . Truncation of hd(n) to a length M−1 is equivalent to multiplying
hd(n) by a “rectangular window,” defined as

w(n) =
{

1, n = 0, 1, . . . ,M − 1
0, otherwise

Thus the unit sample response of the FIR filter becomes

h(n) = hd(n)w(n)

=
{
hd(n), n = 0, 1, . . . ,M − 1
0, otherwise

It is instructive to consider the effect of the window function on the desired
frequency response Hd(ω). Recall that multiplication of the window function w(n)
with hd(n) is equivalent to convolution of Hd(ω) with W(ω), where W(ω) is the
frequency-domain representation (Fourier transform) of the window function, that is,

W(ω) =
M−1∑
n=0

w(n)e−jωn

(2.17)

(2.18)

(2.19)

(2.20)

(2.21)

Design of Digital Filters

H (ω) are given in Fig. 1.2.

2.2

the integral in (2.17).
In general, the unit sample response h (n) obtained from (2.17) is infinite in

680



d

(truncated) FIR filter. That is,

H(ω) = 1
2π

∫ π
−π
Hd(ν)W(ω − ν)dν

The Fourier transform of the rectangular window is

W(ω) =
M−1∑
n=0

e−jωn

= 1− e
−jωM

1− e−jω = e
−jω(M−1)/2 sin(ωM/2)

sin(ω/2)

This window function has a magnitude response

|W(ω)| = | sin(ωM/2)|| sin(ω/2)| , π ≤ ω ≤ π

and a piecewise linear phase

�(ω) =



−ω

(
M − 1

2

)
, when sin(ωM/2) ≥ 0

−ω
(
M − 1

2

)
+ π, when sin(ωM/2) < 0

and 61. The width of the main lobe [width is measured to the first zero of W(ω)]
is 4π/M . Hence, as M increases, the main lobe becomes narrower. However, the
sidelobes of |W(ω)| are relatively high and remain unaffected by an increase in M .
In fact, even though the width of each sidelobe decreases with an increase in M , the
height of each sidelobe increases with an increase inM in such a manner that the area
under each sidelobe remains invariant to changes in M . This characteristic behavior

by M such that the normalized peak values of the sidelobes remain invariant to an
increase in M .

Figure 2.2
Frequency response for
rectangular window of
lengths (a) M = 31,
(b) M = 61.

Thus the convolution of H (ω) with W(ω) yields the frequency response of the

(2.22)

(2.23)

(2.24)

(2.25)

Design of Digital Filters

The magnitude response of the window function is illustrated in Fig. 2.2 forM = 31

is not evident from observation of Fig. 2.2 because W(ω) has been normalized

681



The characteristics of the rectangular window play a significant role in determin-
ing the resulting frequency response of the FIR filter obtained by truncating hd(n) to
lengthM . Specifically, the convolution of Hd(ω) with W(ω) has the effect of smooth-
ingHd(ω). AsM is increased, W(ω) becomes narrower, and the smoothing provided
by W(ω) is reduced. On the other hand, the large sidelobes of W(ω) result in some
undesirable ringing effects in the FIR filter frequency response H(ω), and also in
relatively larger sidelobes in H(ω). These undesirable effects are best alleviated by
the use of windows that do not contain abrupt discontinuities in their time-domain
characteristics, and have correspondingly low sidelobes in their frequency-domain
characteristics.

sponse characteristics.

TABLE 1 Window Functions for FIR Filter Design

Name of Time-domain sequence,

window h(n), 0 ≤ n ≤ M − 1

Bartlett (triangular) 1−
2
∣∣∣∣n− M − 12

∣∣∣∣
M − 1

Blackman 0.42 − 0.5 cos 2πn
M − 1 + 0.08 cos

4πn
M − 1

Hamming 0.54− 0.46 cos 2πn
M − 1

Hanning
1
2

(
1− cos 2πn

M − 1
)

Kaiser

I0


α

√(
M − 1

2

)2
−
(
n− M − 1

2

)2
I0

[
α

(
M − 1

2

)]

Lanczos




sin
[

2π
(
n− M − 1

2

)/
(M − 1)

]

2π
(
n− M − 1

2

)/(
M − 1

2

)



L

, L > 0

1,
∣∣∣∣n− M − 12

∣∣∣∣ ≤ αM − 12 , 0 < α < 1
Tukey

1
2

[
1+ cos

(
n− (1+ a)(M − 1)/2
(1− α)(M − 1)/2 π

)]

α(M − 1)/2 ≤
∣∣∣∣n− M − 12

∣∣∣∣ ≤ M − 12

Design of Digital Filters

Table 1 lists several window functions that possess desirable frequency re-

the windows. The frequency response characteristics of the Hanning, Hamming, and
Figure 2.3 illustrates the time-domain characteristics of

Blackman windows are illustrated in Figs. 2.4 through 2.6. All of these window

682



1

0.8

0.6

0.4

0.2

0
0

1

0.8

0.6

0.4

0.2

0
0

Blackman Lanczos

Kaiser

Bartlett

Tukey

Hanning
Hamming

Rectangular

M − 1 M − 1

M
ag

ni
tu

de

M
ag

ni
tu

de

Figure 2.3 Shapes of several window functions.

Figure 2.4
Frequency responses
of Hanning window
for (a) M = 31 and
(b) M = 61.

Figure 2.5
Frequency responses
for Hamming window
for (a) M = 31 and
(b) M = 61.

Figure 2.6
Frequency responses
for Blackman window
for (a) M = 31 and
(b) M = 61.

Design of Digital Filters

683



functions have significantly lower sidelobes compared with the rectangular window.
However, for the same value of M, the width of the main lobe is also wider for these
windows compared to the rectangular window. Consequently, these window func-
tions provide more smoothing through the convolution operation in the frequency
domain, and as a result, the transition region in the FIR filter response is wider.
To reduce the width of this transition region, we can simply increase the length of

frequency-domain features of the various window functions.
The window technique is best described in terms of a specific example. Suppose

that we want to design a symmetric lowpass linear-phase FIR filter having a desired
frequency response

Hd(ω) =
{

1e−jω(M−1)/2, 0 ≤ |ω| ≤ ωc
0, otherwise

A delay of (M − 1)/2 units is incorporated into Hd(ω) in anticipation of forcing
the filter to be of length M . The corresponding unit sample response, obtained by

hd(n) = 12π
∫ ωc
−ωc

e
jω

(
n−M−12

)
dω

=
sinωc

(
n− M − 1

2

)

π

(
n− M − 1

2

) , n �= M − 1
2

Clearly, hd(n) is noncausal and infinite in duration.

d

TABLE 2 Important Frequency-Domain Characteristics
of Some Window Functions

Approximate
transition width of Peak sidelobe

Type of window main lobe (dB)

Rectangular 4π/M −13
Bartlett 8π/M −25
Hanning 8π/M −31
Hamming 8π/M −41
Blackman 12π/M −57

(2.26)

(2.27)

Design of Digital Filters

the window, which results in a larger filter. Table 2 summarizes these important

evaluating the integral in (2.18), is

If we multiply h (n) by the rectangular window sequence in (2.19), we obtain

684



an FIR filter of length M having the unit sample response

h(n) =
sinωc

(
n− M − 1

2

)

π

(
n− M − 1

2

) , 0 ≤ n ≤M − 1, n �= M − 1
2

If M is selected to be odd, the value of h(n) at n = (M − 1)/2 is

h

(
M − 1

2

)
= ωc
π

The magnitude of the frequency response H(ω) of this filter is illustrated in

ripples occur near the band edge of the filter. The oscillations increase in frequency
as M increases, but they do not diminish in amplitude. As indicated previously, these
large oscillations are the direct result of the large sidelobes existing in the frequency
characteristicW(ω) of the rectangular window. As this window function is convolved
with the desired frequency response characteristic Hd(ω), the oscillations occur as
the large constant-area sidelobes ofW(ω) move across the discontinuity that exists in
d d

tiplication of hd(n) with a rectangular window is identical to truncating the Fourier
series representation of the desired filter characteristic Hd(ω). The truncation of the
Fourier series is known to introduce ripples in the frequency response characteristic
H(ω) due to the nonuniform convergence of the Fourier series at a discontinuity. The
oscillatory behavior near the band edge of the filter is called the Gibbs phenomenon.

To alleviate the presence of large oscillations in both the passband and the stop-
band, we should use a window function that contains a taper and decays toward zero
gradually, instead of abruptly, as it occurs in a rectangular window.

d

effects at the band edge and do result in lower sidelobes at the expense of an increase
in the width of the transition band of the filter.

Figure 2.7 Lowpass filter designed with a rectangular window: (a) M = 61 and (b) M = 101.

(2.28)

(2.29)

Design of Digital Filters

Fig. 2.7 for M = 61 and M = 101. We observe that relatively large oscillations or

H (ω). Since (2.17) is basically a Fourier series representation of H (ω), the mul-

Figures 2.8
through 2.11 illustrate the frequency response of the resulting filter when some
of the window functions listed in Table 1 are used to taper h(n ) . As illustrated
in Figs. 2.8 through 2.11, the window functions do indeed eliminate the ringing

685



Figure 2.8
Lowpass FIR filter designed
with rectangular window
(M = 61).

Figure 2.9
Lowpass FIR filter designed
with Hamming window
(M = 61).

Figure 2.10
Lowpass FIR filter designed
with Blackman window
(M = 61).

Figure 2.11
Lowpass FIR filter designed
with α = 4 Kaiser window
(M = 61).

Design of Digital Filters

686



2.3 Design of Linear-Phase FIR Filters by the
Frequency-Sampling Method

In the frequency-sampling method for FIR filter design, we specify the desired fre-
quency response Hd(ω) at a set of equally spaced frequencies, namely

ωk = 2π
M
(k + α), k = 0, 1, . . . , M − 1

2
, M odd

k = 0, 1, . . . , M
2
− 1, M even

α = 0 or 12
and solve for the unit sample response h(n) of the FIR filter from these equally
spaced frequency specifications. To reduce sidelobes, it is desirable to optimize the
frequency specification in the transition band of the filter. This optimization can be
accomplished numerically on a digital computer by means of linear programming
techniques as shown by Rabiner et al. (1970).

In this section we exploit a basic symmetry property of the sampled frequency
response function to simplify the computations. Let us begin with the desired fre-
quency response of the FIR filter, which is [for simplicity, we drop the subscript in
Hd(ω)],

H(ω) =
M−1∑
n=0

h(n)e−jωn

Suppose that we specify the frequency response of the filter at the frequencies given

H(k + α) ≡ H
(

2π
M
(k + α)

)

H(k + α) ≡
M−1∑
n=0

h(n)e−j2π(k+α)n/M, k = 0, 1, . . . ,M − 1

reduces to Mh(m) exp(−j2παm/M). Thus we obtain

h(n) = 1
M

M−1∑
k=0

H(k + α)ej2π(k+α)n/M, n = 0, 1, . . . ,M − 1

response h(n) from the specification of the frequency samples H(k + α), k = 0,
1, . . . ,M − 1. Note that when α =

(2.30)

(2.31)

(2.32)

(2.33)

Design of Digital Filters

by (2.30). Then from (2.31) we obtain

It is a simple matter to invert (2.32) and express h(n) in terms of H(k + α).
If we multiply both sides of (2.32) by the exponential, exp(j2πkm/M), m = 0,
1, . . . ,M − 1, and sum over k = 0, 1, . . . ,M − 1, the right-hand side of (2.32)

The relationship in (2.33) allows us to compute the values of the unit sample

0, (2.32) reduces to the discrete Fourier

687



(IDFT).
Since {h(n)} is real, we can easily show that the frequency samples {H(k + α)}

satisfy the symmetry condition

H(k + α) = H ∗(M − k − α)
This symmetry condition, along with the symmetry conditions for {h(n)}, can be used
to reduce the frequency specifications from M points to (M+ 1)/2 points for M odd
and M/2 points for M even. Thus the linear equations for determining {h(n)} from
{H(k + α)} are considerably simplified.

k = 2π(k+α)/M , k = 0,
1, . . . ,M − 1, we obtain

H(k + α) = Hr
(

2π
M
(k + α)

)
ej[βπ/2−2π(k+α)(M−1)/2M]

where β = 0 when {h(n)} is symmetric and β = 1 when {h(n)} is antisymmetric. A
simplification occurs by defining a set of real frequency samples {G(k + x)}

G(k + α) = (−1)kHr
(

2π
M
(k + α)

)
, k = 0, 1, . . . ,M − 1

r k

H(k + α) = G(k + α)ejπkej [βπ/2−2π(k+α)(M−1)/2M]

sponding symmetry condition for G(k + α), which can be exploited by substituting

for the four cases α = 0, α = 12 , β = 0, and β = 1. The results are summarized in

Although the frequency sampling method provides us with another means for
designing linear-phase FIR filters, its major advantage lies in the efficient frequency-

The following examples illustrate the design of linear-phase FIR filters based
on the frequency-sampling method. The optimum values for the samples in the

the paper by Rabiner et al. (1970).

EXAMPLE 2.1

Determine the coefficients of a linear-phase FIR filter of lengthM = 15 which has a symmetric
unit sample response and a frequency response that satisfies the conditions

Hr

(
2πk
15

)
=



1, k = 0, 1, 2, 3
0.4, k = 4
0, k = 5, 6, 7

(2.34)

(2.35)

(2.36)

(2.37)

sampling structure, which is obtained when most of the frequency samples are zero.

Design of Digital Filters

transform (DFT) of the sequence {h(n)} and (2.33) reduces to the inverse DFT

In particular, if (2.11) is sampled at the frequencies ω

We use (2.36) in (2.35) to eliminate H (ω ). Thus we obtain

Now the symmetry condition for H(k + α) given in (2.34) translates into a corre-

into (2.33), to simplify the expressions for the FIR filter impulse response {h(n)}

Table 3. The detailed derivations are left as exercises for the reader.

transition band are obtained from tables in

688



TABLE 3 Unit Sample Response: h(n) = ±h(M − 1− n)
Symmetric

α = 0

H(k) = G(k)ejπk/M, k = 0, 1. . . . ,M − 1

G(k) = (−1)kHr
(

2πk
M

)
, G(k) = −G(M − k)

h(n) = 1
M

{
G(0)+ 2

U∑
k=1

G(k) cos
2πk
M

(n+ 1
2
)

}

U =
{
M−1

2 , M odd
M
2 − 1, M even

α = 12

H

(
k + 1

2

)
= G

(
k + 1

2

)
e−jπ/2ejπ(2k+1)/2M

G

(
k + 1

2

)
= (−1)kHr

[
2π
M

(
k + 1

2

)]

G

(
k + 1

2

)
= G

(
M − k − 1

2

)

h(n) = 2
M

∑U
k=0 G

(
k + 12

)
sin 2π

M

(
k + 12

) (
n+ 12

)
Antisymmetric

α = 0

H(k) = G(k)ejπ/2ejπk/M, k = 0, 1, . . . ,M − 1

G(k) = (−1)kHr
(

2πk
M

)
, G(k) = G(M − k)

h(n) = − 2
M

(M−1)/2∑
k=1

G(k) sin
2πk
M

(
n+ 1

2

)
, M odd

h(n) = 1
M

{
(−1)n+1G(M/2)− 2∑(M/2)−1k=1 G(k) sin 2πM k (n+ 12 )} , M even

α = 12

H

(
k + 1

2

)
= G

(
k + 1

2

)
ejπ(2k+1)/2M

G

(
k + 1

2

)
= (−1)kHr

[
2π
M

(
k + 1

2

)]

G

(
k + 1

2

)
= −G

(
M − k − 1

2

)
; G(M/2) = 0 for M odd

h(n) = 2
M

∑V
k=0 G

(
k + 12

)
cos 2π

M

(
k + 12

) (
n+ 12

)
V =

{
M−3

2 , M odd
M
2 − 1, M even

Design of Digital Filters

689



Solution. Since h(n) is symmetric and the frequencies are selected to correspond to the

G(k) = (−1)kHr
(

2πk
15

)
, k = 0, 1, . . . , 7

The result of this computation is

h(0) = h(14) = −0.014112893
h(1) = h(13) = −0.001945309
h(2) = h(12) = 0.04000004
h(3) = h(11) = 0.01223454
h(4) = h(10) = −0.09138802
h(5) = h(9) = −0.01808986
h(6) = h(8) = 0.3133176
h(7) = 0.52

size that Hr(ω) is exactly equal to the values given by the specifications above at ωk = 2πk/15.

Design of Digital Filters

case α = 0, we use the corresponding formula in Table 3 to evaluate h(n). In this case

The frequency response characteristic of this filter is shown in Fig. 2.12. We should empha-

pleFigure 2.12 Frequency response of linear-phase FIR filter in Exam 2.1.

690



Determine the coefficients of a linear-phase FIR filter of length M = 32 which has a symmetric
unit sample response and a frequency response that satisfies the condition

Hr

(
2π(k + α)

32

)
=



1, k = 0, 1, 2, 3, 4, 5
T1, k = 6
0, k = 7, 8, . . . , 15

where T1 = 0.3789795 for α = 0, and T1 = 0.3570496 for α = 12 . These values of T1 were

Solution.
and α = 12 . The

respectively. Note that the bandwidth of the filter for α = 12 is wider than that for α = 0.

TABLE 4

M = 32 M = 32

ALPHA = 0. ALPHA = 0.5

T1 = 0.3789795E+00 T1 = 0.3570496E+00

h( 0) = −0.7141978E-02 h( 0) = −0.4089120E-02
h( 1) = −0.3070801E-02 h( 1) = −0.9973779E-02
h( 2) = 0.5891327E-02 h( 2) = −0.7379891E-02
h( 3) = 0.1349923E-01 h( 3) = 0.5949799E-02

h( 4) = 0.8087033E-02 h( 4) = 0.1727056E-01

h( 5) = −0.1107258E-01 h( 5) = 0.7878412E-02
h( 6) = −0.2420687E-01 h( 6) = −0.1798590E-01
h( 7) = −0.9446550E-02 h( 7) = −0.2670584E-01
h( 8) = 0.2544464E-01 h( 8) = 0.3778549E-02

h( 9) = 0.3985050E-01 h( 9) = 0.4191022E-01

h(10) = 0.2753036E-02 h(10) = 0.2839344E-01

h(11) = −0.5913959E-01 h(11) = −0.4163144E-01
h(12) = −0.6841660E-01 h(12) = −0.8254962E-01
h(13) = 0.3175741E-01 h(13) = 0.2802212E-02

h(14) = 0.2080981E+00 h(14) = 0.2013655E+00

h(15) = 0.3471138E+00 h(15) = 0.3717532E+00

obtained from tables of optimum transition parameters in the paper by Rabiner et al.
(1970).

Design of Digital Filters

EXAMPLE 2.2

The appropriate equations for this computation are given in Table 3 for α = 0
These computations yield the unit sample responses shown in Table 4.

corresponding frequency response characteristics are illustrated in Figs. 2.13 and 2.14,

691



Frequency response of linear-phase FIR filter in

The optimization of the frequency samples in the transition region of the fre-
quency response can be explained by evaluating the system function H(z), given by

terms of G(k + α). Thus for the symmetric filter we obtain

Frequency response of linear-phase FIR filter in
1
2 ).

Design of Digital Filters

Figure 2.13
Example 2.2 (M = 32 and α = 0).

(2.12), on the unit circle and using the relationship in (2.37) to express H(ω) in

Figure 2.14
Example 2.2 (M = 32 and α =

692



H(ω) =




sin
(
ωM

2
− πα

)
M

M−1∑
k=0

G(k + α)
sin

[ω
2
− π
M
(k + α)

]

 e
−jω(M−1)/2

where

G(k + α) =
{−G(M − k), α = 0
G(M − k − 12 ), α = 12

Similarly, for the antisymmetric linear-phase FIR filter we obtain

H(ω) =




sin
(
ωM

2
− πα

)
M

M−1∑
k=0

G(k + α)
sin

[ω
2
− π
M
(k + α)

]

 e
−jω(M−1)/2ejπ/2

where

G(k + α) =
{
G(M − k), α = 0
−G

(
M − k − 12

)
, α = 12

With these expressions for the frequency response H(ω) given in terms of the
desired frequency samples {G(k+α)}, we can easily explain the method for selecting
the parameters {G(k+α)} in the transition band which result in minimizing the peak
sidelobe in the stopband. In brief, the values of G(k + α) in the passband are set to
(−1)k and those in the stopband are set to zero. For any choice of G(k + α) in the
transition band, the value of H(ω) is computed at a dense set of frequencies (e.g., at
ωn = 2πn/K , n = 0, 1, . . . , K − 1, where, for example, K = 10M ). The value of
the maximum sidelobe is determined, and the values of the parameters {G(k + α)}
in the transition band are changed in a direction of steepest descent, which, in effect,
reduces the maximum sidelobe. The computation of H(ω) is now repeated with
the new choice of {G(k + α)}. The maximum sidelobe of H(ω) is again determined
and the values of the parameters {G(k + α)} in the transition band are adjusted in
a direction of steepest descent, which, in turn, reduces the sidelobe. This iterative
process is performed until it converges to the optimum choice of the parameters
{G(k + α)} in the transition band.

There is a potential problem in the frequency-sampling realization of the FIR
linear-phase filter. The frequency-sampling realization of the FIR filter introduces
poles and zeros at equally spaced points on the unit circle. In the ideal situation, the
zeros cancel the poles and, consequently, the actual zeros of H(z) are determined by
the selection of the frequency samples {H(k+ α)}. In a practical implementation of
the frequency-sampling realization, however, quantization effects preclude a perfect
cancellation of the poles and zeros. In fact, the location of poles on the unit circle
provide no damping of the round-off noise that is introduced in the computations.
As a result, such noise tends to increase with time and, ultimately, may destroy the
normal operation of the filter.

(2.38)

(2.39)

(2.40)

(2.41)

Design of Digital Filters

693



To mitigate this problem, we can move both the poles and zeros from the unit
circle to a circle just inside the unit circle, say at radius r = 1− 	 , where 	 is a very
small number. Thus the system function of the linear-phase FIR filter becomes

H(z) = 1− r
Mz−Mej2πα

M

M−1∑
k=0

H(k + α)
1− rej2ωπ(k+α)/Mz−1

The corresponding two-pole filter realization can be modified accordingly. The
damping provided by selecting r < 1 ensures that roundoff noise will be bounded
and thus instability is avoided.

Design of Optimum Equiripple Linear-Phase FIR Filters

The window method and the frequency-sampling method are relatively simple tech-
niques for designing linear-phase FIR filters. However, they also possess some minor

some applications. A major problem is the lack of precise control of the critical
frequencies such as ωp and ωs .

The filter design method described in this section is formulated as a Chebyshev
approximation problem. It is viewed as an optimum design criterion in the sense
that the weighted approximation error between the desired frequency response and
the actual frequency response is spread evenly across the passband and evenly across
the stopband of the filter minimizing the maximum error. The resulting filter designs
have ripples in both the passband and the stopband.

To describe the design procedure, let us consider the design of a lowpass filter
with passband edge frequency ωp and stopband edge frequency ωs . From the gen-

satisfies the condition

1− δ1 ≤ Hr(ω) ≤ 1+ δ1, |ω| ≤ ωp
Similarly, in the stopband, the filter frequency response is specified to fall between
the limits ±δ2 , that is,

−δ2 ≤ Hr(ω) ≤ δ2, |ω| > ωs
Thus δ1 represents the ripple in the passband and δ2 represents the attenuation or
ripple in the stopband. The remaining filter parameter is M , the filter length or the
number of filter coefficients.

Let us focus on the four different cases that result in a linear-phase FIR filter.

Case 1: Symmetric unit sample response. h(n) = h(M − 1− n) and M odd. In this
case, the real-valued frequency response characteristic Hr(ω) is

Hr(ω) = h
(
M − 1

2

)
+ 2

(M−3)/2∑
n=0

h(n) cosω
(
M − 1

2
− n

)

(2.42)

(2.43)

(2.44)

(2.45)

Design of Digital Filters

2.4

disadvantages, described in Section 2.6, which may render them undesirable for

eral specifications given in Fig. 1.2, in the passband, the filter frequency response

These cases were treated in Section 2.2 and are summarized below.

694



If we let k = (M − 1)/2− n and define a new set of filter parameters {a(k)} as

a(k) =



h

(
M − 1

2

)
, k = 0

2h
(
M − 1

2
− k

)
, k = 1, 2, . . . , M − 1

2

Hr(ω) =
(M−1)/2∑
k=0

a(k) cosωk

Case 2: Symmetric unit sample response. h(n) = h(M − 1− n) and M even. In this
case, Hr(ω) is expressed as

Hr(ω) = 2
(M/2)−1∑
n=0

h(n) cosω
(
M − 1

2
− n

)

Again, we change the summation index from n to k = M/2− n and define a new set
of filter parameters {b(k)} as

b(k) = 2h
(
M

2
− k

)
, k = 1, 2, . . . ,M/2

Hr(ω) =
M/2∑
k=1

b(k) cosω
(
k − 1

2

)

the form

Hr(ω) = cos ω2
(M/2)−1∑
k=0

b̃(k) cosωk

where the coefficients {b̃(k)} are linearly related to the coefficients {b(k)}. In fact, it
can be shown that the relationship is

b̃(0) = 12b(1)

b̃(k) = 2b(k)− b̃(k − 1), k = 2, 3, . . . ,M
2
− 2

b̃

(
M

2
− 1

)
= 2b

(
M

2

)

(2.46)

(2.47)

(2.48)

(2.49)

(2.50)

(2.51)

(2.52)

Design of Digital Filters

then (2.45) reduces to the compact form

With these substitutions (2.48) becomes

In carrying out the optimization, it is convenient to rearrange (2.50) further into

695



Case 3: Antisymmetric unit sample response. h(n) = −h(M−1−n) andM odd. The
real-valued frequency response characteristic Hr(ω) for this case is

Hr(ω) = 2
(M−3)/2∑
n=0

h(n) sinω
(
M − 1

2
− n

)

new set of filter parameters {c(k)} as

c(k) = 2h
(
M − 1

2
− k

)
, k = 1, 2, . . . , (M − 1)/2

Hr(ω) =
(M−1)/2∑
k=1

c(k) sinωk

Hr(ω) = sinω
(M−3)/2∑
k=0

c̃(k) cosωk

where the coefficients {c̃(k)} are linearly related to the parameters {c(k)}. This

c̃

(
M − 3

2

)
= c

(
M − 1

2

)

c̃

(
M − 5

2

)
= 2c

(
M − 3

2

)

...
...

c̃(k − 1)− c̃(k + 1) = 2c(k), 2 ≤ k ≤ M − 5
2

c̃(0)+ 12 c̃(2) = c(1)

Case 4: Antisymmetric unit sample response. h(n) = −h(M−1−n) and M even. In
this case, the real-valued frequency response characteristic Hr(ω) is

Hr(ω) = 2
(M/2)−1∑
n=0

h(n) sinω
(
M − 1

2
− n

)

(2.53)

(2.54)

(2.55)

(2.56)

(2.57)

(2.58)

Design of Digital Filters

If we change the summation in (2.53) from n to k = (M − 1)/2 − n and define a

As in the previous case, it is convenient to rearrange (2.55) into the form

then (2.53) becomes

desired relationship can be derived from (2.55) and (2.56) and is simply given as

696



A change in the summation index from n to k = M/2−n combined with a definition
of a new set of filter coefficients {d(k)}, related to {h(n)} according to

d(k) = 2h
(
M

2
− k

)
, k = 1, 2, . . . , M

2

results in the expression

Hr(ω) =
M/2∑
k=1

d(k) sinω
(
k − 1

2

)

form

Hr(ω) = sin ω2
(M/2)−1∑
k=0

d̃(k) cosωk

where the new filter parameters {d̃(k)} are related to {d(k)} as follows:

d̃

(
M

2
− 1

)
= 2d

(
M

2

)

d̃(k − 1)− d̃(k) = 2d(k), 2 ≤ k ≤ M
2
− 1

d̃(0)− 12 d̃(1) = d(1)

TABLE 5 Real-Valued Frequency Response Functions
for Linear-Phase FIR Filters

Filter type Q(ω) P (ω)

h(n) = h(M − 1− n)
M odd

(Case 1)
1

(M−1)/2∑
k=0

a(k) cosωk

h(n) = h(M − 1− n)
M even
(Case 2)

cos
ω

2

(M/2)−1∑
k=0

b̃(k) cosωk

h(n) = −h(M − 1− n)
M odd

(Case 3)
sinω

(M−3)/2∑
k=0

c̃(k) cosωk

h(n) = −h(M − 1− n)
M even
(Case 4)

sin
ω

2

(M/2)−1∑
k=0

d̃(k) cosωk

(2.59)

(2.60)

(2.61)

(2.62)

Design of Digital Filters

As in the previous two cases, we find it convenient to rearrange (2.60) into the

697



r

note that the rearrangements that we made in Cases 2, 3, and 4 have allowed us to
express Hr(ω) as

Hr(ω) = Q(ω)P (ω)
where

Q(ω) =




1 Case 1
cos

ω

2
Case 2

sinω Case 3

sin
ω

2
Case 4

and P(ω) has the common form

P(ω) =
L∑
k=0

α(k) cosωk

with {α(k)} representing the parameters of the filter, which are linearly related to
the unit sample response h(n) of the FIR filter. The upper limit L in the sum is
L = (M − 1)/2 for Case 1, L = (M − 3)/2 for Case 3, and L = M/2 − 1 for Case 2
and Case 4.

In addition to the common framework given above for the representation of
Hr(ω), we also define the real-valued desired frequency response Hdr(ω) and the
weighting function W(ω) on the approximation error. The real-valued desired fre-
quency response Hdr(ω) is simply defined to be unity in the passband and zero in

teristics for Hdr(ω). The weighting function on the approximation error allows us
to choose the relative size of the errors in the different frequency bands (i.e., in the
passband and in the stopband). In particular, it is convenient to normalize W(ω) to
unity in the stopband and set W(ω) = δ2/δ1 in the passband, that is,

W(ω) =
{
δ2/δ1, ω in the passband
1, ω in the stopband

Then we simply select W(ω) in the passband to reflect our emphasis on the relative
size of the ripple in the stopband to the ripple in the passband.

With the specification of Hdr(ω) and W(ω), we can now define the weighted
approximation error as

E(ω) = W(ω)[Hdr(ω)−Hr(ω)]
= W(ω)[Hdr(ω)−Q(ω)P (ω)]

= W(ω)Q(ω)
[
Hdr(ω)

Q(ω)
− P(ω)

]

(2.63)

(2.64)

(2.65)

(2.66)

(2.67)

Design of Digital Filters

The expressions for H (ω) in these four cases are summarized in Table 5. We

the stopband. For example, Fig. 2.15 illustrates several different types of charac-

698



Desired frequency response
characteristics for different
types of filters.

ω
ωp0

1

π

Hdr(ω)

(a)

ω
ωp0

1

π

Hdr(ω)

(b)

ω
ω1 ω20

1

π

Hdr(ω)

(c)

ω
ω2ω1 ω3 ω40

1

π

Hdr(ω)

(d)

For mathematical convenience, we define a modified weighting function Ŵ (ω) and
a modified desired frequency response Ĥdr(ω) as

Ŵ (ω) = W(ω)Q(ω)

Ĥdr(ω) = Hdr(ω)
Q(ω)

Then the weighted approximation error may be expressed as

E(ω) = Ŵ (ω)[Ĥdr(ω)− P(ω)]

for all four different types of linear-phase FIR filters.
Given the error function E(ω), the Chebyshev approximation problem is basi-

cally to determine the filter parameters {α(k)} that minimize the maximum absolute

(2.68)

(2.69)

Design of Digital Filters

Figure 2.15

699



value of E(ω) over the frequency bands in which the approximation is to be per-
formed. In mathematical terms, we seek the solution to the problem

min
over {α(k)}

[
max
ω	S
|E(ω)|

]
= min

over {a(k)}

[
max
ω	S
|Ŵ (ω)[Ĥdr(ω)−

L∑
k=0

α(k) cosωk]|
]

where S represents the set (disjoint union) of frequency bands over which the op-
timization is to be performed. Basically, the set S consists of the passbands and
stopbands of the desired filter.

The solution to this problem is due to Parks and McClellan (1972a), who applied
a theorem in the theory of Chebyshev approximation. It is called the alternation
theorem, which we state without proof.

Alternation Theorem. Let S be a compact subset of the interval [0, π). A necessary
and sufficient condition for

P(ω) =
L∑
k=0

α(k) cosωk

to be the unique, best weighted Chebyshev approximation to Ĥdr(ω) in S , is that the
error function E(ω) exhibit at least L + 2 extremal frequencies in S . That is, there
must exist at least L + 2 frequencies {ωi} in S such that ω1 < ω2 < · · · < ωL+2 ,
E(ωi) = −E(ωi+1), and

|E(ωi)| = max
ω	S
|E(ω)|, i = 1, 2, . . . , L+ 2

We note that the error function E(ω) alternates in sign between two successive
extremal frequencies. Hence the theorem is called the alternation theorem.

To elaborate on the alternation theorem, let us consider the design of a lowpass
filter with passband 0 ≤ ω ≤ ωp and stopband ωs ≤ ω ≤ π . Since the desired
frequency response Hdr(ω) and the weighting function W(ω) are piecewise constant,
we have

dE(ω)

dω
= d
dω
{W(ω)[Hdr(ω)−Hr(ω)]}

= −dHr(ω)
dω

= 0

Consequently, the frequencies {ωi} corresponding to the peaks of E(ω) also corre-
spond to peaks at which Hr(ω) meets the error tolerance. Since Hr(ω) is a trigono-
metric polynomial of degree L, for Case 1, for example,

(2.70)

Design of Digital Filters

700



Hr(ω) =
L∑
k=0

α(k) cosωk

=
L∑
k=0

α(k)

[
k∑
n=0

βnk(cosω)n
]

=
L∑
k=0

α′(k)(cosω)k

it follows that Hr(ω) can have at most L− 1 local maxima and minima in the open
interval 0 < ω < π . In addition, ω = 0 and ω = π are usually extrema of Hr(ω)
and, also, of E(ω). Therefore, Hr(ω) has at most L + 1 extremal frequencies.
Furthermore, the band-edge frequencies ωp and ωs are also extrema of E(ω), since
|E(ω)| is maximum at ω = ωp and ω = ωs . As a consequence, there are at most
L+ 3 extremal frequencies in E(ω) for the unique, best approximation of the ideal
lowpass filter. On the other hand, the alternation theorem states that there are at
least L + 2 extremal frequencies in E(ω). Thus the error function for the lowpass
filter design has either L+ 3 or L+ 2 extrema. In general, filter designs that contain
more than L+ 2 alternations or ripples are called extra ripple filters. When the filter
design contains the maximum number of alternations, it is called a maximal ripple
filter.

The alternation theorem guarantees a unique solution for the Chebyshev opti-
n

the set of equations

Ŵ (ωn)[Ĥdr(ωn)− P(ωn)] = (−1)nδ, n = 0, 1, . . . , L+ 1

where δ represents the maximum value of the error function E(ω). In fact, if we
2

P(ωn)+ (−1)
nδ

Ŵ (ωn)
= Ĥdr(ωn), n = 0, 1, . . . , L+ 1

or, equivalently, in the form

L∑
k=0

α(k) cosωnk + (−1)
nδ

Ŵ (ωn)
= Ĥdr(ωn), n = 0, 1, . . . , L+ 1

(2.71)

(2.72)

(2.73)

Design of Digital Filters

At the desired extremal frequencies {ω }, we havemization problem in (2.70).

select W(ω) as indicated by (2.66), it follows that δ = δ .
The set of linear equations in (2.72) can be rearranged as

701



expressed in matrix form as


1 cosω0 cos 2ω0 · · · cosLω0 1
Ŵ (ω0)

1 cosω1 cos 2ω1 · · · cosLω1 −1
Ŵ (ω1)

...

...

1 cosωL+1 cos 2ωL+1 · · · cosLωL+1 (−1)
L+1

Ŵ (ωL+1)







α(0)

α(1)

...

α(L)

δ




=




Ĥdr(ω0)

Ĥdr(ω1)

...

...

Ĥdr(ωL+1)




Initially, we know neither the set of extremal frequencies {ωn} nor the parameters
{α(k)} and δ . To solve for the parameters, we use an iterative algorithm, called the
Remez exchange algorithm [see Rabiner et al. (1975)], in which we begin by guessing
at the set of extremal frequencies, determine P(ω) and δ , and then compute the error
function E(ω). From E(ω) we determine another set of L+ 2 extremal frequencies
and repeat the process iteratively until it converges to the optimal set of extremal

procedure, matrix inversion is time consuming and inefficient.
A more efficient procedure, suggested in the paper by Rabiner et al. (1975), is

to compute δ analytically, according to the formula

δ = γ0Ĥdr(ω0)+ γ1Ĥdr(ω1)+ · · · + γL+1Ĥdr(ωL+1)
γ0

Ŵ (ω0)
− γ1
Ŵ (ω1)

+ · · · + (−1)
L+1γL+1

Ŵ (ωL+1)

where

γk =
L+1∏
n=0
n�=k

1
cosωk − cosωn

Now since P(ω) is a trigonometric polynomial of the form

P(ω) =
L∑
k=0

α(k)xk, x = cosω

and since we know that the polynomial at the points xn ≡ cosωn , n = 0, 1, . . . , L+1,
has the corresponding values

P(ωn) = Ĥdr(ωn)− (−1)
nδ

Ŵ (ωn)
, n = 0, 1, . . . , L+ 1

(2.74)

(2.75)

(2.76)

(2.77)

Design of Digital Filters

If we treat the {a(k)} and δ as the parameters to be determined, (2.73) can be

frequencies. Although the matrix equation in (2.74) can be used in the iterative

The expression for δ in (2.75) follows immediately from the matrix equation in
(2.74). Thus with an initial guess at the L+2 extremal frequencies, we compute δ .

702



we can use the Lagrange interpolation formula for P(ω). Thus P(ω) can be expressed
as [see Hamming (1962)]

P(ω) =

L∑
k=0

P(ωk)[βk/(x − xk)]

L∑
k=0

[βk/(x − xk)]

n k = cosωk , and

βk =
L∏
n=0
n�=k

1
xk − xn

Having the solution for P(ω), we can now compute the error function E(ω) from

E(ω) = Ŵ (ω)[Ĥdr(ω)− P(ω)]

on a dense set of frequency points. Usually, a number of points equal to 16M , where
M is the length of the filter, suffices. If |E(ω)| ≥ δ for some frequencies on the
dense set, then a new set of frequencies corresponding to the L + 2 largest peaks

repeated. Since the new set of L+ 2 extremal frequencies is selected to correspond
to the peaks of the error function |E(ω)|, the algorithm forces δ to increase in each
iteration until it converges to the upper bound and hence to the optimum solution
for the Chebyshev approximation problem. In other words, when |E(ω)| ≤ δ for all
frequencies on the dense set, the optimal solution has been found in terms of the

to Remez (1957).
Once the optimal solution has been obtained in terms of P(ω), the unit sample

response h(n) can be computed directly, without having to compute the parameters
{α(k)}. In effect, we have determined

Hr(ω) = Q(ω)P (ω)

which can be evaluated at ω = 2πk/M , k = 0, 1, . . . , (M − 1)/2, for M odd, or
M/2 for M even. Then, depending on the type of filter being designed, h(n) can be

A computer program written by Parks and McClellan (1972b) is available for
designing linear-phase FIR filters based on the Chebyshev approximation criterion
and implemented with the Remez exchange algorithm. This program can be used
to design lowpass, highpass, or bandpass filters, differentiators, and Hilbert trans-
formers. The latter two types of filters are described in the following sections. A
number of software packages for designing equiripple linear-phase FIR filters are
now available.

(2.78)

(2.79)

(2.80)

Design of Digital Filters

where P(ω ) is given by (2.77), x = cosω , x

of |E(ω)| are selected and the computational procedure beginning with (2.75) is

polynomial H(ω). A flowchart of the algorithm is shown in Fig. 2.16 and is due

determined from the formulas given in Table 3.

703



Input filter parameters

Initial guess of
M + 2 extremal freq.

Calculate the optimum
δ on extremal set

Interpolate through M + 1 
points to obtain P(ω)

Calculate error E(ω)
and find local maxima
where |E(ω)| � δ

More than
M + 2 
extrema?

Retain M + 2 
largest extrema

Check whether  the
extremal points changed

Best approximation

Changed

Yes

No

Flowchart of Remez algorithm.

The Parks–McClellan program requires a number of input parameters which
determine the filter characteristics. In particular, the following parameters must be
specified:

NFILT: The filter length, denoted above as M .

JTYPE: Type of filter:
JTYPE = 1 results in a multiple passband/stopband filter.
JTYPE = 2 results in a differentiator.
JTYPE = 3 results in a Hilbert transformer.

NBANDS: The number of frequency bands from 2 (for a lowpass filter) to a maxi-
mum of 10 (for a multiple-band filter).

LGRID: The grid density for interpolating the error function E(ω). The default
value is 16 if left unspecified.

EDGE: The frequency bands specified by lower and upper cutoff frequencies,

Design of Digital Filters

Figure 2.16

704



up to a maximum of 10 bands (an array of size 20, maximum). The
frequencies are given in terms of the variable f = ω/2π , where f = 0.5
corresponds to the folding frequency.

FX: An array of maximum size 10 that specifies the desired frequency re-
sponse Hdr(ω) in each band.

WTX: An array of maximum size 10 that specifies the weight function in each
band.

The following examples demonstrate the use of this program to design a lowpass and
a bandpass filter.

Design a lowpass filter of length M = 61 with a passband edge frequency fp = 0.1 and a
stopband edge frequency fs = 0.15.
Solution. The lowpass filter is a two-band filter with passband edge frequencies (0, 0.1) and
stopband edge frequencies (0.15, 0.5). The desired response is (1, 0) and the weight function
is arbitrarily selected as (1, 1).

61, 1, 2
0.0, 0.1, 0.15, 0.5
1.0, 0.0
1.0, 1.0

has a stopband attenuation of −56 dB and a passband ripple of 0.0135 dB.

0 10 20 30 40 50 60
−0.1

0

0.1

0.2

0.3

n

h(
n)

0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5
−100

−80

−60

−40

−20

0

f

M
ag

ni
tu

de
 (

dB
)

Impulse response and frequency response of M = 61 FIR filter in

Design of Digital Filters

EXAMPLE 2.3

The impulse response and frequency response are shown in Fig. 2.17. The resulting filter

Figure 2.17
Example 2.3.

705



If we increase the length of the filter to M = 101 while maintaining all the
other parameters given above the same, the resulting filter has the impulse response

Now, the stopband
attenuation is −85 dB and the passband ripple is reduced to 0.00046 dB.

We should indicate that it is possible to increase the attenuation in the stopband
by keeping the filter length fixed, say at M = 61, and decreasing the weighting
function W(ω) = δ2/δ1 in the passband. With M = 61 and a weighting function
(0.1, 1), we obtain a filter that has a stopband attenuation of −65 dB and a passband
ripple of 0.049 dB.

Design a bandpass filter of length M = 32 with passband edge frequencies fp1 = 0.2 and
fp2 = 0.35 and stopband edge frequencies of fs1 = 0.1 and fs2 = 0.425.

Solution. This passband filter is a three-band filter with a stopband range of (0, 0.1), a
passband range of (0.2, 0.35), and a second stopband range of (0.425, 0.5). The weighting
function is selected as (10.0, 1.0, 10.0), or as (1.0, 0.1, 1.0), and the desired response in the
three bands is (0.0, 1.0, 0.0). Thus the input parameters to the program are

32, 1, 3
0.0, 0.1, 0.2, 0.35, 0.425, 0.5
0.0, 1.0, 0.0
10.0, 1.0, 10.0

0 10 20 30 40 50 60 70 80 90 100
−0.1

0

0.1

0.2

0.3

n

h(
n)

0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5
−100

−80

−60

−40

−20

0

f

M
ag

ni
tu

de
 (

dB
)

Figure 2.18 Impulse response and frequency response of M = 101 FIR filter in

Design of Digital Filters

and frequency response characteristics shown in Fig. 2.18.

EXAMPLE 2.4

Example 2.3.

706



0 5 10 15 20 25 30
−0.4

−0.2

0

0.2

0.4

n

h(
n)

0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5
−100

−80

−60

−40

−20

0

f

M
ag

ni
tu

de
 (

dB
)

Figure 2.19 Impulse response and frequency response of M = 32 FIR filter in

We note that the ripple in the stopbands δ2 is 10 times smaller than the ripple in the passband
due to the fact that errors in the stopband were given a weight of 10 compared to the passband
weight of unity. The impulse response and frequency response of the bandpass filter are

These examples serve to illustrate the relative ease with which optimal lowpass,
highpass, bandstop, bandpass, and more general multiband linear-phase FIR filters
can be designed based on the Chebyshev approximation criterion implemented by
means of the Remez exchange algorithm. In the next two sections we consider the
design of differentiators and Hilbert transformers.

2.5 Design of FIR Differentiators

Differentiators are used in many analog and digital systems to take the derivative of a
signal. An ideal differentiator has a frequency response that is linearly proportional
to frequency. Similarly, an ideal digital differentiator is defined as one that has the
frequency response

Hd(ω) = jω, −π ≤ ω ≤ π (2.81)

Design of Digital Filters

Example 2.4.

illustrated in Fig. 2.19.

707



The unit sample response corresponding to Hd(ω) is

hd(n) = 12π
∫ π
−π
Hd(ω)e

jωndω

= 1
2π

∫ π
−π
jωejωndω

= cosπn
n

, −∞ < n <∞, n �= 0

We observe that the ideal differentiator has an antisymmetric unit sample response
[i.e., hd(n) = −hd(−n)]. Hence, hd(0) = 0.

In this section we consider the design of linear-phase FIR differentiators based
on the Chebyshev approximation criterion. In view of the fact that the ideal differ-
entiator has an antisymmetric unit sample response, we shall confine our attention
to FIR designs in which h(n) = −h(M − 1− n). Hence we consider the filter types
classified in the preceding section as Case 3 and Case 4.

We recall that in Case 3, where M is odd, the real-valued frequency response
of the FIR filter Hr(ω) has the characteristic that Hr(0) = 0. A zero response at
zero frequency is just the condition that the differentiator should satisfy, and we see

differentiator is desired, this is impossible to achieve with an FIR filter having an odd
number of coefficients, since Hr(π) = 0 for M odd. In practice, however, full-band
differentiators are rarely required.

In most cases of practical interest, the desired frequency response characteristic
need only be linear over the limited frequency range 0 ≤ ω ≤ 2πfp , where fp is
called the bandwidth of the differentiator. In the frequency range 2πfp < ω ≤ π ,
the desired response may be either left unconstrained or constrained to be zero.

criterion, the weighting function W(ω) is specified in the program as

W(ω) = 1
ω
, 0 ≤ ω ≤ 2πfp

in order that the relative ripple in the passband be a constant. Thus the absolute
error between the desired response ω and the approximation Hr(ω) increases as ω
varies from 0 to 2πfp
relative error

δ = max
0≤ω≤2πfp

{W(ω)[ω −Hr(ω)]}

= max
0≤ω≤2πfp

[
1− Hr(ω)

ω

]

is fixed within the passband of the differentiator.

Use the Remez algorithm to design a linear-phase FIR differentiator of length M = 60. The
passband edge frequency is 0.1 and the stopband edge frequency is 0.15.

(2.82)

In the design of FIR differentiators based on the Chebyshev approximation

(2.83)

(2.84)

Design of Digital Filters

from Table 5 that both filter types satisfy this condition. However, if a full-band

. However, the weighting function in (2.83) ensures that the

EXAMPLE 2.5

708



Solution. The input parameters to the program are

60, 2, 2
0.0, 0.1, 0.15, 0.5
1.0, 0.0
1.0, 1.0

Also shown in the same
figure is the approximation error over the passband 0 ≤ f ≤ 0.1 of the filter.

The important parameters in a differentiator are its length M , its bandwidth
{ band-edge frequency } fp , and the peak relative error δ of the approximation. The
interrelationship among these three parameters can be easily displayed parametri-
cally. In particular, the value of 20 log10 δ versus fp with M as a parameter is shown

and Schafer (1974a), are useful in the selection of the filter length, given specifications
on the inband ripple and the cutoff frequency fp .

differentiators result in a significantly smaller approximation error δ than comparable
odd-length differentiators. Designs based on M odd are particularly poor if the
bandwidth exceeds fp = 0.45. The problem is basically the zero in the frequency
response at ω = π(f = 1/2). When fp < 0.45, good designs are obtained for M
odd, but comparable-length differentiators with M even are always better in the
sense that the approximation error is smaller.

In view of the obvious advantage of even-length over odd-length differentiators,
a conclusion might be that even-length differentiators are always preferable in prac-
tical systems. This is certainly true for many applications. However, we should note
that the signal delay introduced by any linear-phase FIR filter is (M − 1)/2, which is
not an integer when M is even. In many practical applications, this is unimportant.
In some applications where it is desirable to have an integer-valued delay in the signal
at the output of the differentiator, we must select M to be odd.

These numerical results are based on designs resulting from the Chebyshev ap-
proximation criterion. We wish to indicate it is also possible and relatively easy to
design linear-phase FIR differentiators based on the frequency-sampling method.

band (fp = 0.5) differentiator, of length M = 30. The graph of the absolute value
of the approximation error as a function of frequency is also shown in this figure.

2.6 Design of Hilbert Transformers

An ideal Hilbert transformer is an all-pass filter that imparts a 90◦ phase shift on the
signal at its input. Hence the frequency response of the ideal Hilbert transformer is
specified as

Hd(ω) =
{−j, 0 < ω ≤ π
j, −π < ω < 0 (2.85)

Design of Digital Filters

The frequency response characteristic is illustrated in Fig. 2.20.

in Fig. 2.21 for M even and in Fig. 2.22 for M odd. These results, due to Rabiner

A comparison of the graphs in Figs. 2.21 and 2.22 reveals that even-length

For example, Fig. 2.23 illustrates the frequency response characteristics of a wide-

709



0 10 20 30 40 50

−0.1

−0.05

0

0.05

0.1

n

h(
n)

0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5

0.2

0.4

0.6

0.8

1

f

M
ag

ni
tu

de

0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1

5

0

5

x 10
4

f

E
rr

or

0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1

5

0

5

x 10
3

f

N
or

m
al

iz
ed

 E
rr

or

Figure 2.20 Frequency response and approximation error for M = 60 FIR differentiator of

Design of Digital Filters

Example 2.5.

710



Optimum FIR differentiators

0

−10

−20

−30

−40

−50

−60

−70

−80

−90

−100

−110

−120

−130
0.20 0.25 0.30 0.35 0.40 0.45 0.5

M = 4

8

16

32

64

20
 lo

g 1
0 
δ 

in
 D

ec
ib

el
s

Passband Cutoff Frequency ( fp)

Curves of 20 log10 δ versus fp for M = 4, 8,
16, 32, and 64. [From paper by Rabiner and Schafer (1974a).
Reprinted with permission of AT&T.]

Optimum FIR differentiators
0

−10

−20

−30

−40

−50

−60

−70

−80

−90

−100

−110

−120
−130

0.20 0.25 0.30 0.35 0.40 0.45 0.5

M = 5

9

17

33

65

20
 L

og
10
δ 

 in
 D

ec
ib

el
s

Passband cutoff frequency ( fp)

Figure 2.22 Curves of 20log10δ versus Fp for M =
5, 9, 17,33 and 65 [From paper by Rabiner and Schafer
(1974a). Reprinted with permission of AT&T.]

Design of Digital Filters

Figure 2.21

711



Frequency response and approximation error for M = 30
FIR differentiator designed by frequency-sampling method.

Hilbert transformers are frequently used in communication systems and signal pro-
cessing, as, for example, in the generation of single-sideband modulated signals, radar
signal processing, and speech signal processing.

The unit sample response of an ideal Hilbert transformer is

hd(n) = 12π
∫ π
−π
Hd(ω)e

jωndω

= 1
2π

(∫ 0
−π
jejωndω −

∫ π
0
jejωndω

)

=
{

2
π

sin2(πn/2)
n

, n �= 0
0, n = 0

As expected, hd(n) is infinite in duration and noncausal. We note that hd(n) is
antisymmetric [i.e., hd(n) = −hd(−n)]. In view of this characteristic, we focus our
attention on the design of linear-phase FIR Hilbert transformers with antisymmetric

(2.86)

Design of Digital Filters

Figure 2.23

712



unit sample response [i.e., h(n) = −h(M−1−n)]. We also observe that our choice of
an antisymmetric unit sample response is consistent with having a purely imaginary
frequency response characteristic Hd(ω).

We recall once again that when h(n) is antisymmetric, the real-valued frequency
response characteristic Hr(ω) is zero at ω = 0 for both M odd and even and at
ω = π when M is odd. Clearly, then, it is impossible to design an all-pass digi-
tal Hilbert transformer. Fortunately, in practical signal processing applications, an
all-pass Hilbert transformer is unnecessary. Its bandwidth need only cover the band-
width of the signal to be phase shifted. Consequently, we specify the desired real-
valued frequency response of a Hilbert transform filter as

Hdr(ω) = 1, 2πfl ≤ ω ≤ 2πfu
where fl and fu are the lower and upper cutoff frequencies, respectively.

It is interesting to note that the ideal Hilbert transformer with unit sample re-
d This property is retained by

the FIR Hilbert transformer under some symmetry conditions. In particular, let us
consider the Case 3 filter type for which

Hr(ω) =
(M−1)/2∑
k=1

c(k) sinωk

and suppose that fl = 0.5 − fu . This ensures a symmetric passband about the
midpoint frequency f = 0.25. If we have this symmetry in the frequency response,
r r

(M−1)/2∑
k=1

c(k) sinωk =
(M−1)/2∑
k=1

c(k) sin k(π − ω)

=
(M−1)/2∑
k=1

c(k) sinωk cosπk

=
(M−1)/2∑
k=1

c(k)(−1)k+1 sinωk

or equivalently,
(M−1)/2∑
k=1

[1− (−1)k+1]c(k) sinωk = 0

Clearly, c(k) must be equal to zero for k = 0, 2, 4, . . . .
Now, the relationship between {c(k)} and the unit sample response {h(n)} is,

c(k) = 2h
(
M − 1

2
− k

)

(2.87)

(2.88)

(2.89)

Design of Digital Filters

sponse h (n) as given in (2.86) is zero for n even.

H (ω) = H (π − ω) and hence (2.88) yields

from (2.54),

713



or, equivalently,

h

(
M − 1

2
− k

)
= 1

2
c(k)

h(k) =




0, k = 0, 2, 4, . . . , for M − 1
2

even

0, k = 1, 3, 5, . . . , for M − 1
2

odd

It does not hold for M even. This
means that for comparable values of M , the case M odd is preferable since the com-
putational complexity (number of multiplications and additions per output point) is
roughly one half of that for M even.

When the design of the Hilbert transformer is performed by the Chebyshev
approximation criterion using the Remez algorithm, we select the filter coefficients
to minimize the peak approximation error

δ = max
2πfl≤ω≤2πfu

[Hdr(ω)−Hr(ω)]

= max
2πfl≤ω≤2πfu

[1−Hr(ω)]

Thus the weighting function is set to unity and the optimization is performed over
the single frequency band (i.e., the passband of the filter).

Design a Hilbert transformer with parameters M = 31, fl = 0.05, and fu = 0.45.

Solution. We observe that the frequency response is symmetric, since fu = 0.5 − fl . The
parameters for executing the Remez algorithm are

31, 3, 1
0.05, 0.45

1.0
1.0

(2.90)

(2.91)

(2.92)

Design of Digital Filters

If c(k) is zero for k = 0, 2, 4, . . . , then (2.90) yields

Unfortunately, (2.91) holds only for M odd.

EXAMPLE 2.6

2.24. We observe that, indeed, every other value of h(n) is essentially zero.
The result of this design is the unit sample response and frequency response shown in Fig.

714



0 5 10 15 20 25 30

−0.5

0

0.5

n

h(
n)

0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5
−100

−50

0

f

M
ag

ni
tu

de
 (

dB
)

Rabiner and Schafer (1974b) have investigated the characteristics of Hilbert
transformer designs for both M odd and M even. If the filter design is restricted to a
symmetric frequency response, then there are basically three parameters of interest,
M , δ , and fl . 10 δ versus fl (the transition width)
with M as a parameter. We observe that for comparable values of M , there is no
performance advantage of using M odd over M even, and vice versa. However, the
computational complexity in implementing a filter for M odd is less by a factor of 2
over M even as previously indicated. Therefore, M odd is preferable in practice.

Mfl ≈ −0.61 log10 δ

Hence this formula can be used to estimate the size of one of the three basic filter
parameters when the other two parameters are specified.

2.7 Comparison of Design Methods for Linear-Phase FIR Filters

Historically, the design method based on the use of windows to truncate the impulse
response hd(n) and obtain the desired spectral shaping was the first method pro-
posed for designing linear-phase FIR filters. The frequency-sampling method and
the Chebyshev approximation method were developed in the 1970s and have since
become very popular in the design of practical linear-phase FIR filters.

The major disadvantage of the window design method is the lack of precise
control of the critical frequencies, such as ωp and ωs , in the design of a lowpass FIR

(2.93)

Design of Digital Filters

Figure 2.24 Frequency of FIR Hilbert transform filter in Example 2.6.

Figure 2.25 is a plot of 20 log

For design purposes, the graphs in Fig. 2.25 suggest that, as a rule of thumb,

715



Hilbert transformers
0

−10

−20

−30

−40

−50

−60

−70

−80

−90

−100

−110

−120
0 0.02 0.04 0.06 0.08 0.10

M = 3

4

7

8

15

16

31

32

6364

20
 lo

g 1
0 

δ 
 in

 D
ec

ib
el

s

Transition width (∆f )

Figure 2.25 Curves of 20 log10 δ versus �f for M = 3, 4, 7, 8, 15, 16, 31, 32,
63, 64. [From paper by Rabiner and Schafer (1974b). Reprinted with permission of
AT&T.]

filter. The values of ωp and ωs , in general, depend on the type of window and the
filter length M .

The frequency-sampling method provides an improvement over the window de-
sign method, since Hr(ω) is specified at the frequencies ωk = 2πk/M or ωk =
π(2k+1)/M and the transition band is a multiple of 2π/M . This filter design method
is particularly attractive when the FIR filter is realized either in the frequency domain
by means of the DFT or in any of the frequency-sampling realizations. The attractive
feature of these realizations is that Hr(ωk) is either zero or unity at all frequencies,
except in the transition band.

The Chebyshev approximation method provides total control of the filter speci-
fications, and, as a consequence, it is usually preferable over the other two methods.
For a lowpass filter, the specifications are given in terms of the parameters ωp , ωs ,
δ1 , δ2 , and M . We can specify the parameters ωp , ωs , M , and δ , and optimize the
filters relative to δ2 . By spreading the approximation error over the passband and

Design of Digital Filters

716



the stopband of the filter, this method results in an optimal filter design, in the sense
that for a given set of specifications just described, the maximum sidelobe level is
minimized.

The Chebyshev design procedure based on the Remez exchange algorithm re-
quires that we specify the length of the filter, the critical frequencies ωp and ωs , and
the ratio δ2/δ1 . However, it is more natural in filter design to specify ωp , ωs , δ1 ,
and δ2 and to determine the filter length that satisfies the specifications. Although
there is no simple formula to determine the filter length from these specifications, a
number of approximations have been proposed for estimating M from ωp , ωs , δ1 ,
and δ2 . A particularly simple formula attributed to Kaiser for approximating M is

M̂ = −20 log10
(√
δ1δ2

)− 13
14.6 �f

+ 1

where �f is the transition band, defined as �f = (ωs − ωp)/2π . This formula has
been given in the paper by Rabiner et al. (1975). A more accurate formula proposed
by Herrmann et al. (1973) is

M̂ = D∞(δ1, δ2)− f (δ1, δ2)(�f )
2

�f
+ 1

where, by definition,

D∞(δ1, δ2) = [0.005309(log10 δ1)2 + 0.07114(log10 δ1)− 0.4761](log10 δ2)
− [0.00266(log10 δ1)2 + 0.5941 log10 δ1 + 0.4278]

f (δ1, δ2) = 11.012+ 0.51244(log10 δ1 − log10 δ2)
These formulas are extremely useful in obtaining a good estimate of the filter

length required to achieve the given specifications �f , δ1 , and δ2 . The estimate is
used to carry out the design, and if the resulting δ exceeds the specified δ2 , the length
can be increased until we obtain a sidelobe level that meets the specifications.

3 Design of IIR Filters From Analog Filters

Just as in the design of FIR filters, there are several methods that can be used to
design digital filters having an infinite-duration unit sample response. The techniques
described in this section are all based on converting an analog filter into a digital filter.
Analog filter design is a mature and well-developed field, so it is not surprising that
we begin the design of a digital filter in the analog domain and then convert the
design into the digital domain.

An analog filter can be described by its system function,

Ha(s) = B(s)
A(s)

=

M∑
k=0

βks
k

N∑
k=0

αks
k

(2.94)

(2.95)

(2.96)

(2.97)

(3.1)

Design of Digital Filters

717



where {αk} and {βk} are the filter coefficients, or by its impulse response, which is
related to Ha(s) by the Laplace transform

Ha(s) =
∫ ∞
−∞

h(t)e−st dt

Alternatively, the analog filter having the rational system function H(s) given in

N∑
k=0

αk
dky(t)

dtk
=

M∑
k=0

βk
dkx(t)

dtk

where x(t) denotes the input signal and y(t) denotes the output of the filter.
Each of these three equivalent characterizations of an analog filter leads to alter-

native methods for converting the filter into the digital domain, as will be described in

with system function H(s) is stable if all its poles lie in the left half of the s -plane.
Consequently, if the conversion technique is to be effective, it should possess the
following desirable properties:

1. The j� axis in the s -plane should map into the unit circle in the z-plane. Thus
there will be a direct relationship between the two frequency variables in the
two domains.

2. The left-half plane (LHP) of the s -plane should map into the inside of the unit
circle in the z-plane. Thus a stable analog filter will be converted to a stable
digital filter.

We mentioned in the preceding section that physically realizable and stable IIR filters
cannot have linear phase. Recall that a linear-phase filter must have a system function
that satisfies the condition

H(z) = ±z−NH(z−1)
where z−N represents a delay of N units of time. But if this were the case, the filter
would have a mirror-image pole outside the unit circle for every pole inside the unit
circle. Hence the filter would be unstable. Consequently, a causal and stable IIR
filter cannot have linear phase.

If the restriction on physical realizability is removed, it is possible to obtain a
linear-phase IIR filter, at least in principle. This approach involves performing a time
reversal of the input signal x(n), passing x(−n) through a digital filter H(z), time-
reversing the output of H(z), and finally, passing the result through H(z) again. This
signal processing is computationally cumbersome and appears to offer no advantages
over linear-phase FIR filters. Consequently, when an application requires a linear-
phase filter, it should be an FIR filter.

In the design of IIR filters, we shall specify the desired filter characteristics for the
magnitude response only. This does not mean that we consider the phase response
unimportant. Since the magnitude and phase characteristics are related, as indicated

response that is obtained from the design methodology.

(3.2)

(3.3)

(3.4)

Design of Digital Filters

(3.1) can be described by the linear constant-coefficient differential equation

Sections 3.1 through 3.3. We recall that an analog linear time-invariant system

in Section 1, we specify the desired magnitude characteristics and accept the phase

718



Figure 3.1
Substitution of the
backward difference for
the derivative implies the
mapping s = (1− z−1)/T .

H(s) = s

(a)

dy(t)y(t)
dt

H(z) =

(b)

y(n) − y(n − 1)y(n)
T

1− z −1
T

3.1 IIR Filter Design by Approximation of Derivatives

One of the simplest methods for converting an analog filter into a digital filter is to

tion. This approach is often used to solve a linear constant-coefficient differential
equation numerically on a digital computer.

For the derivative dy(t)/dt at time t = nT , we substitute the backward difference
[y(nT )− y(nT − 1)]/T . Thus

dy(t)

dt

∣∣∣∣
t=nT
= y(nT )− y(nT − T )

T

= y(n)− y(n− 1)
T

where T represents the sampling interval and y(n) ≡ y(nT ). The analog differ-
entiator with output dy(t)/dt has the system function H(s) = s , while the digi-
tal system that produces the output [y(n) − y(n − 1)]/T has the system function
H(z) = (1 − z−1

s = 1− z
−1

T

The second derivative d2y(t)/dt2 is replaced by the second difference, which is
derived as follows:

d2y(t)

dt2

∣∣∣∣
t=nT
= d
dt

[
dy(t)

dt

]
t=nT

= [y(nT )− y(nT − T )]/T − [y(nT − T )− y(nT − 2T )]/T
T

= y(n)− 2y(n− 1)+ y(n− 2)
T 2

s2 = 1− 2z
−1 + z−2
T 2

=
(

1− z−1
T

)2

(3.5)

(3.6)

(3.7)

(3.8)

Design of Digital Filters

approximate the differential equation in (3.3) by an equivalent difference equa-

)/T . Consequently, as shown in Fig. 3.1, the frequency-domain
equivalent for the relationship in (3.5) is

In the frequency domain, (3.7) is equivalent to

719



The mapping
s = (1 − z−1)/T takes LHP
in the s -plane into points
inside the circle of radius
1
2 and center z = 12 in the
z-plane.

Unit circle

z-plane

s-plane

σ

jΩ

1
2

It easily follows from the discussion that the substitution for the kth derivative
of y(t) results in the equivalent frequency-domain relationship

sk =
(

1− z−1
T

)k

Consequently, the system function for the digital IIR filter obtained as a result of the
approximation of the derivatives by finite differences is

H(z) = Ha(s)|s=(1−z−1)/T
whereHa(s) is the system function of the analog filter characterized by the differential

Let us investigate the implications of the mapping from the s -plane to the z-plane

z = 1
1− sT

z = 1
1− j�T

= 1
1+�2T 2 + j

�T

1+�2T 2

As � varies from −∞ to ∞, the corresponding locus of points in the z-plane is a
circle of radius 12 and with center at z = 12

(3.9)

(3.10)

(3.11)

(3.12)

Design of Digital Filters

Figure 3.2

equation given in (3.3).

as given by (3.6) or, equivalently,

If we substitute s = j� in (2.11), we find that

, as illustrated in Fig. 3.2.

720



the s -plane into corresponding points inside this circle in the z-plane, and points in
the right-hand plane (RHP) of the s -plane are mapped into points outside this circle.
Consequently, this mapping has the desirable property that a stable analog filter is
transformed into a stable digital filter. However, the possible location of the poles of
the digital filter are confined to relatively small frequencies and, as a consequence,
the mapping is restricted to the design of lowpass filters and bandpass filters having
relatively small resonant frequencies. It is not possible, for example, to transform a
highpass analog filter into a corresponding highpass digital filter.

In an attempt to overcome the limitations in the mapping given above, more
complex substitutions for the derivatives have been proposed. In particular, an Lth-
order difference of the form

dy(t)

dt

∣∣∣∣
t=nT
= 1
T

L∑
k=1

αk
y(nT + kT )− y(nT − kT )

T

has been proposed, where {αk} are a set of parameters that can be selected to optimize
the approximation. The resulting mapping between the s -plane and the z-plane is
now

s = 1
T

L∑
k=1

αk(z
k − z−k)

When z = ejω , we have

s = j 2
T

L∑
k=1

αk sinωk

which is purely imaginary. Thus

� = 2
T

L∑
k=1

αk sinωk

is the resulting mapping between the two frequency variables. By proper choice of
the coefficients {αk} it is possible to map the j�-axis into the unit circle. Furthermore,
points in the LHP in s can be mapped into points inside the unit circle in z.

the problem of selecting the set of coefficients {αk} remains. In general, this is a
difficult problem. Since simpler techniques exist for converting analog filters into
IIR digital filters, we shall not emphasize the use of the Lth-order difference as a
substitute for the derivative.

Convert the analog bandpass filter with system function

Ha(s) = 1
(s + 0.1)2 + 9

into a digital IIR filter by use of the backward difference for the derivative.

(3.13)

(3.14)

(3.15)

(3.16)

Design of Digital Filters

It is easily demonstrated that the mapping in (3.11) takes points in the LHP of

Despite achieving the two desirable characteristics with the mapping of (3.16),

EXAMPLE 3.1

721



Solution.

H(z) = 1(
1− z−1
T

+ 0.1
)2
+ 9

= T
2/(1+ 0.2T + 9.01T 2)

1− 2(1+ 0.1T )
1+ 0.2T + 9.01T 2 z

−1 + 1
1+ 0.2T + 9.01T 2 z

−2

The system function H(z) has the form of a resonator provided that T is selected small enough
(e.g., T ≤ 0.1), in order for the poles to be near the unit circle. Note that the condition a21 < 4a2
is satisfied, so that the poles are complex valued.

For example, if T = 0.1, the poles are located at

p1,2 = 0.91± j0.27

= 0.949e±j16.5◦

We note that the range of resonant frequencies is limited to low frequencies, due to
the characteristics of the mapping. The reader is encouraged to plot the frequency
response H(ω) of the digital filter for different values of T and compare the results
with the frequency response of the analog filter.

mapping

s = 1
T
(z− z−1)

Solution. By substituting for s in H(s), we obtain

H(z) = 1(
z− z−1
T

+ 0.1
)2
+ 9

= z
2T 2

z4 + 0.2T z3 + (2 + 9.01T 2)z2 − 0.2T z+ 1

We observe that this mapping has introduced two additional poles in the conversion from
Ha(s) to H(z). As a consequence, the digital filter is significantly more complex than the
analog filter. This is a major drawback to the mapping given above.

Design of Digital Filters

Substitution for s from (3.6) into H(s) yields

EXAMPLE 3.2

Convert the analog bandpass filter in Example 3.1 into a digital IIR filter by use of the

722



IIR Filter Design by Impulse Invariance

In the impulse invariance method, our objective is to design an IIR filter having a
unit sample response h(n) that is the sampled version of the impulse response of the
analog filter. That is,

h(n) ≡ h(nT ), n = 0, 1, 2, . . .
where T is the sampling interval.

X(f ) = Fs
∞∑

k=−∞
Xa[(f − k)Fs]

where f = F/Fs is the normalized frequency. Aliasing occurs if the sampling rate
Fs is less than twice the highest frequency contained in Xa(F ).

Expressed in the context of sampling the impulse response of an analog filter with
frequency responseHa(F ), the digital filter with unit sample response h(n) ≡ ha(nT )
has the frequency response

H(f ) = Fs
∞∑

k=−∞
Ha[(f − k)Fs]

or, equivalently,

H(ω) = Fs
∞∑

k=−∞
Ha[(ω − 2πk)Fs]

or

H(�T ) = 1
T

∞∑
k=−∞

Ha

(
�− 2πk

T

)

frequency response of the corresponding digital filter.
It is clear that the digital filter with frequency response H(ω) has the frequency

response characteristics of the corresponding analog filter if the sampling interval
T is selected sufficiently small to completely avoid or at least minimize the effects
of aliasing. It is also clear that the impulse invariance method is inappropriate for
designing highpass filters due to the spectrum aliasing that results from the sampling
process.

To investigate the mapping of points between the z-plane and the s -plane implied

z-transform of h(n) to the Laplace transform of ha(t). This relationship is

H(z)|z=esT =
1
T

∞∑
k=−∞

Ha

(
s − j 2πk

T

)

(3.17)

(3.18)

(3.19)

(3.20)

(3.21)

(3.22)

To examine the implications of (3.17), we refer to when a continuous time sig-
nal  with spectrum  is sampled at a rate  samples per second, 
the spectrum of the sampled signal is the periodic repetition of the scaled spectrum 

 with period Specifically, the relationship is

xa(t) Xa(F ) Fs =1/T

FsXa(F ) Fs .

Design of Digital Filters

3.2

Figure 3.3 depicts the frequency response of a lowpass analog filter and the

by the sampling process, we rely on a generalization of (3.21) which relates the

723



H(ΩT )

ΩT
25T0−25T

Ha(ΩT )

Ω
250−25

Figure 3.3 Frequency response Ha(�) of the analog filter and frequency re-
sponse of the corresponding digital filter with aliasing.

where

H(z) =
∞∑
n=0

h(n)z−n

H(z)|z=esT =
∞∑
n=0

h(n)e−sT n

a

is suppressed in our notation.
Let us consider the mapping of points from the s -plane to the z-plane implied

by the relation
z = esT

If we substitute s = σ + j� and express the complex variable z in polar form as
z = rejω

rejω = eσT ej�T

Clearly, we must have
r = eσT

ω = �T

(3.23)

(3.24)

(3.25)

Design of Digital Filters

Note that when s = j�, (3.22) reduces to (3.21), where the factor of j in H (�)

, (3.24) becomes

724



Consequently, σ < 0 implies that 0 < r < 1 and σ > 0 implies that r > 1. When
σ = 0, we have r = 1. Therefore, the LHP in s is mapped inside the unit circle in z
and the RHP in s is mapped outside the unit circle in z.

Also, the j�-axis is mapped into the unit circle in z as indicated above. However,
the mapping of the j�-axis into the unit circle is not one-to-one. Since ω is unique
over the range (−π, π), the mapping ω = �T implies that the interval −π/T ≤
� ≤ π/T Furthermore, the
frequency interval π/T ≤ � ≤ 3π/T also maps into the interval −π ≤ ω ≤ π and,
in general, so does the interval (2k−1)π/T ≤ � ≤ (2k+1)π/T , when k is an integer.
Thus the mapping from the analog frequency � to the frequency variable ω in the
digital domain is many-to-one, which simply reflects the effects of aliasing due to

To explore further the effect of the impulse invariance design method on the
characteristics of the resulting filter, let us express the system function of the analog
filter in partial-fraction form. On the assumption that the poles of the analog filter
are distinct, we can write

Ha(s) =
N∑
k=1

ck

s − pk

where {pk} are the poles of the analog filter and {ck} are the coefficients in the
partial-fraction expansion. Consequently,

ha(t) =
N∑
k=1

cke
pkt , t ≥ 0

The mapping of z = esT
maps strips of width 2π/T
(for σ < 0) in the s -plane
into points in the unit circle
in the z-plane.

Unit
circle

s-plane

z-plane

σ

jΩ

z = esT
π

T

π

T
−

3π
T

−

3π
T

(3.26)

(3.27)

Design of Digital Filters

the relation in (3.24).

maps into the corresponding values of −π ≤ ω ≤ π .

sampling. Figure 3.4 illustrates the mapping from the s -plane to the z-plane for

Figure 3.4

725



If we sample ha(t) periodically at t = nT , we have
h(n) = ha(nT )

=
N∑
k=1

cke
pkT n

IIR filter becomes

H(z) =
∞∑
n=0

h(n)z−n

=
∞∑
n=0

(
N∑
k=1

cke
pkT n

)
z−n

=
N∑
k=1

ck

∞∑
n=0
(epkT z−1)n

k < 0 and yields

∞∑
n=0
(epkT z−1)n = 1

1− epkT z−1

Therefore, the system function of the digital filter is

H(z) =
N∑
k=1

ck

1− epkT z−1

We observe that the digital filter has poles at

zk = epkT , k = 1, 2, . . . , N
Although the poles are mapped from the s -plane to the z-plane by the relationship

same relationship. Therefore, the impulse invariance method does not correspond

having distinct poles. It can be generalized to include multiple-order poles. For

Convert the analog filter with system function

Ha(s) = s + 0.1
(s + 0.1)2 + 9

into a digital IIR filter by means of the impulse invariance method.

(3.28)

(3.29)

(3.30)

(3.31)

(3.32)

Design of Digital Filters

Now, with the substitution of (3.28), the system function of the resulting digital

The inner sum in (3.29) converges because p

in (3.32), we should emphasize that the zeros in the two domains do not satisfy the

to the simple mapping of points given by (3.24).
The development that resulted in H(z) given by (3.31) was based on a filter

brevity, however, we shall not attempt to generalize (3.31).

EXAMPLE 3.3

726



Solution. We note that the analog filter has a zero at s = −0.1 and a pair of complex-
conjugate poles at

pk = −0.1± j3

We do not have to determine the impulse response ha(t) in order to design the digital
IIR filter based on the method of impulse invariance. Instead, we directly determine H(z), as

a

H(s) =
1
2

s + 0.1− j3 +
1
2

s + 0.1+ j3

Then

H(z) =
1
2

1− e−0.1T ej3T z−1 +
1
2

1− e−0.1T e−j3T z−1

Since the two poles are complex conjugates, we can combine them to form a single two-
pole filter with system function

H(z) = 1− (e
−0.1T cos 3T )z−1

1− (2e−0.1T cos 3T )z−1 + e−0.2T z−1

for T = 0.1 and T = 0.5. For purpose of comparison, we have also plotted the magnitude of

more prevalent when T = 0.5 than when T = 0.1. Also, note the shift of the resonant
frequency as T changes.

Pole–zero locations
for analog filter in

σ

jΩ

Design of Digital Filters

as illustrated in Fig. 3.5.

given by (3.31), from the partial-fraction expansion of H (s). Thus we have

The magnitude of the frequency response characteristic of this filter is plotted in Fig. 3.6

the frequency response of the analog filter in Fig. 3.7. We note that aliasing is significantly

Figure 3.5

Example 3.3.

727



Frequency response
of digital filter in

−

−

−

−

−

Frequency response
of analog filter in

−

−

−

−

The preceding example illustrates the importance of selecting a small value for
T to minimize the effect of aliasing. Due to the presence of aliasing, the impulse
invariance method is appropriate for the design of lowpass and bandpass filters only.

IIR Filter Design by the Bilinear Transformation

The IIR filter design techniques described in the preceding two sections have a severe
limitation in that they are appropriate only for lowpass filters and a limited class of
bandpass filters.

In this section we describe a mapping from the s -plane to the z-plane, called
the bilinear transformation, that overcomes the limitation of the other two design
methods described previously. The bilinear transformation is a conformal mapping
that transforms the j�-axis into the unit circle in the z-plane only once, thus avoiding
aliasing of frequency components. Furthermore, all points in the LHP of s are
mapped inside the unit circle in the z-plane and all points in the RHP of s are
mapped into corresponding points outside the unit circle in the z-plane.

The bilinear transformation can be linked to the trapezoidal formula for nu-
merical integration. For example, let us consider an analog linear filter with system
function

H(s) = b
s + a (3.33)

Design of Digital Filters

Figure 3.6

Example 3.3.

Figure 3.7

Example 3.3.

3.3

728



This system is also characterized by the differential equation

dy(t)

dt
+ ay(t) = bx(t)

Instead of substituting a finite difference for the derivative, suppose that we integrate
the derivative and approximate the integral by the trapezoidal formula. Thus

y(t) =
∫ t
t0

y ′(τ )dτ + y(t0)

where y′(t) denotes the derivative of y(t). The approximation of the integral in
0 = nT − T yields

y(nT ) = T
2

[y ′(nT )+ y ′(nT − T )]+ y(nT − T )

y ′(nT ) = −ay(nT )+ bx(nT )

ference equation for the equivalent discrete-time system. With y(n) ≡ y(nT ) and
x(n) ≡ x(nT ), we obtain the result(

1+ aT
2

)
y(n)−

(
1− aT

2

)
y(n− 1) = bT

2
[x(n)+ x(n− 1)]

The z-transform of this difference equation is

(
1+ aT

2

)
Y (z)−

(
1− aT

2

)
z−1Y (z) = bT

2
(1+ z−1)X(z)

Consequently, the system function of the equivalent digital filter is

H(z) = Y (z)
X(z)

= (bT /2)(1+ z
−1)

1+ aT /2− (1− aT /2)z−1

or, equivalently,

H(z) = b
2
T

(
1− z−1
1+ z−1

)
+ a

Clearly, the mapping from the s -plane to the z-plane is

s = 2
T

(
1− z−1
1+ z−1

)

(3.34)

(3.35)

(3.36)

(3.37)

(3.38)

(3.39)

(3.40)

Design of Digital Filters

(3.35) by the trapezoidal formula at t = nT and t

Now the differential equation in (3.34) evaluated at t = nT yields

We use (3.37) to substitute for the derivative in (3.36) and thus obtain a dif-

729



This is called the bilinear transformation.
Although our derivation of the bilinear transformation was performed for a

first-order differential equation, it holds, in general, for an N th-order differential
equation.

To investigate the characteristics of the bilinear transformation, let

z = rejω

s = σ + j�

s = 2
T

z− 1
z+ 1

= 2
T

rejω − 1
rejω + 1

= 2
T

(
r2 − 1

1+ r2 + 2r cosω + j
2r sinω

1+ r2 + 2r cosω
)

Consequently,

σ = 2
T

r2 − 1
1+ r2 + 2r cosω

� = 2
T

2r sinω
1+ r2 + 2r cosω

First, we note that if r < 1, then σ < 0, and if r > 1, then σ > 0. Consequently,
the LHP in s maps into the inside of the unit circle in the z-plane and the RHP in s
maps into the outside of the unit circle. When r = 1, then σ = 0 and

� = 2
T

sinω
1+ cosω

= 2
T

tan
ω

2

or, equivalently,

ω = 2 tan−1 �T
2

observe a frequency compression or frequency warping, as it is usually called, due to
the nonlinearity of the arctangent function.

(3.41)

(3.42)

(3.43)

(3.44)

Design of Digital Filters

Then (3.40) can be expressed as

The relationship in (3.44) between the frequency variables in the two domains

once into the range −π ≤ ω ≤ π . However, the mapping is highly nonlinear. We
We observe that the entire range in � is mapped onlyis illustrated in Fig. 3.8.

730



−10 −5 5 100 15

ΩT
2

π

2
−

π

π

2

π−

ω = 2 tan−1

ω

ΩT

Mapping between the frequency variables ω and
� resulting from the bilinear transformation.

It is also interesting to note that the bilinear transformation maps the point s = ∞

has a zero at s = ∞, results in a digital filter that has a zero at z = −1.

Convert the analog filter with system function

Ha(s) = s + 0.1
(s + 0.1)2 + 16

into a digital IIR filter by means of the bilinear transformation. The digital filter is to have a
resonant frequency of ωr = π/2.

Solution. First, we note that the analog filter has a resonant frequency �r = 4. This
frequency is to be mapped into ωr = π/2 by selecting the value of the parameter T . From the

= 12 in order to have ωr = π/2. Thus the desired
mapping is

s = 4
(

1− z−1
1+ z−1

)

The resulting digital filter has the system function

H(z) = 0.125+ 0.0061z
−1− 0.1189z−1

1+ 0.0006z−1

We note that the coefficient of the z−1 term in the denominator of H(z) is extremely small
and can be approximated by zero. Thus we have the system function

H(z) = 0.125+ 0.0061z
−1− 0.1189 −2

1+ 0.9 −2512

+ 0.9512z−2

z

z

Design of Digital Filters

Figure 3.8

into the point z = −1. Consequently, the single-pole lowpass filter in (3.33), which

EXAMPLE 3.4

relationship in (3.43), we must select T

731



This filter has poles at
p1,2 = 0.987e±jπ/2

and zeros at
z1,2 = −1, 0.95

Therefore, we have succeeded in designing a two-pole filter that resonates near ω = π/2.

In this example the parameter T was selected to map the resonant frequency
of the analog filter into the desired resonant frequency of the digital filter. Usually,
the design of the digital filter begins with specifications in the digital domain, which
involve the frequency variable ω . These specifications in frequency are converted

In this procedure, the parameter T is
transparent and may be set to any arbitrary value (e.g., T = 1). The following
example illustrates this point.

Design a single-pole lowpass digital filter with a 3-dB bandwidth of 0.2π , using the bilinear
transformation applied to the analog filter

H(s) = �c
s +�c

where �c is the 3-dB bandwidth of the analog filter.

Solution. The digital filter is specified to have its −3-dB gain at ωc = 0.2π . In the frequency
domain of the analog filter ωc = 0.2π corresponds to

�c = 2
T

tan 0.1π

= 0.65
T

Thus the analog filter has the system function

H(s) = 0.65/T
s + 0.65/T

This represents our filter design in the analog domain.

into the desired digital filter. Thus we obtain

H(z) = 0.245(1+ z
−1)

1− 0.509z−1
where the parameter T has been divided out.

The frequency response of the digital filter is

H(ω) = 0.245(1+ e
−jω)

1− 0.509e−jω
At ω = 0, H(0) = 1, and at ω = 0.2π , we have |H(0.2π)| = 0.707, which is the desired
response.

Design of Digital Filters

designed that meets these specifications and converted to a digital filter by means
to the analog domain by means of the relation in (3.43). The analog filter is then

of the bilinear transformation in (3.40).

EXAMPLE 3.5

Now, we apply the bilinear transformation given by (3.40) to convert the analog filter

732



3.4 Characteristics of Commonly Used Analog Filters

discussion, IIR digital filters can easily be obtained
by beginning with an analog filter and then using a mapping to transform the s -
plane into the z-plane. Thus the design of a digital filter is reduced to designing an
appropriate analog filter and then performing the conversion from H(s) to H(z), in
such a way so as to preserve, as much as possible, the desired characteristics of the
analog filter.

Analog filter design is a well-developed field and many books have been writ-
ten on the subject. In this section we briefly describe the important characteristics
of commonly used analog filters and introduce the relevant filter parameters. Our
discussion is limited to lowpass filters. Subsequently, we describe several frequency
transformations that convert a lowpass prototype filter into either a bandpass, high-
pass, or band-elimination filter.

Butterworth filters. Lowpass Butterworth filters are all-pole filters characterized by
the magnitude-squared frequency response

|H(�)|2 = 1
1+ (�/�c)2N =

1
1+ 	2(�/�p)2N

where N is the order of the filter, �c is its −3-dB frequency (usually called the cutoff
frequency), �p is the passband edge frequency, and 1/(1+	2) is the band-edge value
of |H(�)|2 . Since H(s)H(−s) evaluated at s = j� is simply equal to |H(�)|2 , it
follows that

H(s)H(−s) = 1
1+ (−s2/�2c)N

The poles of H(s)H(−s) occur on a circle of radius �c at equally spaced points.

−s2
�2c
= (−1)1/N = ej (2k+1)π/N , k = 0, 1, . . . , N − 1

and hence

sk = �cejπ/2ej (2k+1)π/2N, k = 0, 1, . . . , N − 1

worth filters.
The frequency response characteristics of the class of Butterworth filters are

We note that |H(�)|2 is monotonic
in both the passband and stopband. The order of the filter required to meet an
attenuation δ2 at a specified frequency �s
at � = �s we have

1
1+ 	2(�s/�p)2N = δ

2
2

(3.45)

(3.46)

(3.47)

As we have seen from our   prior

Design of Digital Filters

From (3.46) we find that

For example, Fig. 3.9 illustrates the pole positions for N = 4 and N = 5 Butter-

shown in Fig. 3.10 for several values of N .

is easily determined from (3.45). Thus

733



π π
2 8+

π π
2 8

− −

N = 4

(a)

Poles of
H(s)

Poles of
H(−s)

π π
2 10+

N = 5

(b)

Poles of
H(s)

Poles of
H(−s)

Pole positions for Butterworth filters.

and hence

N = log[(1/δ
2
2)− 1]

2 log(�s/�c)
= log(δ/	)

log(�s/�p)

where, by definition, δ2 = 1/
√

1+ δ2 . Thus the Butterworth filter is completely
characterized by the parameters N , δ2 , 	 , and the ratio �s/�p .

Determine the order and the poles of a lowpass Butterworth filter that has a −3-dB bandwidth
of 500 Hz and an attenuation of 40 dB at 1000 Hz.

Solution. The critical frequencies are the −3-dB frequency �c and the stopband frequency
�s , which are

�c = 1000π
�s = 2000π

(3.48)

Design of Digital Filters

Figure 3.9

EXAMPLE 3.6

734



0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1.0

1.1

|H(Ω)|2

1 + ε2
1

N = 1

N = 2

N = 5 N = 4

N = 3

Ωe
Ω

Figure 3.10 Frequency response of Butterworth filters.

For an attenuation of 40 dB, δ2

N = log10(10
4 − 1)

2 log10 2

= 6.64

To meet the desired specifications, we select N = 7. The pole positions are

sk = 1000πej [π/2+(2k+1)π/14], k = 0, 1, 2, . . . , 6

Chebyshev filters. There are two types of Chebyshev filters. Type I Chebyshev filters
are all-pole filters that exhibit equiripple behavior in the passband and a monotonic

Design of Digital Filters

= 0.01. Hence from (3.48) we obtain

735



characteristic in the stopband. On the other hand, the family of type II Chebyshev
filters contains both poles and zeros and exhibits a monotonic behavior in the pass-
band and an equiripple behavior in the stopband. The zeros of this class of filters lie
on the imaginary axis in the s -plane.

The magnitude squared of the frequency response characteristic of a type I
Chebyshev filter is given as

|H(�)|2 = 1
1+ 	2T 2N(�/�p)

where 	 is a parameter of the filter related to the ripple in the passband and TN(x)
is the N th-order Chebyshev polynomial defined as

TN(x) =
{

cos(N cos−1 x), |x| ≤ 1
cosh(N cosh−1 x), |x| > 1

The Chebyshev polynomials can be generated by the recursive equation

TN+1(x) = 2xTN(x)− TN−1(x), N = 1, 2, . . .

0 1 2
2 − 1, T3(x) =

4x3 − 3x , and so on.
Some of the properties of these polynomials are as follows:

1. |TN(x)| ≤ 1 for all |x| ≤ 1.
2. TN(1) = 1 for all N .
3. All the roots of the polynomial TN(x) occur in the interval −1 ≤ x ≤ 1.

The filter parameter 	 is related to the ripple in the passband, as illustrated in
N

2 = 1.
On the other hand, for N even, TN(0) = 1 and hence |H(0)|2 = 1/(1+ 	2). At the
band-edge frequency � = �p , we have TN(1) = 1, so that

1√
1+ 	2 = 1− δ1

or, equivalently,

	2 = 1
(1− δ1)2 − 1

where δ1 is the value of the passband ripple.

(3.49)

(3.50)

(3.51)

(0) = 0 and hence |H(0)|

(3.52)

Design of Digital Filters

where T (x) = 1 and T (x) = x . From (3.51) we obtain T (x) = 2x

Fig. 3.11, for N odd and N even. For N odd, T

736



Figure 3.11 Type I Chebyshev filter characteristic.

The poles of a type I Chebyshev filter lie on an ellipse in the s -plane with major
axis

r1 = �p β
2 + 1
2β

and minor axis

r2 = �p β
2 − 1
2β

where β is related to 	 according to the equation

β =
[√

1+ 	2 + 1
	

]1/N

The pole locations are most easily determined for a filter of order N by first locating
the poles for an equivalent N th-order Butterworth filter that lie on circles of radius
r1 2
poles of the Butterworth filter as

φk = π2 +
(2k + 1)π

2N
, k = 0, 1, 2, . . . , N − 1

then the positions of the poles for the Chebyshev filter lie on the ellipse at the
coordinates (xk, yk), k = 0, 1, . . . , N − 1, where

xk = r2 cosφk, k = 0, 1, . . . , N − 1
yk = r1 sin φk, k = 0, 1, . . . , N − 1

(3.53)

(3.54)

(3.55)

(3.56)

(3.57)

Design of Digital Filters

or radius r , as illustrated in Fig.3.12. If we denote the angular positions of the

737



Determination of the pole
locations for a Chebyshev
filter.

r2

r1

A type II Chebyshev filter contains zeros as well as poles. The magnitude squared
of its frequency response is given as

|H(�)|2 = 1
1+ 	2[T 2N(�s/�p)/T 2N(�s/�)]

where TN(x) is, again, the N th-order Chebyshev polynomial and �s is the stopband

at the points

sk = j �ssin φk , k = 0, 1, . . . , N − 1

The poles are located at the points (vk, wk), where

vk = �sxk√
x2k + y2k

, k = 0, 1, . . . , N − 1

wk = �syk√
x2k + y2k

, k = 0, 1, . . . , N − 1

k k

stopband through the equation

β =

1+

√
1− δ22
δ2




1/N

(3.58)

(3.59)

(3.60)

(3.61)

(3.62)

Design of Digital Filters

Figure 3.12

frequency as illustrated in Fig. 3.13. The zeros are located on the imaginary axis

where {x } and {y } are defined in (3.57) with β now related to the ripple in the

738



Figure 3.13 Type II Chebyshev filters.

From this description, we observe that the Chebyshev filters are characterized
by the parameters N , 	 , δ2 , and the ratio �s/�p . For a given set of specifications on
	 , δ2 , and �s/�p , we can determine the order of the filter from the equation

N =
log

[(√
1− δ22 +

√
1− δ22(1+ 	2)

)
/	δ2

]

log
[
(�s/�p)+

√
(�s/�p)2 − 1

]

= cosh
−1(δ/	)

cosh−1(�s/�p)

where, by definition, δ2 = 1/
√

1+ δ2 .

Determine the order and the poles of a type I lowpass Chebyshev filter that has a 1-dB ripple
in the passband, a cutoff frequency �p = 1000π , a stopband frequency of 2000π , and an
attenuation of 40 dB or more for � ≥ �s .

Solution. First, we determine the order of the filter. We have

10 log10(1+ 	2) = 1

1+ 	2 = 1.259

	2 = 0.259

	 = 0.5088

Also,
20 log10 δ2 = −40

δ2 = 0.01

(3.63)

Design of Digital Filters

EXAMPLE 3.7

739



N = log10 196.54
log10(2+

√
3)

= 4.0

Thus a type I Chebyshev filter having four poles meets the specifications.

we compute β , r1 , and r2 . Hence

β = 1.429
r1 = 1.06�p
r2 = 0.365�p

The angles {φk} are
φk = π2 +

(2k + 1)π
8

, k = 0, 1, 2, 3

Therefore, the poles are located at

x1 + jy1 = −0.1397�p ± j0.979�p
x2 + jy2 = −0.337�p ± j0.4056�p

On the
other hand, the Chebyshev filter required only four. This result is typical of such
comparisons. In general, the Chebyshev filter meets the specifications with fewer
poles than the corresponding Butterworth filter. Alternatively, if we compare a
Butterworth filter to a Chebyshev filter having the same number of poles and the
same passband and stopband specifications, the Chebyshev filter will have a smaller
transition bandwidth. For a tabulation of the characteristics of Chebyshev filters and
their pole–zero locations, the interested reader is referred to the handbook of Zverev
(1967).

Elliptic filters. Elliptic (or Cauer) filters exhibit equiripple behavior in both the pass-

of filters contains both poles and zeros and is characterized by the magnitude-squared
frequency response

|H(�)|2 = 1
1+ 	2UN(�/�p)

where UN(x) is the Jacobian elliptic function of order N , which has been tabulated
by Zverev (1967), and 	 is a parameter related to the passband ripple. The zeros lie
on the j�-axis.

(3.64)

Design of Digital Filters

Hence from (3.63) we obtain

The pole positions are determined from the relations in (3.53) through (3.57). First,

The filter specifications in Example 3.7 are very similar to the specifications

case the number of poles required to meet the specifications was seven.
given in Example 3.6, which involved the design of a Butterworth filter. In that

band and the stopband, as illustrated in Fig. 3.14 for N odd and N even. This class

740



1 + ε2
1

1

1 + ε2
1

1

Ω Ω
δ

2
2 δ

2
2

|H(Ω)|2 |H(Ω)|2

N even N odd

Figure 3.14 Magnitude-squared frequency characteristics of elliptic filters.

We recall from our discussion of FIR filters that the most efficient designs occur
when we spread the approximation error equally over the passband and the stopband.
Elliptic filters accomplish this objective and, as a consequence, are the most efficient
from the viewpoint of yielding the smallest-order filter for a given set of specifications.
Equivalently, we can say that for a given order and a given set of specifications, an
elliptic filter has the smallest transition bandwidth.

The filter order required to achieve a given set of specifications in passband ripple
δ1 , stopband ripple δ2 , and transition ratio �p/�s is given as

N =
K(�p/�s)K

(√
1− (	2/δ2)

)
K(	/δ)K

(√
1− (�p/�s)2

)
where K(x) is the complete elliptic integral of the first kind, defined as

K(x) =
∫ π/2

0

dθ√
1− x2 sin2 θ

and δ2 = 1/
√

1+ δ2 . Values of this integral have been tabulated in a number of
texts [e.g., the books by Jahnke and Emde (1945) and Dwight (1957)]. The passband
ripple is 10 log10(1+ 	2).

We shall not attempt to describe elliptic functions in any detail because such a dis-
cussion would take us too far afield. Suffice to say that computer programs are avail-
able for designing elliptic filters from the frequency specifications indicated above.

In view of the optimality of elliptic filters, the reader may question the reason for
considering the class of Butterworth or the class of Chebyshev filters in practical ap-
plications. One important reason that these other types of filters might be preferable
in some applications is that they possess better phase response characteristics. The
phase response of elliptic filters is more nonlinear in the passband than a comparable
Butterworth filter or a Chebyshev filter, especially near the band edge.

(3.65)

(3.66)

Design of Digital Filters

741



Bessel filters. Bessel filters are a class of all-pole filters that are characterized by the
system function

H(s) = 1
BN(s)

where BN(s) is the N th-order Bessel polynomial. These polynomials can be ex-
pressed in the form

BN(s) =
N∑
k=0

aks
k

where the coefficients {ak} are given as

ak = (2N − k)!2N−kk!(N − k)! , k = 0, 1, . . . , N

Alternatively, the Bessel polynomials may be generated recursively from the relation

BN(s) = (2N − 1)BN−1(s)+ s2BN−2(s)

with B0(s) = 1 and B1(s) = s + 1 as initial conditions.
An important characteristic of Bessel filters is the linear-phase response over the

Magnitude and phase
responses of Bessel and
Butterworth filters of order
N = 4.

N = 4

Bessel

Butterworth

Magnitude

1.0

0.8

0.6

0.4

0.2

10 2 3 4 5

1 2 3 4 5

0 Ω

Ω

Bessel

Butterworth

180

135

90

45

−45

0

−90

−135
−180

Phase

(3.67)

(3.68)

(3.69)

(3.70)

Design of Digital Filters

passband of the filter. For example, Fig. 3.15 shows a comparison of the magnitude

Figure 3.15

742



and phase responses of a Bessel filter and Butterworth filter of order N = 4. We note
that the Bessel filter has a larger transition bandwidth, but its phase is linear within
the passband. However, we should emphasize that the linear-phase chacteristics of
the analog filter are destroyed in the process of converting the filter into the digital
domain by means of the transformations described previously.

Some Examples of Digital Filter Designs Based on the Bilinear
Transformation

In this section we present several examples of digital filter designs obtained from
analog filters by applying the bilinear transformation to convert H(s) to H(z). These
filter designs are performed with the aid of one of several software packages now
available for use on a personal computer.

A lowpass filter is designed to meet specifications of a maximum ripple of 12 dB
in the passband, 60-dB attenuation in the stopband, a passband edge frequency of
ωp = 0.25π , and a stopband edge frequency of ωs = 0.30π .

A Butterworth filter of order N = 37 is required to satisfy the specifications. Its

Figure 3.16 Frequency response characteristics of a 37-order Butter-
worth filter.

Design of Digital Filters

3.5

frequency response characteristics are illustrated in Fig. 3.16. If a Chebyshev filter

743



is used, a filter of order N = 13 satisfies the specifications. The frequency response

passband ripple of 0.31 dB. Finally, an elliptic filter of order N = 7 is designed which

numerical values for the filter parameters, and the resulting frequency specifications

function H(z):

H(z) =
K∏
i=1

b(i, 0)+ b(i, 1)z−1 + b(i, 2)z−2
1+ a(i, 1)z−1 + a(i, 2)z−2

Although we have described only lowpass analog filters in the preceding section, it is a
simple matter to convert a lowpass analog filter into a bandpass, bandstop, or highpass

The
bilinear transformation is then applied to convert the analog filter into an equivalent
digital filter. As in the case of the lowpass filters described above, the entire design
can be carried out on a computer.

Figure 3.17 Frequency response characteristics of a 13-order type I
Chebyshev filter.

(3.71)

Design of Digital Filters

characteristics for a type I Chebyshev filter are shown in Fig. 3.17. The filter has a

also satisfies the specifications. For illustrative purposes, we show in Table 6, the

are shown in Fig. 3.18. The following notation is used for the parameters in the

analog filter by a frequency transformation, as is described in Section 4.

744



TABLE 6 Filter Coefficients for a 7-Order Elliptic Filter

INFINITE IMPULSE RESPONSE (IIR)
ELLIPTIC LOWPASS FILTER

UNQUANTIZED COEFFICIENTS
FILTER ORDER = 7
SAMPLING FREQUENCY = 2.000 KILOHERTZ
I. A(I, 1) A(I, 2) B(I, 0) B(I, 1) B(I, 2)
1 -.790103 .000000 .104948 .104948 .000000
2 -1.517223 .714088 .102450 -.007817 .102232
3 -1.421773 .861895 .420100 -.399842 .419864
4 -1.387447 .962252 .714929 -.826743 .714841

*** CHARACTERISTICS OF DESIGNED FILTER ***
BAND 1 BAND 2

LOWER BAND EDGE .00000 .30000
UPPER BAND EDGE .25000 1.00000
NOMINAL GAIN 1.00000 .00000
NOMINAL RIPPLE .05600 .00100
MAXIMUM RIPPLE .04910 .00071
RIPPLE IN DB .41634 -63.00399

Figure 3.18 Frequency response characteristics of a 7-order elliptic
filter.

Design of Digital Filters

745



4 Frequency Transformations

The treatment in the preceding section is focused primarily on the design of lowpass
IIR filters. If we wish to design a highpass or a bandpass or a bandstop filter, it is a
simple matter to take a lowpass prototype filter (Butterworth, Chebyshev, elliptic,
Bessel) and perform a frequency transformation.

One possibility is to perform the frequency transformation in the analog domain
and then to convert the analog filter into a corresponding digital filter by a mapping
of the s -plane into the z-plane. An alternative approach is first to convert the analog
lowpass filter into a lowpass digital filter and then to transform the lowpass digital
filter into the desired digital filter by a digital transformation. In general, these two
approaches yield different results, except for the bilinear transformation, in which
case the resulting filter designs are identical. These two approaches are described
below.

4.1 Frequency Transformations in the Analog Domain

First, we consider frequency transformations in the analog domain. Suppose that we
have a lowpass filter with passband edge frequency �p and we wish to convert it to
another lowpass filter with passband edge frequency �′p . The transformation that
accomplishes this is

s −→ �p
�′p
s, (lowpass to lowpass)

Thus we obtain a lowpass filter with system function Hl(s) = Hp[(�p/�′p)s], where
Hp(s) is the system function of the prototype filter with passband edge frequency �p .

If we wish to convert a lowpass filter into a highpass filter with passband edge
frequency �′p , the desired transformation is

s −→ �p�
′
p

s
, (lowpass to highpass)

The system function of the highpass filter is Hh(s) = Hp(�p�′p/s).
The transformation for converting a lowpass analog filter with passband edge

frequency �c into a band filter, having a lower band edge frequency �l and an upper
band edge frequency �u , can be accomplished by first converting the lowpass filter
into another lowpass filter having a band edge frequency�′p = 1 and then performing
the transformation

s −→ s
2 +�l�u
s(�u −�l) , (lowpass to bandpass)

Equivalently, we can accomplish the same result in a single step by means of the
transformation

s −→ �p s
2 +�l�u
s(�u −�l) , (lowpass to bandpass)

(4.1)

(4.2)

(4.3)

(4.4)

Design of Digital Filters

746



TABLE 7 Frequency Transformations for Analog Filters (Pro-
totype Lowpass Filter Has Band Edge Frequency �p )

Band edge
Type of frequencies of

transformation Transformation new filter

Lowpass s −→ �p
�′p
s �′p

Highpass s −→ �p�
′
p

s
�′p

Bandpass s −→ �p s
2 +�l�u
s(�u −�l) �l,�u

Bandstop s −→ �p s(�u −�c)
s2 +�u�l �l,�u

where
�l = lower band edge frequency
�u = upper band edge frequency

Thus we obtain

Hb(s) = Hp
(
�p

s2 +�l�u
s(�u −�l)

)

Finally, if we wish to convert a lowpass analog filter with band-edge frequency �p

additional factor�p serving to normalize for the band-edge frequency of the lowpass
filter. Thus the transformation is

s −→ �p s(�u −�l)
s2 +�u�l , (lowpass to bandstop)

which leads to

Hbs(s) = Hp
(
�p
s(�u −�l)
s2 +�u�l

)

Table The
to distort the frequency

response characteristics of the lowpass filter. However, the effects of the nonlinear
ity on the frequency response are minor, primarily affecting the frequency scale
but preserving the amplitude response characteristics of the filter. Thus an equirip
ple lowpass filter is transformed into an equiripple bandpass or bandstop or high
pass filter.

(4.5)

Design of Digital Filters

into a bandstop filter, the transformation is simply the inverse of (4.3) with the

The mappings in (4.1), (4.2), (4.3), and (4.5) are summarized in 7.
mappings in (4.4) and (4.5) are nonlinear and may appear

-

-
-

747



Transform the single-pole lowpass Butterworth filter with system function

H(s) = �p
s +�p

into a bandpass filter with upper and lower band edge frequencies �u and �l , respectively.

Solution.

H(s) = 1
s2 +�l�u
s(�u −�l) + 1

= (�u −�l)s
s2 + (�u −�l)s +�l�u

The resulting filter has a zero at s = 0 and poles at

s =
−(�u −�l)±

√
�2u +�2l − 6�u�l
2

4.2 Frequency Transformations in the Digital Domain

As in the analog domain, frequency transformations can be performed on a digital
lowpass filter to convert it to either a bandpass, bandstop, or highpass filter. The
transformation involves replacing the variable z−1 by a rational function g(z−1),
which must satisfy the following properties:

1. The mapping z−1 −→ g(z−1)must map points inside the unit circle in the z-plane
into itself.

2. The unit circle must also be mapped into itself.

Condition (2) implies that for r = 1,
e−jω = g(e−jω) ≡ g(ω)

= |g(ω)|ejarg[g(ω)]

It is clear that we must have |g(ω)| = 1 for all ω . That is, the mapping must be all
pass. Hence it is of the form

g(z−1) = ±
n∏
k=1

z−1 − ak
1− akz−1

where |ak| < 1 to ensure that a stable filter is transformed into another stable filter
(i.e., to satisfy condition 1).

mations for converting a prototype digital lowpass filter into either a bandpass, a
bandstop, a highpass, or another lowpass digital filter. These transformations are

(4.6)

Design of Digital Filters

EXAMPLE 4.1

The desired transformation is given by (4.4). Thus we have

From the general form in (4.6), we obtain the desired set of digital transfor-

tabulated in Table 8.

748



Frequency Transformation for Digital Filters (Prototype Lowpass
Filter Has Band Edge Frequency ωp )

Type of
transformation Transformation Parameters

Lowpass z−1 −→ z
−1 − a

1− az−1
ω′p = band edge frequency new filter
a = sin[(ωp − ω

′
p)/2]

sin[(ωp + ω′p)/2]

Highpass z−1 −→ − z
−1 + a

1+ az−1
ω′p = band edge frequency new filter
a = − cos[(ωp + ω

′
p)/2]

cos[(ωp − ω′p)/2]

Bandpass z−1 −→ − z
−2 − a1z−1 + a2
a2z−2 − a1z−1 + 1

ωl = lower band edge frequency
ωu = upper band edge frequency
a1 = 2αK/(K + 1)
a2 = (K − 1)/(K + 1)
α = cos[(ωu + ωl)/2]

cos[(ωu − ωl)/2]
K= cot ωu − ωl

2
tan

ωp

2

Bandstop z−1 −→ z
−2 − a1z−1 + a2
a2z−1 − a1z−1 + 1

ωl = lower band edge frequency
ωu= upper band edge frequency
a1 = 2α/(K + 1)
a2 = (1−K)/(1+K)
α = cos[(ωu + ωl)/2]

cos[(ωu − ωl)/2]
K = tan ωu − ωl

2
tan

ωp

2

Convert the single-pole lowpass Butterworth filter with system function

H(z) = 0.245(1+ z
−1)

1− 0.509z−1

into a bandpass filter with upper and lower cutoff frequencies ωu and ωl , respectively. The
lowpass filter has 3-dB bandwidth, ωp

Solution. The desired transformation is

z−1 −→ − z
−2 − a1z−1 + a2
a2z−2 − a1z−1 + 1

Design of Digital Filters

TABLE 8

EXAMPLE 4.2

= 0.2π (see Example 3.5).

749



where a1 and a2

H(z) =
0.245

[
1− z

−2 − a1z−1 + a2
a2z−2 − a1z−1 + 1

]

1+ 0.509
(
z−2 − a1z−1 + a2
a2z−2 − a1z−1 + 1

)

= 0.245(1− a2)(1− z
−2)

(1+ 0.509a2)− 1.509a1z−1 + (a2 + 0.509)z−2

Note that the resulting filter has zeros at z = ±1 and a pair of poles that depend on the choice
of ωu and ωl .

For example, suppose that ωu = 3π/5 and ωl = 2π/5. Since ωp = 0.2π , we find that
K = 1, a2 = 0, and a1 = 0. Then

H(z) = 0.245(1− z
−2)

1+ 0.509z−2

This filter has poles at z = ±j0.713 and hence resonates at ω = π/2.

Since a frequency transformation can be performed either in the analog domain
or in the digital domain, the filter designer has a choice as to which approach to
take. However, some caution must be exercised depending on the types of filters
being designed. In particular, we know that the impulse invariance method and
the mapping of derivatives are inappropriate to use in designing highpass and many
bandpass filters, due to the aliasing problem. Consequently, one would not employ
an analog frequency transformation followed by conversion of the result into the
digital domain by use of these two mappings. Instead, it is much better to perform
the mapping from an analog lowpass filter into a digital lowpass filter by either of
these mappings, and then to perform the frequency transformation in the digital
domain. Thus the problem of aliasing is avoided.

In the case of the bilinear transformation, where aliasing is not a problem, it
does not matter whether the frequency transformation is performed in the analog
domain or in the digital domain. In fact, in this case only, the two approaches result
in identical digital filters.

Summary and References

We have described in some detail the most important techniques for designing FIR
and IIR digital filters based either on frequency-domain specifications expressed in
terms of a desired frequency response Hd(ω) or on the desired impulse response
hd(n).

As a general rule, FIR filters are used in applications where there is a need for
a linear-phase filter. This requirement occurs in many applications, especially in
telecommunications, where there is a requirement to separate (demultiplex) signals
such as data that have been frequency-division multiplexed, without distorting these

Design of Digital Filters

are defined in Table 8. Substitution into H(z) yields

5

750



signals in the process of demultiplexing. Of the several methods described for design-
ing FIR filters, the frequency-sampling design method and the optimum Chebyshev
approximation method yield the best designs.

IIR filters are generally used in applications where some phase distortion is
tolerable. Of the class of IIR filters, elliptic filters are the most efficient to implement
in the sense that for a given set of specifications, an elliptic filter has a lower order
or fewer coefficients than any other IIR filter type. When compared with FIR filters,
elliptic filters are also considerably more efficient. In view of this, one might consider
the use of an elliptic filter to obtain the desired frequency selectivity, followed then
by an all-pass phase equalizer that compensates for the phase distortion in the elliptic
filter. However, attempts to accomplish this have resulted in filters with a number
of coefficients in the cascade combination that equaled or exceeded the number of
coefficients in an equivalent linear-phase FIR filter. Consequently, no reduction in
complexity is achievable in using phase-equalized elliptic filters.

Such a rich literature now exists on the design of digital filters that it is not
possible to cite all the important references. We shall cite only a few. Some of the
early work on digital filter design was done by Kaiser (1963, 1966), Steiglitz (1965),
Golden and Kaiser (1964), Rader and Gold (1967a), Shanks (1967), Helms (1968),
Gibbs (1969, 1970), and Gold and Rader (1969).

The design of analog filters is treated in the classic books by Storer (1957),
Guillemin (1957), Weinberg (1962), and Daniels (1974).

The frequency-sampling method for filter design was first proposed by Gold and
Jordan (1968, 1969), and optimized by Rabiner et al. (1970). Additional results were
published by Herrmann (1970), Herrmann and Schuessler (1970a), and Hofstetter et
al. (1971). The Chebyshev (minimax) approximation method for designing linear-
phase FIR filters was proposed by Parks and McClellan (1972a,b) and discussed
further by Rabiner et al. (1975). The design of elliptic digital filters is treated in the
book by Gold and Rader (1969) and in the paper by Gray and Markel (1976). The
latter includes a computer program for designing digital elliptic filters.

The use of frequency transformations in the digital domain was proposed by
Constantinides (1967, 1968, 1970). These transformations are appropriate only for
IIR filters. The reader should note that when these transformations are applied to a
lowpass FIR filter, the resulting filter is IIR.

Direct design techniques for digital filters have been considered in a number of
papers, including Shanks (1967), Burrus and Parks (1970), Steiglitz (1970), Deczky
(1972), Brophy and Salazar (1973), and Bandler and Bardakjian (1973).

Problems

1 Design an FIR linear-phase, digital filter approximating the ideal frequency response

Hd(ω) =



1, for |ω| ≤ π
6

0, for
π

6
< |ω| ≤ π

(a) Determine the coefficients of a 25-tap filter based on the window method with a
rectangular window.

Design of Digital Filters

751



(b) Determine and plot the magnitude and phase response of the filter.
(c) Repeat parts (a) and (b) using the Hamming window.
(d) Repeat parts (a) and (b) using a Bartlett window.

2

Hd(ω) =




1, for |ω| ≤ π
6

0, for
π

6
< |ω| < π

3

1, for
π

3
≤ |ω| ≤ π

3
4
5

M = 4 for which the frequency response at ω = 0 and ω = π/2 is specified as

Hr(0) = 1, Hr
(π

2

)
= 1

2

6 Determine the coefficients {h(n)} of a linear-phase FIR filter of length M = 15

the condition

Hr

(
2πk
15

)
=
{

1, k = 0, 1, 2, 3
0, k = 4, 5, 6, 7

7 ific
ations

Hr

(
2πk
15

)
=
{ 1, k = 0, 1, 2, 3

0.4, k = 4
0, k = 5, 6, 7

8 The ideal analog differentiator is described by

ya(t) = dxa(t)
dt

where xa(t) is the input and ya(t) the output signal.

(a) Determine its frequency response by exciting the system with the input xa(t) =
ej2πF t .

(b) Sketch the magnitude and phase response of an ideal analog differentiator band-
limited to B hertz.

(c) The ideal digital differentiator is defined as

H(ω) = jω, |ω| ≤ π

Justify this definition by comparing the frequency response |H(ω)|,�H(ω) with
that in part (b).

Design of Digital Filters

Repeat Problem 1 for a bandstop filter having the ideal response

which has a symmetric unit sample response and a frequency response that satisfies

Redesign the filter of Problem 1 using the Hanning and Blackman windows.

Determine the unit sample response {h(n)} of a linear-phase FIR filter of length
Redesign the filter of Problem 2 using the Hanning and Blackman windows.

Repeat the filter design problem in Problem 6 with the frequency response spec -

752



(d) By computing the frequency response H(ω), show that the discrete-time system

y(n) = x(n)− x(n− 1)

is a good approximation of a differentiator at low frequencies.

(e) Compute the response of the system to the input

x(n) = A cos(ω0n+ θ)

9 Use the window method with a Hamming window to design a 21-tap differentiator

resulting filter.

|Hd(ω)|

π

π−π 0
ω

10 Use the bilinear transformation to convert the analog filter with system function

H(s) = s + 0.1
(s + 0.1)2 + 9

into a digital IIR filter. Select T = 0.1 and compare the location of the zeros in H(z)
with the locations of the zeros obtained by applying the impulse invariance method
in the conversion of H(s).

11
means of the bilinear transformation. Thereby derive the digital filter characteristic

transformation applied to the analog filter results in the same digital bandpass filter.
12 An ideal analog integrator is described by the system function Ha(s) = 1/s . A

digital integrator with system function H(z) can be obtained by use of the bilinear
transformation. That is,

H(z) = T
2

1+ z−1
1− z−1 ≡ Ha(s)|s=(2/T )(1−z−1)/(1+z−1)

(a) Write the difference equation for the digital integrator relating the input x(n) to
the output y(n).

(b) Roughly sketch the magnitude |Ha(j�)| and phase�(�) of the analog integrator.

Design of Digital Filters

as shown in Fig. P9. Compute and plot the magnitude and phase response of the

Figure P9

Convert the analog bandpass filter designed in Example 4.1 into a digital filter by

obtained in Example 4.2 by the alternative approach and verify that the bilinear

753



(c) It is easily verified that the frequency response of the digital integrator is

H(ω) = −j T
2

cos(ω/2)
sin(ω/2)

= −j T
2

cot
ω

2

Roughly sketch |H(ω)| and θ(ω).
(d) Compare the magnitude and phase characteristics obtained in parts (b) and (c).

How well does the digital integrator match the magnitude and phase character-
istics of the analog integrator?

(e) The digital integrator has a pole at z = 1. If you implement this filter on a digital
computer, what restrictions might you place on the input signal sequence x(n)
to avoid computational difficulties?

13
has unity gain at dc.

3 zeros
@ z = −1

| z | = 1
2

| z | = 1

60 

−60 

(a) Determine the system function in the form

H(z) = A
[
(1+ a1z−1)(1+ b1z−1 + b2z−2)
(1+ c1z−1)(1+ d1z−1 + d2z−2)

]

giving numerical values for the parameters A, a1 , b1 , b2 , c1 , d1 , and d2 .

(b) Draw block diagrams showing numerical values for path gains in the following
forms:

(a) Direct form II (canonic form)

(b) Cascade form (make each section canonic, with real coefficients)

Design of Digital Filters

A z-plane pole–zero plot for a certain digital filter is shown in Fig. P13. The filter

Figure P13

754



14

| z | = 4
3

| z | = 3
4

| z | = 1

60 

−60 

(a) Does it represent an FIR filter?
(b) Is it a linear-phase system?
(c) Give a direct-form realization that exploits all symmetries to minimize the num-

ber of multiplications. Show all path gains.
15 A digital low-pass filter is required to meet the following specifications:

Passband ripple: ≤ 1 dB
Passband edge: 4 kHz
Stopband attenuation: ≥ 40 dB
Stopband edge: 6 kHz
Sample rate: 24 kHz

The filter is to be designed by performing a bilinear transformation on an analog
system function. Determine what order Butterworth, Chebyshev, and elliptic analog
designs must be used to meet the specifications in the digital implementation.

16 An IIR digital lowpass filter is required to meet the following specifications:

Passband ripple (or peak-to-peak ripple): ≤ 0.5 dB
Passband edge: 1.2 kHz
Stopband attenuation: ≥ 40 dB
Stopband edge: 2.0 kHz
Sample rate: 8.0 kHz

Use the design formulas in the book to determine the required filter order for

(a) A digital Butterworth filter
(b) A digital Chebyshev filter
(c) A digital elliptic filter

Design of Digital Filters

Consider the pole–zero plot shown in Fig. P14.

Figure P14

755



17 Determine the system function H(z) of the lowest-order Chebyshev digital filter that
meets the following specifications:

(a) 1-dB ripple in the passband 0 ≤ |ω| ≤ 0.3π .
(b) At least 60 dB attentuation in the stopband 0.35π ≤ |ω| ≤ π . Use the bilinear

transformation.

18 Determine the system function H(z) of the lowest-order Chebyshev digital filter that
meets the following specifications:

(a) 12 -dB ripple in the passband 0 ≤ |ω| ≤ 0.24π .
(b) At least 50-dB attenuation in the stopband 0.35π ≤ |ω| ≤ π . Use the bilinear

transformation.

19 An analog signal x(t) consists of the sum of two components x1(t) and x2(t). The

is bandlimited to 40 kHz and it is sampled at a rate of 100 kHz to yield the sequence
x(n).

|X(F )|

0 20 40
F

Spectrum of
x2(t)

Spectrum of
x1(t)

Frequency in kilohertz

It is desired to suppress the signal x2(t) by passing the sequence x(n) through
a digital lowpass filter. The allowable amplitude distortion on |X1(f )| is ±2%(δ1 =
0.02) over the range 0 ≤ |F | ≤ 15 kHz. Above 20 kHz, the filter must have an
attenuation of at least 40 dB (δ2 = 0.01).
(a) Use the Remez exchange algorithm to design the minimum-order linear-phase

FIR filter that meets the specifications above. From the plot of the magni-
tude characteristic of the filter frequency response, give the actual specifications
achieved by the filter.

(b) Compare the order M obtained in part (a) with the approximate formulas given

(c) For the orderM obtained in part (a), design an FIR digital lowpass filter using the
window technique and the Hamming window. Compare the frequency response
characteristics of this design with those obtained in part (a).

(d) Design the minimum-order elliptic filter that meets the given amplitude speci-
fications. Compare the frequency response of the elliptic filter with that of the
FIR filter in part (a).

Design of Digital Filters

spectral characteristics of x(t) are shown in the sketch in Fig. P19. The signal x(t)

Figure P19

in equations (2.94) and (2.95).

756



(e) Compare the complexity of implementing the FIR filter in part (a) versus the
elliptic filter obtained in part (d). Assume that the FIR filter is implemented in
the direct form and the elliptic filter is implemented as a cascade of two-pole
filters. Use storage requirements and the number of multiplications per output
point in the comparison of complexity.

ha(t)

0 5 10

5

t

(a) Let h(n) = ha(nT ), where T = 1, be the impulse response of a discrete-time
filter. Determine the system function H(z) and the frequency response H(ω)
for this FIR filter.

(b) Sketch (roughly) |H(ω) and compare this frequency response characteristic with
|Ha(j�)|.

21 In this problem you will be comparing some of the characteristics of analog and
digital implementations of the single-pole low-pass analog system

Ha(s) = α
s + α ⇔ ha(t) = e

−αt

(a) What is the gain at dc? At what radian frequency is the analog frequency re-
sponse 3 dB down from its dc value? At what frequency is the analog frequency
response zero? At what time has the analog impulse response decayed to 1/e of
its initial value?

(b) Give the digital system function H(z) for the impulse-invariant design for this
filter. What is the gain at dc? Give an expression for the 3-dB radian frequency.
At what (real-valued) frequency is the response zero? How many samples are
there in the unit sample time-domain response before it has decayed to 1/e of
its initial value?

(c) “Prewarp” the parameter α and perform the bilinear transformation to obtain
the digital system function H(z) from the analog design. What is the gain at
dc? At what (real-valued) frequency is the response zero? Give an expression
for the 3-dB radian frequency. How many samples are there in the unit sample
time-domain response before it has decayed to 1/e of its initial value?

Design of Digital Filters

The impulse response of an analog filter is shown in Fig. P20.20

Figure P20

757



22 We wish to design a FIR bandpass filter having a durationM = 201. Hd(ω) represents

0.5π−0.4π−0.4π−0.5π 0
ω

Hd(ω)

π−π

(a) Determine the unit sample (impulse) response hd(n) corresponding to Hd(ω).

(b) Explain how you would use the Hamming window

w(n) = 0.54+ 0.46 cos
(

2πn
N − 1

)
, −M − 1

2
≤ n ≤ M − 1

2

to design a FIR bandpass filter having an impulse response h(n) for 0 ≤ n ≤ 200.
(c) Suppose that you were to design the FIR filter with M = 201 by using the

frequency-sampling technique in which the DFT coefficients H(k) are specified
instead of h(n). Give the values of H(k) for 0 ≤ k ≤ 200 corresponding to
Hd(e

jω) and indicate how the frequency response of the actual filter will differ
from the ideal. Would the actual filter represent a good design? Explain your
answer.

23 We wish to design a digital bandpass filter from a second-order analog lowpass But-
terworth filter prototype using the bilinear transformation. The specifications on the

The cutoff frequencies (measured at the
half-power points) for the digital filter should lie at ω = 5π/12 and ω = 7π/12.

The analog prototype is given by

H(s) = 1
s2 +√2s + 1

with the half-power point at � = 1.
(a) Determine the system function for the digital bandpass filter.

(b) Using the same specs on the digital filter as in part (a), determine which of the

directly using the bilinear transformation to give the proper digital filter. Only
the plot of the magnitude squared of the frequency is given.

Design of Digital Filters

the ideal characteristic of the noncausal bandpass filter as shown in Fig. P22.

Figure P22

digital filter are shown in Fig. P23 (a).

analog bandpass prototype filters shown in Fig. P23 (b) could be transformed

758



0

ω

|H(ω)|2

|H(Ω)|2

ππ

1

2

π

6

2
1

200 285

1

2
1

I.

Ω

|H(Ω)|2

111 200

1

2
1

II.

Ω

|H(Ω)|2

300 547

1

2
1

III.

Ω

|H(Ω)|2

600 1019

1

2
1

IV.

Ω

(b)

(a)

power points

Design of Digital Filters

Figure P23

759



24

+

+

+

+

+

+ +

+

+

z −1

z −1
z −12

z −1

+

z −1

z −1

−

x(n)

6
1

12
1

12
1

2
1

−1

−1

−1

−1

−1

2
1

ν(n)

(a) Sketch a z-plane pole–zero plot for this filter.
(b) Is the filter lowpass, highpass, or bandpass?
(c) Determine the magnitude response |H(ω)| at the frequencies ωk = πk/6 for

k = 0, 1, 2, 3, 4, 5, 6.
(d) Use the results of part (c) to sketch the magnitude response for 0 ≤ ω ≤ π and

confirm your answer to part (b).
25 An analog signal of the form xa(t) = a(t) cos 2000πt is bandlimited to the range

A/D
converter

D/A
converterX H(ω)xa(t)

x(n) ω(n) ν(n) â(t)

Rx =    = 2500
1
Tx

cos (0.8 πn)

Design of Digital Filters

Figure P24 shows a digital filter designed using the frequency-sampling method.

Figure P24

900 ≤ F ≤ 1100 Hz. It is used as an input to the system shown in Fig. P25.

Figure P25

760



(a) Determine and sketch the spectra for the signals x(n) and w(n).

(b) Use a Hamming window of length M = 31 to design a lowpass linear phase FIR
filter H(ω) that passes {a(n)}.

(c) Determine the sampling rate of the A/D converter that would allow us to elimi-

26 System identification Consider an unknown LTI system and an FIR system model

The problem is to determine the coefficients {h(n), 0 ≤ n ≤ M−1} of the FIR model
of the system to minimize the average squared error between the outputs of the two
systems.

y(n)

y(n)

+
+

−

FIR
model

Unknown
LTI

system

Minimize
the sum of
squared 

errors

x(n)

ˆ

(a) Determine the equation for the FIR filter coefficients {h(n), 0 ≤ n ≤ M−1} that
minimize the least-squares error

E =
N∑
n=0

[y(n)− ŷ(n)]2

where

ŷ(n) =
M−1∑
k=0

h(k)x(n− k), n = K,K + 1, . . . , N

and N � M .
(b) Repeat part (a) if the output of the unknown system is corrupted by an additive

white noise {w(n)} sequence with variance σ 2w .

Design of Digital Filters

nate the frequency conversion in Fig. P25.

as shown in Fig. P26. Both systems are excited by the same input sequence {x(n)}.

Figure P26

761



27 A linear time-invariant system has an input sequence x(n) and an output sequence
y(n). The user has access only to the system output y(n). In addition, the following
information is available:

The input signal is periodic with a given fundamental period N and has a flat spectral
envelope, that is,

x(n) =
N−1∑
k=0

cxk e
j (2π/N)kn, all n

where cxk = 1 for all k .
The system H(z) is all pole, that is,

H(z) = 1

1+
P∑
k=1

akz
−k

but the order p and the coefficients (ak, 1 ≤ k ≤ p) are unknown. Is it possible to
determine the order p and the numerical values of the coefficients {ak, 1 ≤ k ≤ p}
by taking measurements on the output y(n)? If yes, explain how. Is this possible for
every value of p?

28 FIR system modeling Consider an “unknown” FIR system with impulse response
h(n), 0 ≤ n ≤ 11, given by

h(0) = h(11) = 0.309828× 10−1

h(1) = h(10) = 0.416901× 10−1

h(2) = h(9) = −0.577081× 10−1

h(3) = h(8) = −0.852502× 10−1

h(4) = h(7) = 0.147157× 100

h(5) = h(6) = 0.449188× 100

A potential user has access to the input and output of the system but does not have
any information about its impulse response other than that it is FIR. In an effort to
determine the impulse response of the system, the user excites it with a zero-mean,
random sequence x(n) uniformly distributed in the range [−0.5, 0.5], and records
the signal x(n) and the corresponding output y(n) for 0 ≤ n ≤ 199.
(a) By using the available information that the unknown system is FIR, the user

employs the method of least squares to obtain an FIR model h(n), 0 ≤ n ≤
M − 1. Set up the system of linear equations, specifying the parameters h(0),
h(1), . . . , h(M − 1). Specify formulas we should use to determine the necessary
autocorrelation and crosscorrelation values.

Design of Digital Filters

762



(b) Since the order of the system is unknown, the user decides to try models of
different orders and check the corresponding total squared error. Clearly, this
error will be zero (or very close to it if the order of the model becomes equal to
the order of the system). Compute the FIR models hM(n), 0 ≤ n ≤ M − 1 for
M = 8, 9, 10, 11, 12, 13, 14 as well as the corresponding total squared errors EM ,
M = 8, 9, . . . , 14. What do you observe?

(c) Determine and plot the frequency response of the system and the models for
M = 11, 12, 13. Comment on the results.

(d) Suppose now that the output of the system is corrupted by additive noise, so
instead of the signal y(n), 0 ≤ n ≤ 199, we have available the signal

v(n) = y(n)+ 0.01w(n)
where w(n) is a Gaussian random sequence with zero mean and variance σ 2 = 1.

the results. The quality of the model can be also determined by the quantity

Q =

∞∑
n=0

[h(n)− ĥ(n)]2

∞∑
n=0

h2(n)

29 Filter design by Padé approximation Let the desired impulse response hd(n), n ≥ 0,
of an IIR filter be specified. The filter to be designed to approximate {hd(n)} has the
system function

H(z) =
∑M

k=0 bkz
−k

1+∑Nk=1 akz−k =
∞∑
k=0

h(k)z−k

H(z) has L = M + N + 1 parameters, namely, the coefficients {ak} and {bk} to be
determined. Suppose the input to the filter is x(n) = δ(n). Then, the response of the
filter is y(n) = h(n), and, hence,

h(n) = −a1h(n− 1)− a2h(n− 2)− . . .− aNh(n−N)
+ b0δ(n)+ b1δ(n− 1)+ . . .+ bMδ(n−M) (1)

(a) Show that equation (1) reduces to

h(n) = −a1h(n− 1)− a2h(n− 2)− . . .− aNh(n−N)+ bn, 0 ≤ n ≤ M (2)
(b) Show that for n > M , equation (1) reduces to

h(n) = −a1h(n− 1)− a2h(n− 2)− . . .− aNh(n−N), n > M (3)
(c) Explain how equations (2) and (3) can be used to determine {ak} and {bk} by

letting h(n) = hd(n) for 0 ≤ n ≤ N +M . (This filter design method in which
h(n) exactly matches the desired response hd(n) for 0 ≤ n ≤ M + N is called
the Padé approximation method.)

Design of Digital Filters

Repeat part (b) of Problem 27 by using v(n) instead of y(n) and comment on

763



30 Suppose the desired unit sample response is

hd(n) = 2
(

1
2

)n
u(n)

(b) Compare the frequency response of H(ω) with that of the desired filter response
Hd(ω).

31 Shanks method for least-squares filter design Suppose that we are given the desired
response hd(n), n ≥ 0, and we wish to determine the coefficients {ak} and {bk} of an
IIR filter with system function

H(z) =
∑M

k=0 bkz
−k

1+∑Nk=1 akz−k
such that the sum of squared errors between hd(n) and h(n) is minimized.

(a) If the input to H(z) is x(n) = δ(n), what is the difference equation satisfied by
the filter H(z)?

(b) Show that for n > M , an estimate of hd(n) is

ĥd(n) = −
N∑
k=1

akhd(n− k)

and determine the equations for the coefficients {ak} by minimizing the sum of
squared errors

E1 =
∞∑

n=M+1
[hd(n)− ĥd(n)]2

Thus, the filter coefficients {ak} that specify the denominator of H(z) are deter-
mined.

k

where

H1(z) = 1
1+∑Nk=1 âkz−k

H2(z) =
N∑
k=1

bkz
−k

and {âk} are the coefficients determined in part (b).
If the response of H1(z) to the input δ(n) is denoted as

v(n) = −
N∑
k=1

âkv(n− k)+ δ(n)

Design of Digital Filters

(a) Use the Padé approximation described in Problem 29 to determine h(n).

(c) To determine the parameters {b } consider the system shown in Fig. P31,

764



ν(n)δ(n) all-pole
filter

All-zero 
filter

H1(z) H2(z)

hd(n)

and the output of H2(z) is denoted as ĥd(n), determine the equation for the
parameters {bk} that minimize the sum of squared errors

E2 =
∞∑
n=0

[hd(n)− ĥd(n)]2

(The preceding least-squares method for filter design is due to Shanks (1967).)

32
the parameters {ak} and {bk} of

H(z) =
∑M

k=0 bkz
−k

1+∑Nk=1 akz−k
when the desired response is the impulse response of the three-pole and three-zero
type II lowpass Chebyshev digital filter having a system function

Hd(z) = 0.3060(1+ z
−1)(0.2652− 0.09z−1 + 0.2652z−2)

(1− 0.3880z−1)(1− 1.1318z−1 + 0.5387z−2)
(a) Plot hd(n) and then observe that hd(n) ≈ 0 for n > 50.
(b) Determine the pole and zero positions for the filter H(z) obtained by the Shanks

method for (N,M) = (3, 2), (N,M) = (3, 3) and (N,M) = (4, 3), and compare
these results with the poles and zeros of Hd(z). Comment on the similarities and
differences.

Answers to Selected Problems

1 hd(n) = sin
π
6 (n−12)

π(n−12) ; h(n) = hd(n)w(n)
where w(n) is a rectangular window of length N = 25.

2 hd(n) = δ(n)− sin
π
3 (n−2)

π(n−12) +
sin π6 (n−12)
π(n−12) ; h(n) = hd(n)w(n)

where w(n) is a rectangular window of length 25.

5 Hr(ω) = 2
∑1

n=0 h(n) cos
[
ω
(

3
2 − n

)]
From Hr(0) = 1 and Hr

(
π

2

) = 1/2, we obtain h(0) = 0.073, h(1) = 0.427, h(2) = h(1) and
h(3) = h(0)

7 h(n) = {0.3133, −0.0181, −0.0914, 0.0122, 0.0400, −0.0019, −0.0141, 0.52, 0.52, −0.0141,
−0.0019, 0.0400, 0.0122, −0.0914, −0.0181, 0.3133}

9 hd(n) = cosπ(n− 10)
(n− 10) , 0 ≤ n ≤ 20, n 	= 10

= 0, n = 10
Then h(n) = hd(n)w(n), where w(n) is the Hamming window of length 21.

12 (a) Let T = 2. Then H(z) = 1+z−1
1−z−1 ⇒ y(n) = y(n− 1)+ x(n)+ x(n− 1)

13 H(z) = A
(

1+z−1
)(

1+2z−1+z−2
)

(
1− 12 z−1

)(
1− 12 z−1+ 14 z−2

)
H(z)|z=1 = 1; A = 364 , b1 = 2, b2 = 1, a1 = 1, c1 = − 12 , d1 = − 12 , d2 = 14

Design of Digital Filters

Figure P31

Use the Shanks method for filter design as described in Problem 31 to determine

765



15

Butterworth filter: Nmin ≥ log ηlog k = 9.613⇒ N = 10

Chebyshev filter: Nmin ≥ cosh
−1η

cos h−1k
= 5.212⇒ N = 6

Elliptic filter: Nmin ≥ k(sin α)
k(cosα)

,
k(cos β)
k(sin β)

= 3.78⇒ N = 4

19 (a) MATLAB is used to design the FIR filter using the Remez algorithm. We find that a filter
of length M = 37 meets the specifications. We note that in MATLAB, the frequency scale
is normalized to 12 of the sampling frequency.

(b) δ1 = 0.02, δ2 = 0.01, �f = 20100 − 15100 = 0.05
ˆ −20 log10

(√
δ1δ2

)
−13

14.6�f + 1 ≈ 34
∞(δ1δ2) = 1.7371; f (δ1δ2) = 11.166

and M̂ = D∞(δ1δ2)−f (δ1δ2)(�f )2
�f

+ 1 ≈ 36

21 (a) dc gain: Ha(0) = 1; 3 dB frequency: 	c = α
For all 	 , only H(j∞) = 0; ha(τ ) = 1e ha(0) = 1e ; r = 1α

24 H(z) = 16 (1− z−6)(1− z−1)
(
2+ z−1 + 32 z−α + 12 z−3 + z−4

)
This filter is FIR with zeros at z = 1, e±j π6 , e±j π2 , e±j5 π6 , −0.55528±j0.6823 and 0.3028±j0.7462

25 (a) fL = 9002500 = 0.36; fH = 11002500 = 0.44
hd(n) = 2 sin 0.08π(n−15)(n−15) ; h(n) = hd(n)wH (n)
wH (n) = 0.54− 0.46 cos 2π(n−15)30

Design of Digital Filters

From the design specifications we obtain ε = 0.509, δ = 99.995, fp = 16 , fs = 14

With equation (2.94) we obtain M =
With equation (2.95) we obtain D

Note (2.95) is a better approximation of M .

766



John G. Proakis, Dimitris G. Manolakis. Copyright © 2007 by Pearson Education, Inc. All rights reserved.
From Chapter 11 of Digital Signal Processing: Principles, Algorithms, and Applications, Fourth Edition.

Multirate Digital Signal
Processing

767



Multirate Digital Signal
Processing

In many practical applications of digital signal processing, one is faced with the
problem of changing the sampling rate of a signal, either increasing it or decreasing
it by some amount. For example, in telecommunication systems that transmit and
receive different types of signals (e.g., teletype, facsimile, speech, video, etc.), there
is a requirement to process the various signals at different rates commensurate with
the corresponding bandwidths of the signals. The process of converting a signal from
a given rate to a different rate is called sampling rate conversion. In turn, systems
that employ multiple sampling rates in the processing of digital signals are called
multirate digital signal processing systems.

Sampling rate conversion of a digital signal can be accomplished in one of two
general methods. One method is to pass the digital signal through a D/A converter,
filter it if necessary, and then to resample the resulting analog signal at the desired
rate (i.e., to pass the analog signal through an A/D converter). The second method
is to perform the sampling rate conversion entirely in the digital domain.

One apparent advantage of the first method is that the new sampling rate can be
arbitrarily selected and need not have any special relationship to the old sampling
rate. A major disadvantage, however, is the signal distortion, introduced by the D/A
converter in the signal reconstruction, and by the quantization effects in the A/D
conversion. Sampling rate conversion performed in the digital domain avoids this
major disadvantage.

In this chapter we describe sampling rate conversion and multirate signal pro-
cessing in the digital domain. First we describe sampling rate conversion by a rational
factor and present several methods for implementing the rate converter, including
single-stage and multistage implementations. Then, we describe a method for sam-
pling rate conversion by an arbitrary factor and discuss its implementation. We

768



present several applications of sampling rate conversion in multirate signal process-
ing systems, which include the implementation of narrowband filters, digital filter
banks, subband coding, transmultiplexers, and quadrature mirror filters.

1

The process of sampling rate conversion can be developed and understood using the
idea of “resampling after reconstruction.” In this theoretical approach, a discrete-
time signal is ideally reconstructed and the resulting continuous-time signal is resam-
pled at a different sampling rate. This idea leads to a mathematical formulation that
enables the realization of the entire process by means of digital signal processing.

Let x(t) be a continuous-time signal that is sampled at a rate Fx = 1/Tx to
generate a discrete-time signal x(nTx). From the samples x(nTx) we can generate a
continuous-time signal using the interpolation formula

y(t) =
∞∑

n=−∞
x(nTx)g(t − nTx)

If the bandwidth of x(t) is less than Fx/2 and the interpolation function is given by

g(t) = sin(πt/Tx)
πt/Tx

F←→ G(F) =
{
Tx, |F | ≤ Fx/2
0, otherwise

then y(t) = x(t); otherwise y(t) �= x(t). In practice, perfect recovery of x(t) is not

summation.

t = mTy , where Fy = 1/Ty is the desired sampling frequency. Therefore, the general
formula for sampling rate conversion becomes

y(mTy) =
∞∑

n=−∞
x(nTx)g(mTy − nTx)

which expresses directly the samples of the desired sequence in terms of the sam-
ples of the original sequence and sampled values of the reconstruction function at
positions (mTy − nTx). The computation of y(nTy) requires (a) the input sequence
x(nTx), (b) the reconstruction function g(t), and (c) the time instants nTx and mTy
of the input and output samples. The values y(mTy) calculated by this equation are
accurate only if Fy > Fx . If Fy < Fx , we should filter out the frequency components
of x(t) above Fy/2 before resampling in order to prevent aliasing. Therefore, the

y y

and X(F) = 0 for |F | ≥ min{Fx/2, Fy/2}.
If Ty = Tx

sponds to an LTI system. y �= Tx , we
rearrange the argument of g(t) as follows:

y(mTy) =
∞∑

n=−∞
x(nTx)g

(
Tx

(
mTy

Tx
− n

))

(1.1)

(1.2)

(1.3)

(1.4)

Multirate Digital Signal Processing

Introduction

possible because the infinite summation in (1.1) should be replaced by a finite

To perform sampling rate conversion we simply evaluate (1.1) at time instants

sampling rate conversion yields y(mT ) = x(mT ) if we use (1.2)

, equation (1.3) becomes a convolution summation, which corre-
To understand the meaning of (1.3) for T

formula (1.3)

769



Multirate Digital Signal Processing

The term mTy/Tx can be decomposed into an integer part km and a fractional part
�m , 0 ≤ �m < 1, as

mTy

Tx
= km +�m

where

km =
⌊
mTy

Tx

⌋

and

�m = mTy
Tx
−

⌊
mTy

Tx

⌋

The symbol �a� denotes the largest integer contained in a . The quantity�m specifies
the position of the current sample within the sample period Tx

y(mTy) =
∞∑

n=−∞
x(nTx)g((km +�m − n)Tx)

m − n, we have

y(mTy) = y((km +�m)Tx)

=
∞∑

k=−∞
g(kTx +�mTx)x((km − k)Tx)

mentation of sampling rate conversion.
We note that (a) given Tx and Ty the input and output sampling times are fixed,
(b) the function g(t) is shifted for each m such that the value g(�mTx) is positioned
at t = mTy , and (c) the required values of g(t) are determined at the input sampling
times. For each value of m, the fractional interval �m determines the impulse re-
sponse coefficients whereas the index km specifies the corresponding input samples

km Tx 

m Ty (m+ 1)  Ty (m−1) Ty 

(km+1)Tx (km+2)Tx (km −1)Tx 

km Tx
=











∆mTx

t

Output sampling 
times

Input sampling 
times

  

x(t)

g(∆mTx)

g(t)

Fractional interval

mT y

Illustration of timing relations for sampling rate conversion.

(1.5)

(1.6)

(1.7)

(1.8)

(1.9)

. Substituting (1.5)
into (1.4) we obtain

If we change the index of summation in (1.8) from n to k = k

Equation (1.9) provides the fundamental equation for the discrete-time imple-
This process is illustrated in Figure 1.1.

Figure 1.1

770



Figure 1.2
Discrete-time linear
time-varying system for
sampling rate conversion. x(n)

x(nTx) y(mTy)

gm(n)

g(nTx + ∆mTx)

y(m)

needed to compute the sample y(mTy). Since for any given value of m the index km is
an integer number, y(mTy) is the convolution between the input sequence x(nTx) and
an impulse response g ((n+�m) Tx). is
that the first shifts a “changing” reconstruction function whereas the second shifts
the “fixed” input sequence.

and time-varying system, because it requires a different impulse response

gm(nTx) = g((n+�m)Tx)

for each output sample y(mTy). Therefore, a new set of coefficients should be com-
puted or retrieved from storage for the computation of every output sample (see

cated and the number of required values is large. This dependence onm prohibits the
use of recursive structures because the required past output values must be computed
using an impulse response for the present value of �m .

A significant simplification results when the ratio Ty/Tx is constrained to be a
rational number, that is,

Ty

Tx
= Fx
Fy
= D
I

where D and I are relatively prime integers. To this end, we express the offset �m as

�m = mD
I
−

⌊
mD

I

⌋
= 1
I

(
mD −

⌊
mD

I

⌋
I

)
= 1
I
(mD)I

where (k)I m can
take on only I unique values 0, 1/I, . . . , (I − 1)/I , so that there are only I distinct
impulse responses possible. Since gm(nTx) can take on I distinct sets of values, it is
periodic in m; that is,

gm(nTx) = gm+rI (nTx), r = 0,±1,±2, . . .

Thus the system gm(nTx) is a linear and periodically time-varying discrete-time sys-
tem. Such systems have been widely studied for a wide range of applications (Meyers
and Burrus, 1975). This is a great simplification compared to the continuously time-

To illustrate these concepts we consider two important special cases. We start
with the process of reducing the sampling rate by an integer factor D , which is known
as decimation or downsampling. If we set Ty = DTx

y(mTy) = y(mDTx) =
∞∑

k=−∞
x(kTx)g((mD − k)Tx)

(1.10)

(1.11)

(1.12)

(1.13)

(1.14)

Multirate Digital Signal Processing

The difference between (1.8) and (1.9)

The sampling rate conversion process defined by (1.9) is a discrete-time linear

Figure 1.2). This procedure may be inefficient when the function g(t) is compli-

denotes the value of k modulo I . From (1.12) it is clear that �

varying discrete-time system in (1.10).

in (1.3), we have

771



We note that the input signal and the impulse response are sampled with a period
Tx . However, the impulse response is shifted at increments of Ty = DTx because we
need to compute only one out of every D samples. Since I = 1, we have �m = 0
and therefore there is only one impulse response g(nTx), for all m. This process is

We consider now the process of increasing the sampling rate by an integer factor
I , which is called upsampling or interpolation. If we set Ty x
have

y(mTy) =
∞∑

k=−∞
x(kTx)g(m(Tx/I)− kTx)

We note that both x(t) and g(t) are sampled with a period Tx ; however, the impulse
response is shifted at increments of Ty = Tx/I for the computation of each output
sample. This is required to “fill-in” an additional number of (I − 1) samples within
each period Tx . = 2. Each “frac-
tional shifting” requires that we resample g(t), resulting in a new impulse response
gm(nTx) = g(nTx x

sequence g(nTy) and create a new sequence v(nTy) by inserting (I −1) zero samples
between successive samples of x(nTx), we can compute y(mTy) as the convolution
of the sequences g(nTy) and x(nTy).
I = 2.

In the next few sections we discuss in more detail the properties, design, and
structures for the implementation of sampling rate conversion entirely in the discrete-
time domain. For convenience, we usually drop the sampling periods Tx and Ty from
the argument of discrete-time signals. However, occasionally, it will be beneficial to
the reader to reintroduce and think in terms of continuous-time quantities and units.

t = mTy t = (m + 1)Ty

(m − 1)Ty

(n − 1)Tx nTx (n + 1)Tx (n + 2)Tx

mTy (m + 1)Ty

t

x(t)

g(t)

t

y(t)

by an integer factor D = 2. A single impulse response, sampled with
period Tx y = DTx to generate the output
samples.

(1.15)

Multirate Digital Signal Processing

illustrated in Figure 1.3 for D = 2.

= T /I in (1.3), we

This is illustrated in Figure 1.4(a)–(b) for I

+mT /I), form = 0, 1, . . . , I−1 in agreement with (1.14). Care-
ful inspection of Figure 1.4(a)–(b) shows that if we determine an impulse response

This idea is illustrated in Figure 1.4(c) for

Illustration of timing relations for sampling rate decrease

, is shifted at steps equal to T

Figure 1.3

772



mTy 

t = m Ty 

(m +1) Ty 

t = (m + 1) Ty

(m − 1) Ty 

nTx (n + 1)Tx (n + 2)Tx (n−1)Tx t

x(t)

g(t)

t

y(t)

mTy (m + 2)Ty t

x(t)

(a)

(b)

(c)

g0(nTx)

g(nTy)

y(nTy)

x(nTx)

v(nTy)

g1(nTx)

Figure 1.4 Illustration of timing relations for sampling rate increase
by an integer factor I = 2. The approach in (a) requires one impulse
response for the even-numbered and one for the odd-numbered output
samples. The approach in (c) requires only one impulse response, ob-
tained by interleaving the impulse responses in (a).

2 Decimation by a Factor D

Let us assume that the signal x(n) with spectrum X(ω) is to be downsampled by an
integer factor D . The spectrum X(ω) is assumed to be nonzero in the frequency
interval 0 ≤ |ω| ≤ π or, equivalently, |F | ≤ Fx/2. We know that if we reduce the
sampling rate simply by selecting every D th value of x(n), the resulting signal will
be an aliased version of x(n), with a folding frequency of Fx/2D . To avoid aliasing,
we must first reduce the bandwidth of x(n) to Fmax = Fx/2D or, equivalently, to
ωmax = π/D . Then we may downsample by D and thus avoid aliasing.

passed through a lowpass filter, characterized by the impulse response h(n) and a
frequency response HD(ω), which ideally satisfies the condition

HD(ω) =
{

1, |ω| ≤ π/D
0, otherwise

Thus the filter eliminates the spectrum of X(ω) in the range π/D < ω < π . Of
course, the implication is that only the frequency components of x(n) in the range
|ω| ≤ π/D are of interest in further processing of the signal.

(2.1)

Multirate Digital Signal Processing

The decimation process is illustrated in Fig. 2.1. The input sequence x(n) is

773



x(n) �(n)
h(n)

y(m)

Fx = 1
Tx

Fy = Fx
D

Downsampler
  D↓

Figure 2.1 Decimation by a factor D .

The output of the filter is a sequence v(n) given as

v(n) =
∞∑
k=0

h(k)x(n− k)

which is then downsampled by the factor D to produce y(m). Thus

y(m) = v(mD)

=
∞∑
k=0

h(k)x(mD − k)

Although the filtering operation on x(n) is linear and time invariant, the down-
sampling operation in combination with the filtering results in a time-variant system.
This is easily verified. Given the fact that x(n) produces y(m), we note that x(n−n0)
does not imply y(n−n0) unless n0 is a multiple of D . Consequently, the overall linear
operation (linear filtering followed by downsampling) on x(n) is not time invariant.

The frequency-domain characteristics of the output sequence y(m) can be ob-
tained by relating the spectrum of y(m) to the spectrum of the input sequence x(n).
First, it is convenient to define a sequence ṽ(n) as

ṽ(n) =
{
v(n), n = 0,±D,±2D, . . .
0, otherwise

Clearly, ṽ(n) can be viewed as a sequence obtained by multiplying v(n)with a periodic

series representation of p(n) is

p(n) = 1
D

D−1∑
k=0

ej2πkn/D

Hence

ṽ(n) = v(n)p(n)

and

y(m) = ṽ(mD) = v(mD)p(mD) = v(mD)

(2.2)

(2.3)

(2.4)

(2.5)

(2.6)

(2.7)

Multirate Digital Signal Processing

train of impulses p(n), with periodD , as illustrated in Fig. 2.2. The discrete Fourier

774



Figure 2.2
Steps required to facilitate
the mathematical
description of
downsampling by a factor
D , using a sinusoidal
sequence for illustration.

0

0

0

0

3

3

3

3

n

n

n

n

6

6

6

6

v(n)

p(n)

v(n) = v(n)p(n)

y(n) = v(nD) = v(nD)

~

~

Now the z-transform of the output sequence y(m) is

Y (z) =
∞∑

m=−∞
y(m)z−m

=
∞∑

m=−∞
ṽ(mD)z−m

Y (z) =
∞∑

m=−∞
ṽ(m)z−m/D

where the last step follows from the fact that ṽ(m) = 0, except at multiples of D . By

Y (z) =
∞∑

m=−∞
v(m)

[
1
D

D−1∑
k=0

ej2πmk/D

]
z−m/D

= 1
D

D−1∑
k=0

∞∑
m=−∞

v(m)(e−j2πk/Dz1/D)−m

= 1
D

D−1∑
k=0

V (e−j2πk/Dz1/D)

= 1
D

D−1∑
k=0

HD(e
−j2πk/Dz1/D)X(e−j2πk/Dz1/D)

where the last step follows from the fact that V (z) = HD(z)X(z).

(2.8)

(2.9)

Multirate Digital Signal Processing

making use of the relations in (2.5) and (2.6) in (2.8), we obtain

775



By evaluating Y (z) in the unit circle, we obtain the spectrum of the output signal
y(m). Since the rate of y(m) is Fy = 1/Ty , the frequency variable, which we denote
as ωy , is in radians and is relative to the sampling rate Fy ,

ωy = 2πF
Fy
= 2πFTy

Since the sampling rates are related by the expression

Fy = Fx
D

it follows that the frequency variables ωy and

ωx = 2πF
Fx
= 2πFTx

are related by
ωy = Dωx

Thus, as expected, the frequency range 0 ≤ |ωx | ≤ π/D is stretched into the corre-
sponding frequency range 0 ≤ |ωy | ≤ π by the downsampling process.

y

on the unit circle, can be expressed as

Y (ωy) = 1
D

D−1∑
k=0

HD

(
ωy − 2πk

D

)
X

(
ωy − 2πk

D

)

With a properly designed filter HD(ω), the aliasing is eliminated and, consequently,

Y (ωy) = 1
D
HD

(ωy
D

)
X

(ωy
D

)

= 1
D
X

(ωy
D

)

for 0 ≤ |ωy | ≤ π . The spectra for the sequences x(n), v(n), and y(m) are illustrated

EXAMPLE 2.1

Design a decimator that downsamples an input signal x(n) by a factor D = 2. Use the Remez
algorithm to determine the coefficients of the FIR filter that has a 0.1-dB ripple in the passband
and is down by at least 30 dB in the stopband.

Solution. A filter length M = 30 achieves the design specifications given above. The
c = π/2.

(2.10)

(2.11)

(2.12)

(2.13)

(2.14)

(2.15)

Multirate Digital Signal Processing

We conclude that the spectrum Y (ω ), which is obtained by evaluating (2.9)

all but the first term in (2.14) vanish. Hence

in Fig. 2.3.

frequency response is illustrated in Fig. 2.4. Note that the cutoff frequency is ω

776



Spectra of signals in the
decimation of x(n) by a
factor D .

ωx0

X(ωx)

ωx0

HD(ωx)

ωx0

V(ωx)

ωy0

Y(ωy)

−π

−π

−π

π

−π π

π

D
− π

D

π

D
− π

D

Magnitude response of linear-phase FIR filter of length M =

Multirate Digital Signal Processing

Figure 2.3

Figure 2.4
30 in Example 2.1.

777



3 Interpolation by a Factor I

An increase in the sampling rate by an integer factor of I can be accomplished
by interpolating I − 1 new samples between successive values of the signal. The
interpolation process can be accomplished in a variety of ways. We shall describe a
process that preserves the spectral shape of the signal sequence x(n).

Let v(m) denote a sequence with a rate Fy = IFx , which is obtained from x(n)
by adding I − 1 zeros between successive values of x(n). Thus

v(m) =
{
x(m/I), m = 0,±I,±2I, . . .
0, otherwise

and its sampling rate is identical to the rate of y(m). This sequence has a z-transform

V (z) =
∞∑

m=−∞
v(m)z−m

=
∞∑

m=−∞
x(m)z−mI

= X(zI )

circle. Thus
V (ωy) = X(ωyI)

where ωy denotes the frequency variable relative to the new sampling rate Fy (i.e.,
ωy = 2πF/Fy ). Now the relationship between sampling rates is Fy = IFx and hence,
the frequency variables ωx and ωy are related according to the formula

ωy = ωx
I

x y We observe that the
sampling rate increase, obtained by the addition of I − 1 zero samples between
successive values of x(n), results in a signal whose spectrum V (ωy) is an I -fold
periodic repetition of the input signal spectrum X(ωx).

Since only the frequency components of x(n) in the range 0 ≤ ωy ≤ π/I are
unique, the images of X(ω) above ωy = π/I should be rejected by passing the
sequence v(m) through a lowpass filter with frequency response HI(ωy) that ideally
has the characteristic

HI(ωy) =
{
C, 0 ≤ |ωy | ≤ π/I
0, otherwise

where C is a scale factor required to properly normalize the output sequence y(m).
Consequently, the output spectrum is

Y (ωy) =
{
CX(ωyI), 0 ≤ |ωy | ≤ π/I
0, otherwise

(3.1)

(3.2)

(3.3)

(3.4)

(3.5)

(3.6)

Multirate Digital Signal Processing

The corresponding spectrum of v(m) is obtained by evaluating (3.2) on the unit

The spectra X(ω ) and V (ω ) are illustrated in Fig. 3.1.

778



Spectra of x(n) and v(n)
where V (ωy) = X(ωyl).

0 π ωx−π

0 π ωy−π π
I

π

I
−

π

I
π

I
−

0 ωyπ
I

2π
I

0 π ωy−π

|X(ωx)|

|V(ωy)|

|HI(ωy)|

|Y(ωy)|

2π
I

− π
I

−

I = 2

The scale factor C is selected so that the output y(m) = x(m/I) for m = 0, ±I ,
+2I, . . . . For mathematical convenience, we select the point m = 0. Thus

y(0) = 1
2π

∫ π
−π
Y (ωy)dωy

= C
2π

∫ π/I
−π/I

X(ωyI)dωy

Since ωy x

y(0) = C
I

1
2π

∫ π
−π
X(ωx)dωx

= C
I
x(0)

Therefore, C = I is the desired normalization factor.
Finally, we indicate that the output sequence y(m) can be expressed as a convo-

lution of the sequence v(n) with the unit sample response h(n) of the lowpass filter.
Thus

y(m) =
∞∑

k=−∞
h(m− k)v(k)

(3.7)

(3.8)

(3.9)

Multirate Digital Signal Processing

Figure 3.1

= ω /I , (3.7) can be expressed as

779



Magnitude response of linear-phase FIR filter of length M =

y(m) =
∞∑

k=−∞
h(m− kI)x(k)

Design a interpolator that increases the input sampling rate by a factor of l = 5. Use the
Remez algorithm to determine the coefficients of the FIR filter that has a 0.1-dB ripple in the
passband and is down by at least 30 dB in the stopband.

Solution. A filter length M = 30 achieves the design specifications given above. The

is ωc = π/5.

Sampling Rate Conversion by a Rational Factor I /D

Having discussed the special cases of decimation (downsampling by a factor D)
and interpolation (upsampling by a factor I ), we now consider the general case
of sampling rate conversion by a rational factor I/D . Basically, we can achieve
this sampling rate conversion by first performing interpolation by the factor I and
then decimating the output of the interpolator by the factor D . In other words, a
sampling rate conversion by the rational factor I/D is accomplished by cascading an

We emphasize that the importance of performing the interpolation first and
the decimation second is to preserve the desired spectral characteristics of x(n).

with impulse response {hu(k)} and {hd(k)} are operated at the same rate, namely IFx ,
and hence can be combined into a single lowpass filter with impulse response h(k)

The frequency response H(ωv) of the combined filter

(3.10)

Multirate Digital Signal Processing

Figure 3.2
30 in Example 3.1.

Since v(k) = 0 except at multiples of I , where v(kI) = x(k), (3.9) becomes

EXAMPLE 3.1

frequency response of the FIR filter is illustrated in Fig. 3.2. Note that the cutoff frequency

4

interpolator with a decimator, as illustrated in Fig. 4.1.

Furthermore, with the cascade configuration illustrated in Fig. 4.1, the two filters

as illustrated in Fig. 4.2.

780



x(n) y(m)

Interpolator Decimator

Upsampler
↑I

Filter
hu(k)

Filter
hd(k)

Downsampler
  D↓Rate Fx

Rate = I Fx Rate = I
D

Fx = Fy

Method for sampling rate conversion by a factor I/D .

must incorporate the filtering operations for both interpolation and decimation, and
hence it should ideally possess the frequency response characteristic

H(ωv) =
{
I, 0 ≤ |ωv| ≤ min(π/D, π/I)
0, otherwise

where ωv = 2πF/Fv = 2πF/IFx = ωx/I .
In the time domain, the output of the upsampler is the sequence

v(l) =
{
x(l/I ), l = 0,±I,±2I, . . .
0, otherwise

and the output of the linear time-invariant filter is

w(l) =
∞∑

k=−∞
h(l − k)v(k)

=
∞∑

k=−∞
h(l − kI)x(k)

Finally, the output of the sampling rate converter is the sequence {y(m)}, which is

x(n) ν(k) w(l) y(m)Upsampler
↑I

Lowpass
filter
h(k)

Downsampler
  D↓Rate = Fx

Rate = IFx = Fν

Rate= I
D

Fx= F y

Method for sampling rate conversion by a factor I/D .

(4.1)

(4.2)

(4.3)

Multirate Digital Signal Processing

Figure 4.1

Figure 4.2

781



obtained by downsampling the sequence {w(l)} by a factor of D . Thus

y(m) = w(mD)

=
∞∑

k=−∞
h(mD − kI)x(k)

variable. Let

k =
⌊
mD

I

⌋
− n

where the notation �r� denotes the largest integer contained in r . With this change

y(m) =
∞∑

n=−∞
h

(
mD −

⌊
mD

I

⌋
I + nI

)
x

(⌊
mD

I

⌋
− n

)

We note that

mD −
⌊
mD

I

⌋
I = mD, modulo I

= (mD)I

y(m) =
∞∑

n=−∞
h(nI + (mD)I )x

(⌊
mD

I

⌋
− n

)

It is apparent from this form that the output y(m) is obtained by passing the
input sequence x(n) through a time-variant filter with impulse response

g(n,m) = h(nI + (mD)I ), −∞ < m, n <∞

where h(k) is the impulse response of the time-invariant lowpass filter operating at
the sampling rate IFx . We further observe that for any integer k ,

g(n,m+ kI) = h(nI + (mD + kDI)I )
= h(nI + (mD)I )
= g(n,m)

Hence g(n,m) is periodic in the variable m with period I .

(4.4)

(4.5)

(4.6)

(4.7)

(4.8)

(4.9)

Multirate Digital Signal Processing

It is illuminating to express (4.4) in a different form by making a change in

in variable, (4.4) becomes

Consequently, (4.6) can be expressed as

which is the discrete-time version of (1.9).

782



The frequency-domain relationships can be obtained by combining the results
of the interpolation and decimation processes. Thus the spectrum at the output of
the linear filter with impulse response h(l) is

V (ωv) = H(ωv)X(ωvI)

=
{
IX(ωvI), 0 ≤ |ωv| ≤ min(π/D, π/I)
0, otherwise

The spectrum of the output sequence y(m), obtained by decimating the sequence
v(n) by a factor of D , is

Y (ωy) = 1
D

D−1∑
k=0

V

(
ωy − 2πk

D

)

where ωy v

y



I

D
X

(ωy
D

)
, 0 ≤ |ωy | ≤ min

(
π,
πD

I

)
0, otherwise

Design a sample rate converter that increases the sampling rate by a factor of 2.5. Use the
Remez algorithm to determine the coefficients of the FIR filter that has a 0.1-dB ripple in
the passband and is down by at least 30 dB in the stopband. Specify the sets of time-varying
coefficients g(n,m) used in the realization of the sampling rate converter.

Solution. The FIR filter that meets the specifications of this problem is exactly the same as

g(n,m) = h(nI + (mD)I )

= h
(
nI +mD − �mD

I
�I

)

By substituting I = 5 and D = 2 we obtain

g(n,m) = h
(

5n+ 2m− 5�2m
5
�
)

By evaluating g(n,m) for n = 0, 1, . . . , 5 and m = 0, 1, . . . , 4 we obtain the following coeffi-
cients for the time-variant filter:

g(0,m) = {h(0) h(2) h(4) h(1) h(3) }
g(1,m) = {h(5) h(7) h(9) h(6) h(8) }
g(2,m) = {h(10) h(12) h(14) h(11) h(13)}
g(3,m) = {h(15) h(17) h(19) h(16) h(18)}
g(4,m) = {h(20) h(22) h(24) h(21) h(23)}
g(0,m) = {h(25) h(27) h(29) h(26) h(28)}

(4.10)

(4.11)

(4.12)

Multirate Digital Signal Processing

= Dω . Since the linear filter prevents aliasing as implied by (4.10), the
spectrum of the output sequence given by (4.11) reduces to

Y (ω ) =

EXAMPLE 4.1

the filter designed in Example 3.1. Its bandwidth is π/5.
The coefficients of the FIR filter are given by (4.8) as

783



In summary, sampling rate conversion by the factor I/D can be achieved by
first increasing the sampling rate by I , accomplished by inserting I − 1 zeros be-
tween successive values of the input signal x(n), followed by linear filtering of the
resulting sequence to eliminate unwanted images of X(ω) and, finally, by downsam-
pling the filtered signal by the factor D . When Fy > Fx , the lowpass filter acts as
an anti-imaging postfilter that removes the spectral replicas at multiples of Fx , but
not at multiples of IFx . When Fy < Fx , the lowpass filter acts as an anti-aliasing
prefilter that removes the down-shifted spectral replicas at multiples of Fy to avoid
overlapping.

5 Implementation of Sampling Rate Conversion

In this section we consider the efficient implementation of sampling rate conversion
systems using polyphase filter structures. Additional computational simplifications

Polyphase Filter Structures

Polyphase structures for FIR filters were developed for the efficient implementation
of sampling rate converters; however, they can be used in other applications. The
polyphase structure is based on the fact that any system function can be split as

H(z) = · · · + h(0) + h(M)z−M + · · ·
· · · + h(1)z−1 + h(M + 1)z−(M+1) + · · ·

...

· · · + h(M − 1)z−(M−1) + h(2M − 1)z−(2M−1) + · · ·
If we next factor out the term z−(i−1) at the i th row, we obtain

H(z) = [· · · + h(0) + h(M)z−M + · · ·]
+ z−1[· · · + h(1) + h(M + 1)z−M + · · ·]

...

+ z−(M−1)[· · · + h(M − 1)+ h(2M − 1)z−M + · · ·]
The last equation can be compactly expressed as

H(z) =
M−1∑
i=0

z−iPi(zM)

where

Pi(z) =
∞∑

n=−∞
h(nM + i)z−n

(5.1)

(5.2)

Multirate Digital Signal Processing

can be obtained using the multistage approach described in Section 6.

5.1

784



z−1

z−1

x(n) y(n)
P0(z3)

P1(z3)

P2(z3)

Block diagram of polyphase filter structure for M = 3.

i

polyphase components of H(z). Each subsequence

pi(n) = h(nM + i), i = 0, 1, . . . ,M − 1

is obtained by downsampling a delayed (“phase-shifted”) version of the original
impulse response.

to express the z-transform of the output sequence as

Y (z) = H(z)X(z)
= P0(z3)X(z)+ z−1P1(z3)X(z)+ z−2P2(z3)X(z)
= P0(z3)X(z)+ z−1{P1(z3)X(z)+ z−1[P2(z3)X(z)]}

This is known as transpose
polyphase structure because it is similar to the transpose FIR filter realization. The
obtained polyphase structures are valid for any filter, FIR or IIR, and any finite value
of M and are sufficient for our needs. Additional structures and details can be found
in Vaidyanathan (1993).

5.2 Interchange of Filters and Downsamplers/Upsamplers

In general, the order of a sampling rate converter (which is a linear time-varying
system) and a linear time-invariant system cannot be interchanged. We next derive
two identities, known as noble identities, that help to swap the position of a filter with
a downsampler or an upsampler by properly modifying the filter.

(5.3)

(5.4)

(5.5)

Multirate Digital Signal Processing

Figure 5.1

Relation (5.1) is called the M-component polyphase decomposition and P (z) the

To develop an M -component polyphase filter structure, we use (5.1) for M = 3

Equation (5.4) leads to the polyphase structure of Figure 5.1. Similarly, (5.5)
yields the polyphase structure shown in Figure 5.2.

785



z−1

z−1

x(n) y(n)
P0(z3)

P1(z3)

P2(z3)

Figure 5.2 Illustration of transpose polyphase filter structure for M = 3.

description of a downsampler is

y(n) = x(nD) Z←→ Y (z) = 1
D

D−1∑
i=0

X(z1/DW iD)

where WD = e−j2π/D

Y (z) = 1
D

D−1∑
i=0

V1(z
1/DWiD) =

1
D

D−1∑
i=0

H(zWiDD )X(z
1/DWiD)

because V1(z) = H(zD)X(z). Taking into consideration WiDD = 1 and Figure

Y (z) = 1
D
H(z)

D−1∑
i=0

X(z1/DWiD) = H(z)V2(z)

Two equivalent
downsampling systems (first
noble identity).

x(n) y(m)
H(zD) D

x(n) v1(m)

v2(n)

y(m)
H(z)D

(a)

(b)

(5.6)

(5.7)

(5.8)

Multirate Digital Signal Processing

To prove the first identity (see Figure 5.3), we recall that the input/output

. The output of the system in Figure 5.3(a) can be written as

which shows the equivalence of the two structures in Figure 5.3.

Figure 5.3

5.3(b), relation (5.7) results in

786



Two equivalent upsampling
systems (second noble
identity).

x(n) y(m)
H(zI)

x(n) v1(n)

v2(m)

y(m)
H(z) I

I

(a)

(b)

A similar identity can be shown to hold for upsampling. To start, we recall that
the input/output description of an upsampler is

y(n) =
{
x
(
n
I

)
, n = 0,±I,±2I, . . .

0, otherwise
Z←→ Y (z) = X(zI )

Y (z) = H(zI )V1(z) = H(zI )X(zI )

1
I

Y (z) = V2(zI ) = H(zI )X(zI )

identical.
In conclusion, we have shown that it is possible to interchange the operation of

linear filtering and downsampling or upsampling if we properly modify the system
function of the filter.

5.3 Sampling Rate Conversion with Cascaded Integrator Comb Filters

The hardware implementation of the lowpass filter required for sampling rate con-
version can be significantly simplified if we choose a comb filter with system function

H(z) =
M−1∑
k=0

z−k = 1− z
−M

1− z−1

This system can be implemented by cascading either the “integrator” 1/(1−z−1) with
the comb filter (1 − z−M) or vice versa. This leads to the name cascaded integrator
comb (CIC) filter structure. The CIC structure does not require any multiplications
or storage for the filter coefficients.

To obtain an efficient decimation structure, we start with an integrator-comb
CIC filter followed by the downsampler and then we apply the first noble identity,

(5.9)

(5.10)

(5.11)

(5.12)

Multirate Digital Signal Processing

Figure 5.4

The output of the system in Figure 5.4(a) can be written as

because V (z) = X(z ). The output of the system in Figure 5.4(b) is given by

which is equal to (5.10). This shows that the two systems in Figure 5.4 are

as shown in Figure 5.5. For the interpolation case, we use an upsampler followed

787



x(n) y(m)
D

(a)

(b)

1 − z−D
1 − z−1

1

x(n) y(m)
D 1 − z−1

1 − z−1
1

Using the first noble identity to obtain an efficient CIC
filter structure for decimation.

by a comb-integrator CIC filter and then we use the second noble identity, as shown

rate conversion, we can cascade K CIC filters. In this case, we order all integra-
tors on one side of the filter and the comb filters on the other side, and then we
apply the noble identities as in the single-stage case. The integrator 1/(1 − z−1) is
an unstable system. Therefore, its output may grow without limits, resulting in over-
flow when the integrator section comes first, as in the decimation structure shown in

However, this overflow can be tolerated if the entire filter is im-
plemented using two’s-complement fixed-point arithmetic. If D �= M or I �= M , the
comb filter 1−z−1 −M/D
or 1 − z−M/I , respectively. A detailed treatment of CIC filters for decimation and
interpolation can be found in Hogenauer (1981). Finally, we note that CIC filters are

If the CIC filter order is a power of 2, that is, M = 2K , we can decompose the

H(z) = (1+ z−1)(1+ z−2)(1+ z−4) . . . (1+ z−2K−1)

Using this decomposition we can develop decimator structures using nonrecursive

x(n) y(m)
I

I

(a)

(b)

1 − z−I
1 − z−1

1

x(n) y(m)
1 − z−1

1 − z−1
1

Using the second noble identity to obtain an efficient CIC
filter structure for interpolation.

(5.13)

special cases of frequency sampling structure.

Multirate Digital Signal Processing

Figure 5.5

in Figure 5.6. To improve the lowpass frequency response required for sampling

Figure 5.5(b).

in Figures 5.5(a) and 5.6(b) should be replaced by 1−z

system function (5.12) as follows:

Figure 5.6

788



z−1

2

z−1z−1

2 2
x(n) y(m)

Efficient filter structure for decimation by D = 8 using comb filters.

CIC filters. = M = 8.
Cascading of N CIC filters can be obtained by providing M first-order sections
(1− z−1) between each decimation stage. The constraint M = 2K can be relaxed by
factoring M into a product of prime numbers, as shown in Jang and Yang (2001).

5.4 Polyphase Structures for Decimation and Interpolation Filters

To develop a polyphase structure for decimation, we start with the straightforward
The decimated

sequence is obtained by passing the input sequence x(n) through a linear filter and
then downsampling the filter output by a factor D . In this configuration, the filter is
operating at the high sampling rate Fx , while only one out of every D output sam-
ples is actually needed. A logical solution would be to find a structure where only
the needed samples are computed. We will develop such an efficient implementa-

Since downsampling

In this fil-
tering structure, only the needed samples are computed and all multiplications and
additions are performed at the lower sampling rate Fx/D . Thus, we have achieved
the desired efficiency. Additional reduction in computation can be achieved by using
an FIR filter with linear phase and exploiting the symmetry of its impulse response.

In practice it is more convenient to implement the polyphase decimator using

clockwise starting at time n = 0 and distributes a block of D input samples to the
polyphase filters starting at filter i = D − 1 and continuing in reverse order until
i = 0. For every block of D input samples, the polyphase filters receive a new input
and their outputs are computed and summed to produce one sample of the output
signal y(m). The operation of this realization can be also understood by a careful

Next, let us consider the efficient implementation of an interpolator, which is
realized by first inserting I − 1 zeros between successive samples of x(n) and then

x(n) y(m) = v(mD)
H(z) D

v(n)

Figure 5.8 Decimation system.

Multirate Digital Signal Processing

Figure 5.7

Figure 5.7 shows an example for a decimator with D

implementation of the decimation process shown in Figure 5.8.

tion by exploitating the polyphase structure in Figure 5.1.
commutes with addition, combining the structures in Figures 5.8 and 5.1 yields
the structure in Figure 5.9(a). If we next apply the identity in Figure 5.3, we
obtain the desired implementation structure shown in Figure 5.9(b).

a commutator model as shown in Figure 5.10. The commutator rotates counter-

inspection of Figure 1.3.

789



z−1

z−1

x(n) y(m)
P0(z3)

P1(z3)

P2(z3)

3

3

3

z−1

z−1

x(n) y(m)
P0(z)3

P1(z)3

P2(z)3

(a)

(b)

Figure 5.9 Implementation of a decimation system using a polyphase
structure before (a) and after (b) the use of the first noble identity.

The major problem with this
structure is that the filter computations are performed at the high sampling rate IFx .

x(n)

y(m)
P0(z)

P1(z)

PD −1(z)

Commutator rate = Fx 

Fx 

Fy = Fx / D

.

..
.
..

Figure 5.10 Decimation using a polyphase filter and a commutator.

Multirate Digital Signal Processing

filtering the resulting sequence (see Figure 5.11).

The desired simplification is achieved by first replacing the filter in Figure 5.11 with
the transpose polyphase structure in Figure 5.2, as illustrated in Figure 5.12(a).

790



x(n) y(m) 
H(z)I

v(m)

Figure 5.11 Interpolation system.

z−1

z−1

x(n) y(m)
P0(z3)

P1(z3)

P2(z3)

3

3

3

z−1

z−1

x(n) y(m)
P0(z) 3

P1(z) 3

P2(z) 3

(a)

(b)

Figure 5.12 Implementation of an interpolation system using a polyphase
structure before (a) and after (b) the use of the second noble identity.

Multirate Digital Signal Processing

791



Fx . It is interesting to note that the structure for an interpolator can be obtained
by transposing the structure of a decimator, and vice versa (Crochiere and Rabiner,
1981).

For every input sample, the polyphase filters produce I output samples y0(n),
y1(n), . . ., yI−1(n). Because the output yi(n) of the i th filter is followed by (I − 1)
zeros and it is delayed by i th samples, the polyphase filters contribute nonzero sam-
ples at different time slots. In practice, we can implement the part of the structure
including the 1-to-I expanders, delays, and adders using the commutator model

n = 0 at branch i = 0. For each input sample x(n) the commutator reads the output
of every polyphase filter to obtain I samples of the output (interpolated) signal y(m).
The operation of this realization can be also understood by a careful inspection of

data using its unique set of coefficients. Therefore, we can obtain the same results
using a single filter by sequentially loading a different set of coefficients.

5.5 Structures for Rational Sampling Rate Conversion

A sampling rate converter with a ratio I/D can be efficiently implemented using a
polyphase interpolator followed by a downsampler. However, since the downsam-
pler keeps only every Dth polyphase subfilter output, it is not necessary to compute
all I interpolated values between successive input samples. To determine which
polyphase subfilter outputs to compute, we consider an example with I = 5 and
D = 3. The interpolator polyphase structure has I = 5 subfilters which provide
interpolated samples at an effective sampling period T = Tx/I . The downsampler
picks every Dth of these samples, resulting in a discrete-time signal with sampling

x(n)

y(m)

P0(z)

P1(z)

PI−1(z)

.

..

Commutator rate = I Fx 

Fy = I Fx 

Fx 

Interpolation using a polyphase filter and a commutator.

Multirate Digital Signal Processing

Then, we use the second noble identity (see Figure 5.4) to obtain the structure in
Figure 5.12(b). Thus, all the filtering multiplications are performed at the low rate

shown in Figure 5.13. The commutator rotates counterclockwise starting at time

Figure 1.4. Each polyphase filter in Figure 5.13 operates on the same input

Figure 5.13

792



Ty Tx 

t
0

0 1 2 3 4 5 6 7 8 9 10 15 20 25

0 1 2 3 4

0 0 1 1 2

0 3 1 4 2

5 6 7 8 9

3 3 4 4 5

0 3 1 4 2

1 2 3 4 5

... ... ...

n

k

m

km

im

Tblock = 3Tx = 5Ty = 15T 
T

Input

Output

Interpolated

Figure 5.14 Illustration of index computation for polyphase implementation of
sampling rate conversion for a rational ratio I/D = 5/3.

period Ty x

Tblock = ITy = DTx = IDT

which contain L output samples or I input samples. The relative time positions
The

input sequence x(nTx) is interpolated to produce a sequence v(kT ), which is then
decimated to yield y(mTy). If we use an FIR filter with M = KI coefficients, the
polyphase subfilters are given by pi(n) = h(nI + i), i = 0, 1, . . . , I − 1, where n =
0, 1, . . . , K − 1. To compute the output sample y(m), we use the polyphase subfilter
with index im which requires the input samples x(km), x(km− 1), . . . , x(km−K + 1).

km =
⌊
mD

I

⌋
and im = (Dm)I

For D = 3 and I = 5 the first data block includes D = 3 input samples and
I = 5 output samples. To compute the samples {y(0), y(1), y(2), y(3), y(4)}, we use
the polyphase subfilter specified by the index im = {0, 3, 1, 4, 2}, respectively. The
samples in the filter memory are updated only when km changes value. This discussion
provides the basic ideas for the efficient software implementation of rational sampling
rate conversion using FIR filters.

6 Multistage Implementation of Sampling Rate Conversion

In practical applications of sampling-rate conversion we often encounter decima-
tion factors and interpolation factors that are much larger than unity. For exam-
ple, suppose that we are given the task of altering the sampling rate by the factor

= DT = DT /I . It is convenient to think in terms of blocks of duration

(5.14)

(5.15)

Multirate Digital Signal Processing

of the various sequences and a block of data are illustrated in Figure 5.14.

From relation (1.9) and Figure 5.14, we can easily deduce that

793



x(n)

Fx

y(m)

Fy = IFx
↑I1

I1Fx

I1I2Fx

h1(n) ↑IL hL(n)↑I2 h2(n) …

Stage 1 Stage 2 Stage L

Figure 6.1 Multistage implementation of interpolation by a factor I .

I/D = 130/63. Although, in theory, this rate alteration can be achieved exactly, the
implementation would require a bank of 130 polyphase filters and may be compu-
tationally inefficient. In this section we consider methods for performing sampling
rate conversion for either D � 1 and/or I � 1 in multiple stages.

First, let us consider interpolation by a factor I � 1 and let us assume that I
can be factored into a product of positive integers as

I =
L∏
i=1
Ii

Then, interpolation by a factor I can be accomplished by cascading L stages of
Note that the filter in each of

the interpolators eliminates the images introduced by the upsampling process in the
corresponding interpolator.

In a similar manner, decimation by a factor D , where D may be factored into a
product of positive integers as

D =
J∏
i=1
Di

can be implemented as a cascade of J stages of filtering and decimation as illustrated

Fi = Fi−1
Di

, i = 1, 2, . . . , J

where the input rate for the sequence {x(n)} is F0 = Fx .

x(n)

Fx

y(m)
h1(n) ↓D1 h2(n) ↓D2 hJ(n) ↓DJ…

Stage 1 Stage 2 Stage J

Fx
D1

Fx
D1D2

Fx
D

Multistage implementation of decimation by a factor D .

(6.1)

(6.2)

(6.3)

Multirate Digital Signal Processing

interpolation and filtering, as shown in Fig. 6.1.

in Fig. 6.2. Thus the sampling rate at the output of the i th stage is

Figure 6.2

794



To ensure that no aliasing occurs in the overall decimation process, we can design
each filter stage to avoid aliasing within the frequency band of interest. To elaborate,
let us define the desired passband and the transition band in the overall decimator as

Passband: 0 ≤ F ≤ Fpc
Transition band: Fpc ≤ F ≤ Fsc

where Fsc ≤ Fx/2D . Then, aliasing in the band 0 ≤ F ≤ Fsc is avoided by selecting
the frequency bands of each filter stage as follows:

Passband: 0 ≤ F ≤ Fpc
Transition band: Fpc ≤ F ≤ Fi − Fsc

Stopband: Fi − Fsc ≤ F ≤ Fi−12
For example, in the first filter stage we have F1 = Fx/D1 , and the filter is designed

to have the following frequency bands:

Passband: 0 ≤ F ≤ Fpc
Transition band: Fpc ≤ F ≤ F1 − Fsc

Stopband: F1 − Fsc ≤ F ≤ F02
After decimation by D1 , there is aliasing from the signal components that fall in the
filter transition band, but the aliasing occurs at frequencies above Fsc . Thus there
is no aliasing in the frequency band 0 ≤ F ≤ Fsc . By designing the filters in the

aliasing occurs in the primary frequency band 0 ≤ F ≤ Fsc .

Consider an audio-band signal with a nominal bandwidth of 4 kHz that has been sampled
at a rate of 8 kHz. Suppose that we wish to isolate the frequency components below 80 Hz
with a filter that has a passband 0 ≤ F ≤ 75 and a transition band 75 ≤ F ≤ 80. Hence
Fpc = 75 Hz and Fsc = 80. The signal in the band 0 ≤ F ≤ 80 may be decimated by the factor
D = Fx/2Fsc = 50. We also specify that the filter have a passband ripple δ1 = 10−2 and a
stopband ripple of δ2 = 10−4 .

M̂ = −10 log10 δ1δ2 − 13
14.6�f

+ 1

where �f is the normalized (by the sampling rate) width of the transition region [i.e., �f =
(Fsc − Fpc)/Fs ]. A more accurate formula proposed by Herrmann et al. (1973) is

M̂ = D∞(δ1, δ2)− f (δ1, δ2)(�f )
2

�f
+ 1

(6.4)

(6.5)

(6.6)

(6.7)

(6.8)

Multirate Digital Signal Processing

subsequent stages to satisfy the specifications given in (6.5), we ensure that no

EXAMPLE 6.1

The length of the linear phase FIR filter required to satisfy these specifications can be

oximating the length M, attributed to Kaiser, is
-estimated from a well-known formula. Recall that a particularly simple formula for appr

795



where D∞(δ1 , δ2 ) and f (δ1 , δ2) are defined as

D∞(δ1, δ2) = [0.005309(log10 δ1)2 + 0.07114(log10 δ1)
− 0.4761] log10 δ2
− [0.00266(log10 δ1)2 + 0.5941 log10 δ1 + 0.4278]

f (δ1, δ2) = 11.012+ 0.51244[log10 δ1 − log10 δ2]
Now a single FIR filter followed by a decimator would require (using the Kaiser formula)

a filter of (approximate) length

M̂ = −10 log10 10
−6 − 13

14.6(5/8000)
+ 1 ≈ 5152

As an alternative, let us consider a two-stage decimation process with D1 = 25 and D2 = 2.
In the first stage we have the specifications F1 = 320 Hz and

Passband: 0 ≤ F ≤ 75

Transition band: 75 < F ≤ 240

�f = 165
8000

δ11 = δ12 , δ21 = δ2

Note that we have reduced the passband ripple δ1 by a factor of 2, so that the total passband
ripple in the cascade of the two filters does not exceed δ1 . On the other hand, the stopband
ripple is maintained at δ2 in both stages. Now the Kaiser formula yields an estimate of M1 as

M̂1 = −10 log10 δ11δ21 − 1314.6�f + 1 ≈ 167

For the second stage, we have F2 = F1/2 = 160 and the specifications

Passband: 0 ≤ F ≤ 75

Transition band: 75 < F ≤ 80

�f = 5
320

δ12 = δ12 , δ22 = δ2

Hence the estimate of the length M2 of the second filter is

M̂2 ≈ 220
Therefore, the total length of the two FIR filters is approximately M̂1 + M̂2 = 387. This
represents a reduction in the filter length by a factor of more than 13.

The reader is encouraged to repeat the computation above with D1 = 10 and D2 = 5.

(6.9)

(6.10)

Multirate Digital Signal Processing

796



filter length results from increasing the factor �f , which appears in the denominator

width of the transition region through a reduction in the sampling rate.
In the case of a multistage interpolator, the sampling rate at the output of the

i th stage is
Fi−1 = IiFi, i = J, J − 1, . . . , 1

and the output rate is F0 = IFJ when the input sampling rate is FJ . The correspond-
ing frequency band specifications are

Passband: 0 ≤ F ≤ Fp
Transition band: Fp < F ≤ Fi − Fsc

The following example illustrates the advantages of multistage interpolation.

having a passband 0 ≤ F ≤ 75 and a transition band of 75 ≤ F ≤ 80. We wish to interpolate
by a factor of 50. By selecting I1 = 2 and I2 = 25, we have basically a transposed form

two-stage decimator to achieve the two-stage interpolator with I1 = 2, I2 = 25, M̂1 ≈ 220,
and M̂2 ≈ 167.

7 Sampling Rate Conversion of Bandpass Signals

To be specific, suppose that we wish to decimate by a factor D an integer-
positioned bandpass signal with spectrum confined to the bands

(k − 1) π
D
< |ω| < k π

D

where k is a positive integer. A bandpass filter defined by

HBP(ω) =
{

1, (k − 1) π
D
< |ω| < k π

D

0, otherwise

would normally be used to eliminate signal frequency components outside the desired
frequency range. Then direct decimation of the filtered signal v(n) by the factor D

(7.1)

(7.2)

In this section we consider the decimation and interpolation of bandpass signals. 
We begin by noting that any bandpass signal can be converted into an equivalent 
lowpass signal whose sampling rate can be changed using the already developed 
techniques. However, a simpler and more widely used approach concerns bandpass 
discrete-time signals with integer-band positioning. The concept is similar to one for 
continuous-time bandpass signals.

Multirate Digital Signal Processing

It is apparent from the computations in Example 6.1 that the reduction in the

in (6.7) and (6.8). By decimating in multiple stages, we are able to increase the

EXAMPLE 6.2

Let us reverse the filtering problem described in Example 6.1 by beginning with a signal

of the decimation problem considered in Example 6.1. Thus we can simply transpose the

797



The spectrum of the decimated signal y(m) is obtained by
scaling the frequency axis by ωy x

with even band positioning (k = 4). In the case where k is odd, there is an inversion
The inversion can be

undone by the simple process y′(m) = (−1)my(m). Note that

The process of bandpass interpolation by an integer factor I is the inverse of that
of bandpass decimation and can be accomplished in a similar manner. The process
of upsampling by inserting zeros between the samples of x(n) produces I images
in the band 0 ≤ ω ≤ π . The desired image can be selected by bandpass filtering.

Note that the
process of interpolation also provides us with the opportunity to achieve frequency
translation of the spectrum.

Finally, sampling rate conversion for a bandpass signal by a rational factor I/D
can be accomplished by cascading a decimator with an interpolator in a manner that
depends on the choice of the parameters D and I . A bandpass filter preceding
the sampling converter is usually required to isolate the signal frequency band of

x(n) y(m) = v(mD)
HBP(z) D

v(n)

ωx

ωx

ωy

V(ω)

Y(ω)

Y(ω)

π 2π−2π −π 0

0

0 2π/D−2π/D 3π/D−3π/D

2π/D−2π/D 3π/D−3π/D

D = 4

D = 4

D = 4

(a)

(b)

(c)

(d)

Spectral interpretation of bandpass decimation for integer-
band positioning (odd integer positioning).

Multirate Digital Signal Processing

according to (2.14).
results in a periodic replication of the bandpass spectrum V (ω) every 2π/D radians

= Dω . This process is illustrated in Figure 7.1
for bandpass signal with odd band positioning (k = 3) and in Figure 7.2 for signals

violation of the
bandwidth constraint given by (7.1) results in signal aliasing.

This can be seen by “reversing” the process shown in Figure 7.1.

Figure 7.1

of the spectrum of the signal as in the continuous-time case.

798



ωx

ωy

V(ω)

Y(ω)

π 2π−2π −π 0

0 3π/D−3π/D 4π/D−4π/D

D = 4

(a)

(b)

Spectral interpretation of bandpass decimation for integer-
band positioning (even integer positioning).

interest. Note that this approach provides us with a modulation-free method for
achieving frequency translation of a signal by selecting D = I .

8 Sampling Rate Conversion by an Arbitrary Factor

Efficient implementation of sampling rate conversion by a polyphase structure re-
quires that the rates Fx and Fy are fixed and related by a rational factor I/D . In
some applications, it is either inefficient or sometimes impossible to use such an exact
rate conversion scheme.

For example, suppose we need to perform rate conversion by the rational number
I/D , where I is a large integer (e.g., I/D = 1023/511). Although we can achieve
exact rate conversion by this number, we would need a polyphase filter with 1023
subfilters. Such an implementation is obviously inefficient in memory usage because
we need to store a large number of filter coefficients.

In some applications, the exact conversion rate is not known when we design the
rate converter, or the rate is continuously changing during the conversion process.
For example, we may encounter the situation where the input and output samples
are controlled by two independent clocks. Even though it is still possible to define a
nominal conversion rate that is a rational number, the actual rate would be slightly
different, depending on the frequency difference between the two clocks. Obviously,
it is not possible to design an exact converter in this case.

In principle, we can convert from any rate Fx to any rate Fy (fixed or variable)

y(mTy) =
K2∑
k=K1

g(kTx +�mTx)x((km − k)Tx)

This requires the computation of a new impulse response pm(k) = g(kTx+�mTx) for
each output sample. However, if �m is measured with finite accuracy, there is only a
finite set of impulse responses, which may be precomputed and loaded from memory

(8.1)

Multirate Digital Signal Processing

Figure 7.2

using formula (1.9), which we repeat here for convenience:

799



as needed. We next discuss two practical approaches for sampling rate conversion
by an arbitrary factor.

8.1 Arbitrary Resampling with Polyphase Interpolators

If we use a polyphase interpolator with I subfilters we can generate samples with
spacing Tx/I . Therefore, the number I of stages determines the granularity of the
interpolating process. If Tx/I is sufficiently small so that successive values of the
signal do not change significantly or the change is less than the quantization step, we
can determine the value at any location t = nTx +�Tx , 0 ≤ � ≤ 1, using the value
of the nearest neighbor (zero-order hold interpolation).

Additional improvement can be obtained using two-point linear interpolation

y(nTx +�Tx) = (1−�)x(n)+�x(n+ 1)

The performance of these interpolation techniques by analyzing their frequency-
domain characteristics. Additional practical details can be found in Ramstad (1984).

8.2 Arbitrary Resampling with Farrow Filter Structures

In practice, we typically implement polyphase sampling rate converters by a rational
factor using causal FIR lowpass filters. If we use an FIR filter with M = KI coeffi-
cients, the coefficients of the polyphase filters are obtained by the mapping

pi(n) = h(nI + i), i = 0, 1, . . . , I − 1

This mapping can be easily visualized as mapping the one-dimensional sequence
h(n) into a two-dimensional array with I rows and K columns by filling successive
columns in natural order as follows:

p0(k) �→ h(0) h(I ) . . . h((K − 1)I )
p1(k) �→ h(1) h(I + 1) . . . h((K − 1)I + 1)

...

pi(k) �→ h(i) h(I + i) . . . h((K − 1)I + i)
pi+1(k) �→ h(i + 1) h(I + i + 1) . . .

...

pI−1(k) �→ h(I − 1) h(2I − 1) . . . h(KI − 1)

The polyphase filters pi(n) are used to compute samples at I equidistant locations
t = nTx+ i(Tx/I ), i = 0, 1, . . . , I−1 covering each input sampling interval. Suppose
now that we wish to compute a sample at t = nTx + �Tx , where � �= i/I and
0 ≤ � ≤ 1. This requires a nonexisting polyphase subfilter, denoted as p�(k),
which would “fall” between two existing subfilters, say, pi(k) and pi+1(k). This set
of coefficients would create a row between the rows with indexes i and i + 1. We

(8.2)

(8.3)

(8.4)

Multirate Digital Signal Processing

note that each column of (8.4) consists of a segment of I consecutive samples of

800



the impulse response h(n) and covers one sampling interval Tx . Suppose next that
we can approximate the coefficient set in each column by an L-degree polynomial

Bk(�) =
L∑
�=0

b
(k)
� �

�, k = 0, 1, . . . , K − 1

i

polyphase subfilter. The type of the polynomial (Lagrange, Chebyschev, etc.) and
the order L can be chosen to avoid any performance degradation compared to the
original filter h(n). The sample at location t = nTx +�Tx is determined by

y((n+�)Tx) =
K−1∑
k=0

Bk(�)x((n− k)Tx), 0 ≤ � ≤ 1

If we substitute

of summations, we obtain

y((n+�)Tx) =
K−1∑
k=0

L∑
�=0

b
(k)
� �

�x((n− k)Tx)

=
L∑
�=0

��
K−1∑
k=0

b
(k)
� x((n− k)Tx)

The last equation can be written as

y((n+�)Tx) =
L∑
�=0

v(�)��

where

v(�) =
K−1∑
k=0

b
(k)
� x((n− k)Tx), � = 0, 1, . . . , L

sequence, where the terms v(�) are successive local derivatives determined from the

with system functions

H�(z) =
K−1∑
k=0

b
(k)
� z
−k

Horner’s rule, which is illustrated next for L = 4:
y(�) = c0 + c1�+ c2�2 + c3�3 + c4�4

= c0 +�(c1 +�(c2 +�(c3 +�c4)))

(8.5)

(8.6)

(8.7)

(8.8)

(8.9)

(8.10)

Multirate Digital Signal Processing

We note that evaluating (8.5) at � = i/I will provide the coefficients of p (k)ψ

where the required filter coefficients are computed using (8.5).
the polynomials (8.5) into the filtering formula (8.6) and we change the order

Equation (8.7) can be interpreted as a Taylor series representation of the output

input sequence. Relation (8.8) can be implemented using FIR filtering structures

The most efficient computation of polynomial (8.7) can be done using the nested

801



HL(z) HL−1(z) H1(z) H0(z)

v(L) v(L−1) v(0)v(1)

∆

...

...

x(nTx)

y(nTx+∆Tx)

Figure 8.1 Block diagram of the Farrow structure for sampling rate change by an
arbitrary factor.

known as Farrow structure (Farrow 1988). Basically, the Farrow structure performs
interpolation between signal values by interpolating between filter coefficients. More
details can be found in Gardner (1993), Erup et al. (1993), Ramstad (1984), Har-
ris (1997), and Laakso et al. (1996).

9 Applications of Multirate Signal Processing

There are numerous practical applications of multirate signal processing. In this
section we describe a few of these applications.

9.1 Design of Phase Shifters

Suppose that we wish to design a network that delays the signal x(n) by a fraction
of a sample. Let us assume that the delay is a rational fraction of a sampling interval
Tx [i.e., d = (k/I)Tx , where k and I are relatively prime positive integers]. In the
frequency domain, the delay corresponds to a linear phase shift of the form

�(ω) = −kω
I

The design of an all-pass linear-phase filter is relatively difficult. However, we
can use the methods of sample-rate conversion to achieve a delay of (k/I)Tx , exactly,
without introducing any significant distortion in the signal. To be specific, let us

I using a standard interpolator. The lowpass filter eliminates the images in the
spectrum of the interpolated signal, and its output is delayed by k samples at the
sampling rate IFx . The delayed signal is decimated by a factor D = I . Thus we have
achieved the desired delay of (k/I)Tx .

Lowpass
filter

Delay by
k samples

x(n)

Fx

y(n)

FxIFx IFx IFx
↑I ↓I

Figure 9.1 Method for generating a delay in a discrete-time signal.

(9.1)

Multirate Digital Signal Processing

This approach leads to the block diagram realization shown in Figure 8.1, which is

consider the system shown in Fig. 9.1. The sampling rate is increased by a factor

802



Polyphase filter structure
for implementing the

p0(n)
x(n)

p1(n)

p2(n)

pk(n)

pI−1(n)

…
…

Output

Rate = IFx

An efficient implementation of the interpolator is the polyphase filter illustrated

commutator at the output of the kth subfilter. Since decimation by D = I means
that we take one out of every I samples from the polyphase filter, the commutator
position can be fixed to the output of the kth subfilter. Thus a delay in k/I can be
achieved by using only the k th subfilter of the polyphase filter. We note that the
polyphase filter introduces an additional delay of (M−1)/2 samples, where M is the
length of its impulse response.

Finally, we mention that if the desired delay is a nonrational factor of the sample
interval Tx

9.2 Interfacing of Digital Systems with Different Sampling Rates

In practice we frequently encounter the problem of interfacing two digital systems
that are controlled by independently operating clocks. An analog solution to this
problem is to convert the signal from the first system to analog form and then resample
it at the input to the second system using the clock in this system. However, a simpler
approach is one where the interfacing is done by a digital method using the basic
sample-rate conversion methods described in this chapter.

To be specific, let us consider interfacing the two systems with independent clocks
x is fed to an interpolator

which increases the sampling rate by I . The output of the interpolator is fed at the
rate IFx to a digital sample-and-hold which serves as the interface to system B at
the high sampling rate IFx . Signals from the digital sample-and-hold are read out
into system B at the clock rate DFy of system B. Thus the output rate from the
sample-and-hold is not synchronized with the input rate.

Multirate Digital Signal Processing

Figure 9.2

system shown in Fig. 9.1.

in Fig. 9.2. The delay of k samples is achieved by placing the initial position of the

, the methods described in Section 8 can be used to obtain the delay.

as shown in Fig. 9.3. The output of system A at rate F

803



System
A

Clock
A

↑I
Interpolation

↓D
Decimator

System
B

x(m)x(n)

Fx

Clock
B

DFy Fy

IFx

IFx

Digital sample-
and-hold

IFy

Figure 9.3 Interfacing of two digital systems with different sampling rates.

In the special case where D = I and the two clock rates are comparable but not
identical, some samples at the output of the sample-and-hold may be repeated or
dropped at times. The amount of signal distortion resulting from this method can be
kept small if the interpolator/decimator factor is large. By using linear interpolation
in place of the digital sample-and-hold we can further reduce the distortion and thus
reduce the size of the interpolator factor.

9.3 Implementation of Narrowband Lowpass Filters

conversion often provides for a more efficient realization, especially when the filter
specifications are very tight (e.g., a narrow passband and a narrow transition band).
Under similar conditions, a lowpass, linear-phase FIR filter may be more efficiently
implemented in a multistage decimator-interpolator configuration. To be more spe-
cific, we can employ a multistage implementation of a decimator of size D , followed
by a multistage implementation of an interpolator of size I , where I = D .

We demonstrate the procedure by means of an example for the design of a low-

Design a linear-phase FIR filter that satisfies the following specifications:

Sampling frequency: 8000 Hz

Passband: 0 ≤ F ≤ 75 Hz
Transition band: 75 Hz ≤ F ≤ 80 Hz

Stopband 80 Hz ≤ F ≤ 4000 Hz
Passband ripple: δ1 = 10−2

Stopband ripple: δ2 = 10−4

Solution. If this filter were designed as a single-rate linear-phase FIR filter, the length of
the filter required to meet the specifications is (from Kaiser’s formula)

M̂ ≈ 5152

Multirate Digital Signal Processing

In Section 6 we demonstrated that a multistage implementation of sampling-rate

pass filter which has the same specifications as the filter that is given in Example 6.1.

EXAMPLE 9.1

804



Now, suppose that we employ a multirate implementation of the lowpass filter based on
a decimation and interpolation factor of D = I = 100. A single-stage implementation of the
decimator-interpolator requires an FIR filter of length

M̂1 = −10 log10(δ1δ2/2)− 1314.6�f + 1 ≈ 5480

However, there is a significant savings in computational complexity by implementing the
decimator and interpolator filters using their corresponding polyphase filters. If we employ
linear-phase (symmetric) decimation and interpolation filters, the use of polyphase filters
reduces the multiplication rate by a factor of 100.

A significantly more efficient implementation is obtained by using two stages of decima-
tion followed by two stages of interpolation. For example, suppose that we select D1 = 50,
D2 = 2, I1 = 2, and I2 = 50. Then the required filter lengths are

M̂1 = −10 log(δ1δ2/4)− 1314.6�f + 1 ≈ 177

M̂1 = −10 log10(δ1δ2/4)− 1314.6�f + 1 ≈ 233

Thus we obtain a reduction in the overall filter length of 2(5480)/2(177 + 233) ≈ 13.36. In
addition, we obtain further reduction in the multiplication rate by using polyphase filters. For
the first stage of decimation, the reduction in multiplication rate is 50, while for the second stage
the reduction in multiplication rate is 100. Further reductions can be obtained by increasing
the number of stages of decimation and interpolation.

9.4 Subband Coding of Speech Signals

A variety of techniques have been developed to efficiently represent speech signals
in digital form for either transmission or storage. Since most of the speech energy
is contained in the lower frequencies, we would like to encode the lower-frequency
band with more bits than the high-frequency band. Subband coding is a method
where the speech signal is subdivided into several frequency bands and each band is
digitally encoded separately.

that the speech signal is sampled at a rate Fs samples per second. The first frequency
subdivision splits the signal spectrum into two equal-width segments, a lowpass signal
(0 ≤ F ≤ Fs/4) and a highpass signal (Fs/4 ≤ F ≤ Fs/2). The second frequency
subdivision splits the lowpass signal from the first stage into two equal bands, a
lowpass signal (0 < F ≤ Fs/8) and a highpass signal (Fs/8 ≤ F ≤ Fs/4). Finally, the
third frequency subdivision splits the lowpass signal from the second stage into two
equal-bandwidth signals. Thus the signal is subdivided into four frequency bands,

Multirate Digital Signal Processing

An example of a frequency subdivision is shown in Fig. 9.4(a). Let us assume

covering three octaves, as shown in Fig. 9.4(b).

805



To
channel

Decimator
D = 2

Encoder
Lowpass

filter

Decimator
D = 2

Lowpass
filter

To
channel

Decimator
D = 2

Encoder
Highpass

filter

To
channel

Decimator
D = 2

Encoder
Highpass

filter

Decimator
D = 2

Lowpass
filter

To
channel

(a)

Decimator
D = 2

Encoder
Highpass

filter

Speech
signal

(b)

8
π

4
π

2
π π

ω

1

0

2 3 4

Figure 9.4 Block diagram of a subband speech coder.

Decimation by a factor of 2 is performed after frequency subdivision. By allo-
cating a different number of bits per sample to the signal in the four subbands, we
can achieve a reduction in the bit rate of the digitalized speech signal.

Filter design is particularly important in achieving good performance in sub-
band coding. Aliasing resulting from decimation of the subband signals must be
negligible. It is clear that we cannot use brickwall filter characteristics as shown in

solution to the aliasing problem is to use quadrature mirror filters (QMF), which

The synthesis method for the subband encoded speech signal is basically the
reverse of the encoding process. The signals in adjacent lowpass and highpass fre-

of QMF is used in the signal synthesis for each octave of the signal.

Subband coding is also an effective method to achieve data compression in image
signal processing. By combining subband coding with vector quantization for each
subband signal, Safranek et al. (1988) have obtained coded images with approxi-
mately 12 bit per pixel, compared with 8 bits per pixel for the uncoded image.

In general, subband coding of signals is an effective method for achieving band-
width compression in a digital representation of the signal, when the signal energy is
concentrated in a particular region of the frequency band. Multirate signal processing
notions provide efficient implementations of the subband encoder.

Multirate Digital Signal Processing

Fig. 9.5(a), since such filters are physically unrealizable. A particularly practical

have the frequency response characteristics shown in Fig. 9.5(b). These filters are
described in Section 11.

quency bands are interpolated, filtered, and combined as shown in Fig. 9.6. A pair

806



Filter characteristics for
subband coding.

Decoder ↑2

↑2

↑2

Filter

Filter

Filter

Decoder Filter

Decoder Filter

Decoder Filter

+

+

Output+

↑2

↑2

↑2

Synthesis of subband-encoded signals.

Multirate Digital Signal Processing

Figure 9.5

Figure 9.6

807



10 Digital Filter Banks

Filter banks are generally categorized as two types, analysis filter banks and synthesis
filter banks. An analysis filter bank consists of a set of filters, with system functions

k

response characteristics of this filter bank split the signal into a corresponding number
of subbands. On the other hand, a synthesis filter bank consists of a set of filters with

k

inputs {yk(n)}. The outputs of the filters are summed to form the synthesized signal
{x(n)}.

Filter banks are often used for performing spectrum analysis and signal synthesis.
When a filter bank is employed in the computation of the discrete Fourier transform
(DFT) of a sequence {x(n)}, the filter bank is called a DFT filter bank. An analysis
filter bank consisting of N filters {Hk(z), k = 0, 1, . . . , N − 1} is called a uniform
DFT filter bank if Hk(z), k = 1, 2, . . . , N − 1, are derived from a prototype filter

H0(z)

x(n) H1(z)

y1(n)

y2(n)

yN−1(n)

y1(n)

y2(n)

yN−1(n)

HN−1(z)

…

Analysis filter bank

(a)

G0(z)

x(n)G1(z)

GN−1(z)

…

Synthesis filter bank

(b)

+

Figure 10.1 A digital filter bank.

Multirate Digital Signal Processing

{H (z)}, arranged in a parallel bank as illustrated in Fig. 10.1(a). The frequency

system functions {G (z)}, arranged as shown in Fig. 10.1(b), with corresponding

808



N
−π

N
2π 2π

N
2π(N−1)

N
3π

N
4π

N
5π

N
π0

H0(ω) H1(ω) H2(ω) HN−1(ω)

. . . . . . . . .
ω

Figure 10.2 Illustration of frequency response characteristics of the N fil-
ters.

H0(z), where

Hk(ω) = H0
(
ω − 2πk

N

)
, k = 1, 2, . . . , N − 1

Hence the frequency response characteristics of the filters {Hk(z), k = 0, 1, . . . , N−1}

filter by multiples of 2π/N . In the time domain the filters are characterized by their
impulse responses, which can be expressed as

hk(n) = h0(n)ej2πnk/N , k = 0, 1, . . . , N − 1

where h0(n) is the impulse response of the prototype filter, which in general may be
either an FIR or an IIR filter. If H0(z) denotes the transfer function of the prototype
filter, the transfer function of the kth filter is

Hk(z) = H0(ze−j2πk/N), 1 ≤ k ≤ N − 1

teristics of the N filters.

where the frequency components in the sequence {x(n)} are translated in frequency
to lowpass by multiplying x(n) with the complex exponentials exp(−j2πnk/N), k =
1, . . . , N−1, and the resulting product signals are passed through a lowpass filter with
impulse response h0(n). Since the output of the lowpass filter is relatively narrow in
bandwidth, the signal can be decimated by a factor D ≤ N . The resulting decimated
output signal can be expressed as

Xk(m) =
∑
n

h0(mD − n)x(n)e−j2πnk/N , k = 0, 1, . . . , N − 1
m = 0, 1, . . .

where {Xk(m)} are samples of the DFT at frequencies ωk = 2πk/N .

(10.1)

are simply obtained by uniformly shifting the frequency response of the prototype

(10.2)

(10.3)

(10.4)

Multirate Digital Signal Processing

Figure 10.2 provides a conceptual illustration of the frequency response charac-

The uniform DFT analysis filter bank can be realized as shown in Fig. 10.3(a),

809



x(n) …

h0(n) X0(m)

Y0(m)

…

Analysis
(a)

ν(n)

g0(n)

Synthesis
(b)

+

e−jω0n

e jω0n

h0(n) X1(m)

e−jω1n

h0(n) Xk(m)

e−jωkn

…

…

h0(n) XN−1(m)

e−jωN−1n

Y1(m)
g0(n)

e jω1n

YN−1(m)
g0(n)

e jωN−1n

↑D

↑D

↑D

↓D

↓D

↓D

↓D

ωk = 2πk
N

… …

…
…

Figure 10.3 A uniform DFT filter bank.

The corresponding synthesis filter for each element in the filter bank can be
k

1, . . . , N − 1} are upsampled by a factor of I = D , filtered to remove the im-
ages, and translated in frequency by multiplication by the complex exponentials
{exp(j2πnk/N), k = 0, 1, . . . , N − 1}. The resulting frequency-translated signals
from the N filters are then summed. Thus we obtain the sequence

v(n) = 1
N

N−1∑
k=0

ej2πnk/N

[∑
m

Yk(m)g0(n−mI)
]

=
∑
m

g0(n−mI)
[

1
N

N−1∑
k=0

Yk(m)e
j2πnk/N

]

=
∑
m

g0(n−mI)yn(m)

(10.5)

Multirate Digital Signal Processing

viewed as shown in Fig. 10.3(b), where the input signal sequences {Y (m), k = 0,

810



where the factor 1/N is a normalization factor, {yn(m)} represent samples of the
inverse DFT sequence corresponding to {Yk(m)}, {g0(n)} is the impulse response of
the interpolation filter, and I = D .

The relationship between the output {Xk(n)} of the analysis filter bank and the
input {Yk(m)} to the synthesis filter bank depends on the application. Usually, {Yk(m)}
is a modified version of {Xk(m)}, where the specific modification is determined by
the application.

An alternative realization of the analysis and synthesis filter banks is illustrated

hk(n) = h0(n)ej2πnk/N , k = 0, 1, . . . , N − 1

The output of each bandpass filter is decimated by a factor D and multiplied by
exp(−j2πmk/N) to produce the DFT sequence {Xk(m)}. The modulation by the
complex exponential allows us to shift the spectrum of the signal from ωk = 2πk/N
to ω0

x(n)

…

… …
h0(n) X0(m)

Y0(m)

Analysis
(a)

ν(n)

g0(n)

Synthesis
(b)

+

e−jω0mD

e jω0mD

h1(n) X1(m)

e−jω1mD

hN−1(n) XN−1(m)

e−jωN−1mD… …

Y1(m)
g1(n)

ejω1mD

YN−1(m)
gN−1(n)

ejωN−1mD

↓D

↓D

↓D

↑D

↑D

↑D

Figure 10.4 Alternative realization of a uniform DFT filter bank.

(10.6)

Multirate Digital Signal Processing

in Fig. 10.4. The filters are realized as bandpass filters with impulse responses

= 0. Hence this realization is equivalent to the realization given in Fig. 10.3.

811



The analysis filter bank output can be written as

Xk(m) =
[∑

n

x(n)h0(mD − n)ej2πk(mD−n)/N
]
e−j2πmkD/N

(b), where the input sequences are first multiplied by the exponential factors [exp(j2π
kmD/N)], upsampled by the factor I = D , and the resulting sequences are filtered
by the bandpass interpolation filters with impulse responses

gk(n) = g0(n)ej2πnk/N
where {g0(n)} is the impulse response of the prototype filter. The outputs of these
filters are then summed to yield

v(n) = 1
N

N−1∑
k=0

{∑
m

[Yk(m)ej2πkmI/N ]gk(n−mI)
}

where I = D .
In the implementation of digital filter banks, computational efficiency can be

achieved by use of polyphase filters for decimation and interpolation. Of particular
interest is the case where the decimation factor D is selected to be equal to the
number N of frequency bands. When D = N , we say that the filter bank is critically
sampled.

10.1 Polyphase Structures of Uniform Filter Banks

For the analysis filter bank, let us define a set of N = D polyphase filters with impulse
responses

pk(n) = h0(nN − k), k = 0, 1, . . . , N − 1
and the corresponding set of decimated input sequences

xk(n) = x(nN + k), k = 0, 1, . . . , N − 1
Note that this definition of {pk(n)} implies that the commutator for the decimator
rotates clockwise.

The structure of the analysis filter bank based on the use of polyphase filters can
the

summation into the form

Xk(m) =
N−1∑
n=0

[∑
l

pn(l)xn(m− l)
]
e−j2πnk/N , k = 0, 1, . . . , D − 1

where N = D . Note that the inner summation represents the convolution of {pn(l)}
with {xn(l)}. The outer summation represents the N -point DFT of the filter outputs.

Each sweep of the commutator results in N outputs, denoted as {rn(m), n = 0,
1, . . . , N−1} from the N polyphase filters. The N -point DFT of this sequence yields
the spectral samples {Xk(m)}. For large values of N , the FFT algorithm provides an
efficient means for computing the DFT.

(10.8)

(10.9)

(10.10)

(10.11)

(10.12)

Multirate Digital Signal Processing

(10.7)

The corresponding filter bank synthesizer can be realized as shown in Fig. 10.4-

be obtained by substituting (10.10) and (10.11) into (10.7) and rearranging

The filter structure corresponding to this computation is illustrated in Fig. 10.5.

812



x(l)

p0(m) r0(m) X0(m)
x0(m)

p1(m) r1(m) X1(m)
x1(m)

pn(m) rn(m) Xk(m)
xn(m)

… … …

… … …

pN−1(m) rN−1(m) XN−1(m)
xN−1(m)

N-Point
DFT

m = 0

Digital filter bank structure for the computation of

Now suppose that the spectral samples {Xk(m)} are modified in some manner,
prescribed by the application, to produce {Yk(m)}. A filter bank synthesis filter based
on a polyphase filter structure can be realized in a similar manner. First, we define
the impulse response of the N (D = I = N) polyphase filters for the interpolation
filter as

qk(n) = g0(nN + k), k = 0, 1, . . . , N − 1
and the corresponding set of output signals as

vk(n) = v(nN + k), k = 0, 1, . . . , N − 1
Note that this definition of {qk(n)} implies that the commutator for the interpolator
rotates counterclockwise.

l

l th polyphase filter as

vl(n) =
∑
m

ql(n−m)
[

1
N

N−1∑
k=0

Yk(m)e
j2πkl/N

]
, l = 0, 1, . . . , N − 1

The term in brackets is the N -point inverse DFT of {Yk(m)}, which we denote as
{yl(m)}. Hence

vl(n) =
∑
m

ql(n−m)yl(m), l = 0, 1, . . . , N − 1

It is

(10.13)

(10.14)

(10.15)

(10.16)

Multirate Digital Signal Processing

Figure 10.5
(10.12).

By substituting (10.13) into (10.5), we can express the output v (n) of the

The synthesis structure corresponding to (10.16) is shown in Fig. 10.6.
interesting to note that by defining the polyphase interpolation filter as in (10.13),
the structure in Fig. 10.6 is the transpose of the polyphase analysis filter shown in
Fig. 10.5.

813



ν(n)

q0(m)
Y0(m) ν0(m)

q1(m)
Y1(m) ν1(m)

ql(m)
Yk(m) νl(m)

……
…

qN−1(m)
YN−1(m) νN−1(m)

y0(m)

y1(m)

yl(m)

…
…

yN−1(m)

…

Inverse
DFT

m = 0

Figure 10.6 Digital filter bank structure for the computa-

In our treatment of digital filter banks we considered the important case of
critically sampled DFT filter banks, where D = N . Other choices of D and N can be
employed in practice, but the implementation of the filters becomes more complex.
Of particular importance is the oversampled DFT filter bank, where N = KD , D
denotes the decimation factor and K is an integer that specifies the oversampling
factor. In this case it can be shown that the polyphase filter bank structures for the
analysis and synthesis filters can be implemented by use of N subfilters and N -point
DFTs and inverse DFTs.

10.2 Transmultiplexers

In a transmultiplexer for TDM-to-FDM conversion, the input signal {x(n)} is
a time-division-multiplexed signal consisting of L signals, which are separated by a
commutator switch. Each of these L signals is then modulated on a different carrier
frequency to obtain an FDM signal for transmission. In a transmultiplexer for FDM-
to-TDM conversion, the composite signal is separated by filtering into the L signal
components which are then time-division multiplexed.

In telephony, single-sideband transmission is used with channels spaced at a nom-
inal 4-kHz bandwidth. Twelve channels are usually stacked in frequency to form a
basic group channel, with a bandwidth of 48 kHz. Larger-bandwidth FDM signals are
formed by frequency translation of multiple groups into adjacent frequency bands.
We shall confine our discussion to digital transmultiplexers for 12-channel FDM and
TDM signals.

Let us first consider FDM-to-TDM conversion. The analog FDM signal is passed
The digital signal is then

Multirate Digital Signal Processing

tion of (10.16).

through an A/D converter as shown in Fig. 10.7(a).

An application of digital filter banks is in the design and implementation of digital 
transmultiplexers, which are devices for converting between time-division-multi-
plexed (TDM) signals and frequency-division-multiplexed (FDM) signals.

814



A/D
converter

SSB
demodulator Decimator

FDM

signal

TDM

signal

TDM

Multiplexer

SSB
demodulator Decimator

s2(n)

SSB
demodulator Decimator

sN(n)

…
…

(a)

x(n)

LPF
h(n)

LPF
h(n)

(b)

cos ωkn

−sin ωkn

↓D

↓D

s1(n)

Figure 10.7 Block diagram of FDM-to-TDM transmultiplexer.

demodulated to baseband by means of single-sideband demodulators. The output
of each demodulator is decimated and fed to the commutator of the TDM system.

To be specific, let us assume that the 12-channel FDM signal is sampled at the
Nyquist rate of 96 kHz and passed through a filter-bank demodulator. The basic
building block in the FDM demodulator consists of a frequency converter, a lowpass

efficiently implemented by the DFT filter bank described previously. The lowpass
filter and decimator are efficiently implemented by use of the polyphase filter struc-
ture. Thus the basic structure for the FDM-to-TDM converter has the form of a DFT
filter bank analyzer. Since the signal in each channel occupies a 4-kHz bandwidth,
its Nyquist rate is 8 kHz, and hence the polyphase filter output can be decimated by a
factor of 12. Consequently, the TDM commutator is operating at a rate of 12×8 kHz
or 96 kHz.

In TDM-to-FDM conversion, the 12-channel TDM signal is demultiplexed into
the 12 individual signals, where each signal has a rate of 8 kHz. The signal in each
channel is interpolated by a factor of 12 and frequency converted by a single-sideband

modulators are summed and fed to the D/A converter. Thus we obtain the analog

Multirate Digital Signal Processing

filter, and a decimator, as illustrated in Fig. 10.7(b). Frequency conversion can be

modulator, as shown in Fig. 10.8. The signal outputs from the 12 single-sideband

815



TDM
signal

FDM
signal

… …

… …

+

Interpolator
SSB

modulator

Interpolator
SSB

modulator D/A
convertor

Interpolator
SSB

modulator

Figure 10.8 Block diagram of TDM-to-FDM transmultiplexer.

FDM signal for transmission. As in the case of FDM-to-TDM conversion, the inter-
polator and the modulator filter are combined and efficiently implemented by use of
a polyphase filter. The frequency translation can be accomplished by the DFT. Con-
sequently, the TDM-to-FDM converter encompasses the basic principles introduced
previously in our discussion of DFT filter bank synthesis.

11 Two-Channel Quadrature Mirror Filter Bank

The basic building block in applications of quadrature mirror filters (QMF) is the two-

that employs two decimators in the “signal analysis” section and two interpolators
in the “signal synthesis” section. The lowpass and highpass filters in the analysis
section have impulse responses h0(n) and h1(n), respectively. Similarly, the lowpass
and highpass filters contained in the synthesis section have impulse responses g0(n)
and g1(n), respectively

The Fourier transforms of the signals at the outputs of the two decimators are

Xa0(ω) = 12
[
X

(ω
2

)
H0

(ω
2

)
+X

(
ω − 2π

2

)
H0

(
ω − 2π

2

)]

Xa1(ω) = 12
[
X

(ω
2

)
H1

(ω
2

)
+X

(
ω − 2π

2

)
H1

(
ω − 2π

2

)]

↓2 G0(z)

x(n)x(n)
+

~ ~

HPF
H1(z)

LPF
H0(z)

G1(z)
Xa1 Xs1

Xa0 Xs0

~ ~

ˆ

Analysis
section

Synthesis
section

↑2

↑2↓2

Figure 11.1 Two-channel QMF bank.

(11.1)

Multirate Digital Signal Processing

channel QMF bank shown in Fig. 11.1. This is a multirate digital filter structure

816



If Xs0(ω) and Xs1(ω) represent the two inputs to the synthesis section, the output is
simply

X̂(ω) = Xs0(2ω)G0(ω)+Xs1(2ω)G1(ω)
Now, suppose that we connect the analysis filter to the corresponding synthesis filter,
so that Xa0(ω) = Xs0(ω) and Xa1(ω) = Xs1

X̂(ω) = 1
2

[H0(ω)G0(ω)+H1(ω)G1(ω)]X(ω)

+ 1
2

[H0(ω − π)G0(ω)+H1(ω − π)G1(ω)]X(ω − π)

second term represents the effect of aliasing, which we would like to eliminate.

X̂(z) = 1
2

[H0(z)G0(z)+H1(z)G1(z)]X(z)

+ 1
2

[H0(−z)G0(z)+H1(−z)G1(z)]X(−z)

= Q(z)X(z)+ A(z)X(−z)

where, by definition,

Q(z) = 1
2

[H0(z)G0(z)+H1(z)G1(z)]

A(z) = 1
2

[H0(−z)G0(z)+H1(−z)G1(z)]

11.1 Elimination of Aliasing

To eliminate aliasing, we require that A(z) = 0, i.e.,

H0(−z)G0(z)+H1(−z)G1(z) = 0

In the frequency domain, this condition becomes

H0(ω − π)G0(ω)+H1(ω − π)G1(ω) = 0

This condition can be simply satisfied by selecting G0(ω) and G1(ω) as

G0(ω) = H1(ω − π), G1(ω) = −H0(ω − π)

(11.2)

(11.3)

(11.4)

(11.5)

(11.6)

(11.7)

(11.8)

Multirate Digital Signal Processing

into (11.2), we obtain

The first term in (11.3) is the desired signal output from the QMF bank. The

In the z-transform domain (11.3) is expressed as

Thus, the second term in (11.3) vanishes and the filter bank is alias-free.

(ω). Then, by substituting from (11.1)

817



π

2
π0

1
H0(ω) H1(ω)

ω

Figure 11.2 Mirror image characteristics of the analysis filters
H0(ω) and H1(ω).

To elaborate, let us assume that H0(ω) is a lowpass filter and H1(ω) is a mirror-
Then we can express H0(ω) and

H1(ω) as
H0(ω) = H(ω)
H1(ω) = H(ω − π)

where H(ω) is the frequency response of a lowpass filter.
corresponding relations are

h0(n) = h(n)
h1(n) = (−1)nh(n)

As a consequence, H0(ω) and H1(ω) have mirror-image symmetry about the fre-
To be consistent with the constraint in

0

G0(ω) = H(ω)
and the highpass filter G1(ω) as

G1(ω) = −H(ω − π)
In the time domain, these relations become

g0(n) = h(n)
g1(n) = (−1)nh(n)

In the z-transform domain, the relations for the elimination of aliasing are:

H0(z) = H(z)
H1(z) = H(−z)
G0(z) = H(z)
G1(z) = −H(−z)

(11.9)

In the time domain, the

(11.10)

(11.11)

(11.12)

(11.13)

(11.14)

Multirate Digital Signal Processing

image highpass filter as shown in Fig. 11.2.

quency ω = π/2, as shown in Fig.11.2.
(11.8), we select the lowpass filter G (ω) as

818



11.2 Condition for Perfect Reconstruction

With A(z) = 0, we now consider the condition for which the output x(n) of the QMF
bank is identical to the input x(n), except for an arbitrary delay, for all possible inputs.
When this condition is satisfied, the filter bank is called a perfect reconstruction QMF
bank. Thus, we require that

Q(z) = 1
2

[H0(z)G0(z)+H1(z)G1(z)] = z−k

tion may be expressed as
H 2(z)−H 2(−z) = 2z−k

or, equivalently,
H 2(ω)−H 2(ω − π) = 2e−jωk

Therefore, for perfect reconstruction, the frequency response H(ω) of the lowpass
filter in the two-channel QMF bank must satisfy the magnitude condition

∣∣∣H 2(ω)−H 2(ω − π)∣∣∣ = C
where C is a positive constant, e.g., C = 2. We note that if H(ω) satisfies the

QMF output x(n) is simply a delayed version of the input sequence x(n). However,
linear phase is not a necessary condition for perfect reconstruction.

11.3 Polyphase Form of the QMF Bank

The two-channel alias-free QMF bank can be realized efficiently by employing poly-
phase filters. Toward this goal, H0(z) and H1(z) can be expressed as

H0(z) = P0(z2)+ z−1P1(z2)
H1(z) = P0(z2)− z−1P1(z2)

sentation of the filters G0(z) and G1(z) as

G0(z) = P0(z2)+ z−1P1(z2)

G1(z) = −
[
P0(z

2)− z−1P1(z2)
]

Thus, we obtain the polyphase realization of the QMF bank as shown in Figure
11.3(a). The corresponding computationally efficient polyphase realization is

(11.15)

(11.16)

(11.17)

(11.18)

(11.20)

Multirate Digital Signal Processing

By making use of the relations in (11.14), the condition for perfect reconstruc-

magnitude condition in (11.18) and is designed to have linear phase, then the

where we have used the relationship given in (11.14).

(11.19)

Similarly, using the relations given in (11.14), we obtain the polyphase repre-

shown in Figure 11.3(b).

819



x(n)

x(n)

+ ++ P0(z2)P1(z2)
ˆ↑2↓2

++ P1(z2)P0(z2)
↑2↓2

z−1 z−1

(a)
x(n)

+ ++ P0(z2)P1(z2)
x(n)ˆ↑2↓2

++ P1(z2)P0(z2) ↑2↓2

z−1 z−1

(b)

Synthesis SectionAnalysis Section

− −

− −

Figure 11.3 Polyphase realization of the two-channel QMF bank.

11.4 Linear Phase FIR QMF Bank

Now, let us consider the use of a linear phase filter H(ω). Hence H(ω) may be
expressed in the form

H(ω) = Hr(ω)e−jω(N−1)/2

where N is the filter length. Then

H 2(ω) = H 2r (ω)e−jω(N−1)

= |H(ω)|2e−jω(N−1)

and
H 2(ω − π) = H 2r (ω − π)e−j (ω−π)(N−1)

= (−1)N−1|H(ω − π)|2e−jω(N−1)

Therefore, the overall transfer function of the two-channel QMF which employs
linear-phase FIR filters is

X̂(ω)

X(ω)
=

[
|H(ω)|2 − (−1)N−1|H(ω− π)|2

]
e−jω(N−1)

Note that the overall filter has a delay ofN−1 samples and a magnitude characteristic

M(ω) = |H(ω)|2 − (−1)N−1|H(ω − π)|2

We also note that when N is odd, M(π/2) = 0, because |H(π/2)| = |H(3π/2)|. This
is an undesirable property for a QMF design. On the other hand, when N is even,

M(ω) = |H(ω)|2 + |H(ω − π)|2

(11.21)

(11.22)

(11.23)

(11.24)

(11.25)

(11.26)

Multirate Digital Signal Processing

820



QMF should satisfy the condition

M(ω) = |H(ω)|2 + |H(ω − π)|2 = 1 for all ω

Unfortunately, the only filter frequency response
2 = cos2 aω. Con-

sequently, any nontrivial linear-phase FIR filter H(ω) introduces some amplitude
distortion.

The amount of amplitude distortion introduced by a nontrivial linear phase FIR
filter in the QMF can be minimized by optimizing the FIR filter coefficients. A
particularly effective method is to select the filter coefficients of H(ω) such that
M(ω) is made as flat as possible while simultaneously minimizing (or constraining)
the stopband energy ofH(ω). This approach leads to the minimization of the integral
squared error

J = w
∫ π
ωs

|H(ω)|2dω + (1− w)
∫ π

0
[M(ω)− 1]2dω

where w is a weighting factor in the range 0 < w < 1. In performing the optimiza-
tion, the filter impulse response is constrained to be symmetric (linear phase). This
optimization is easily done numerically on a digital computer. This approach has
been used by Johnston (1980) and Jain and Crochiere (1984) to design two-channel
QMFs. Tables of optimum filter coefficients have been tabulated by Johnston (1980).

11.5 IIR QMF Bank

As an alternative to linear-phase FIR filters, we can design an IIR filter that satisfies
For this purpose, elliptic filters provide

especially efficient designs. Since the QMF would introduce some phase distortion,
the signal at the output of the QMF can be passed through an all-pass phase equalizer
designed to minimize phase distortion.

11.6 Perfect Reconstruction Two-Channel FIR QMF Bank

We have observed that neither linear-phase FIR filters nor IIR filters described above
yield perfect reconstruction in a two-channel QMF bank. However, as shown by
Smith and Barnwell (1984), perfect reconstruction can be achieved by designing
H(ω) as a linear-phase FIR half-band filter of length 2N − 1.

A half-band filter is defined as a zero-phase FIR filter whose impulse response
{b(n)} satisfies the condition

b(2n) =
{

constant, n = 0
0, n �= 0

which avoids the problem of a zero at ω = π/2. For N even, the ideal two-channel

(11.27)

(11.28)

(11.29)

Multirate Digital Signal Processing

which follows from (11.25).
function that satisfies (11.27) is the trivial function |H(ω)|

the all-pass constraint given by (11.18).

821



1 + δ1

δ2

−δ2

1 − δ1

0
ωp ωs

δ1 = δ2 ωp + ωs = π

Figure 11.4 Frequency response characteristic of FIR half-band
filter.

Hence all the even-numbered samples are zero except at n = 0. The zero-phase
requirement implies that b(n) = b(−n). The frequency response of such a filter is

B(ω) =
K∑

n=−K
b(n)e−jωn

where K is odd. Furthermore, B(ω) satisfies the condition that B(ω)+ B(π − ω) is
equal to a constant for all frequencies. The typical frequency response characteristic

We note that the filter response is
symmetric with respect to π/2, the band edges frequencies ωp and ωs are symmetric
about ω = π/2, and the peak passband and stopband errors are equal. We also note
that the filter can be made causal by introducing a delay of K samples.

Now, suppose that we design an FIR half-band filter of length 2N−1, where N is

another half-band filter with frequency response

B+(ω) = B(ω)+ δe−jω(N−1)

Note that B+(ω) is nonnegative and hence it has the
spectral factorization

B+(z) = H(z)H(z−1)z−(N−1)

or, equivalently,
B+(ω) = |H(ω)|2e−jω(N−1)

where H(ω) is the frequency response of an FIR filter of length N with real co-
efficients. Due to the symmetry of B+(ω) with respect to ω = π/2, we also have

(11.30)

(11.31)

(11.32)

(11.33)

Multirate Digital Signal Processing

of a half-band filter is shown in Fig. 11.4.

even, with frequency response as shown in Fig. 11.5(a). From B(ω) we construct

as shown in Fig. 11.5(b).

822



1 + δ

1 − δ

δ

−δπ
0

1

ωp ωs

ωp + ωs = π

2

(a)

B(ω)
amplitude

response of 
G(z)

1 + 2δ

1

2δ

π
0

ωp ωs
2

(b)

B+(ω)
amplitude

response of 
B+(z)

ω

δ

2δ
0

(c)

Magnitude
responses of

analysis 
filters

ω

H0 H1

Figure 11.5 Frequency response characteristic of FIR half-band filters
B(ω) and B+(ω). (From Vaidyanathan (1987))

B+(z)+ (−1)N−1B+(−z) = αz−(N−1)

or, equivalently,

B+(ω)+ (−1)N−1B+(ω − π) = αe−jω(N−1)

H(z)H(z−1)+H(−z)H(−z−1) = α

(11.34)

(11.35)

(11.36)

Multirate Digital Signal Processing

where α is a constant. Thus, by substituting (11.32) into (11.34), we obtain

823



0
H1(−z) and G1(z) = −H0(−z), it follows that these conditions are satisfied by choos-
ing H1(z),G0(z), and G1(z) as

H0(z) = H(z)
H1(z) = −z−(N−1)H0(−z−1)
G0(z) = z−(N−1)H0(z−1)
G1(z) = z−(N−1)H1(z−1) = −H0(−z)

Thus aliasing distortion is eliminated and, since X̂(ω)/X(ω) is a constant, the QMF
performs perfect reconstruction so that x(n) = αx(n − N + 1). However, we note
that H(z) is not a linear-phase filter. The FIR filters H0(z),H1(z),G0(z), and G1(z)
in the two-channel QMF bank are efficiently realized as polyphase filters as shown
previously.

11.7 Two-Channel QMF Banks in Subband Coding

based on subdividing the signal into several subbands and encoding each subband

four subbands, namely, 0 ≤ F ≤ Fs/16, Fs/16 < F ≤ Fs/8, Fs/8 < F ≤ Fs/4, and
Fs/4 < F ≤ Fs/2, where Fs is the sampling frequency. The subdivision into four
subbands can be accomplished by use of three two-channel QMF analysis sections.
After encoding and transmission through the channel, each subband signal is decoded
and reconstructed by passing the subbands through three two-channel QMF synthesis
filters. The system configuration for subband coding employing four subbands is

(a)

Output

signal

Encoder

Channel
Encoder

Encoder

Encoder

Decoder

Decoder

Decoder

Decoder

Synthesis
QMF

Signal

samples

Analysis
QMF

Analysis
QMF

Analysis
QMF

Synthesis
QMF

Synthesis
QMF

x(n) x(n)ˆ

(b) (c)

+ +P0(z)
x(n)ˆ↑2

+ P1(z) ↑2

z−1

Synthesis Section of QMF

x(n)

+P1(z)↓2

+

− −

P0(z)↓2

z−1

Analysis Section of QMF

Figure 11.6 System for subband coding using two-channel QMF banks.

(11.37)

Multirate Digital Signal Processing

Since H(z) satisfies (11.36) and since aliasing is eliminated when we have G (z) =

In Section 9.4 we described a method for efficient encoding of a speech signal

separately. For example, in Figure 9.4 we illustrated the separation of a signal into

illustrated in Figure 11.6.

824



12 M -Channel QMF Bank

In this section, we consider the generalization of the QMF bank to M channels.

the input to the analysis section, x(a)k (n), 0 ≤ k ≤ M − 1, are the outputs of the
analysis filters, x(s)k (n), 0 ≤ k ≤ M − 1, are the inputs to the synthesis filters and x̂(n)
is the output of the synthesis section.

The M outputs from the analysis filters may be expressed in the z-transform
domain as

X
(a)
k (z) =

1
M

M−1∑
m=0

Hk

(
z1/MWmM

)
X

(
z1/MWmM

)
, 0 ≤ k ≤ M − 1

where WM = e−j2π/M . The output from the synthesis section is

X̂(z) =
M−1∑
k=0

X
(s)
k

(
zM

)
Gk(z)

As in the case of the two-channel QMF bank, we set X(a)k (z) = X(s)k (z). Then, if we

X̂(z) =
M−1∑
k=0

Gk(z)

[
1
M

M−1∑
m=0

Hk
(
zWmM

)
X

(
zWmM

)]

=
M−1∑
m=0

[
1
M

M−1∑
k=0

Gk(z)Hk
(
zWmM

)]
X

(
zWmM

)

x(n)

x(z)
+

H0(z)

H1(z)

H2(z)

HM−1(z) ↓M

↓M

↓M

↓M

...

G0(z)

G1(z)

G2(z)

GM−1(z)

......

↓

M

↓

M

↓

M

↓

M

~ ~ x
(s)(n)x(a)(n)0 0

~ ~ x
(s)(n)x(a)(n)1 1

~ ~ x
(s)(n)x(a)(n)2 2

~ ~x 
(a) (n)M−1 x 

(s) (n)M−1 

x(n)ˆ

Xz

Figure 12.1 An M -channel QMF bank.

(12.1)

(12.2)

(12.3)

Multirate Digital Signal Processing

Figure 12.1 illustrates the structure of an M -channel QMF bank, where x(n) is

substitute (12.1) into (12.2), we obtain

825



It is convenient to define the term in the brackets as

Rm(z) = 1
M

M−1∑
k=0

Gk(z)Hk(zW
m
M), 0 ≤ m ≤ M − 1

X̂(z) =
M−1∑
m=0

Rm(z)X(zW
m
N )

= R0(z)X(z)+
M−1∑
m=1

Rm(z)X
(
zWmM

)

the second term is the aliasing component.

12.1 Alias-Free and Perfect Reconstruction Condition

Rm(z) = 0, 1 ≤ m ≤ M − 1

With the elimination of the alias terms, the M -channel QMF bank becomes a linear
time-invariant system that satisfies the input-output relation

X̂(z) = R0(z)X(z)

where

R0(z) = 1
M

M−1∑
k=0

Hk(z)Gk(z)

Then, the condition for a perfect reconstruction M -channel QMF bank becomes

R0(z) = Cz−k

where C and k are positive constants.

12.2 Polyphase Form of the M -Channel QMF Bank

An efficient implementation of the M -channel QMF bank is achieved by employing
polyphase filters. To obtain the polyphase form for the analysis filter bank, the k th
filter Hk(z) is represented as

Hk(z) =
M−1∑
m=0

z−mPkm(z), 0 ≤ k ≤ M − 1

(12.4)

(12.5)

(12.6)

(12.7)

(12.8)

(12.9)

(12.10)

Multirate Digital Signal Processing

Then, (12.3) may be expressed as

We note the first term in (12.5) is the alias-free component of the QMF bank and

From (12.5), it is clear that aliasing is eliminated by forcing the condition

826



We may express the equations for the M polyphase filter in matrix form as

H(z) = P(zM)a(z)
where

H(z) = [H0(z)H1(z) · · · HM−1(z)]t

a(z) =
[
1 z−1 z−2 · · · z−(M−1)

]t
and

P(z) =




P00(z) P01(z) · · · P0M−1(z)
P10(z) P11(z) · · · P1M−1(z)

...

PM−1 0(z) PM−1 1(z) · · · PM−1 M−1(z)




and after applying the first noble identity we obtain the structure shown in Fig-

The synthesis section can be constructed in a similar manner. Suppose we use

the filters {Gk(z)}. Thus,

Gk(z) =
M−1∑
m=0

z−(M−1−m)Qkm(zM), 0 ≤ k ≤ M − 1

G(z) = z−(M−1)Q(zM)a(z−1)

G(z) = [G0(z) G1(z) · · · GM−1(z) ]t

Q(z) =




Q00(z) Q01(z) · · · Q0 M−1(z)
Q10(z) Q11(z) · · · Q1 M−1(z)
...

QM−1 0(z) QM−1 1(z) · · · QM−1 M−1(z)




(a) (b)

x(n)

z−1

z−1

z−1

P(zM)

↓M

↓M

↓M

↓M x(a)(n)

x(a)(n)1

0

x(a)(n)2

x (a) (n)M−1 

..
.

..
.

x(n)

z−1

z−1

z−1

P(z)

↓M

↓M

↓M

↓M x(a)(n)

x(a)(n)1

0

x(a)(n)2

x (a) (n)M−1 

..
.

..
.

Polyphase structure of the analysis section of an M -channel
QMF bank (a) before and (b) after applying the first noble identity.

(12.11)

(12.12)

(12.13)

(12.14)

(12.15)

(12.16)

Multirate Digital Signal Processing

The polyphase form of the analysis filter bank is shown in Figure 12.2(a),

ure 12.2(b).

a type II (transpose) form (see Problem 15) for the polyphase representation of

When expressed in matrix form, (12.14) becomes

where a(z) is defined in (12.12) and

Figure 12.2

827



(b)

Q(z)

x(s)(n)1

x(s)(n)0

x(s)(n)2

x (s) (n)M−1 

+

+

z−1

+

z−1

z−1

..
.

↓

M

↓

M

↓

M

↓

M x(n)ˆ

(a)

+

+

z−1

+

z−1

z−1

Q(zM)

x(s)(n)1

x(s)(n)0

x(s)(n)2

x (s) (n)M−1 

..
.

↓

M

↓

M

↓

M

↓

M x(n)ˆ

Polyphase structure of the synthesis section of an M -channel QMF
bank (a) before and (b) after applying the first noble identity.

Q(z)P(z) = Cz−kI

where I is the M×M identity matrix. Hence, if the polyphase matrix P(z) is known,
then the polyphase synthesis matrix Q(z) is

Q(z) = Cz−k[P(z)]−1

Polyphase

M -channel QMF
bank

Q(z)P(z)

+

+

z−1z−1

z−1

+

z−1

z−1z−1

..
...
.

..
.

↓

M

↓

M

↓

M

↓

M x(n)

x(n)

ˆ

↓M

↓M

↓M

↓M

(12.17)

(12.18)

Multirate Digital Signal Processing

Figure 12.3

observe that the perfect reconstruction condition can be restated as
From the structure of the M -channel QMF bank shown in Figure 12.4, we

realization of the

Figure 12.4

Therefore, the synthesis section of the M-channel QMF bank is realized as shown 
in Figure 11.3. By combining Figures 12.2(b) and 12.3(b), we obtain the poly-
phase structure of the complete M-channel QMF bank shown in Figure 12.4.

828



Suppose the polyphase matrix for a three-channel perfect reconstruction FIR QMF bank is

P(z3) =

 1 1 22 3 1

1 2 1




Determine the analysis and the synthesis filters in the QMF bank.

Solution.


H0(z)H1(z)
H2(z)


 =


 1 1 22 3 1

1 2 1





 1z−1
z−2




Hence,

H0(z) = 1+ z−1 + 2z−2, H1(z) = 2 + 3z−1 + z−2, H2(z) = 1+ 2z−1 + z−2

The inverse of P(z3) is

[
P(z3)

]−1 = 1
2


 1 3 −5−1 −1 3

1 −1 1




We may scale this inverse by the factor 2, so that

Q(z3) = 2[P(z3)]−1


G0(z)G1(z)
G2(z)


 = z−2


 1 3 −5−1 −1 3

1 −1 1





 1z
z2




Hence,

G0(z) = −5+ 3z−1 + z−2, G1(z) = 3− z−1 − z−2, G3(z) = 1− z−1 + z−2

Vaidyanathan (1992) treats the design of M -channel perfect reconstruction QMF
banks by selecting the analysis filters Hk(z) to be FIR with a paraunitary polyphase
structure, i.e.,

P̃(z)P(z) = dI, d > 0
where P̃(z) is the paraconjugate of P(z). That is, P̃(z) is formed by the transpose of
P(1/z), with the coefficients of P(z) replaced by their complex conjugates. Then, the
polyphase filters in the synthesis section are designed to satisfy

Q(z) = Cz−kP̃(z), C > 0, k > 0

(12.19)

(12.20)

Multirate Digital Signal Processing

EXAMPLE 12.1

The analysis filters are given by (12.11) as

Then, by applying (12.15), we obtain the synthesis filters as

829



Magnitude response for
analysis filters in an M = 3
QMF bank.

π

3
2π

3
π0

ω

1
H0 H1 H2

It can be shown (see Vaidyanathan et al. (1989)) that any causal Lth-degree FIR
paraunitary matrix P(z) can be expressed in a product form as

P(z) = VL(z)VL−1(z) · · ·V1(z)U
where U is a unitary matrix and {Vm(z)} are paraunitary matrices that have the form

Vm(z) = I− υmυHm + z−1υmυHm
where υm is an M -dimensional vector of unit norm. Then, the design of the synthesis
filters reduces to the optimization of the components of υm and ui by minimizing an
objective function, which may be a generalization of the two-channel QMF objective

J =
M−1∑
k=0

∫
kth stopband

|Hk(ω)|2dω

by employing a nonlinear optimization technique to determine the components of
υm and ui . Thus, the vectors υm and ui completely determine P(z) and, hence, the

k

For example, consider the design of a perfect reconstruction three-channel QMF
The design procedure

described above yields the magnitude responses for the analysis filters that are shown

of the filters is approximately 20 dB.

Magnitude response of
optimized analysis filters
for an M = 3 FIR perfect
reconstruction QMF bank.
(From Multirate Systems
and Filter Banks, by
P.P. Vaidyanathan, ©1993
by Prentice Hall. Reprinted
with permission of the
publisher.)

−10

0

−20

−30

−40

−50

−60
0.0 0.1 0.2 0.3 0.4 0.5

Normalized frequency (ω/2π)

dB

H0(ω) H1(ω) H2(ω)

(12.21)

(12.22)

(12.23)

Multirate Digital Signal Processing

Figure 12.5

function given by (11.28). In particular, we may minimize the objective function

analysis filters H (z). The synthesis filters are then determined from (12.20).

bank with magnitude responses as shown in Fig. 12.5.

in Fig. 12.6. The filter length is N = 14. We observe that the stopband attenuation

Figure 12.6

830



13 Summary and References

The need for sampling rate conversion arises frequently in digital signal processing
applications. In this chapter we first treated sampling rate reduction (decimation)
and sampling rate increase (interpolation) by integer factors and then demonstrated
how the two processes can be combined to obtain sampling rate conversion by any
rational factor. Then, we described a method to achieve sampling rate conversion by
an arbitrary factor. In the special case where the signal to be resampled is a bandpass
signal, we described methods for performing the sampling rate conversion.

In general, the implementation of sampling rate conversion requires the use of a
linear time-variant filter. We described methods for implementing such filters, includ-
ing the class of polyphase filter structures, which are especially simple to implement.
We also described the use of multistage implementations of multirate conversion as
a means of simplifying the complexity of the filter required to meet the specifications.

We also described a number of applications that employ multirate signal process-
ing, including the implementation of narrowband filters, phase shifters, filter banks,
subband speech coders, quadrature mirror filters and transmultiplexers. These are
just a few of the many applications encountered in practice where multirate signal
processing is used.

The first comprehensive treatment of multirate signal processing was given in the
book by Crochiere and Rabiner (1983). In the technical literature, we cite the papers
by Schafer and Rabiner (1973), and Crochiere and Rabiner (1975, 1976, 1981, 1983).
The use of interpolation methods to achieve sampling rate conversion by an arbitrary
factor is treated in a paper by Ramstad (1984). A thorough tutorial treatment of
multirate digital filters and filter banks, including quadrature mirror filters, is given by
Vetterli (1987), and by Vaidyanathan (1990, 1993), where many references on various
applications are cited. A comprehensive survey of digital transmultiplexing methods
is found in the paper by Scheuermann and Gockler (1981). Subband coding of speech
has been considered in many publications. The pioneering work on this topic was
done by Crochiere (1977, 1981) and by Garland and Esteban (1980). Subband coding
has also been applied to coding of images. We mention the papers by Vetterli (1984),
Woods and O’Neil (1986), Smith and Eddins (1988), and Safranek et al. (1988) as
just a few examples. In closing, we wish to emphasize that multirate signal processing
continues to be a very active research area.

Problems

1 An analog signal xa(t) is bandlimited to the range 900 ≤ F ≤ 1100 Hz. It is used as

filter with cutoff frequency Fc = 125 Hz.
xa(t) x(n)

Fx = = 2500

w(n) v(n) y(n)
H(ω)

cos(0.8πn)

A/D
converter ↓10

1
Tx

Fy = = 250
1
Ty

Multirate Digital Signal Processing

an input to the system shown in Fig. P1. In this system, H(ω) is an ideal lowpass

Figure P1

831



(a) Determine and sketch the spectra for the signals x(n), w(n), v(n), and y(n).
(b) Show that it is possible to obtain y(n) by sampling xa(t) with period T = 4

milliseconds.
2 Consider the signal x(n) = anu(n), |a| < 1.

(a) Determine the spectrum X(ω).
(b) The signal x(n) is applied to a decimator that reduces the rate by a factor of 2.

Determine the output spectrum.
(c) Show that the spectrum in part (b) is simply the Fourier transform of x(2n).

3 The sequence x(n) is obtained by sampling an analog signal with period T . From
this signal a new signal is derived having the sampling period T/2 by use of a linear
interpolation method described by the equation

y(n) =


x(n/2), n even
1
2

[
x

(
n− 1

2

)
+ x

(
n+ 1

2

)]
, n odd

(a) Show that this linear interpolation scheme can be realized by basic digital signal
processing elements.

(b) Determine the spectrum of y(n) when the spectrum of x(n) is

X(ω) =
{

1, 0 ≤ |ω| ≤ 0.2π
0, otherwise

(c) Determine the spectrum of y(n) when the spectrum of x(n) is

X(ω) =
{

1, 0.7π ≤ |ω| ≤ 0.9π
0, otherwise

4 Consider a signal x(n) with Fourier transform

X(ω) = 0, ωn < |ω| ≤ π
fm < |f | ≤ 12

(a) Show that the signal x(n) can be recovered from its samples x(mD) if the sam-
pling frequency ωs = 2π/D ≤ 2ωm(fs = 1/D ≥ 2fm).

(b) Show that x(n) can be reconstructed using the formula

x(n) =
∞∑

k=−∞
x(kD)hr(n− kD)

where

hr(n) = sin(2πfcn)2πn ,
fm < fc < fs − fm
ωm < ωc < ωs − ωm

(c) Show that the bandlimited interpolation in part (b) can be thought of as a two-
step process, first, increasing the sampling rate by a factor of D by inserting
(D−1) zero samples between successive samples of the decimated signal xa(n) =
x(nD), and second, filtering the resulting signal using an ideal lowpass filter with
cutoff frequency ωc .

Multirate Digital Signal Processing

832



5 In this problem we illustrate the concepts of sampling and decimation for discrete-
time signals. To this end consider a signal x(n) with Fourier transform X(ω) as in

0

x(n)

1 2 n−2 −1 …

…

0π
3

X(ω)

1

− π π ω
3

−π

(a) Sampling x(n) with a sampling period D = 2 results in the signal

xs(n) =
{
x(n), n = 0,±2,±4, . . .
0, n = ±1,±3,±5, . . .

Compute and sketch the signal xs(n) and its Fourier transform Xs(ω). Can we
reconstruct x(n) from xs(n)? How?

(b) Decimating x(n) by a factor of D = 2 produces the signal

xd(n) = x(2n), all n

Show that Xd(ω) = Xs(ω/2). Plot the signal xd(n) and its transform Xd(ω). Do
we lose any information when we decimate the sampled signal xs(n)?

6 Design a decimator that downsamples an input signal x(n) by a factor D = 5. Use
the Remez algorithm to determine the coefficients of the FIR filter that has a 0.1-dB
ripple in the passband (0 ≤ ω ≤ π/5) and is down by at least 30 dB in the stopband.
Also determine the corresponding polyphase filter structure for implementing the
decimator.

7 Design an interpolator that increases the input sampling rate by a factor of I = 2. Use
the Remez algorithm to determine the coefficients of the FIR filter that has a 0.1-dB
ripple in the passband (0 ≤ ω ≤ π/2) and is down by at least 30 dB in the stopband.
Also, determine the corresponding polyphase filter structure for implementing the
interpolator.

8 Design a sample rate converter that reduces the sampling rate by a factor 25 . Use
the Remez algorithm to determine the coefficients of the FIR filter that has a 0.1-dB
ripple in the passband and is down by at least 30 dB in the stopband. Specify the
sets of time-variant coefficients g(n,m) and the corresponding coefficients in the
polyphase filter realization of the sample rate converter.

Multirate Digital Signal Processing

Fig. P5.

Figure P5

833



9 Consider the two different ways of cascading a decimator with an interpolator shown

x(n) y1(n)

(a)

x(n) y2(n)

(b)

↓D↑I

↑I↓D

(a) If D = I , show that the outputs of the two configurations are different. Hence,
in general, the two systems are not identical.

(b) Show that the two systems are identical if and only if D and I are relatively
prime.

10 Prove the equivalence of the two decimator and interpolator configurations shown

x(n) y(n)
H(z)

(a)

–––
x(n) y(n)

H(zD)

x(n) y(n)

(b)

–––
x(n) y(n)

H(z) H(zI)

↓D ↓D

↑I ↑I

11 Consider an arbitrary digital filter with transfer function

H(z) =
∞∑

n=−∞
h(n)z−n

(a) Perform a two-component polyphase decomposition of H(z) by grouping the
even-numbered samples h0(n) = h(2n) and the odd-numbered samples h1(n) =
h(2n+ 1). Thus show that H(z) can be expressed as

H(z) = H0(z2)+ z−1H1(z2)

and determine H0(z) and H1(z).

Multirate Digital Signal Processing

in Fig. P9.

Figure P9

Figure P10

in Fig. P10 (noble identities) (see Vaidyanathan, 1990).

834



(b) Generalize the result in part (a) by showing that H(z) can be decomposed into
a D-component polyphase filter structure with transfer function

H(z) =
D−1∑
k=0

z−kHk(zD)

Determine Hk(z).

(c) For the IIR filter with transfer function

H(z) = 1
1− az−1

determine H0(z) and H1(z) for the two-component decomposition.

12 A sequence x(n) is upsampled by I = 2, it passes through an LTI system H1(z),
and then it is downsampled by D = 2. Can we replace this process with a single
LTI system H2(z)? If the answer is positive, determine the system function of this
system.

13 Plot the signals and their corresponding spectra for rational sampling rate conversion
by (a) I/D = 5/3 and (b) I/D = 3/5. Assume that the spectrum of the input signal
x(n) occupies the entire range −π ≤ ωx ≤ π.

14 We wish to design an efficient nonrecursive decimator for D = 8 using the factoriza-
tion

H(z) = [(1+ z−1)(1+ z−2)(1+ z−4) . . . (1+ z−2K−1)]5

(a) Derive an efficient implementation using filters with system function Hk(z) =
(1+ z−1)5 .

(b) Show that each stage of the obtained decimator can be implemented more effi-
ciently using a polyphase decomposition.

15 Let us consider an alternative polyphase decomposition by defining new polyphase
filters Qm(zN) as

H(z) =
N−1∑
m=0

z−(N−1−m)Qm(zN)

This polyphase decomposition is called a type II form to distinguish it from the
conventional decomposition based on polyphase filters Pm(zN).

(a) Show that the type II form polyphase filters Qm(z) are related to the polyphase
filters Pm(zN) as follows:

Qm(z
N) = PN−1−m(zN)

(b) Sketch the polyphase filter structure for H(z) based in the polyphase filters
Qm(z

N) and, thus, show that this structure is an alternative transpose form.

Multirate Digital Signal Processing

835



16

17

D = 100
Passband: 0 ≤ F ≤ 50

Transition band: 50 ≤ F ≤ 55
Input sampling rate: 10,000 Hz

Ripple: δ1 = 10−1, δ2 = 10−3

18 Design a linear-phase FIR filter that satisfies the following specifications based on a
single-stage and a two-stage multirate structure:

Sampling rate: 10,000 Hz

Passband: 0 ≤ F ≤ 60
Transition band: 60 ≤ F ≤ 65

Ripple: δ1 = 10−1, δ2 = 10−3

19
coefficients are zero.

20 Design one-stage and two-stage interpolators to meet the following specifications:

I = 20
Input sampling rate: 10,000 Hz

Passband: 0 ≤ F ≤ 90
Transition band: 90 ≤ F ≤ 100

Ripple: δ1 = 10−2, δ2 = 10−3

21

22 Show that the transpose of an L-stage interpolator for increasing the sampling rate by
an integer factor I is equivalent to an L-stage decimator that decreases the sampling
rate by a factor D = I .

23 Sketch the polyphase filter structure for achieving a time advance of (k/I)Tx in a
sequence x(n).

24 Prove the following expressions for an interpolator of order I .
(a) The impulse response h(n) can be expressed as

h(n) =
I−1∑
k=0

pk(n− k)

        
polator 

Design a two-stage decimator for the following specifications:

Multirate Digital Signal Processing

Use the result in Problem 15 to determine the type II form of the I = 3 inter-
in Figure 5.12(b).

Prove that the half-band filter that satisfies (11.35) is always odd and the even

By using (10.15) derive the equations corresponding to the structure for the poly-
phase synthesis section shown in Fig. 10.6.

836



where

pk(n) =
{
pk(n/I), n = 0,±I,±2I, . . .
0, otherwise

(b) H(z) may be expressed as

H(z) =
I−1∑
k=0

z−kpk(z)

(c) Pk(z) = 1
I

∞∑
n=−∞

I−1∑
l=0

h(n)ej2πl(n−k)/I z−(n−k)/I

Pk(ω) = 1
I

I−1∑
l=0

H

(
ω − 2πl

I

)
ej (ω−2πl)k/I

25 Consider the interpolation of a signal by a factor I . The interpolation filter with
transfer function H(z) is implemented by a polyphase filter structure based on an
alternative (type II) decomposition, namely

H(z) =
I−1∑
m=0

z−(I−1−m)Qm(zI )

Determine and sketch the structure of the interpolator that employs the polyphase
filters Qm(z), 0 ≤ m ≤ I − 1.

26 In the polyphase filter structures for a uniform DFT filter bank described in Sec-

for the lowpass prototype filter, H0(z), as

H0(z) =
N−1∑
i=0

z−iPi(zN)

where

Pi(z) =
∞∑
n=0

h0(nN + i)z−n, 0 ≤ i ≤ N − 1

Then,
Hk(z) = H0(z e−j2πk/N) = H0(zWkN)

where WN = e−j2π/N .
(a) Show that the filters Hk(z), 1 ≤ k ≤ N − 1, can be expressed as

Hk(z) =
[
1 W−kN W

−2k
N · · ·W−(N−1)kN

]



P0(z
N)

z−1P1(zN)
...

z−(N−1)PN−1(zN)




Multirate Digital Signal Processing

tion 10.1, the polyphase filters in the analysis section were defined by equation
(10.10). Instead of this definition, suppose we define the N -band polyphase filters

837



(b) Show that 


H0(z)

H1(z)
...

HN−1(z)


 = NW−1




P0(z
N)

z−1P1(zN)
...

z−(N−1)PN−1(zN)




where W is the DFT matrix

W =




1 1 1 · · · 1
1 W 1N W

2
N · · · W(N−1)N

1 W 2N W
4
N · · · W 2(N−1)N

...
...

...
...

1 W(N−1)N W
2(N−1)
N · · · W(N−1)(N−1)N




(c) Sketch the analysis section of the uniform DFT filter bank.

(d) Determine and sketch the synthesis section of the uniform DFT filter bank.

27 The prototype filter in a four-channel uniform DFT filter bank is characterized by
the transfer function

H0(z) = 1+ z−1 + 3z−2 + 4z−4

(a) Determine the transfer functions of the filters H1(z), H2(z) and H3(z) in the
analysis section.

(b) Determine the transfer functions of the filters in the synthesis section.

(c) Sketch the analysis and synthesis sections of the uniform DFT filter bank.

28 Consider the following FIR filter transfer function:

H(z) = −3+ 19z−2 + 32z−3 + 19z−4 − 3z−6

(a) Show that H(z) is a linear-phase filter.

(b) Show that H(z) is a half-band filter.

(c) Plot the magnitude and phase responses of the filter.

29 The analysis filter H0(z) in a two-channel QMF has the transfer function

H0(z) = 1+ z−1

(a) Determine the polyphase filters P0(z2) and P1(z2).

(b) Determine the analysis filter H1(z) and sketch the two-channel analysis section
that employs polyphase filters.

(c) Determine the synthesis filters G0(z) and G1(z) and sketch the entire two-
channel QMF based on polyphase filters.

(d) Show that the QMF bank results in perfect reconstruction.

Multirate Digital Signal Processing

838



30 The analysis filters in a three-channel QMF bank have the transfer functions

H0(z) = 1+ z−1 + z−2

H1(z) = 1− z−1 + z−2

H2(z) = 1− z−2

(a) Determine the polyphase matrix P(z3) and express the analysis filters in the form

(b) Determine the synthesis filters G0(z), G1(z), and G2(z) that result in perfect
reconstruction.

(c) Sketch the three-channel QMF bank that employs polyphase filters in the anal-
ysis and synthesis sections.

31 Zoom-frequency analysis

0 ω0 ω0 + ∆ω

X(ω)

π ω←π

(b)

(a)

X

LPF

LPF

DFT

yI(n)

yR(n)

x(n)

e j2πf0n

↓D

↓D

(a) Sketch the spectrum of the signal y(n) = yR(n)+ jyI (n) if the input signal x(n)

(b) Suppose that we are interested in the analysis of the frequencies in the band
f0 ≤ f ≤ f0 + �f , where f0 = π/6 and �f = π/3. Determine the cutoff of
a lowpass filter and the decimation factor D required to retain the information
contained in this band of frequencies.

Multirate Digital Signal Processing

(12.11).

Consider the system in Fig. P31(a).

Figure P31

has the spectrum shown in Fig. P31(b).

839



(c) Assume that

x(n) =
p−1∑
k=0

(
1− k

2p

)
cos 2πfkn

where p = 40 and fk = k/p , k = 0, 1, . . . , p − 1. Compute and plot the
1024-point DFT of x(n).

(d) Repeat part (b) for the signal x(n) given in part (c) by using an appropriately
designed lowpass linear phase FIR filter to determine the decimated signal s(n) =
sR(n)+ jsI (n).

(e) Compute the 1024-point DFT of s(n) and investigate to see if you have obtained
the expected results.

Multirate Digital Signal Processing

840



Linear Prediction and
Optimum Linear Filters

The design of filters to perform signal estimation is a problem that frequently arises
in the design of communication systems, control systems, in geophysics, and in many
other applications and disciplines. In this chapter we treat the problem of optimum
filter design from a statistical viewpoint. The filters are constrained to be linear and
the optimization criterion is based on the minimization of the mean-square error. As
a consequence, only the second-order statistics (autocorrelation and crosscorrelation
functions) of a stationary process are required in the determination of the optimum
filters.

Included in this treatment is the design of optimum filters for linear prediction.
Linear prediction is a particularly important topic in digital signal processing, with
applications in a variety of areas, such as speech signal processing, image processing,
and noise suppression in communication systems. As we shall observe, determi-
nation of the optimum linear filter for prediction requires the solution of a set of
linear equations that have some special symmetry. To solve these linear equations,
we describe two algorithms, the Levinson–Durbin algorithm and the Schur algo-
rithm, which provide the solution to the equations through computationally efficient
procedures that exploit the symmetry properties.

The last section of the chapter treats an important class of optimum filters called
Wiener filters. Wiener filters are widely used in many applications involving the
estimation of signals corrupted with additive noise.

1 Random Signals, Correlation Functions, and Power Spectra

We begin with a brief review of the characterization of random signals in terms of
statistical averages expressed in both the time domain and the frequency domain. The

John G. Proakis, Dimitris G. Manolakis. Copyright © 2007 by Pearson Education, Inc. All rights reserved.
From Chapter 12 of Digital Signal Processing: Principles, Algorithms, and Applications, Fourth Edition.

841



Linear Prediction and Optimum Linear Filters

reader is assumed to have a background in probability theory and random processes,
at the level given in the books of Helstrom (1990), Peebles (1987), and Stark and
Woods (1994).

1.1 Random Processes

Many physical phenomena encountered in nature are best characterized in statistical
terms. For example, meteorological phenomena such as air temperature and air
pressure fluctuate randomly as a function of time. Thermal noise voltages generated
in the resistors of electronic devices, such as a radio or television receiver, are also
randomly fluctuating phenomena. These are just a few examples of random signals.
Such signals are usually modeled as infinite-duration infinite-energy signals.

Suppose that we take the set of waveforms corresponding to the air temperature
in different cities around the world. For each city there is a corresponding waveform

Sample functions of a random process.

that is a function of time, as illustrated in Fig. 1.1. The set of all possible waveforms

Figure 1.1

842



is called an ensemble of time functions or, equivalently, a random process. The
waveform for the temperature in any particular city is a single realization or a sample
function of the random process.

Similarly, the thermal noise voltage generated in a resistor is a single realization
or a sample function of the random process consisting of all noise voltage waveforms
generated by the set of all resistors.

The set (ensemble) of all possible noise waveforms of a random process is de-
noted as X(t, S), where t represents the time index and S represents the set (sample
space) of all possible sample functions. A single waveform in the ensemble is de-
noted by x(t, s). Usually, we drop the variable s (or S) for notational convenience,
so that the random process is denoted as X(t) and a single realization is denoted as
x(t).

Having defined a random process X(t) as an ensemble of sample functions, let
us consider the values of the process for any set of time instants t1 > t2 > · · · > tn ,
where n is any positive integer. In general, the samples Xti ≡ x(ti), i = 1, 2, . . . , n
are n random variables characterized statistically by their joint probability density
function (PDF) denoted as p(xt1 , xt2 , . . . , xtn) for any n.

Stationary Random Processes

Suppose that we have n samples of the random process X(t) at t = ti , i = 1, 2, . . . , n,
and another set of n samples displaced in time from the first set by an amount τ .
Thus the second set of samples are Xti+τ ≡ X(ti + τ), i = 1, 2, . . . , n, as shown

This second set of n random variables is characterized by the joint
probability density function p(xti+τ , . . . , xtn+τ ). The joint PDFs of the two sets of
random variables may or may not be identical. When they are identical, then

p(xt1, xt2 , . . . , xtn) = p(xt1+τ , xt2+τ , . . . , xtn+τ )

for all τ and all n, and the random process is said to be stationary in the strict sense.
In other words, the statistical properties of a stationary random process are invariant
to a translation of the time axis. On the other hand, when the joint PDFs are different,
the random process is nonstationary.

1.3 Statistical (Ensemble) Averages

Let us consider a random process X(t) sampled at time instant t = ti . Thus X(ti)
is a random variable with PDF p(xti ). The l th moment of the random variable is
defined as the expected value of Xl(ti), that is,

E(Xlti ) =
∫ ∞
−∞

xlti p(xti )dxti

In general, the value of the l th moment depends on the time instant ti , if the PDF
of Xti depends on ti . When the process is stationary, however, p(xti+τ ) = p(xti ) for
all τ . Hence the PDF is independent of time and, consequently, the l th moment is
independent of time (a constant).

(1.1)

(1.2)

Linear Prediction and Optimum Linear Filters

1.2

in Fig. 1.1.

843



Next, let us consider the two random variables Xti = X(ti), i = 1, 2, corre-
sponding to samples of X(t) taken at t = t1 and t = t2 . The statistical (ensemble)
correlation between Xt1 and Xt2 is measured by the joint moment

E(Xt1Xt2) =
∫ ∞
−∞

∫ ∞
−∞

xt1xt2p(xt1xt2) dx1 dx2

Since the joint moment depends on the time instants t1 and t2 , it is denoted as γxx(t1 ,
t2 ) and is called the autocorrelation function of the random process. When the process
X(t) is stationary, the joint PDF of the pair (Xt1 , Xt2) is identical to the joint PDF
of the pair (Xt1+τ , Xt2+τ ) for any arbitrary τ . This implies that the autocorrelation
function of X(t) depends on the time difference t1 − t2 = τ . Hence for a stationary
real-valued random process the autocorrelation function is

γxx(τ ) = E[Xt1+τXt1]

On the other hand,

γxx(−τ) = E(Xt1−τXt1) = E(Xt ′1Xt ′1+τ ) = γxx(τ )

Therefore, γxx(τ ) is an even function. We also note that γxx(0) = E(X2t1) is the
average power of the random process.

There exist nonstationary processes with the property that the mean value of the
process is a constant and the autocorrelation function satisfies the property γxx(t1 ,
t2) = γxx(t1 − t2). Such a process is called wide-sense stationary. Clearly, wide-
sense stationarity is a less stringent condition than strict-sense stationarity. In our
treatment we shall require only that the processes be wide-sense stationary.

Related to the autocorrelation function is the autocovariancefunction, which is
defined as

cxx(t1, t2) = E{[Xt1 −m(t1)][Xt2 −m(t2)]}
= γxx(t1, t2)−m(t1)m(t2)

where m(t1) = E(Xt1) and m(t2) = E(Xt2) are the mean values of Xt1 and Xt2 ,
respectively. When the process is stationary,

cxx(t1, t2) = cxx(t1 − t2) = cxx(τ ) = γxx(τ )−m2x

where τ = t1 − t2 . Furthermore, the variance of the process is σ 2x = cxx(0) =
γxx(0)−m2x .

1.4 Statistical Averages for Joint Random Processes

Let X(t) and Y (t) be two random processes and let Xti ≡ X(ti), i = 1, 2, . . . , n,
and Yt ′

j
≡ Y (t ′j ), j = 1, 2, . . . , m, represent the random variables at times t1 > t2 >

(1.3)

(1.4)

(1.5)

(1.6)

(1.7)

Linear Prediction and Optimum Linear Filters

844



· · · > tn and t ′1 > t ′2 > · · · > t ′m , respectively. The two sets of random variables are
characterized statistically by the joint PDF

p(xt1 , xt2 , . . . , xtn , yt ′1
, yt ′2

, . . . , yt ′m)

for any set of time instants {ti} and {t ′j } and for any positive integer values of m and n.
The crosscorrelation function of X(t) and Y (t), denoted as γxy(t1 , t2), is defined

by the joint moment

γxy(t1, t2) ≡ E(Xt1Yt2) =
∫ ∞
−∞

∫ ∞
−∞

xt1yt2p(xt1, yt2)dxt1dyt2

and the crosscovariance is

cxy(t1, t2) = γxy(t1, t2)−mx(t1)my(t2)
When the random processes are jointly and individually stationary, we have γxy(t1 ,
t2) = γxy(t1 − t2) and cxy(t1 , t2) = cxy(t1 − t2). In this case

γxy(−τ) = E(Xt1Yt1+τ ) = E(Xt ′1−τYt ′1) = γyx(τ )

The random processes X(t) and Y (t) are said to be statistically independent if
and only if

p(xt1 , xt2 , . . . , xtn , yt ′1
, yt ′2 , . . . , yt

′
m
) = p(xt1 , . . . , xtn )p(yt ′1, . . . , yt ′m)

for all choices of ti , t ′i and for all positive integers n and m. The processes are said
to be uncorrelated if

γxy(t1, t2) = E(Xt1)E(Yt2)
so that cxy(t1 , t2) = 0.

A complex-valued random process Z(t) is defined as

Z(t) = X(t)+ jY (t)
where X(t) and Y (t) are random processes. The joint PDF of the complex-valued
random variables Zti ≡ Z(ti), i = 1, 2, . . . , is given by the joint PDF of the com-
ponents (Xti , Yti ), i = 1, 2, . . . , n. Thus the PDF that characterizes Zti , i = 1,
2, . . . , n is

p(xt1, xt2 , . . . , xtn , yt1, yt2 , . . . , ytn)

A complex-valued random process Z(t) is encountered in the representation
of the in-phase and quadrature components of the lowpass equivalent of a narrow-
band random signal or noise. An important characteristic of such a process is its
autocorrelation function, which is defined as

γzz(t1, t2) = E(Zt1Z∗t2)

= E[(Xt1 + jYt1)(Xt2 − jYt2)]
= γxx(t1, t2)+ γyy(t1, t2)+ j [γyx(t1, t2)− γxy(t1, t2)]

(1.8)

(1.9)

(1.10)

(1.11)

(1.12)

(1.13)

Linear Prediction and Optimum Linear Filters

845



When the random processes X(t) and Y (t) are jointly and individually stationary,
the autocorrelation function of Z(t) becomes

γzz(t1, t2) = γzz(t1 − t2) = γzz(τ )

where τ = t1 2
γ ∗zz(τ ) = E(Z∗t1Zt1−τ ) = γzz(−τ)

Now, suppose that Z(t) = X(t) + jY (t) and W(t) = U(t) + jV (t) are two
complex-valued random processes. Their crosscorrelation function is defined as

γzw(t1, t2) = E(Zt1W ∗t2)

= E[(Xt1 + jYt1)(Ut2 − jVt2)]
= γxu(t1, t2)+ γyv(t1, t2)+ j [γyu(t1, t2)− γxv(t1, t2)]

When X(t), Y (t), U(t), and V (t) are pairwise stationary, the crosscorrelation func-
1 − t2 . In addition,

we have
γ ∗zw(τ ) = E(Z∗t1Wt1−τ ) = E(Z∗t1+τWt1) = γwz(−τ)

1.5 Power Density Spectrum

A stationary random process is an infinite-energy signal and hence its Fourier trans-
form does not exist. The spectral characteristic of a random process is obtained
according to the Wiener–Khintchine theorem, by computing the Fourier transform
of the autocorrelation function. That is, the distribution of power with frequency is
given by the function

�xx(F ) =
∫ ∞
−∞

γxx(τ )e
−j2πFτ dτ

The inverse Fourier transform is given as

γxx(τ ) =
∫ ∞
−∞

�xx(F )e
j2πFτ dF

We observe that

γxx(0) =
∫ ∞
−∞

�xx(F )dF

= E(X2t ) ≥ 0
Since E(X2t ) = γxx(0) represents the average power of the random process, which
is the area under �xx(F ), it follows that �xx(F ) is the distribution of power as a
function of frequency. For this reason, �xx(F ) is called the power density spectrum
of the random process.

(1.14)

(1.15)

(1.16)

(1.17)

(1.18)

(1.19)

Linear Prediction and Optimum Linear Filters

− t . The complex conjugate of (1.13) is

tions in (1.15) become functions of the time difference τ = t

846



If the random process is real, γxx(τ ) is real and even and hence �xx(F ) is real
and even. If the random process is complex valued, γxx(τ ) = γ ∗xx(−τ) and, hence

�∗xx(F ) =
∫ ∞
−∞

γ ∗xx(τ )e
j2πFτ dτ =

∫ ∞
−∞

γ ∗xx(−τ)e−j2πFτ dτ

=
∫ ∞
−∞

γxx(τ )e
−j2πFτ dτ = �xx(F )

Therefore, �xx(F ) is always real.
The definition of the power density spectrum can be extended to two jointly

stationary random processes X(t) and Y (t), which have a crosscorrelation function
γxy(τ ). The Fourier transform of γxy(τ ) is

�xy(F ) =
∫ ∞
−∞

γxy(τ )e
−j2πFτ dτ

which is called the cross-power density spectrum. It is easily shown that �∗xy(F ) =
�yx(−F). For real random processes, the condition is �yx(F ) = �xy(−F).

1.6 Discrete-Time Random Signals

This characterization of continuous-time random signals can be easily carried over
to discrete-time signals. Such signals are usually obtained by uniformly sampling a
continuous-time random process.

A discrete-time random process X(n) consists of an ensemble of sample se-
quences x(n). The statistical properties of X(n) are similar to the characterization
of X(t), with the restriction that n is now an integer (time) variable. To be specific,
we state the form for the important moments that we use in this text.

The l th moment of X(n) is defined as

E(Xln) =
∫ ∞
−∞

xlnp(xn)dxn

and the autocorrelation sequence is

γxx(n, k) = E(XnXk) =
∫ ∞
−∞

∫ ∞
−∞

xnxkp(xn, xk)dxndxk

Similarly, the autocovariance is

cxx(n, k) = γxx(n, k)− E(Xn)E(Xk)

For a stationary process, we have the special forms (m = n− k)

γxx(n− k) = γxx(m)
cxx(n− k) = cxx(m) = γxx(m)−m2x

(1.20)

(1.21)

(1.22)

(1.23)

(1.24)

Linear Prediction and Optimum Linear Filters

847



where mx = E(Xn) is the mean of the random process. The variance is defined as
σ 2 = cxx(0) = γxx(0)−m2x .

For a complex-valued stationary process Z(n) = X(n)+ jY (n), we have

γzz(m) = γxx(m)+ γyy(m)+ j [γyx(m)− γxy(m)]

and the crosscorrelation sequence of two complex-valued stationary sequences is

γzw(m) = γxu(m)+ γyv(m)+ j [γyu(m)− γxv(m)]

As in the case of a continuous-time random process, a discrete-time random
process has infinite energy but a finite average power and is given as

E(X2n) = γxx(0)

By use of the Wiener–Khintchine theorem, we obtain the power density spectrum of
the discrete-time random process by computing the Fourier transform of the auto-
correlation sequence γxx(m), that is,

�xx(f ) =
∞∑

m=−∞
γxx(m)e

−j2πfm

The inverse transform relationship is

γxx(m) =
∫ 1/2
−1/2

�xx(f )e
j2πfmdf

We observe that the average power is

γxx(0) =
∫ 1/2
−1/2

�xx(f )df

so that �xx(f ) is the distribution of power as a function of frequency, that is, �xx(f )
is the power density spectrum of the random process X(n). The properties we have
stated for �xx(F ) also hold for �xx(f ).

1.7 Time Averages for a Discrete-Time Random Process

Although we have characterized a random process in terms of statistical averages,
such as the mean and the autocorrelation sequence, in practice, we usually have
available a single realization of the random process. Let us consider the problem of
obtaining the averages of the random process from a single realization. To accomplish
this, the random process must be ergodic.

By definition, a random process X(n) is ergodic if, with probability 1, all the
statistical averages can be determined from a single sample function of the process.
In effect, the random process is ergodic if time averages obtained from a single

(1.25)

(1.26)

(1.27)

(1.28)

(1.29)

(1.30)

Linear Prediction and Optimum Linear Filters

848



realization are equal to the statistical (ensemble) averages. Under this condition we
can attempt to estimate the ensemble averages using time averages from a single
realization.

To illustrate this point, let us consider the estimation of the mean and the au-
tocorrelation of the random process from a single realization x(n). Since we are
interested only in these two moments, we define ergodicity with respect to these
parameters. For additional details on the requirements for mean ergodicity and au-
tocorrelation ergodicity which are given below, the reader is referred to the book of
Papoulis (1984).

1.8 Mean-Ergodic Process

Given a stationary random process X(n) with mean

mx = E(Xn)

let us form the time average

m̂x = 12N + 1
N∑

n=−N
x(n)

In general, we view m̂x
value will vary with the different realizations of the random process. Hence m̂x is a
random variable with a PDF p(m̂x). Let us compute the expected value of m̂x over
all possible realizations of X(n). Since the summation and the expectation are linear
operations, we can interchange them, so that

E(m̂x) = 12N + 1
N∑

n=−N
E[x(n)] = 1

2N + 1
N∑

n=−N
mx = mx

Since the mean value of the estimate is equal to the statistical mean, we say that the
estimate m̂x is unbiased.

Next, we compute the variance of m̂x . We have

var(m̂x) = E(|m̂x |2)− |mx |2

But

E(|m̂x |2) = 1
(2N + 1)2

N∑
n=−N

N∑
k=−N

E[x∗(n)x(k)]

= 1
(2N + 1)2

N∑
n=−N

N∑
k=−N

γxx(k − n)

= 1
2N + 1

2N∑
m=−2N

(
1− |m|

2N + 1
)
γxx(m)

(1.31)

(1.32)

Linear Prediction and Optimum Linear Filters

in (1.31) as an estimate of the statistical mean whose

849



Therefore,

var(m̂x) = 12N + 1
2N∑

m=−2N

(
1− |m|

2N + 1
)
γxx − |mx |2

= 1
2N + 1

2N∑
m=−2N

(
1− |m|

2N + 1
)
cxx(m)

If var(mx)→ 0 asN →∞, the estimate converges with probability 1 to the statistical
mean mx . Therefore, the process X(n) is mean ergodic if

lim
N→∞

1
2N + 1

2N∑
m=−2N

(
1− |m|

2N + 1
)
cxx(m) = 0

Under this condition, the estimate m̂x in the limit as N →∞ becomes equal to the
statistical mean, that is,

mx = lim
N→∞

1
2N + 1

N∑
n=−N

x(n)

Thus the time-average mean, in the limit as N →∞, is equal to the ensemble mean.

∞∑
m=−∞

|cxx(m)| <∞

which implies that cxx(m)→ 0 as m→∞. This condition holds for most zero-mean
processes encountered in the physical world.

1.9 Correlation-Ergodic Processes

Now, let us consider the estimate of the autocorrelation γxx(m) from a single real-
ization of the process. Following our previous notation, we denote the estimate (for
a complex-valued signal, in general) as

rxx(m) = 12N + 1
N∑

n=−N
x∗(n)x(n+m)

Again, we regard rxx(m) as a random variable for any given lagm, since it is a function
of the particular realization. The expected value (mean value over all realizations) is

E[rxx(m)] = 12N + 1
N∑

n=−N
E[x∗(n)x(n+m)]

= 1
2N + 1

N∑
n=−N

γxx(m) = γxx(m)

(1.33)

(1.34)

(1.35)

(1.36)

(1.37)

(1.38)

Linear Prediction and Optimum Linear Filters

A sufficient condition for (1.34) to hold is if

850



Therefore, the expected value of the time-average autocorrelation is equal to the
statistical average. Hence we have an unbiased estimate of γxx(m).

To determine the variance of the estimate rxx(m), we compute the expected
value of |rxx(m)|2 and subtract the square of the mean value. Thus

var[rxx(m)] = E[|rxx(m)|2]− |γxx(m)|2

But

E[|rxx(m)|2] = 1
(2N + 1)2

N∑
n=−N

N∑
k=−N

E[x∗(n)x(n+m)x(k)x∗(k +m)]

The expected value of the term x∗(n)x(n+m)x(k)x∗(k+m) is just the autocorrelation
sequence of a random process defined as

vm(n) = x∗(n)x(n+m)

E[|rxx(m)|2] = 1
(2N + 1)2

N∑
n=−N

N∑
k=−N

γ (m)vv (n− k)

= 1
2N + 1

2N∑
n=−2N

(
1− |n|

2N + 1
)
γ (m)vv (n)

and the variance is

var[rxx(m)] = 12N + 1
2N∑

n=−2N

(
1− |n|

2N + 1
)
γ (m)vv (n)− |γxx(m)|2

If var[rxx(m)]→ 0 as N → ∞, the estimate rxx(m) converges with probability
1 to the statistical autocorrelation γxx(m). Under these conditions, the process is
correlation ergodic and the time-average correlation is identical to the statistical
average, that is,

lim
N→∞

1
2N + 1

N∑
n=−N

x∗(n)x(n+m) = γxx(m)

In our treatment of random signals, we assume that the random processes are
mean ergodic and correlation ergodic, so that we can deal with time averages of the
mean and the autocorrelation obtained from a single realization of the process.

(1.39)

(1.40)

(1.41)

(1.42)

(1.43)

Linear Prediction and Optimum Linear Filters

Hence (1.40) may be expressed as

851



2 Innovations Representation of a Stationary Random Process

In this section we demonstrate that a wide-sense stationary random process can be
represented as the output of a causal and causally invertible linear system excited
by a white noise process. The condition that the system is causally invertible also
allows us to represent the wide-sense stationary random process by the output of the
inverse system, which is a white noise process.

Let us consider a wide-sense stationary process {x(n)} with autocorrelation se-
quence {γxx(m)} and power spectral density �xx(f ), |f | ≤ 12 . We assume that �xx(f )
is real and continuous for all |f | ≤ 12 . The z-transform of the autocorrelation se-
quence {γxx(m)} is

�xx(z) =
∞∑

m=−∞
γxx(m)z

−m

from which we obtain the power spectral density by evaluating �xx(z) on the unit
circle [i.e., by substituting z = exp(j2πf )].

Now, let us assume that log�xx(z) is analytic (possesses derivatives of all orders)
in an annular region in the z-plane that includes the unit circle (i.e., r1 < |z| < r2
where r1 < 1 and r2 > 1). Then, log�xx(z) can be expanded in a Laurent series of
the form

log�xx(z) =
∞∑

m=−∞
v(m)z−m

where the {v(m)} are the coefficients in the series expansion. We can view {v(m)}
as the sequence with z-transform V (z) = log�xx(z). Equivalently, we can evaluate
log�xx(z) on the unit circle,

log�xx(f ) =
∞∑

m=−∞
v(m)e−j2πfm

so that the {v(m)} are the Fourier coefficients in the Fourier series expansion of the
periodic function log�xx(f ). Hence

v(m) =
∫ 1

2

− 12
[log�xx(f )]ej2πfm df, m = 0,±1, . . .

We observe that v(m) = v(−m), since �xx(f ) is a real and even function of f .

�xx(z) = exp
[ ∞∑
m=−∞

v(m)z−m
]

= σ 2wH(z)H(z−1)

(2.1)

(2.2)

(2.3)

(2.4)

(2.5)

Linear Prediction and Optimum Linear Filters

From (2.2) it follows that

852



where, by definition, σ 2w = exp[v(0)] and

H(z) = exp
[ ∞∑
m=1

v(m)z−m
]
, |z| > r1

the power spectral density as

�xx(f ) = σ 2w|H(f )|2

We note that

log�xx(f ) = log σ 2w + logH(f )+ logH ∗(f )

=
∞∑

m=−∞
v(m)e−j2πfm

with H(z−1). The Fourier series coefficients {v(m)} are the cepstral coefficients and
xx

|z| > r1 < 1. Hence, in this region, it has a Taylor series expansion as a causal system
of the form

H(z) =
∞∑
m=0

h(n)z−n

The output of this filter in response to a white noise input sequence w(n) with power
spectral density σ 2w is a stationary random process {x(n)} with power spectral density
�xx(f ) = σ 2w|H(f )|2. Conversely, the stationary random process {x(n)} with power
spectral density �xx(f ) can be transformed into a white noise process by passing
{x(n)} through a linear filter with system function 1/H(z). We call this filter a noise
whitening filter. Its output, denoted as {w(n)} is called the innovations process as-
sociated with the stationary random process {x(n)}. These two relationships are

Filters for generating
(a) the random process
x(n) from white noise and
(b) the inverse filter.

Linear
causal
filter
H(z)

w(n)
White noise

x(n) =  Σ h(k)w(n − k)
k = 0

(a)

Linear
causal
filter

1/H(z)

x(n) w(n)

White noise

(b)

(2.6)

(2.7)

(2.8)

(m)}.the sequence {v(m)} is called the cepstrum of the sequence {γ .

Linear Prediction and Optimum Linear Filters

If (2.5) is evaluated on the unit circle, we have the equivalent representation of

From the definition of H (z) given by (2.6), it is clear that the causal part of the
Fourier series in (2.3) is associated with H(z) and the anticausal part is associated

The filter with system function H(z) given by (2.6) is analytic in the region

illustrated in Fig. 2.1.

Figure 2.1

853



The representation of the stationary stochastic process {x(n)} as the output of

sequence {w(n)} is called the Wold representation.

2.1 Rational Power Spectra

Let us now restrict our attention to the case where the power spectral density of the
stationary random process {x(n)} is a rational function, expressed as

�xx(z) = σ 2w
B(z)B(z−1)
A(z)A(z−1)

, r1 < |z| < r2

where the polynomials B(z) and A(z) have roots that fall inside the unit circle in the
z-plane. Then the linear filter H(z) for generating the random process {x(n)} from
the white noise sequence {w(n)} is also rational and is expressed as

H(z) = B(z)
A(z)

=

q∑
k=0

bkz
−k

1+
p∑
k=1

akz
−k

|z| > r1

where {bk} and {ak} are the filter coefficients that determine the location of the zeros
and poles of H(z), respectively. Thus H(z) is causal, stable, and minimum phase.
Its reciprocal 1/H(z) is also a causal, stable, and minimum-phase linear system.
Therefore, the random process {x(n)} uniquely represents the statistical properties
of the innovations process {w(n)}, and vice versa.

the output x(n) is related to the input w(n) by the difference equation

x(n)+
p∑
k=1

akx(n− k) =
q∑
k=0

bkw(n− k)

We will distinguish among three specific cases.

Autoregressive (AR) process. b0 = 1, bk = 0, k > 0. In this case, the linear filter
H(z) = 1/A(z) is an all-pole filter and the difference equation for the input–output
relationship is

x(n)+
p∑
k=1

akx(n− k) = w(n)

In turn, the noise-whitening filter for generating the innovations process is an all-zero
filter.

(2.9)

(2.10)

(2.11)

(2.12)

Linear Prediction and Optimum Linear Filters

an IIR filter with system function H(z) given by (2.8) and excited by a white noise

For the linear system with the rational system function H(z) given by (2.10),

854



Moving average (MA) process. ak = 0, k ≥ 1. In this case, the linear filter
H(z) = B(z) is an all-zero filter and the difference equation for the input–output
relationship is

x(n) =
q∑
k=0

bkw(n− k)

The noise-whitening filter for the MA process is an all-pole filter.

Autoregressive, moving average (ARMA) process. In this case, the linear filter
H(z) = B(z)/A(z) has both finite poles and zeros in the z-plane and the corre-

The inverse system for gen-
erating the innovations process from x(n) is also a pole–zero system of the form
1/H(z) = A(z)/B(z).

2.2 Relationships Between the Filter Parameters
and the Autocorrelation Sequence

When the power spectral density of the stationary random process is a rational func-
tion, there is a basic relationship between the autocorrelation sequence {γxx(m)} and
the parameters {ak} and {bk} of the linear filter H(z) that generates the process by
filtering the white noise sequence w(n). This relationship can be obtained by mul-

∗
value of both sides of the resulting equation. Thus we have

E[x(n)x∗(n−m)] = −
p∑
k=1

akE[x(n− k)x∗(n−m)]

+
q∑
k=0

bkE[w(n− k)x∗(n−m)]

Hence

γxx(m) = −
p∑
k=1

akγxx(m− k)+
q∑
k=0

bkγwx(m− k)

where γwx(m) is the crosscorrelation sequence between w(n) and x(n).
The crosscorrelation γwx(m) is related to the filter impulse response. That is,

γwx(m) = E[x∗(n)w(n+m)]

= E
[ ∞∑
k=0

h(k)w∗(n− k)w(n+m)
]

= σ 2wh(−m)
where, in the last step, we have used the fact that the sequence w(n) is white. Hence

γwx(m) =
{

0, m > 0
σ 2wh(−m), m ≤ 0

(2.13)

(2.14)

(2.15)

(2.16)

(2.17)

Linear Prediction and Optimum Linear Filters

sponding difference equation is given by (2.11).

tiplying the difference equation in (2.11) by x (n − m) and taking the expected

855



γxx(m) =




−
p∑
k=1

akγxx(m− k), m > q

−
p∑
k=1

akγxx(m− k)+ σ 2w
q−m∑
k=0

h(k)bk+m, 0 ≤ m ≤ q
γ ∗xx(−m), m < 0

This represents a nonlinear relationship between γxx(m) and the parameters {ak},
{bk}.

γxx(m) =




−
p∑
k=1

akγxx(m− k), m > 0

−
p∑
k=1

akγxx(m− k)+ σ 2w, m = 0
γ ∗xx(−m), m < 0

Thus we have a linear relationship between γxx(m) and the {ak} parameters. These
equations, called the Yule–Walker equations, can be expressed in the matrix form

γxx(0) γxx(−1) γxx(−2) · · · γxx(−p)
γxx(1) γxx(0) γxx(−1) · · · γxx(−p + 1)
...

...
...

...

γxx(p) γxx(p − 1) γxx(p − 2) · · · γxx(0)






1
a1
...

ap


 =



σ 2w
0
...

0




This correlation matrix is Toeplitz, and hence it can be efficiently inverted by use of

Finally, by setting ak k
obtain the relationship for the autocorrelation sequence in the case of a MA process,
namely,

γxx(m) =



σ 2w

q∑
k=0

bkbk+m, 0 ≤ m ≤ q
0, m > q
γ ∗xx(−m), m < 0

3 Forward and Backward Linear Prediction

Linear prediction is an important topic in digital signal processing with many practical
applications. In this section we consider the problem of linearly predicting the value
of a stationary random process either forward in time or backward in time. This
formulation leads to lattice filter structures and to some interesting connections to
parametric signal models.

(2.18)

(2.19)

(2.20)

(2.21)

Linear Prediction and Optimum Linear Filters

By combining (2.17) with (2.15), we obtain the desired relationship:

The relationship in (2.18) applies, in general, to the ARMA process. For an
AR process, (2.18) simplifies to

the algorithms described in Section 4.
= 0 , 1 ≤ k ≤ p , and h(k) = b , 0 ≤ k ≤ q , in (2.18), we

856



3.1 Forward Linear Prediction

Let us begin with the problem of predicting a future value of a stationary random
process from observation of past values of the process. In particular, we consider
the one-step forward linear predictor, which forms the prediction of the value x(n)
by a weighted linear combination of the past values x(n− 1), x(n− 2), . . . , x(n−p).
Hence the linearly predicted value of x(n) is

x̂(n) = −
p∑
k=1

ap(k)x(n− k)

where the {−ap(k)} represent the weights in the linear combination. These weights
are called the prediction coefficients of the one-step forward linear predictor of order
p . The negative sign in the definition of x(n) is for mathematical convenience and
conforms with current practice in the technical literature.

The difference between the value x(n) and the predicted value x(n) is called the
forward prediction error, denoted as fp(n):

fp(n) = x(n)− x̂(n)

= x(n)+
p∑
k=1

ap(k)x(n− k)

We view linear prediction as equivalent to linear filtering where the predictor

error filter with input sequence {x(n)} and output sequence {fp(n)}. An equivalent

direct-form FIR filter with system function

Ap(z) =
p∑
k=0

ap(k)z
−k

where, by definition, ap(0) = 1.

Forward linear prediction.

+
+x(n)

x(n − 1) x(n)

fp(n)

−

z −1
Forward

linear
predictor

ˆ

(3.1)

(3.2)

(3.3)

Linear Prediction and Optimum Linear Filters

is embedded in the linear filter , as shown in Fig. 3.1. This is called a prediction-

realization for the prediction-error filter is shown in Fig. 3.2. This realization is a

Figure 3.1

857



+

fp(n)

ap(2)

x(n) z −1z −1 z −1 z −1

ap(3) ap(p)ap(p − 1)ap(1)1

Prediction-error filter.

f0(n) = g0(n) = x(n)
fm(n) = fm−1(n)+Kmgm−1(n− 1) m = 1, 2, . . . , p
gm(n) = K∗mfm−1(n)+ gm−1(n− 1) m = 1, 2, . . . , p

where {Km} are the reflection coefficients and gm(n) is the backward prediction error
defined in the following section. Note that for complex-valued data, the conjugate of
Km is used in the equation for gm
in block diagram form along with a typical stage showing the computations given by

As a consequence of the equivalence between the direct-form prediction-error
FIR filter and the FIR lattice filter, the output of the p-stage lattice filter is expressed

First
stage

f0(n)

g0(n)

f1(n)

g1(n)
pth

stage

fp(n)

fm(n)fm−1(n)

gp(n)
Second
stage

f2(n)

g2(n)
x(n)

…

…

+

gm(n)

Km

 K*m

gm−1(n) +z −1

p-stage lattice filter.

(3.4)

The direct-form FIR filter is equivalent to an all-zero lattice filter. The lattice 
filter is generally described by the following set of orderrecursive eq u ations:

Linear Prediction and Optimum Linear Filters

Figure 3.2

(n). Figure 3.3 illustrates a p-stage lattice filter

(3.4).

Figure 3.3

858



as

fp(n) =
p∑
k=0

ap(k)x(n− k), ap(0) = 1

Fp(z) = Ap(z)X(z)

or, equivalently,

Ap(z) = Fp(z)
X(z)

= Fp(z)
F0(z)

The mean-square value of the forward linear prediction error fp(n) is

Efp = E[|fp(n)|2]

= γxx(0)+ 2�
[

p∑
k=1

a∗p(k)γxx(k)

]
+

p∑
k=1

p∑
l=1

a∗p(l)ap(k)γxx(l − k)

E
f
p is a quadratic function of the predictor coefficients and its minimization leads to

the set of linear equations

γxx(l) = −
p∑
k=1

ap(k)γxx(l − k), l = 1, 2, . . . , p

These are called the normal equations for the coefficients of the linear predictor. The
minimum mean-square prediction error is simply

min[Efp ] ≡ Efp = γxx(0)+
p∑
k=1

ap(k)γxx(−k)

In the following section we extend the development above to the problem of
predicting the value of a time series in the opposite direction, namely, backward in
time.

3.2 Backward Linear Prediction

Let us assume that we have the data sequence x(n), x(n−1), . . . , x(n−p+1) from a
stationary random process and we wish to predict the value x(n− p) of the process.
In this case we employ a one-step backward linear predictor of order p . Hence

x̂(n− p) = −
p−1∑
k=0

bp(k)x(n− k)

(3.5)

(3.6)

(3.7)

(3.8)

(3.9)

(3.10)

(3.11)

Linear Prediction and Optimum Linear Filters

Since (3.5) is a convolution sum, the z-transform relationship is

859



The difference between the value x(n − p) and the estimate x̂(n − p) is called the
backward prediction error, denoted as gp(n):

gp(n) = x(n− p)+
p−1∑
k=0

bp(k)x(n− k)

=
p∑
k=0

bp(k)x(n− k), bp(p) = 1

The backward linear predictor can be realized either by a direct-form FIR filter
The

as the forward linear predictor.
The weighting coefficients in the backward linear predictor are the complex

conjugates of the coefficients for the forward linear predictor, but they occur in
reverse order. Thus we have

bp(k) = a∗p(p − k), k = 0, 1, . . . , p

Gp(z) = Bp(z)X(z)

or, equivalently,

Bp(z) = Gp(z)
X(z)

= Gp(z)
G0(z)

where Bp(z) represents the system function of the FIR filter with coefficients bp(k).
Since bp(k) = a∗(p − k), Gp(z) is related to Ap(z)

Bp(z) =
p∑
k=0

bp(k)z
−k

=
p∑
k=0

a∗p(p − k)z−k

= z−p
p∑
k=0

a∗p(k)z
k

= z−pA∗p(z−1)

function Bp(z) are simply the (conjugate) reciprocals of the zeros of Ap(z). Hence
Bp(z) is called the reciprocal or reverse polynomial of Ap(z).

(3.12)

(3.13)

(3.14)

(3.15)

(3.16)

Linear Prediction and Optimum Linear Filters

structure similar to the structure shown in Fig. 3.1 or as a lattice structure.
lattice structure shown in Fig. 3.3 provides the backward linear predictor as well

In the z-domain, the convolution sum in (3.12) becomes

The relationship in (3.16) implies that the zeros of the FIR filter with system

860



Now that we have established these interesting relationships between the for-
ward and backward one-step predictors, let us return to the recursive lattice equations

F0(z) = G0(z) = X(z)
Fm(z) = Fm−1(z)+Kmz−1Gm−1(z), m = 1, 2, . . . , p
Gm(z) = K∗mFm−1(z)+ z−1Gm−1(z), m = 1, 2, . . . , p

If we divide each equation by X(z), we obtain the desired results in the form

A0(z) = B0(z) = 1
Am(z) = Am−1(z)+Kmz−1Bm−1(z), m = 1, 2, . . . , p
Bm(z) = K∗mAm−1(z)+ z−1Bm−1(z), m = 1, 2, . . . , p

Thus a lattice filter is described in the z-domain by the matrix equation

[
Am(z)

Bm(z)

]
=
[

1 Kmz−1
K∗m z

−1

] [
Am−1(z)
Bm−1(z)

]

m(z) and Bm(z) allow us to obtain the direct-form FIR
filter coefficients {am(k)} from the reflection coefficients {Km}, and vice versa.

The lattice structure with parameters K1 , K2, . . . , Kp corresponds to a class
of p direct-form FIR filters with system functions A1(z), A2(z), . . . , Ap(z). It is
interesting to note that a characterization of this class of p FIR filters in direct form
requires p(p + 1)/2 filter coefficients. In contrast, the lattice-form characterization
requires only the p reflection coefficients {Ki}. The reason the lattice provides a
more compact representation for the class of p FIR filters is because appending
stages to the lattice does not alter the parameters of the previous stages. On the
other hand, appending the pth stage to a lattice with (p − 1) stages is equivalent to
increasing the length of an FIR filter by one coefficient. The resulting FIR filter with
system function Ap(z) has coefficients totally different from the coefficients of the
lower-order FIR filter with system function Ap−1(z).

The formula for determining the filter coefficients {ap(k)} recursively is easily

Am(z) = Am−1(z)+Kmz−1Bm−1(z)
m∑
k=0

am(k)z
−k =

m−1∑
k=0

am−1(k)z−k +Km
m−1∑
k=0

a∗m−1(m− 1− k)z−(k+1)

By equating the coefficients of equal powers of z−1 and recalling that am(0) = 1
for m = 1, 2, . . . , p , we obtain the desired recursive equation for the FIR filter

(3.17)

(3.18)

(3.19)

(3.20)

Linear Prediction and Optimum Linear Filters

in (3.4) and transform them to the z-domain. Thus we have

The relations in (3.18) for A

derived from polynomial relationships (3.18). We have

861



coefficients in the form

am(0) = 1
am(m) = Km
am(k) = am−1(k)+Kma∗m−1(m− k)

= am−1(k)+ am(m)a∗m−1(m− k),
1 ≤ k ≤ m− 1
m = 1, 2, . . . , p

The conversion formula from the direct-form FIR filter coefficients {ap(k)} to
the lattice reflection coefficients {Ki} is also very simple. For the p-stage lattice we
immediately obtain the reflection coefficient Kp = ap(p). To obtain Kp−1, . . . , K1 ,
we need the polynomials Am

Am−1(z) = Am(z)−KmBm(z)1− |Km|2 , m = p, . . . , 1

which is just a step-down recursion. Thus we compute all lower-degree polynomials
Am(z) beginning with Ap−1(z) and obtain the desired lattice reflection coefficients
from the relation Km = am(m). We observe that the procedure works as long as
|Km 	
it is relatively easy to obtain a formula for recursively and directly computing Km ,
m = p − 1, . . . , 1. For m = p − 1, . . . , 1, we have

Km = am(m)

am−1(k) = am(k)−Kmbm(k)1− |Km|2

= am(k)− am(m)a
∗
m(m− k)

1− |am(m)|2

which is just the recursion in the Schur–Cohn stability test for the polynomial Am(z).

lattice parameters |Km| = 1. If this occurs, it is indicative that the polynomialAm−1(z)
has a root located on the unit circle. Such a root can be factored out from Am−1(z)

Finally, let us consider the minimization of the mean-square error in a backward
linear predictor. The backward prediction error is

gp(n) = x(n− p)+
p−1∑
k=0

bp(k)x(n− k)

= x(n− p)+
p∑
k=1

a∗p(k)x(n− p + k)

(3.21)

(3.22)

| = 1 for m = 1, 2, . . . , p−1. From this step-down recursion for the polynomials,

(3.23)

(3.24)

Linear Prediction and Optimum Linear Filters

(z) for m = p − 1, . . . , 1. From (3.19) we obtain

As just indicated , the recursive equation in (3.23) breaks down if any of the

and the iterative process in (3.23) carried out for the reduced-order system.

862



and its mean-square value is
Ebp = E[|gp(n)|2]

The minimization of Ebp with respect to the prediction coefficients yields the same set

min[Ebp ] ≡ Ebp = Efp

3.3 The Optimum Reflection Coefficients for the Lattice Forward
and Backward Predictors

predictor coefficients that minimize the mean-square value of the prediction error.
In this section we consider the problem of optimizing the reflection coefficients in
the lattice predictor and expressing the reflection coefficients in terms of the forward
and backward prediction errors.

The forward prediction error in the lattice filter is expressed as

fm(n) = fm−1(n)+Kmgm−1(n− 1)

The minimization of E[|fm(n)|2] with respect to the reflection coefficient Km yields
the result

Km =
−E[fm−1(n)g∗m−1(n− 1)]

E[|gm−1(n− 1)|2]
or, equivalently,

Km =
−E[fm−1(n)g∗m−1(n− 1)]√

E
f

m−1E
b
m−1

where Ef
m−1 = Ebm−1 = E[|gm−1(n− 1)|2] = E[|fm−1(n)|2].

We observe that the optimum choice of the reflection coefficients in the lattice
predictor is the negative of the (normalized) crosscorrelation coefficients between
the forward and backward errors in the lattice.1

that |Km| ≤ 1, it follows that the minimum mean-square value of the prediction
error, which can be expressed recursively as

Efm = (1− |Km|2)Efm−1

is a monotonically decreasing sequence.

1 The normalized crosscorrelation coefficients between the forward and backward error in the lattice
(i.e., {−Km}) are often called the partial correlation (PARCOR) coefficients.

(3.25)

(3.26)

(3.27)

(3.28)

(3.29)

(3.30)

Linear Prediction and Optimum Linear Filters

of linear equations as in (3.9). Hence the minimum mean-square error (MSE) is

which is given by (3.10).

In Sections 3.1 and 3.2 we derived the set of linear equations which provide the

Since it is apparent from (3.28)

863



3.4 Relationship of an AR Process to Linear Prediction

The parameters of an AR(p) process are intimately related to a predictor of order
p for the same process. To see the relationship, we recall that in an AR(p) process,
the autocorrelation sequence {γxx(m)} is related to the parameters {ak} by the Yule–

The corresponding equations for

A direct comparison of these two sets of relations reveals that there is a one-
to-one correspondence between the parameters {ak} of the AR(p) process and the
predictor coefficients {ap(k)} of the pth-order predictor. In fact, if the underlying
process {x(n)} is AR(p), the prediction coefficients of the pth-order predictor are
identical to {ak}. Furthermore, the minimum mean-square error (MMSE) in the
pth-order predictor Efp is identical to σ 2w , the variance of the white noise process.
In this case, the prediction-error filter is a noise-whitening filter which produces the
innovations sequence {w(n)}.

4 Solution of the Normal Equations

In the preceding section we observed that the minimization of the mean-square value

be expressed in the compact form

p∑
k=0

ap(k)γxx(l − k) = 0 l = 1, 2, . . . , p

ap(0) = 1

If we augment

normal equations, which may be expressed as

p∑
k=0

ap(k)γxx(l − k) =
{
E
f
p , l = 0

0, l = 1, 2, . . . , p

We also noted that if the random process is an AR(p) process, the MMSE Efp = σ 2w .
In this section we describe two computationally efficient algorithms for solving

the normal equations. One algorithm, originally due to Levinson (1947) and mod-
ified by Durbin (1959), is called the Levinson–Durbin algorithm. This algorithm
is suitable for serial processing and has a computation complexity of O(p2). The
second algorithm, due to Schur (1917), also computes the reflection coefficients in
O(p2) operations but with parallel processors, the computations can be performed
in O(p) time. Both algorithms exploit the Toeplitz symmetry property inherent in
the autocorrelation matrix.

We begin by describing the Levinson–Durbin algorithm.

(4.1)

(4.2)

Linear Prediction and Optimum Linear Filters

Walker equations given in (2.19) or (2.20).
the predictor of order p are given by (3.9) and (3.10).

of the predictor given by (3.9). These equations, called the normal equations, may
of the forward prediction error resulted in a set of linear equations for the coefficients

The resulting minimum MSE (MMSE) is given by (3.10).
(3.10) to the normal equations given by (4.1) we obtain the set of augmented

864



4.1 The Levinson–Durbin Algorithm

The Levinson-Durbin algorithm is a computationally efficient algorithm for solv-
This algorithm

exploits the special symmetry in the autocorrelation matrix

�p =




γxx(0) γ ∗xx(1) · · · γ ∗xx(p − 1)
γxx(1) γxx(0) · · · γ ∗xx(p − 2)
...

...

γxx(p − 1) γxx(p − 2) · · · γxx(0)




Note that �p(i, j) = �p(i−j), so that the autocorrelation matrix is a Toeplitz matrix.
Since �p(i, j) = �∗p(j, i), the matrix is also Hermitian.

The key to the Levinson–Durbin method of solution, which exploits the Toeplitz
property of the matrix, is to proceed recursively, beginning with a predictor of order
m = 1 (one coefficient), and then to increase the order recursively, using the lower-
order solutions to obtain the solution to the next-higher order. Thus the solution to

a1(1) = −γxx(1)
γxx(0)

and the resulting MMSE is

E
f

1 = γxx(0)+ a1(1)γxx(−1)
= γxx(0)[1− |a1(1)|2]

Recall that a1(1) = K1 , the first reflection coefficient in the lattice filter.
The next step is to solve for the coefficients {a2(1), a2(2)} of the second-order

predictor and express the solution in terms of a1(1). The two equations obtained

a2(1)γxx(0)+ a2(2)γ ∗xx(1) = −γxx(1)
a2(1)γxx(1)+ a2(2)γxx(0) = −γxx(2)

xx(1), we obtain the solution

a2(2) = −γxx(2)+ a1(1)γxx(1)
γxx(0)[1− |a1(1)|2]

= −γxx(2)+ a1(1)γxx(1)
E
f

1

a2(1) = a1(1)+ a2(2)a∗1(1)

Thus we have obtained the coefficients of the second-order predictor. Again, we
note that a2(2) = K2 , the second reflection coefficient in the lattice filter.

(4.3)

(4.4)

(4.5)

(4.6)

(4.7)

Linear Prediction and Optimum Linear Filters

ing the normal equations in ( 4.1) for the prediction coefficients.

the first-order predictor obtained by solving (4.1) is

from (4.1) are

By using the solution in (4.4) to eliminate γ

865



Proceeding in this manner, we can express the coefficients of the mth-order
predictor in terms of the coefficients of the (m− 1)st-order predictor. Thus we can
write the coefficient vector am as the sum of two vectors, namely,

am =



am(1)
am(2)
...

am(m)


 =




am−1

· · ·
0


+




dm−1

· · ·
Km




where am−1 is the predictor coefficient vector of the (m − 1)st-order predictor and
the vector dm−1 and the scalar Km are to be determined. Let us also partition the
m×m autocorrelation matrix �xx as

�m =
[

�m−1 γb
∗
m−1

γbt
m−1 γxx(0)

]

where γbt
m−1 = [γxx(m− 1) γxx(m− 2) · · · γxx(1)] = (γbm−1)t , the asterisk (∗) de-

notes the complex conjugate, and γ tm denotes the transpose of γm
b on γm−1 denotes the vector γ tm−1 = [γxx(1) γxx(2) · · · γxx(m− 1) ] with ele-
ments taken in reverse order.

The solution to the equation �mam = −γm can be expressed as[
�m−1 γb

∗
m−1

γbt
m−1 γxx(0)

]{[
am−1

0

]
+
[

dm−1
Km

]}
= −

[
γm−1
γxx(m)

]

This is the key step in the Levinson–Durbin algorithm.
two equations, namely,

�m−1am−1 + �m−1dm−1 +Kmγb∗m−1 = −γm−1
γbtm−1am−1 + γbtm−1dm−1 +Kmγxx(0) = −γxx(m)

Since �m−1am−1 = −γm−1
dm−1 = −Km�−1m−1γb

∗
m−1

But γb
∗
m−1 is just γm−1 with elements taken in reverse order and conjugated. Therefore,

dm−1 = Kmab∗m−1 = Km



a∗
m−1(m− 1)
a∗
m−1(m− 2)

...

a∗
m−1(1)




m . If we eliminate dm−1

Km[γxx(0)+ γbtm−1ab
∗
m−1]+ γbtm−1am−1 = −γxx(m)

(4.8)

(4.9)

. The superscript

(4.10)

(4.11)

(4.12)

(4.13)

(4.14)

Linear Prediction and Optimum Linear Filters

From (4.10) we obtain

the solution in (4.13) is simply

, (4.11) yields the solution

The scalar equation (4.12) can now be used to solve for K
in (4.12) by using (4.14), we obtain

866



Hence

Km = −
γxx(m)+ γbtm−1am−1
γxx(0)+ γbtm−1ab

∗
m−1

we obtain the desired recursion for the predictor coefficients in the Levinson–Durbin
algorithm as

am(m) = Km = −
γxx(m)+ γbtm−1am−1
γxx(0)+ γbtm−1ab

∗
m−1
= −γxx(m)+ γ

bt
m−1am−1

E
f

m−1

am(k) = am−1(k)+Kma∗m−1(m− k)

= am−1(k)+ am(m)a∗m−1(m− k),
k = 1, 2, . . . , m− 1
m = 1, 2, . . . , p

Am(z) and Bm(z). Furthermore, Km is the reflection coefficient in the mth stage of
the lattice predictor. This development clearly illustrates that the Levinson–Durbin
algorithm produces the reflection coefficients for the optimum lattice prediction filter
as well as the coefficients of the optimum direct-form FIR predictor.

Finally, let us determine the expression for the MMSE. For the mth-order pre-
dictor, we have

Efm = γxx(0)+
m∑
k=1

am(k)γxx(−k)

= γxx(0)+
m∑
k=1

[am−1(k)+ am(m)a∗m−1(m− k)]γxx(−k)

= Ef
m−1[1− |am(m)|2] = Efm−1(1− |Km|2), m = 1, 2, . . . , p

where Ef0 = γxx(0). Since the reflection coefficients satisfy the property that |Km| ≤
1, the MMSE for the sequence of predictors satisfies the condition

E
f

0 ≥ Ef1 ≥ Ef2 ≥ · · · ≥ Efp

This concludes the derivation of the Levinson–Durbin algorithm for solving the
linear equations �mam = −γm , for m = 0, 1, . . . , p . We observe that the linear equa-
tions have the special property that the vector on the right-hand side also appears
as a vector in �m . In the more general case, where the vector on the right-hand
side is some other vector, say cm , the set of linear equations can be solved recur-
sively by introducing a second recursive equation to solve the more general linear

(4.15)

(4.16)

(4.17)

(4.18)

(4.19)

Linear Prediction and Optimum Linear Filters

Therefore, by substituting the solutions in (4.14) and ( 4.15) into (4.8),

The reader should note that the recursive relation in (4.17) is identical to the recur-
sive relation in (3.21) for the predictor coefficients, obtained from the polynomials

867



equations �mbm = cm . The result is a generalized Levinson–Durbin algorithm (see

tions and additions (operations) to go from stage m to stage m + 1. Therefore, for
p stages it takes on the order of 1+ 2+ 3+ · · · + p(p + 1)/2, or O(p2), operations
to solve for the prediction filter coefficients, or the reflection coefficients, compared
with O(p3) operations if we did not exploit the Toeplitz property of the correlation
matrix.

If the Levinson–Durbin algorithm is implemented on a serial computer or signal
processor, the required computation time is on the order of O(p2) time units. On the
other hand, if the processing is performed on a parallel processing machine utilizing
as many processors as necessary to exploit the full parallelism in the algorithm,

carried out simultaneously. Therefore, this computation can be performed in O(p)
time units.
takes additional time. Certainly, the inner products involving the vectors am−1 and
γb
m−1 can be computed simultaneously by employing parallel processors. However,

the addition of these products cannot be done simultaneously, but instead, requires
O(logp) time units. Hence, the computations in the Levinson–Durbin algorithm,
when performed by p parallel processors, can be accomplished in O(p logp) time.

In the following section we describe another algorithm, due to Schur (1917), that
avoids the computation of inner products, and therefore is more suitable for parallel
computation of the reflection coefficients.

4.2 The Schur Algorithm

The Schur algorithm is intimately related to a recursive test for determining the
positive definiteness of a correlation matrix. To be specific, let us consider the auto-
correlation matrix �p+1 associated with the augmented normal equations given by

R0(z) = γxx(1)z
−1 + γxx(2)z−2 + · · · + γxx(p)z−p

γxx(0)+ γxx(1)z−1 + · · · + γxx(p)z−p
and the sequence of functions Rm(z) defined recursively as

Rm(z) = Rm−1(z)− Rm−1(∞)
z−1[1− R∗

m−1(∞)Rm−1(z)]
, m = 1, 2, . . .

Schur’s theorem states that a necessary and sufficient condition for the correla-
tion matrix to be positive definite is that |Rm(∞)| < 1 for m = 1, 2, . . . , p .

Let us demonstrate that the condition for positive definiteness of the autocor-
relation matrix �p+1 is equivalent to the condition that the reflection coefficients in
the equivalent lattice filter satisfy the condition |Km| < 1, m = 1, 2, . . . , p .

0

R1(z) = γxx(1)+ γxx(2)z
−1 + · · · + γxx(p)z−p+1

γxx(0)+ γxx(1)z−1 + · · · + γxx(p)z−p

(4.20)

(4.21)

(4.22)

Linear Prediction and Optimum Linear Filters

Problem 12).
The Levinson–Durbin recursion given by (4.17) requires O(m) multiplica-

the multiplications as well as the additions required to compute (4.17) can be

However, the computation in (4.16) for the reflection coefficients

(4.2). From the elements of this matrix we form the function

First, we note that R (∞) = 0. Then, from (4.21) we have

868



Hence R1(∞) = γxx(1)/γxx(0). We observe that R1(∞) = −K1 .
2

z = ∞. Thus we obtain

R2(∞) = γxx(2)+K1γxx(1)
γxx(0)(1− |K1|2)

Again, we observe that R2(∞) = −K2 . By continuing this procedure, we find that
Rm(∞) = −Km , for m = 1, 2, . . . , p . Hence the condition |Rm(∞)| < 1 for m = 1,
2, . . . , p , is identical to the condition |Km| < 1 for m = 1, 2, . . . , p , and ensures the
positive definiteness of the autocorrelation matrix �p+1 .

Since the reflection coefficients can be obtained from the sequence of functions
Rm(z), m = 1, 2, . . . , p , we have another method for solving the normal equations.
We call this method the Schur algorithm.

Schur algorithm. Let us first rewrite Rm(z) as

Rm(z) = Pm(z)
Qm(z)

, m = 0, 1, . . . , p

where
P0(z) = γxx(1)z−1 + γxx(2)z−2 + · · · + γxx(p)z−p

Q0(z) = γxx(0)+ γxx(1)z−1 + · · · + γxx(p)z−p

Since K0 = 0 and Km = −Rm(∞) for m = 1, 2, . . . , p , the recursive equation
m(z) and

Qm(z):

[
Pm(z)

Qm(z)

]
=
[

1 Km−1
K∗
m−1z

−1 z−1

] [
Pm−1(z)
Qm−1(z)

]
, m = 1, 2, . . . , p

Thus we have

P1(z) = P0(z) = γxx(1)z−1 + γxx(2)z−2 + · · · + γxx(p)z−p

Q1(z) = z−1Q0(z) = γxx(0)z−1 + γxx(1)z−2 + · · · + γxx(p)z−p−1

and

K1 = − P1(z)
Q1(z)

∣∣∣∣
z=∞
= −γxx(1)

γxx(0)

Next the reflection coefficient K2 is obtained by determining P2(z) and Q2(z) from
2 2

(4.23)

(4.24)

(4.25)

(4.26)

(4.27)

Linear Prediction and Optimum Linear Filters

Second, we compute R (z) according to (4.21) and evaluate the result at

(4.21) implies the following recursive equations for the polynomials P

(4.25), dividing P (z) by Q (z) and evaluating the result at z = ∞. Thus we find

869



that
P2(z) = P1(z)+K1Q1(z)

= [γxx(2)+K1γxx(1)]z−2 + · · ·
+ [γxx(p)+K1γxx(p − 1)]z−p

Q2(z) = z−1[Q1(z)+K∗1P1(z)]
= [γxx(0)+K∗1γxx(1)]z−2 + · · ·
+ [γxx(p − 2)+K∗1γxx(p − 1)]z−p

where the terms involving z−p−1 have been dropped. Thus we observe that the

Based on these relationships, the Schur algorithm is described by the following
recursive procedure.

Initialization. Form the 2x(p + 1) generator matrix

G0 =
[

0 γxx(1) γxx(2) · · · γxx(p)
γxx(0) γxx(1) γxx(2) · · · γxx(p)

]

where the elements of the first row are the coefficients of P0(z) and
the elements of the second row are the coefficients of Q0(z).

Step 1. Shift the second row of the generator matrix to the right by one place
and discard the last element of this row. A zero is placed in the vacant
position. Thus we obtain a new generator matrix,

G1 =
[

0 γxx(1) γxx(1) γxx(2) · · · γxx(p)
0 γxx(0) γxx(1) γxx(1) · · · γxx(p − 1)

]

The (negative) ratio of the elements in the second column yields the
reflection coefficient K1 = −γxx(1)/γxx(0).

Step 2. Multiply the generator matrix by the 2× 2 matrix

V1 =
[

1 K1
K∗1 1

]

Thus we obtain

V1G1 =
[

0 0 γxx(2)+K1γxx(1) γxx(p)+K1γxx(p − 1)
0 γxx(0)+K∗1γxx(1) · · · γxx(p − 1)+K∗1 γxx(p)

]

Step 3. Shift the second row of V1G1 by one place to the right and thus form
the new generator matrix

G2 =
[

0 0 γxx(2)+K1γxx(1) · · · γxx(p)+K1γxx(p − 1)
0 0 γxx(0)+K∗1 γxx(1) · · · γxx(p − 2)+K∗1 γxx(p − 1)

]

The negative ratio of the elements in the third column ofG2 yields K2 .

(4.28)

(4.29)

(4.30)

(4.31)

(4.32)

(4.33)

Linear Prediction and Optimum Linear Filters

recursive equation in (4.25) is equivalent to (4.21).

870



Steps 2 and 3 are repeated until we have solved for all p reflection coefficients. In
general, the 2× 2 matrix in Step 2 is

Vm =
[

1 Km
K∗m 1

]

and multiplication of Vm by Gm yields VmGm . In Step 3 we shift the second row of
VmGm one place to the right and obtain the new generator matrix Gm+1 .

We observe that the shifting operation of the second row in each iteration is
equivalent to multiplication by the delay operator z−1 in the second recursive equa-

m(z) by the poly-
nomial Qm(z) and the evaluation of the quotient at z = ∞ is equivalent to dividing
the elements in the (m + 1)st column of Gm . The computation of the p reflection
coefficients can be accomplished by use of parallel processors in O(p) time units.
Below we describe a pipelined architecture for performing these computations.

Another way of demonstrating the relationship of the Schur algorithm to the
Levinson–Durbin algorithm and the corresponding lattice predictor is to determine
the output of the lattice filter obtained when the input sequence is the correlation
sequence {γxx(m), m = 0, 1, . . .}. Thus, the first input to the lattice filter is γxx(0),
the second input is γxx(1), and so on [i.e., f0(n) = γxx(n)]. After the delay in the
first stage, we have g0(n− 1) = γxx(n− 1). Hence, for n = 1, the ratio f0(1)/g0(0) =
γxx(1)/γxx(0), which is the negative of the reflection coefficient K1 . Alternatively,
we can express this relationship as

f0(1)+K1g0(0) = γxx(1)+K1γxx(0) = 0
Furthermore, g0(0) = γxx(0) = Ef0 . At time n = 2, the input to the second stage is,

f1(2) = f0(2)+K1g0(1) = γxx(2)+K1γxx(1)
and after the unit of delay in the second stage, we have

g1(1) = K∗1f0(1)+ g0(0) = K∗1γxx(1)+ γxx(0)
Now the ratio f1(2)/g1(1) is

f1(2)
g1(1)

= γxx(2)+K1γxx(1)
γxx(0)+K∗1γxx(1)

= γxx(2)+K1γxx(1)
E
f

1

= −K2

Hence
f1(2)+K2g1(1) = 0

g1(1) = Ef1
By continuing in this manner, we can show that at the input to the mth lattice stage,
the ratio fm−1(m)/gm−1(m−1) = −Km and gm−1(m−1) = Efm−1 . Consequently, the
lattice filter coefficients obtained from the Levinson algorithm are identical to the
coefficients obtained in the Schur algorithm. Furthermore, the lattice filter structure
provides a method for computing the reflection coefficients in the lattice predictor.

(4.34)

Linear Prediction and Optimum Linear Filters

tion in (4.25). We also note that the division of the polynomial P

according to (3.4),

871



A pipelined architecture for implementing the Schur algorithm. Kung and Hu
(1983) developed a pipelined lattice-type processor for implementing the Schur al-
gorithm. The processor consists of a cascade of p lattice-type stages, where each
stage consists of two processing elements (PEs), which we designate as upper PEs
denoted as A1 , A2, . . . , Ap , and lower PEs denoted as B1 , B2, . . . , Bp , as shown in

The PE designated as A1 is assigned the task of performing divisions.
The remaining PEs perform one multiplication and one addition per iteration (one
clock cycle).

Initially, the upper PEs are loaded with the elements of the first row of the
0

elements of the second row of the generator matrix G0 . The computational process
begins with the division PE, A1 , which computes the first reflection coefficient as
K1 = −γxx(1)/γxx(0). The value of K1 is sent simultaneously to all the PEs in the
upper branch and lower branch.

The second step in the computation is to update the contents of all processing
elements simultaneously. The contents of the upper and lower PEs are updated as
follows:

PE Am: Am← Am +K1Bm, m = 2, 3, . . . , p
PE Bm: Bm← Bm +K∗1Am, m = 1, 2, . . . , p

The third step involves the shifting of the contents of the upper PEs one place
to the left. Thus we have

PE Am: Am−1 ← Am, m = 2, 3, . . . , p

At this point, PE A1 contains γxx(2) + K1γxx(1) while PE B1 contains γxx(0) +
K∗1γxx(1). Hence the processor A1 is ready to begin the second cycle by computing
the second reflection coefficient K2 = −A1/B1 . The three computational steps
beginning with the division A1/B1 are repeated until all p reflection coefficients are
computed. Note that PE B1 provides the minimum mean-square error E

f
m for each

iteration.
If τd denotes the time for PE A1 to perform a (complex) division and τma is

the time required to perform a (complex) multiplication and an addition, the time
required to compute all p reflection coefficients is p(τd+τma) for the Schur algorithm.

γxx(1)

γxx(0)

B1

A1 A2
K*m

K*m

Km

K*m

γxx(2)

γxx(1)

B2

Km

K*m

A3

γxx(3)

γxx(2)

B3

Km

K*m

Ap−1

γxx(p−1)

γxx(p−2)

Bp−1

Km

K*m

Ap

γxx(p)

γxx(p−1)

Bp

Km

K*m

…
…

…

E fm

Pipelined parallel processor for computing the reflection coefficients.

Linear Prediction and Optimum Linear Filters

Fig. 4.1.

generator matrix G , as illustrated in Fig. 4.1. The lower PEs are loaded with the

Figure 4.1

872



5 Properties of the Linear Prediction-Error Filters

Linear prediction filters possess several important properties, which we now describe.
We begin by demonstrating that the forward prediction-error filter is minimum phase.

Minimum-phase property of the forward prediction-error filter. We have already
demonstrated that the reflection coefficients {Ki} are correlation coefficients, and
consequently, |Ki | ≤ 1 for all i . This condition and the relationEfm = (1−|Km|2)Efm−1
can be used to show that the zeros of the prediction-error filter are either all inside
the unit circle or they are all on the unit circle.

First, we show that if Efp > 0, the zeros |zi | < 1 for every i . The proof is by
induction. Clearly, for p = 1 the system function for the prediction-error filter is

A1(z) = 1+K1z−1

Hence z1 = −K1 and Ef1 = (1− |K1|2)Ef0 > 0. Now, suppose that the hypothesis is
true for p − 1. Then, if zi p

Ap(zi) = Ap−1(zi)+Kpz−1i Bp−1(zi)

= Ap−1(zi)+Kpz−pi A∗p−1
(

1
zi

)
= 0

Hence
1
Kp
= −

z
−p
i A

∗
p−1(1/zi)

Ap−1(zi)
≡ Q(zi)

We note that the function Q(z) is all pass. In general, an all-pass function of the form

P(z) =
N∏
k=1

zz∗k + 1
z+ zk , |zk | < 1

satisfies the property that |P(z)| > 1 for |z| < 1, |P(z)| = 1 for |z| = 1, and |P(z)| < 1
for |z| > 1. Since Q(z) = −P(z)/z, it follows that |zi | < 1 if |Q(z)| > 1. Clearly, this
is the case since Q(zi) = 1/Kp and Efp > 0.

On the other hand, suppose that Ef
p−1 > 0 and E

f
p = 0. In this case |Kp| = 1 and

|Q(zi)| = 1. Since the MMSE is zero, the random process x(n) is called predictable
or deterministic. Specifically, a purely sinusoidal random process of the form

x(n) =
M∑
k=1

αke
j (nωk+θk)

where the phases {θk} are statistically independent and uniformly distributed over
(0, 2π), has the autocorrelation

γxx(m) =
M∑
k=1

α2k e
jmωk

(5.1)

(5.2)

(5.3)

(5.4)

(5.5)

(5.6)

Linear Prediction and Optimum Linear Filters

is a root of A (z), we have from (3.16) and (3.18),

873



and the power density spectrum

�xx(f ) =
M∑
k=1

α2k δ(f − fk), fk =
ωk

2π

This process is predictable with a predictor of order p ≥ M .
To demonstrate the validity of the statement, consider passing this process through

a prediction error filter of order p ≥ M . The MSE at the output of this filter is

Efp =
∫ 1/2
−1/2

�xx(f )|Ap(f )|2df

=
∫ 1/2
−1/2

[
M∑
k=1

α2k δ(f − fk)
]
|Ap(f )|2df

=
M∑
k=1

α2k |Ap(fk)|2

By choosing M of the p zeros of the prediction-error filter to coincide with the
frequencies {fk}, the MSE Efp can be forced to zero. The remaining p −M zeros
can be selected arbitrarily to be anywhere inside the unit circle.

Finally, the reader can prove that if a random process consists of a mixture of
a continuous power spectral density and a discrete spectrum, the prediction-error
filter must have all its roots inside the unit circle.

Maximum-phase property of the backward prediction-error filter. The system
function for the backward prediction-error filter of order p is

Bp(z) = z−pA∗p(z−1)

Consequently, the roots of Bp(z) are the reciprocals of the roots of the forward
prediction-error filter with system function Ap(z). Hence if Ap(z) is minimum phase,
then Bp(z) is maximum phase. However, if the process x(n) is predictable, all the
roots of Bp(z) lie on the unit circle.

Whitening property. Suppose that the random process x(n) is an AR(p) stationary
random process that is generated by passing white noise with variance σ 2w through
an all-pole filter with system function

H(z) = 1

1+
p∑
k=1

akz
−k

(5.7)

(5.8)

(5.9)

(5.10)

Linear Prediction and Optimum Linear Filters

874



Then the prediction-error filter of order p has the system function

Ap(z) = 1+
p∑
k=1

ap(k)z
−k

where the predictor coefficients ap(k) = ak . The response of the prediction-error
filter is a white noise sequence {w(n)}. In this case the prediction-error filter whitens
the input random process x(n) and is called a whitening filter, as indicated in Sec-

More generally, even if the input process x(n) is not an AR process, the prediction-
error filter attempts to remove the correlation among the signal samples of the input
process. As the order of the predictor is increased, the predictor output x̂(n) be-
comes a closer approximation to x(n) and hence the difference f (n) = x̂(n)− x(n)
approaches a white noise sequence.

Orthogonality of the backward prediction errors. The backward prediction errors
{gm(k)} from different stages in the FIR lattice filter are orthogonal. That is,

E[gm(n)g∗l (n)] =
{

0, 0 ≤ l ≤ m− 1
Ebm, l = m

This property is easily proved by substituting for gm ∗l
carrying out the expectation. Thus

E[gm(n)g∗l (n)] =
m∑
k=0

bm(k)

l∑
j=0

b∗l (j )E[x(n− k)x∗(n− j)]

=
l∑

j=0
b∗l (j )

m∑
k=0

bm(k)γxx(j − k)

But the normal equations for the backward linear predictor require that

m∑
k=0

bm(k)γxx(j − k) =
{

0, j = 1, 2, . . . , m− 1
Ebm, j = m

Therefore,

E[gm(n)g∗l (n)] =
{
Ebm = Efm, m = l
0, 0 ≤ l ≤ m− 1

Additional properties. There are a number of other interesting properties regard-
ing the forward and backward prediction errors in the FIR lattice filter. These are
given here for real-valued data. Their proof is left as an exercise for the reader.

(5.11)

(5.12)

(5.13)

(5.14)

(5.15)

Linear Prediction and Optimum Linear Filters

tion 3.4.

(n) and g (n) into (5.12) and

875



(a) E[fm(n)x(n− i)] = 0, 1 ≤ i ≤ m
(b) E[gm(n)x(n− i)] = 0, 0 ≤ i ≤ m− 1
(c) E[fm(n)x(n)] = E[gm(n)x(n−m)] = Em
(d) E[fi(n)fj (n)] = Emax(i, j)

(e) E[fi(n)fj (n− t)] = 0, for
{

1 ≤ t ≤ i − j, i > j
−1 ≥ t ≥ i − j, i < j

(f) E[gi(n)gj (n− t)] = 0, for
{

0 ≤ t ≤ i − j, i > j
0 ≥ t ≥ i − j + 1, i < j

(g) E[fi(n+ i)fj (n+ j)] =
{
Ei, i = j
0, i 	= j

(h) E[gi(n+ i)gj (n+ j)] = Emax (i,j)
(i) E[fi(n)gj (n)] =

{
KjEi, i ≥ j, i, j ≥ 0, K0 = 1
0, i < j

(j) E[fi(n)gi(n− 1)] = −Ki+1Ei
(k) E[gi(n− 1)x(n)] = E[fi(n+ 1)x(n− 1)] = −Ki+1Ei
(l) E[fi(n)gj (n− 1)] =

{
0, i > j
−Kj+1Ei, i ≤ j

6 AR Lattice and ARMA Lattice-Ladder Filters

diction. The linear predictor with transfer function,

Ap(z) = 1+
p∑
k=1

ap(k)z
−k

when excited by an input random process {x(n)}, produces an output that approaches
a white noise sequence as p → ∞. On the other hand, if the input process is an
AR(p), the output of Ap(z) is white. Since Ap(z) generates an MA(p) process when
excited with a white noise sequence, the all-zero lattice is sometimes called an MA
lattice.

In the following section, we develop the lattice structure for the inverse filter
1/Ap(z), called the AR lattice, and the lattice-ladder structure for an ARMA process.

6.1 AR Lattice Structure

Let us consider an all-pole system with system function

H(z) = 1

1+
p∑
k=1

ap(k)z
−k

(6.1)

(6.2)

Linear Prediction and Optimum Linear Filters

In Section 3 we showed the relationship of the all-zero FIR lattice to linear pre-

876



The difference equation for this IIR system is

y(n) = −
p∑
k=1

ap(k)y(n− k)+ x(n)

Now suppose that we interchange the roles of the input and output [i.e., interchange

x(n) = −
p∑
k=1

ap(k)x(n− k)+ y(n)

or, equivalently,

y(n) = x(n)+
p∑
k=1

ap(k)x(n− k)

function Ap(z). Thus an all-pole IIR system can be converted to an all-zero system
by interchanging the roles of the input and output.

Based on this observation, we can obtain the structure of an AR(p) lattice from
an MA(p) lattice by interchanging the input with the output. Since the MA(p)
lattice has y(n) = fp(n) as its output and x(n) = f0(n) is the input, we let

x(n) = fp(n)
y(n) = f0(n)

These definitions dictate that the quantities {fm(n)} be computed in descending order.
This computation can be accomplished by rearranging the recursive equation for
{fm m−1(n) in terms of fm(n). Thus we obtain

fm−1(n) = fm(n)−Kmgm−1(n− 1), m = p, p − 1, . . . , 1

The equation for gm(n) remains unchanged. The result of these changes is the set of
equations

x(n) = fp(n)
fm−1(n) = fm(n)−Kmgm−1(n− 1)
gm(n) = K∗mfm−1(n)+ gm−1(n− 1)
y(n) = f0(n) = g0(n)

the all-pole lattice structure has an all-zero path with input g0(n) and output gp(n),
which is identical to the all-zero path in the MA(p) lattice structure. This is not
surprising, since the equation for gm(n) is identical in the two lattice structures.

(6.3)

(6.4)

(6.5)

(6.6)

Linear Prediction and Optimum Linear Filters

x(n) with y(n) in (6.3)], obtaining the difference equation

We observe that (6.4) is a difference equation for an FIR system with system

(n)} in (3.4) and solving for f

The corresponding structure for the AR( p) lattice is shown in Fig. 6.1. Note that

877



x(n) = fp(n) fp−1(n) f2(n)

−Kp K*p

gp(n) g2(n)
+ z −1

+

−K2 K*2

+ z −1

+
f1(n)

g1(n) g0(n)

f0(n) = y(n)

−K1 K*1

+ z −1

+
Input Output

…

…

Lattice structure for an all-pole system.

We also observe that the AR(p) and MA(p) lattice structures are characterized
by the same parameters, namely, the reflection coefficients {Ki}. Consequently,

parameters {ap(k)} in the direct-form realizations of the all-zero system Ap(z) and
the lattice parameters {Ki} of the MA(p) lattice structure apply as well to the all-pole
structures.

6.2 ARMA Processes and Lattice-Ladder Filters

The all-pole lattice provides the basic building block for lattice-type structures that
implement IIR systems that contain both poles and zeros. To construct the appro-
priate structure, let us consider an IIR system with system function

H(z) =

q∑
k=0

cq(k)z
−k

1+
p∑
k=1

ap(k)z
−k
= Cq(z)
Ap(z)

Without loss of generality, we assume that p ≥ q .
This system is described by the difference equations

v(n) = −
p∑
k=1

ap(k)v(n− k)+ x(n)

y(n) =
q∑
k=0

cq(k)v(n− k)

obtained by viewing the system as a cascade of an all-pole system followed by an
all-zero system.
combination of delayed outputs from the all-pole system.

Since zeros result from forming a linear combination of previous outputs, we can
carry over this observation to construct a pole–zero system using the all-pole lattice
structure as the basic building block. We have clearly observed that gm(n) in the

(6.7)

(6.8)

Linear Prediction and Optimum Linear Filters

Figure 6.1

the equations given in (3.21) and (3.23) for converting between the system

From (6.8) we observe that the output y(n) is simply a linear

878



all-pole lattice can be expressed as a linear combination of present and past outputs.
In fact, the system

Hb(z) ≡ Gm(z)
Y (z)

= Bm(z)

is an all-zero system. Therefore, any linear combination of {gm(n)} is also an all-zero
filter.

Let us begin with an all-pole lattice filter with coefficients Km , 1 ≤ m ≤ p , and
add a ladder part by taking as the output a weighted linear combination of {gm(n)}.

Its output is

y(n) =
q∑
k=0

βkgk(n)

where {βk} are the parameters that determine the zeros of the system. The system

H(z) = Y (z)
X(z)

=
q∑
k=0

βk
Gk(z)

X(z)

Stage
pgp(n)

βp β0

fp−1(n)

gp−1(n)

fm(n)

Stage
p − 1

fp−2(n)

gp−2(n)

f1(n)

g1(n)
Stage

1

f0(n)

g0(n)

+ +

−Km K*m

gm(n)

fm−1(n)

gm−1(n)

+

+

+ z −1

x(n) = fp(n) …

…

…

βp−2

+

β1βp−1

Output

Input

(a) Pole–zero system

(b) mth stage of lattice

Lattice-ladder structure for pole–zero system.

(6.9)

(6.10)

(6.11)

Linear Prediction and Optimum Linear Filters

The result is a pole–zero filter that has the lattice-ladder structure shown in Fig. 6.2.

function corresponding to (6.10) is

Figure 6.2

879



p 0 0

H(z) =
q∑
k=0

βk
Gk(z)

G0(z)

F0(z)

Fp(z)

= 1
Ap(z)

q∑
k=0

βkBk(z)

Therefore,

Cq(z) =
q∑
k=0

βkBk(z)

This is the desired relationship that can be used to determine the weighting coeffi-
cients {βk}

Given the polynomials Cq(z) and Ap(z), where p ≥ q , the reflection coefficients
{Ki} are determined first from the coefficients {ap(k)}. By means of the step-down

k

expressed as

Cm(z) =
m−1∑
k=0

βkBk(z)+ βmBm(z)

= Cm−1(z)+ βmBm(z)
or, equivalently,

Cm−1(z) = Cm(z)− βmBm(z), m = p, p − 1, . . . , 1

By running this recursive relation backward, we can generate all the lower-degree
polynomials, Cm(z), m = p − 1, . . . , 1. Since bm(m) = 1, the parameters βm are

βm = cm(m), m = p, p − 1, . . . , 1, 0

When excited by a white noise sequence, this lattice-ladder filter structure gen-
erates an ARMA(p, q) process that has a power density spectrum

�xx(f ) = σ 2w
|Cq(f )|2
|Ap(f )|2

2
w is the variance of

the input white noise sequence.

(6.12)

(6.13)

(6.14)

(6.15)

(6.16)

.

Linear Prediction and Optimum Linear Filters

Since X(z) = F (z) and F (z) = G (z), (6.11), can be expressed as

recursive relation given by (3.22), we also obtain the polynomials B(z) , k = 1,
2, . . . , p . Then the ladder parameters can be obtained from (6.13), which can be

determined from (6.15) by setting

and an autocorrelation function that satisfies (2.18), where σ

880



7 Wiener Filters for Filtering and Prediction

In many practical applications we are given an input signal {x(n)}, consisting of the
sum of a desired signal {s(n)} and an undesired noise or interference {w(n)}, and we
are asked to design a filter that suppresses the undesired interference component. In
such a case, the objective is to design a system that filters out the additive interference
while preserving the characteristics of the desired signal {s(n)}.

In this section we treat the problem of signal estimation in the presence of an
additive noise disturbance. The estimator is constrained to be a linear filter with
impulse response {h(n)}, designed so that its output approximates some specified

lem.
The input sequence to the filter is x(n) = s(n) + w(n), and its output sequence

is y(n). The difference between the desired signal and the filter output is the error
sequence e(n) = d(n)− y(n).

We distinguish three special cases:

1. If d(n) = s(n), the linear estimation problem is referred to as filtering.
2. If d(n) = s(n + D), where D > 0, the linear estimation problem is referred

to as signal prediction. Note that this problem is different than the prediction
considered earlier in this chapter, where d(n) = x(n+D), D ≥ 0.

3. If d(n) = s(n − D), where D > 0, the linear estimation problem is referred to
as signal smoothing.

Our treatment will concentrate on filtering and prediction.
The criterion selected for optimizing the filter impulse response {h(n)} is the min-

imization of the mean-square error. This criterion has the advantages of simplicity
and mathematical tractability.

The basic assumptions are that the sequences {s(n)}, {w(n)}, and {d(n)} are zero
mean and wide-sense stationary. The linear filter will be assumed to be either FIR or
IIR. If it is IIR, we assume that the input data {x(n)} are available over the infinite
past. We begin with the design of the optimum FIR filter. The optimum linear filter,
in the sense of minimum mean-square error (MMSE), is called a Wiener filter.

Model for linear estimation
problem.

++
+

s(n)
Signal

x(n)

w(n)
Noise

y(n)

d(n)

e(n)−

Optimum
linear
filter

Linear Prediction and Optimum Linear Filters

desired signal sequence {d(n)}. Figure 7.1 illustrates the linear estimation prob-

Figure 7.1

881



7.1 FIR Wiener Filter

Suppose that the filter is constrained to be of length M with coefficients {hk, 0 ≤
k ≤ M − 1). Hence its output y(n) depends on the finite data record x(n), x(n −
1), . . . , x(n−M + 1),

y(n) =
M−1∑
k=0

h(k)x(n− k)

The mean-square value of the error between the desired output d(n) and y(n) is

EM = E|e(n)|2

= E
∣∣∣∣∣d(n)−

M−1∑
k=0

h(k)x(n− k)
∣∣∣∣∣
2

Since this is a quadratic function of the filter coefficients, the minimization of EM
yields the set of linear equations

M−1∑
k=0

h(k)γxx(l − k) = γdx(l), l = 0, 1, . . . ,M − 1

where γxx(k) is the autocorrelation of the input sequence {x(n)} and γdx(k) =
E[d(n)x∗(n−k)] is the crosscorrelation between the desired sequence {d(n)} and the
input sequence {x(n), 0 ≤ n ≤ M − 1}. The set of linear equations that specify the
optimum filter is called the Wiener–Hopf equation. These equations are also called
the normal equations, encountered earlier in the chapter in the context of linear
one-step prediction.

�MhM = γd

where �M is an M ×M (Hermitian) Toeplitz matrix with elements �lk = γxx(l − k)
and γd is the M×1 crosscorrelation vector with elements γdx(l), l = 0, 1, . . . ,M−1.
The solution for the optimum filter coefficients is

hopt = �−1M γd

and the resulting minimum MSE achieved by the Wiener filter is

MMSEM = min
hM

EM = σ 2d −
M−1∑
k=0

hopt(k)γ
∗
dx(k)

or, equivalently,
MMSEM = σ 2d − γ

∗t
d �
−1
M γd

(7.1)

(7.2)

(7.3)

(7.4)

(7.5)

(7.6)

(7.7)

Linear Prediction and Optimum Linear Filters

In general, the equations in (7.3) can be expressed in matrix form as

882



where σ 2d = E|d(n)|2 .
If we are dealing with filtering,

the d(n) = s(n). Furthermore, if s(n) and w(n) are uncorrelated random sequences,
as is usually the case in practice, then

γxx(k) = γss(k)+ γww(k)
γdx(k) = γss(k)

M−1∑
k=0

h(k)[γss(l − k)+ γww(l − k)] = γss(l), l = 0, 1, . . . ,M − 1

If we are dealing with prediction, then d(n) = s(n+D) where D > 0. Assuming
that s(n) and w(n) are uncorrelated random sequences, we have

γdx(k) = γss(l +D)

Hence the equations for the Wiener prediction filter become

M−1∑
k=0

h(k)[γss(l − k)+ γww(l − k)] = γss(l +D), l = 0, 1, . . . ,M − 1

In all these cases, the correlation matrix to be inverted is Toeplitz. Hence the
(generalized) Levinson–Durbin algorithm may be used to solve for the optimum
filter coefficients.

Let us consider a signal x(n) = s(n)+ w(n), where s(n) is an AR(1) process that satisfies the
difference equation

s(n) = 0.6s(n− 1)+ v(n)
where {v(n)} is a white noise sequence with variance σ 2v = 0.64, and {w(n)} is a white noise
sequence with variance σ 2w = 1. We will design a Wiener filter of length M = 2 to estimate
{s(n)}.

Solution. Since {s(n)} is obtained by exciting a single-pole filter by white noise, the power
spectral density of s(n) is

�ss(f ) = σ 2v |H(f )|2

= 0.64|1− 0.6e−j2πf |2

= 0.64
1.36− 1.2 cos 2πf

(7.8)

(7.9)

(7.10)

(7.11)

Linear Prediction and Optimum Linear Filters

Let us consider some special cases of (7.3).

and the normal equations in (7.3) become

EXAMPLE 7.1

883



The corresponding autocorrelation sequence {γss(m)} is
γss(m) = (0.6)|m|

The equations for the filter coefficients are

2h(0)+ 0.6h(1) = 1

0.6h(0)+ 2h(1) = 0.6
Solution of these equations yields the result

h(0) = 0.451, h(1) = 0.165
The corresponding minimum MSE is

MMSE2 = 1− h(0)γss (0)− h(1)γss (1)

= 1− 0.451− (0.165)(0.6)

= 0.45
This error can be reduced further by increasing the length of the Wiener filter (see Prob-

7.2 Orthogonality Principle in Linear Mean-Square Estimation

obtained directly by applying the orthogonality principle in linear mean-square es-
timation. Simply stated, the mean-square error EM

data points in the estimate,

E[e(n)x∗(n− l)] = 0, l = 0, 1, . . . ,M − 1
where

e(n) = d(n)−
M−1∑
k=0

h(k)x(n− k)

When viewed geometrically, the output of the filter, which is the estimate

d̂(n) =
M−1∑
k=0

h(k)x(n− k)

is a vector in the subspace spanned by the data {x(k), 0 ≤ k ≤ M − 1}. The error
ˆ ˆ

The orthogonality principle states that the length EM = E|e(n)|2 is a minimum when
e(n) is perpendicular to the data subspace [i.e., e(n) is orthogonal to each data point
x(k), 0 ≤ k ≤ M − 1].

filter coefficients {h(k)} are selected such that the error is orthogonal to each of the

(7.12)

(7.13)

(7.14)

Linear Prediction and Optimum Linear Filters

lem 35).

The normal equations for the optimum filter coefficients given by (7.3) can be

in (7.2) is a minimum if the

Conversely, if the filter coefficients satisfy (7.12),the resulting MSE is a minimum.

e(n) is a vector from d(n) to d (n) [i.e., d(n) = e(n)+ d(n)], as shown in Fig. 7.2.

884



Geometric interpretation of
linear MSE problem. x(2)

h(1)x(2)

h(0)x(1)
x(1)

d(n)
e(n)

d(n)ˆ

unique if the data {x(n)} in the estimate d̂(n) are linearly independent. In this case,
the correlation matrix �M is nonsingular. On the other hand, if the data are linearly
dependent, the rank of �M is less than M and therefore the solution is not unique.
In this case, the estimate d̂(n) can be expressed as a linear combination of a reduced
set of linearly independent data points equal to the rank of �M .

Since the MSE is minimized by selecting the filter coefficients to satisfy the
orthogonality principle, the residual minimum MSE is simply

MMSEM = E[e(n)d∗(n)]

7.3 IIR Wiener Filter

In the preceding section we constrained the filter to be FIR and obtained a set of M
linear equations for the optimum filter coefficients. In this section we allow the filter
to be infinite in duration (IIR) and the data sequence to be infinite as well. Hence
the filter output is

y(n) =
∞∑
k=0

h(k)x(n− k)

The filter coefficients are selected to minimize the mean-square error between the
desired output d(n) and y(n), that is,

E∞ = E|e(n)|2

= E
∣∣∣∣∣d(n)−

∞∑
k=0

h(k)x(n− k)
∣∣∣∣∣
2

(7.15)

(7.16)

(7.17)

Linear Prediction and Optimum Linear Filters

Figure 7.2

We note that the solution obtained from the normal equations in (7.3) is

which yields the result given in (7.6).

885



Application of the orthogonality principle leads to the Wiener–Hopf equation,

∞∑
k=0

h(k)γxx(l − k) = γdx(l), l ≥ 0

The residual MMSE is simply obtained by application of the condition given by

MMSE∞ = min
h

E∞ = σ 2d −
∞∑
k=0

hopt(k)γ
∗
dx(k)

z-transform techniques, because the equation holds only for l ≥ 0. We shall solve
for the optimum IIR Wiener filter based on the innovations representation of the
stationary random process {x(n)}.

Recall that a stationary random process {x(n)} with autocorrelation γxx(k) and
power spectral density �xx(f ) can be represented by an equivalent innovations pro-
cess, {i(n)}, by passing {x(n)} through a noise-whitening filter with system function
1/G(z), where G(z) is the minimum-phase part obtained from the spectral factor-
ization of �xx(z):

�xx(z) = σ 2i G(z)G(z−1)
Hence G(z) is analytic in the region |z| > r1 , where r1 < 1.

Now, the optimum Wiener filter can be viewed as the cascade of the whitening
filter 1/G(z) with a second filter, say Q(z), whose output y(n) is identical to the
output of the optimum Wiener filter. Since

y(n) =
∞∑
k=0

q(k)i(n− k)

and e(n) = d(n) − y(n), application of the orthogonality principle yields the new
Wiener–Hopf equation as

∞∑
k=0

q(k)γii(l − k) = γdi(l), l ≥ 0

But since {i(n)} is white, it follows that γii(l − k) = 0 unless l = k . Thus we obtain
the solution as

q(l) = γdi(l)
γii(0)

= γdi(l)
σ 2i

, l ≥ 0

The z-transform of the sequence {q(l)} is

Q(z) =
∞∑
k=0

q(k)z−k

= 1
σ 2i

∞∑
k=0

γdi(k)z
−k

(7.18)

(7.19)

(7.20)

(7.21)

(7.22)

(7.23)

(7.24)

Linear Prediction and Optimum Linear Filters

(7.15). Thus we obtain

The Wiener–Hopf equation given by (7.18) cannot be solved directly with

886



If we denote the z-transform of the two-sided crosscorrelation sequence γdi(k) by
�di(z)

�di(z) =
∞∑

k=−∞
γdi(k)z

−k

and define [�di(z)]+ as

[�di(z)]+ =
∞∑
k=0

γdi(k)z
−k

then

Q(z) = 1
σ 2i

[�di(z)]+

To determine [�di(z)]+ , we begin with the output of the noise-whitening filter,
which can be expressed as

i(n) =
∞∑
k=0

v(k)x(n− k)

where {v(k), k ≥ 0} is the impulse response of the noise-whitening filter,
1

G(z)
≡ V (z) =

∞∑
k=0

v(k)z−k

Then

γdi(k) = E[d(n)i∗(n− k)]

=
∞∑
m=0

v(m)E[d(n)x∗(n−m− k)]

=
∞∑
m=0

v(m)γdx(k +m)

The z-transform of the crosscorrelation γdi(k) is

�di(z) =
∞∑

k=−∞

[ ∞∑
m=0

v(m)γdx(k +m)
]
z−k

=
∞∑
m=0

v(m)

∞∑
k=−∞

γdx(k +m)z−k

=
∞∑
m=0

v(m)zm
∞∑

k=−∞
γdx(k)z

−k

= V (z−1)�dx(z) = �dx(z)
G(z−1)

(7.25)

(7.26)

(7.27)

(7.28)

(7.29)

(7.30)

(7.31)

Linear Prediction and Optimum Linear Filters

887



Therefore,

Q(z) = 1
σ 2i

[
�dx(z)

G(z−1)

]
+

Finally, the optimum IIR Wiener filter has the system function

Hopt(z) = Q(z)
G(z)

= 1
σ 2i G(z)

[
�dx(z)

G(z−1)

]
+

In summary, the solution for the optimum IIR Wiener filter requires that we
perform a spectral factorization of �xx(z) to obtain G(z), the minimum-phase com-
ponent, and then we solve for the causal part of �dx(z)/G(z−1). The following
example illustrates the procedure.

Solution. For this signal we have

�xx(z) = �ss(z)+ 1 =
1.8(1− 13 z−1)(1− 13 z)
(1− 0.6z−1)(1− 0.6z)

where σ 2i = 1.8 and

G(z) = 1−
1
3 z
−1

1− 0.6z−1
The z-transform of the crosscorrelation γdx(m) is

�dx(z) = �ss(z) = 0.64
(1− 0.6z−1)(1− 0.6z)

Hence [
�dx(z)

G(z−1)

]
+
=
[

0.64

(1− 13 z)(1− 0.6z−1)

]
+

=
[

0.8
1− 0.6z−1 +

0.266z

1− 13z

]
+

= 0.8
1− 0.6z−1

The optimum IIR filter has the system function

Hopt(z) = 11.8

(
1− 0.6z−1
1− 13z−1

)(
0.8

1− 0.6z−1
)

=
4
9

1− 13z−1

(7.32)

(7.33)

Linear Prediction and Optimum Linear Filters

EXAMPLE 7.2

Let us determine the optimum IIR Wiener filter for the signal given in Example 7.1.

888



and an impulse response

hopt(n) = 49
( 1

3

)n
, n ≥ 0

in terms of the frequency-domain characteristics of the filter. First, we note that
σ 2d ≡ E|d(n)|2 is simply the value of the autocorrelation sequence {γdd
at k = 0. Since

γdd(k) = 12πj
∮

Ĉ
�dd(z)z

k−1dz

it follows that

σ 2d = γdd(0) =
1

2πj

∮
Ĉ

�dd(z)

z
dz

where the contour integral is evaluated along a closed path encircling the origin in
the region of convergence of �dd(z).

by application of Parseval’s theorem. Since hopt(k) = 0 for k < 0, we have

∞∑
k=−∞

hopt(k)γ
∗
dx(k) =

1
2πj

∮
Ĉ
Hopt(z)�dx(z

−1)z−1dz

where C is a closed contour encircling the origin that lies within the common region
of convergence of Hopt(z) and �dx(z−1).

MMSE∞ in the form

MMSE∞ = 12πj
∮

Ĉ
[�dd(z)−Hopt(z)�dx(z−1)]z−1dz

MMSE∞ = 12πj
∮

Ĉ

[
0.3555

(z− 13 )(1− 0.6z)

]
dz

There is a single pole inside the unit circle at z = 13 . By evaluating the residue at the pole, we
obtain

MMSE∞ = 0.444
We observe that this MMSE is only slightly smaller than that for the optimum two-tap Wiener

(k)} evaluated

(7.34)

(7.35)

(7.36)

(7.37)

Linear Prediction and Optimum Linear Filters

We conclude this section by expressing the minimum MSE given by (7.19)

The second term in (7.19) is also easily transformed to the frequency domain

By combining (7.35) with (7.36), we obtain the desired expression for the

EXAMPLE 7.3

For the optimum Wiener filter derived in Example 7.2, the minimum MSE is

filter in Example 7.1.

889



7.4 Noncausal Wiener Filter

In the preceding section we constrained the optimum Wiener filter to be causal [i.e.,
hopt(n) = 0 for n < 0]. In this section we drop this condition and allow the filter to
include both the infinite past and the infinite future of the sequence {x(n)} in forming
the output y(n), that is,

y(n) =
∞∑

k=−∞
h(k)x(n− k)

The resulting filter is physically unrealizable. It can also be viewed as a smoothing
filter in which the infinite future signal values are used to smooth the estimate d̂(n) =
y(n) of the desired signal d(n).

Application of the orthogonality principle yields the Wiener–Hopf equation for
the noncausal filter in the form

∞∑
k=−∞

h(k)γxx(l − k) = γdx(l), −∞ < l <∞

and the resulting MMSEnc as

MMSEnc = σ 2d −
∞∑

k=−∞
h(k)γ ∗dx(k)

to yield the optimum noncausal Wiener filter as

Hnc(z) = �dx(z)
�xx(z)

The MMSEnc can also be simply expressed in the z-domain as

MMSEnc = 12πj
∮

Ĉ
[�dd(z)−Hnc(z)�dx(z−1)]z−1dz

In the following example we compare the form of the optimal noncausal filter
with the optimal causal filter obtained in the previous section.

�dx(z) = �ss(z) = 0.64
(1− 0.6z−1)(1− 0.6z)

and
�xx(z) = �ss(z)+ 1

= 2(1− 0.3z
−1 − 0.3z)

(1− 0.6z−1)(1− 0.6z)

(7.38)

(7.39)

(7.40)

(7.41)

(7.42)

Linear Prediction and Optimum Linear Filters

Since (7.39) holds for −∞ < l <∞, this equation can be transformed directly

EXAMPLE 7.4

The optimum noncausal Wiener filter for the signal characteristics given in Example 7.1 is
given by (7.41), where

890



Then,

Hnc(z) = 0.3555
(1− 13z−1)(1− 13z)

This filter is clearly noncausal.

integrand is
1
z
�ss(z)[1−Hnc(z)] = 0.3555

(z− 13 )(1− 13z)

The only pole inside the unit circle is z = 13 . Hence the residue is

0.3555

1− 13 z

∣∣∣∣∣
z= 13

= 0.3555
8/9

= 0.40

Hence the minimum achievable MSE obtained with the optimum noncausal Wiener filter is

MMSEnc = 0.40

Note that this is lower than the MMSE for the causal filter, as expected.

8 Summary and References

The major focal point in this chapter is the design of optimum linear systems for
linear prediction and filtering. The criterion for optimality is the minimization of
the mean-square error between a specified desired filter output and the actual filter
output.

In the development of linear prediction, we demonstrated that the equations for
the forward and backward prediction errors specified a lattice filter whose param-
eters, the reflection coefficients {Km}, were simply related to the filter coefficients
{am(k)} of the direct-form FIR linear predictor and the associated prediction-error
filter. The optimum filter coefficients {Km} and {am(k)} are easily obtained from the
solution of the normal equations.

We described two computationally efficient algorithms for solving the normal
equations, the Levinson–Durbin algorithm and the Schur algorithm. Both algorithms
are suitable for solving a Toeplitz system of linear equations and have a computational
complexity ofO(p2)when executed on a single processor. However, with full parallel
processing, the Schur algorithm solves the normal equations in O(p) time, whereas
the Levinson–Durbin algorithm requires O(p logp) time.

In addition to the all-zero lattice filter resulting from linear prediction, we also
derived the AR lattice (all-pole) filter structure and the ARMA lattice-ladder (pole–
zero) filter structure. Finally, we described the design of the class of optimum linear
filters, called Wiener filters.

Linear estimation theory has had a long and rich history of development over
the past four decades. Kailath (1974) presents a historical account of the first three

Linear Prediction and Optimum Linear Filters

The minimum MSE achieved by this filter is determined from evaluating (7.42). The

891



decades. The pioneering work of Wiener (1949) on optimum linear filtering for sta-
tistically stationary signals is especially significant. The generalization of the Wiener
filter theory to dynamical systems with random inputs was developed by Kalman
(1960) and Kalman and Bucy (1961). Kalman filters are treated in the books by
Meditch (1969), Brown (1983), and Chui and Chen (1987). The monograph by
Kailath (1981) treats both Wiener and Kalman filters.

There are numerous references on linear prediction and lattice filters. Tutorial
treatments on these subjects have been published in the journal papers by Makhoul
(1975, 1978) and Friedlander (1982a, b). The books by Haykin (1991), Markel and
Gray 1976), and Tretter (1976) provide comprehensive treatments of these subjects.
Applications of linear prediction to spectral analysis are found in the books by Kay
(1988) and Marple (1987), to geophysics in the book by Robinson and Treitel (1980),
and to adaptive filtering in the book by Haykin (1991).

The Levinson–Durbin algorithm for solving the normal equations recursively
was given by Levinson (1947) and later modified by Durbin (1959). Variations of
this classical algorithm, called split Levinson algorithms, have been developed by
Delsarte and Genin (1986) and by Krishna (1988). These algorithms exploit addi-
tional symmetries in the Toeplitz correlation matrix and save about a factor of 2 in
the number of multiplications.

The Schur algorithm was originally described by Schur (1917) in a paper pub-
lished in German. An English translation of this paper appears in the book edited
by Gohberg (1986). The Schur algorithm is intimately related to the polynomials
{Am(z)}, which can be interpreted as orthogonal polynomials. A treatment of or-
thogonal polynomials is given in the books by Szegö (1967), Grenander and Szegö
(1958), and Geronimus (1958). The thesis of Vieira (1977) and the papers by Kailath
et al. (1978), Delsarte et al. (1978), and Youla and Kazanjian (1978) provide ad-
ditional results on orthogonal polynomials. Kailath (1985, 1986) provides tutorial
treatments of the Schur algorithm and its relationship to orthogonal polynomials
and the Levinson–Durbin algorithm. The pipelined parallel processing structure for
computing the reflection coefficients based on the Schur algorithm and the related
problem of solving Toeplitz systems of linear equations is described in the paper by
Kung and Hu (1983). Finally, we should mention that some additional computational
efficiency can be achieved in the Schur algorithm, by further exploiting symmetry
properties of Toeplitz matrices, as described by Krishna (1988). This leads to the
so-called split-Schur algorithm, which is analogous to the split-Levinson algorithm.

Problems

1 The power density spectrum of an AR process {x(n)} is given as

�xx(ω) = σ
2
w

|A(ω)|2

= 25|1− e−jω + 12e−j2ω|2

where σ 2w is the variance of the input sequence.

Linear Prediction and Optimum Linear Filters

892



(a) Determine the difference equation for generating the AR process when the
excitation is white noise.

(b) Determine the system function for the whitening filter.

2 An ARMA process has an autocorrelation {γxx(m)} whose z-transform is given as

�xx(z) = 9
(z− 13)(z− 3)
(z− 12 )(z− 2)

, 12 < |z| < 2

(a) Determine the filter H(z) for generating {x(n)} from a white noise input se-
quence. Is H(z) unique? Explain.

(b) Determine a stable linear whitening filter for the sequence {x(n)}.
3 Consider the ARMA process generated by the difference equation

x(n) = 1.6x(n− 1)− 0.63x(n− 2)+ w(n)+ 0.9w(n− 1)
(a) Determine the system function of the whitening filter and its poles and zeros.

(b) Determine the power density spectrum of {x(n)}.
4 Determine the lattice coefficients corresponding to the FIR filter with system function

H(z) = A3(z) = 1+ 1324z−1 + 58z−2 + 13z−3

5 Determine the reflection coefficients {Km} of the lattice filter corresponding to the
FIR filter described by the system function

H(z) = A2(z) = 1+ 2z−1 + 13z−2

6 (a) Determine the zeros and sketch the zero pattern for the FIR lattice filter with
reflection coefficients

K1 = 12 , K2 = − 13 , K3 = 1

(b) Repeat part (a) but with K3 = −1.
(c) You should have found that the zeros lie on the unit circle. Can this result be

generalized? How?

7 Determine the impulse response of the FIR filter that is described by the lattice
coefficients K1 = 0.6, K2 = 0.3, K3 = 0.5, and K4 = 0.9.

8 p
process is a forward linear prediction-error filter of order p . Show that the backward
linear prediction-error filter of order p is the noise-whitening filter of the correspond-
ing anticausal AR(p) process.

9 Use the orthogonality principle to determine the normal equations and the resulting
minimum MSE for a forward predictor of order p that predicts m samples (m > 1)
into the future (m-step forward predictor). Sketch the prediction-error filter.

10

Linear Prediction and Optimum Linear Filters

In Section 3.4 we indicated that the noise-whitening filterA (z) for a causal AR(p)

Repeat Problem 9 for an m-step backward predictor.

893



11 Determine a Levinson–Durbin recursive algorithm for solving for the coefficients
of a backward prediction-error filter. Use the result to show that coefficients of the
forward and backward predictors can be expressed recursively as

am =
[

am−1
0

]
+Km

[
bm−1

1

]

bm =
[

bm−1
0

]
+K∗m

[
am−1

1

]

12
tions

�mam = −γm
where the right-hand side of this equation has elements of the autocorrelation se-
quence that are also elements of the matrix �. Let us consider the more general
problem of solving the linear equations

�mbm = cm
where cm is an arbitrary vector. (The vector bm is not related to the coefficients of
the backward predictor.) Show that the solution to �mbm = cm can be obtained from
a generalized Levinson–Durbin algorithm which is given recursively as

bm(m) =
c(m)− γbt

m−1bm−1
E
f

m−1
bm(k) = bm−1(k)− bm(m)a∗m−1(m− k),

k = 1, 2, . . . , m− 1
m = 1, 2, . . . , p

where b1(1) = c(1)/γxx (0) = c(1)/Ef0 and am
recursion is required to solve the equation �mbm = cm .

13 Use the generalized Levinson–Durbin algorithm to solve the normal equations re-
cursively for the m-step forward and backward predictors.

14 Consider the AR(3) process generated by the equation

x(n) = 1424x(n− 1)+ 924x(n− 2)− 124x(n− 3)+w(n)

where w(n) is a stationary white noise process with variance σ 2w .
(a) Determine the coefficients of the optimum p = 3 linear predictor.
(b) Determine the autocorrelation sequence γxx(m), 0 ≤ m ≤ 5.
(c) Determine the reflection coefficients corresponding to the p = 3 linear predic-

tor.
15 The z-transform of the autocorrelation γxx(m) of an ARMA(1, 1) process is

�xx(z) = σ 2wH(z)H(z−1)

�xx(z) = 4σ
2
w

9
5− 2z − 2z−1
10− 3z−1 − 3z

Linear Prediction and Optimum Linear Filters

The Levinson–Durbin algorithm described in Section 4.1 solved the linear equa-

(k) is given by (4.17). Thus a second

894



(a) Determine the minimum-phase system function H(z).

(b) Determine the system function H(z) for a mixed-phase stable system.
16 Consider an FIR filter with coefficient vector

[ 1 −2r cos θ r2 ]
(a) Determine the reflection coefficients for the corresponding FIR lattice filter.

(b) Determine the values of the reflection coefficients in the limit as r → 1.
17 An AR(3) process is characterized by the prediction coefficients

a3(1) = −1.25, a3(2) = 1.25, a3(3) = −1
(a) Determine the reflection coefficients.

(b) Determine γxx(m) for 0 ≤ m ≤ 3.
(c) Determine the mean-square prediction error.

18 The autocorrelation sequence for a random process is

γxx(m) =




1, m = 0
−0.5, m = ±1

0.625, m = ±2
−0.6875, m = ±3

0 otherwise

Determine the system functions Am(z) for the prediction-error filters for m = 1, 2,
3, the reflection coefficients {Km}, and the corresponding mean-square prediction
errors.

19 The autocorrelation sequence for an AR process x(n) is

γxx(m) = ( 14 )|m|

(a) Determine the difference equation for x(n).

(b) Is your answer unique? If not, give any other possible solutions.
20

γxx(m) = a|m| cos πm2
where 0 < a < 1.

21 Prove that a FIR filter with system function

Ap(z) = 1+
p∑
k=1

ap(k)z
−k

and reflection coefficients |Kk| < 1 for 1 ≤ k ≤ p − 1 and |Kp| > 1 is maximum
phase [all the roots of Ap(z) lie outside the unit circle].

Linear Prediction and Optimum Linear Filters

Repeat Problem 19 for an AR process with autocorrelation

895



Show that the transformation

Vm =
[

1 Km
K∗m 1

]

in the Schur algorithm satisfies the special property

VmJVtm = (1− |Km|2)J
where

J =
[

1 0
0 −1

]

Thus Vm is called a J -rotation matrix. Its role is to rotate or hyperbolate the row of
Gm to lie along the first coordinate direction (Kailath, 1985).

23 Prove the additional properties (a) through (l) of the prediction-error filters given in

24 Extend the additional properties (a) through (l) of the prediction-error filters given

25 Determine the reflection coefficient K3 in terms of the autocorrelations {γxx(m)}
from the Schur algorithm and compare your result with the expression for K3 ob-
tained from the Levinson–Durbin algorithm.

26 Consider an infinite-length (p = ∞) one-step forward predictor for a stationary
random process {x(n)} with a power density spectrum of �xx(f ). Show that the
mean-square error of the prediction-error filter can be expressed as

Ef∞ = 2π exp{
∫ 1/2
−1/2

ln�xx(f ) df }

27 Determine the output of an infinite-length (p = ∞) m-step forward predictor and
the resulting mean-square error when the input signal is a first-order autoregressive
process of the form

x(n) = ax(n− 1)+ w(n)
28 An AR(3) process {x(n)} is characterized by the autocorrelation sequence γxx(0) =

1, γxx(1) = 12 , γxx(2) = 18 , and γxx(3) = 164 .
(a) Use the Schur algorithm to determine the three reflection coefficients K1 , K2 ,

and K3 .
(b) Sketch the lattice filter for synthesizing {x(n)} from a white noise excitation.

29 The purpose of this problem is to show that the polynomials {Am(z)}, which are the
system functions of the forward prediction-error filters of order m, m = 0, 1, . . . , p ,
can be interpreted as orthogonal on the unit circle. Toward this end, suppose that
�xx(f ) is the power spectral density of a zero-mean random process {x(n)} and let
{Am(z)}, m = 0, 1, . . . , p}, be the system functions of the corresponding prediction-
error filters. Show that the polynomials {Am(z)} satisfy the orthogonality property∫ 1/2

−1/2
�xx(f )Am(f )A

∗
n(f )df = Efmδmn, m, n = 0, 1, . . . , p

Linear Prediction and Optimum Linear Filters

22

Section 5.

in Section 5 to complex-valued signals.

896



30 Determine the system function of the all-pole filter described by the lattice coeffi-
cients K1 = 0.6, K2 = 0.3, K3 = 0.5, and K4 = 0.9.

31 Determine the parameters and sketch the lattice-ladder filter structure for the system
with system function

H(z) = 1− 0.8z
−1 + 0.15z−2

1+ 0.1z−1 − 0.72z−2
32 Consider a signal x(n) = s(n) + w(n), where s(n) is an AR(1) process that satisfies

the difference equation
s(n) = 0.8s(n− 1)+ v(n)

where {v(n)} is a white noise sequence with variance σ 2v = 0.49 and {w(n)} is a
white noise sequence with variance σ 2w = 1. The processes {v(n)} and {w(n)} are
uncorrelated.

(a) Determine the autocorrelation sequences {γss(m)} and {γxx(m)}.
(b) Design a Wiener filter of length M = 2 to estimate {s(n)}.
(c) Determine the MMSE for M = 2.

33
and the corresponding MMSE∞ .

34 Determine the system function for the noncausal IIR Wiener filter for the signal
nc .

35 Determine the optimum FIR Wiener filter of length M = 3 for the signal in Ex-
3 . Compare MMSE3 with MMSE2 and

comment on the difference.
36 An AR(2) process is defined by the difference equation

x(n) = x(n− 1)− 0.6x(n− 2)+ w(n)

where {w(n)} is a white noise process with variance σ 2w . Use the Yule–Walker equa-
tions to solve for the values of the autocorrelation γxx(0), γxx(1), and γxx(2).

37 An observed random process {x(n)} consists of the sum of an AR(p) process of the
form

s(n) = −
p∑
k=1

ap(k)s(n− k)+ v(n)

and a white noise process {w(n)} with variance σ 2w . The random process {v(n)} is
also white with variance σ 2v . The sequences {v(n)} and {w(n)} are uncorrelated.

Show that the observed process {x(n) = s(n) + w(n)} is ARMA(p, p) and de-
termine the coefficients of the numerator polynomial (MA component) in the cor-
responding system function.

Linear Prediction and Optimum Linear Filters

Determine the optimum causal IIR Wiener filter for the signal given in Problem 32

given in Problem 32 and the corresponding MMSE

ample 7.1 and the corresponding MMSE

897



This page intentionally left blank 



John G. Proakis, Dimitris G. Manolakis. Copyright © 2007 by Pearson Education, Inc. All rights reserved.
From Chapter 13   of Digital Signal Processing: Principles, Algorithms, and Applications, Fourth Edition.

Adaptive Filters

899



Adaptive Filters

1 Applications of Adaptive Filters

Adaptive filters have been used widely in communication systems, control systems,
and various other systems in which the statistical characteristics of the signals to be
filtered are either unknown a priori or, in some cases, are slowly time-variant (non-
stationary signals). Numerous applications of adaptive filters have been described in
the literature. Some of the more noteworthy applications include: (1) adaptive an-
tenna systems, in which adaptive filters are used for beam steering and for providing

In contrast to filter design techniques based on knowledge of the second-order sta-
tistics of the signals, there are many digital signal processing applications in which 
these statistics cannot be specified a priori. Such applications include channel equal-
ization, echo cancellation, and system modeling among others, as described in this 
chapter. In these applications, filters with adjustable coefficients, called adaptive 
filters, are employed. Such filters incorporate algorithms that allow the filter coef-
ficients to adapt to the signal statistics.

Adaptive filters have received considerable attention from researchers over the 
past 25 years. As a result, many computationally efficient algorithms for adaptive 
filtering have been developed. In this chapter, we describe two basic algorithms: 
the least-mean-square (LMS) algorithm, which is based on a gradient optimization 
for determining the coefficients, and the class of recursive least-squares algorithms, 
which includes both direct-form FIR (finite-duration impulse response) and lattice 
realizations. Before we describe the algorithms, we present several practical appli-
cations in which adaptive filters have been successfully used in the estimation of 
signals corrupted by noise and other interference.

900



nulls in the beam pattern to remove undesired interference (Widrow, Mantey, and
Griffiths (1967)); (2) digital communication receivers, in which adaptive filters are
used to provide equalization of intersymbol interference and for channel identifica-
tion (Lucky (1965), Proakis and Miller (1969), Gersho (1969), George, Bowen, and
Storey (1971), Proakis (1970; 1975), Magee and Proakis (1973), Picinbono (1978) and
Nichols, Giordano, and Proakis (1977)); (3) adaptive noise cancelling techniques, in
which adaptive filters are used to estimate and eliminate a noise component in a
desired signal (Widrow et al. (1975), Hsu and Giordano (1978), and Ketchum and
Proakis (1982)); (4) system modeling, in which adaptive filters are used as models
to estimate the characteristics of an unknown system. These are just a few of the
best-known examples of the use of adaptive filters.

Of the various FIR filter structures that are possible, the direct form and the
lattice form are the ones used in adaptive filtering applications. The direct-form FIR

other hand, the adjustable parameters in an FIR lattice structure are the reflection
coefficients Kn .

An important consideration in the use of an adaptive filter is the criterion for
optimizing the adjustable filter parameters. The criterion must not only provide a
meaningful measure of filter performance, but it must also result in a practically
realizable algorithm.

For example, a desirable performance index in a digital communication system is
the average probability of error. Consequently, in implementing an adaptive equal-
izer, we might consider the selection of the equalizer coefficients to minimize the av-
erage probability of error as the basis for our optimization criterion. Unfortunately,

z�1 z�1 z�1 z�1

�

Coefficient
adjustment

h(0) h(1) h(2) h(3) h(4)

Input

Output

Figure 1.1 Direct-form adaptive FIR filter.

been considered for adaptive filtering, the FIR filter is by far the most practical 
and widely used. The reason for this preference is quite simple: the FIR filter has 
only adjustable zeros; hence, it is free of stability problems associated with adaptive 
IIR filters, which have adjustable poles as well as zeros. We should not conclude, 
however, that adaptive FIR filters are always stable. On the contrary, the stability of 
the filter depends critically on the algorithm for adjusting its coefficients, as will be 
demonstrated in Sections 2 and 3.

Adaptive Filters

filter structure with adjustable coefficients h(n) is illustrated in Figure 1.1. On the

Although both IIR (infinite-duration impulse response) and FIR filters have 

901



however, the performance index (average probability of error) for this criterion is a
highly nonlinear function of the filter coefficients and the signal statistics. As a conse-
quence, the implementation of an adaptive filter that optimizes such a performance
index is complex and impractical.

In some cases, a performance index that is a nonlinear function of the filter
parameters possesses many relative minima (or maxima), so that one is not certain
whether the adaptive filter has converged to the optimum solution or to one of the
relative minima (or maxima). For such reasons, some desirable performance indices,
such as the average probability of error in a digital communication system, must be
rejected on the grounds that they are impractical to implement.

Two criteria that provide good measures of performance in adaptive filtering
applications are the least-squares criterion and its counterpart in a statistical formu-
lation of the problem, namely, the mean-square-error (MSE) criterion. The least-
squares (and MSE) criterion results in a quadratic performance index as a function
of the filter coefficients and, hence, it possesses a single minimum. The resulting
algorithms for adjusting the coefficients of the filter are relatively easy to implement,

In the following section, we describe several applications of adaptive filters that
serve as a motivation for the mathematical development of algorithms derived in

these
examples. Although we will not develop the recursive algorithms for automat

-
ically

adjusting the filter coefficients in this section, it is instructive to formulate the opti
mization of the filter coefficients as a least-squares optimization problem. This
development will serve to establish a common framework for the algorithms derived
in the next two sections.

1.1 System Identification or System Modeling

In the formulation of this problem we have an unknown system, called a plant, that
we wish to identify. The system is modeled by an FIR filter with M adjustable
coefficients. Both the plant and model are excited by an input sequence x(n). If y(n)
denotes the output of the plant and ŷ(n) denotes the output of the model,

ŷ(n) =
M−1∑
k=0

h(k)x(n− k)

We may form the error sequence

e(n) = y(n)− ŷ(n), n = 0, 1, . . .

and select the coefficients h(k) to minimize

�M =
N∑
n=0

[
y(n)−

M−1∑
k=0

h(k)x(n− k)
]2

where N + 1 is the number of observations.

(1.1)

(1.2)

(1.3)

Adaptive Filters

as we demonstrate in Sections 2 and 3.

Sections 2 and 3. We find it convenient to use the direct-form FIR structure in

902



Figure 1.2
Application of adaptive
filtering to system
identification.

Unknown
time-variant

system

FIR filter
model

Adaptive
algorithm

�

�

Error signal

Noise

x(n)

d(n)

y(n)

y(n)

w(n)

ˆ

The least-squares criterion leads to the set of linear equations for determining
the filter coefficients, namely,

M−1∑
k=0

h(k)rxx(l − k) = ryx(l), l = 0, 1, . . . ,M − 1

xx (l) is the autocorrelation of the sequence x(n) and ryx(l) is the cross -corr
elation of the system output with the input sequence.

parameters are obtained directly from measurement data at the input and output of
the system, without prior knowledge of the plant, we call the FIR filter model an
adaptive filter.

If our only objective were to identify the system by use of the FIR model, the
In control systems applications, however, the

system being modeled may be time variant, changing slowly with time, and our
purpose for having a model is to ultimately use it for designing a controller that
controls the plant. Furthermore, measurement noise is usually present at the output
of the plant. This noise introduces uncertainty in the measurements and corrupts
the estimates of the filter coefficients in the model. Such a scenario is illustrated in

characteristics of the plant in the presence of measurement noise at the output of the
system

identification problem.

1.2 Adaptive Channel Equalization

Figure 1.3 shows a block diagram of a digital communication system in which an
adaptive equalizer is used to compensate for the distortion caused by the transmission
medium (channel). The digital sequence of information symbols a(n) is fed to the
transmitting filter, whose output is

s(t) =
∞∑
k=0

a(k)p(t − kTs)

(1.4)

(1.5)

Adaptive Filters

In (1.4), r

By solving (1.4), we obtain the filter coefficients for the model. Since the filter

solution of (1.4) would suffice.

Figure 1.2. In this case, the adaptive filter must identify and track the time-variant

plant. The algorithms to be described in Sections 2 and 3 are applicable to this

903



Transmitter
(filter)

Receiver
(filter)

Channel
(time-variant

filter) SamplerData
sequence

Error signal

a(n)

â(n)
â(n)

d(n)

Noise

Adaptive
equalizer

Adaptive
algorithm

Decision
device

Reference
signal

� �

Figure 1.3 Application of adaptive filtering to adaptive channel equalization.

where p(t) is the impulse response of the filter at the transmitter and Ts is the time
interval between information symbols; that is, 1/Ts is the symbol rate. For purposes
of this discussion, we may assume that a(n) is a multilevel sequence that takes on
values from the set ±1,±3,±5, . . . ,±(K − 1), where K is the number of possible
symbol values.

Typically, the pulse p(t) is designed to have the characteristics illustrated in
s

nTs , n = ±1,±2, . . . . As a consequence, successive pulses transmitted sequentially
every Ts seconds do not interfere with one another when sampled at the time instants
t = nTs . Thus, a(n) = s(nTs).

The channel, which is usually well modeled as a linear filter, distorts the pulse and,
thus, causes intersymbol interference. For example, in telephone channels, filters are
used throughout the system to separate signals in different frequency ranges. These
filters cause phase and amplitude distortion.
channel distortion on the pulse p(t) as it might appear at the output of a telephone
channel. Now, we observe that the samples taken every Ts seconds are corrupted by

�5Ts 5Ts�4Ts 4Ts�3Ts 3Ts�2Ts 2Ts�Ts Ts

1

0
t

p(t)

Figure 1.4 Pulse shape for digital transmission of symbols at
a rate of 1/Ts symbols per second.

Adaptive Filters

Figure 1.4. Note that p(t) has amplitude p(0) = 1 at t = 0 and p(nT ) = 0 at t =

Figure 1.5 illustrates the effect of

904



�Ts Ts0
t

q(t)

�5Ts 5Ts�4Ts 4Ts�3Ts 3Ts�2Ts 2Ts

Figure 1.5 Effect of channel distortion on the signal pulse in Figure 1.4.

interference from several adjacent symbols. The distorted signal is also corrupted by
additive noise, which is usually wideband.

At the receiving end of the communication system, the signal is first passed
through a filter that is designed primarily to eliminate the noise outside of the fre-
quency band occupied by the signal. We may assume that this filter is a linear-phase
FIR filter that limits the bandwidth of the noise but causes negligible additional
distortion on the channel-corrupted signal.

Samples of the received signal at the output of this filter reflect the presence
of intersymbol interference and additive noise. If we ignore for the moment the
possible time variations in the channel, we may express the sampled output at the
receiver as

x(nTs) =
∞∑
k=0

a(k)q(nTs − kTs)+ w(nTs)

= a(n)q(0)+
∞∑
k=0
k �=n

a(k)q(nTs − kTs)+ w(nTs)

where w(t) represents the additive noise and q(t) represents the distorted pulse at
the output of the receiver filter.

To simplify our discussion, we assume that the sample q(0) is normalized to unity
by means of an automatic gain control (AGC) contained in the receiver. Then, the

(1.6)

Adaptive Filters

905



x(n) = a(n)+
∞∑
k=0
k �=n

a(k)q(n− k)+ w(n)

s s s is
the desired symbol at the nth sampling instant. The second term,

∞∑
k=0
k �=n

a(k)q(n− k)

constitutes the intersymbol interference due to the channel distortion, and w(n)
represents the additive noise in the system.

In general, the channel distortion effects embodied through the sampled values
q(n) are unknown at the receiver. Furthermore, the channel may vary slowly with
time such that the intersymbol interference effects are time variant. The purpose of
the adaptive equalizer is to compensate the signal for the channel distortion, so that
the resulting signal can be detected reliably. Let us assume that the equalizer is an
FIR filter with M adjustable coefficients h(n). Its output may be expressed as

â(n) =
M−1∑
k=0

h(k)x(n+D − k)

where D is some nominal delay in processing the signal through the filter and â(n)
represents an estimate of the nth information symbol. Initially, the equalizer is
trained by transmitting a known data sequence d(n). Then, the equalizer output,
â(n), is compared with d(n) and an error is generated that is used to optimize the
filter coefficients.

If we again adopt the least-squares error criterion, we select the coefficients h(k)
to minimize the quantity

�M =
N∑
n=0

[d(n)− â(n)]2 =
N∑
n=0

[
d(n)−

M−1∑
k=0

h(k)x(n+D − k)
]2

The result of the optimization is a set of linear equations of the form

M−1∑
k=0

h(k)rxx(l − k) = rdx(l −D), l = 0, 1, 2, . . . ,M − 1

where rxx(l) is the autocorrelation of the sequence x(n) and rdx(l) is the crosscorre-
lation between the desired sequence d(n) and the received sequence x(n).

(1.7)

(1.8)

(1.9)

(1.10)

Adaptive Filters

sampled signal given in (1.6) may be expressed as

where x(n) ≡ x(nT ), q(n) ≡ q(nT ), and w(n) ≡ w(nT ). The term a(n) in (1.7)

906



strated in the following two sections), in principle, we observe that these equations
result in values of the coefficients for the initial adjustment of the equalizer. After
the short training period, which usually lasts less than one second for most channels,

the possible time variations in the channel, the equalizer coefficients must continue
to be adjusted in an adaptive manner while receiving data. As illustrated in Fig-

decision device as correct, and using the decisions in place of the reference d(n) to
generate the error signal. This approach works quite well when decision errors occur
infrequently (for example, less than one decision error per hundred symbols). The
occasional decision errors cause only small misadjustments in the equalizer coeffi-

adjus
ting the equalizer coefficients.

1.3 Echo Cancellation in Data Transmission over Telephone
Channels

In the transmission of data over telephone channels, modems (modulator/demod-
ulators) are used to provide an interface between the digital data sequence and the

in which two terminals, labeled A and B, transmit data by using modems A and B to
interface to a telephone channel. As shown, a digital sequence a(n) is transmitted
from terminal A to terminal B while another digital sequence b(n) is transmitted
from terminal B to A. This simultaneous transmission in both directions is called
full-duplex transmission.

As described, the two transmitted signals may be represented as

sA(t) =
∞∑
k=0

a(k)p(t − kTs)

sB(t) =
∞∑
k=0

b(k)p(t − kTs)

Data
terminal

A

Data
terminal

B

Transmitter
A

Transmitter
B

Receiver
A

Receiver
B

Modem A Modem BFour-wire
telephone
channel

a(n)

b(n)ˆ

a(n)

b(n)

ˆ

Figure 1.6 Full-duplex data transmission over telephone channels.

(1.11)

(1.12)

Adaptive Filters

the transmitter begins to transmit the information sequence a(n). In order to track

Although the solution of ( 1.10) is obtained recursively in practice (as demon-

                           

                    

  

ure 1.3, this is usually accomplished by treating the decisions at the output of the

cients. In Sections 2 and 3, we describe the adaptive algorithms for recursively -

analog channel. Shown in Figure 1.6 is a block diagram of a communication system

907



where p(t) is a pulse as shown in Figure 1.4.

When a subscriber leases a private line from a telephone company for the purpose
of transmitting data between terminals A and B, the telephone line provided is a four-
wire line, which is equivalent to having two dedicated telephone (two-wire) channels,
one (pair of wires) for transmitting data in one direction and one (pair of wires) for
receiving data from the other direction. In such a case the two transmission paths are
isolated and, consequently, there is no “crosstalk” or mutual interference between the
two signal paths. Channel distortion is compensated by use of an adaptive equalizer,
as previously described, at the receiver of each modem.

four-wire telephone channel. If the volume of traffic is high and the telephone chan-
nel is used either continuously or frequently, as in banking transactions systems or

Otherwise, it will not be.

An alternative solution for low-volume, infrequent transmission of data is to use
the dial-up switched telephone network. In this case, the local communication link
between the subscriber and the local central telephone office is a two-wire line, called
the local loop. At the central office, the subscriber two-wire line is connected to the
main four-wire telephone channels that interconnect different central offices, called
trunk lines, by a device called a hybrid. By using transformer coupling, the hybrid is
tuned to provide isolation between the transmission and reception channels in full-
duplex operation. However, due to impedance mismatch between the hybrid and
the telephone channel, the level of isolation is often insufficient and, consequently,
some of the signal on the transmitter side leaks back and corrupts the signal on the
receiver side, causing an “echo” that is often heard in voice communications over
telephone channels.

To mitigate the echoes in voice transmissions, the telephone companies employ a
device called an echo suppressor. In data transmission, the solution is to use an echo
canceller within each modem. The echo cancellers are implemented as adaptive
filters with automatically adjustable coefficients, just as in the case of transversal
equalizers.

With the use of hybrids to couple a two-wire to a four-wire channel, and echo
cancellers at each modem to estimate and subtract the echoes, the data communica-

A hybrid is needed at each modem to isolate the transmitter from the receiver and to
couple to the two-wire local loop. Hybrid A is physically located at the central office
of subscriber A while hybrid B is located at the central office to which subscriber
B is connected. The two central offices are connected by a four-wire line, one pair
used for transmission from A to B and the other pair used for transmission in the
reverse direction, from B to A. An echo at terminal A due to the hybrid A is called
a near-end echo, while an echo at terminal A due to the hybrid B is termed a far-end
echo. Both types of echoes are usually present in data transmission and must be
removed by the echo canceller.

Adaptive Filters

The major problem with the system shown in Figure 1.6 is the cost of leasing a

airline reservation systems, the system pictured in Figure 1.6 may be cost effective.

tion system for the dial-up switched network takes the form shown in Figure 1.7.

908



Transmitter
A

Receiver
A

Echo
canceller

Adaptive
algorithm

Transmitter
B

Receiver
B

Echo
canceller

Adaptive
algorithm

Hybrid Hybrid
Hybrid

A
Hybrid

B

�

�

�
�

Modem A

Telephone channel

Modem B

Input
data

Output
data

Input
data

Output
data

Local
loop

Local
loop

Figure 1.7 Block diagram model of a digital communication system that uses echo cancellers in the modems.

A
dap

tive
Filters

909



Suppose we neglect the channel distortion for purposes of this discussion, and
deal with the echoes only. The signal received at modem A may be expressed as

sRA(t) = A1sB(t)+A2sA(t − d1)+ A3sA(t − d2)
where sB(t) is the desired signal to be demodulated at modem A; sA(t−d1) is the near-
end echo due to hybrid A, sA(t−d2) is the far-end echo due to hybrid B;Ai, i = 1, 2, 3,
are the corresponding amplitudes of the three signal components; and d1 and d2 are
the delays associated with the echo components. A further disturbance that corrupts
the received signal is additive noise, so that the received signal at modem A is

rA(t) = sRA(t)+ w(t)
where w(t) represents the additive noise process.

The adaptive echo canceller attempts to estimate adaptively the two echo com-
ponents. If its coefficients are h(n), n = 0, 1, . . . ,M − 1, its output is

ŝA(n) =
M−1∑
k=0

h(k)a(n− k)

which is an estimate of the echo signal components. This estimate is subtracted from
the sampled received signal, and the resulting error signal can be minimized in the
least-squares sense to optimally adjust the coefficients of the echo canceller. There
are several possible configurations for placement of the echo canceller in the mo-
dem, and for forming the corresponding error signal.
configuration, in which the canceller output is subtracted from the sampled output
of the receiver filter with input rA(t).
tion, in which the echo canceller is generating samples at the Nyquist rate instead of
the symbol rate; in this case the error signal used to adjust the coefficients is simply
the difference between rA(n), the sampled received signal, and the canceller out-

adaptive equalizer.

Transmitter
filter

Receiver
filter

Decision
device

Symbol rate
sampler

Echo
canceller

Adaptive
algorithm

Hybrid

�
�

��

Input data a(n)

b(n)

rA(t)

ˆ

Figure 1.8 Symbol rate echo canceller.

(1.13)

(1.14)

(1.15)

Adaptive Filters

Figure 1.8 illustrates one

Figure 1.9 illustrates a second configura-

put. Finally, Figure 1.10 illustrates the canceller operating in combination with an

910



Transmitter
filter

Receiver
filter

Decision
device

Nyquist rate
sampler

Adaptive
algorithm

Hybrid

a(n)

b(n) ��

Echo
canceller

Error signal

ˆ

Figure 1.9 Nyquist rate echo canceller.

Application of the least-squares criterion in any of the configurations shown in
echo

canceller. The reader is encouraged to derive the equations corresponding to the
three configurations.

1.4 Suppression of Narrowband Interference in a Wideband
Signal

We now discuss a problem that arises in practice, especially in signal detection and
in digital communications. Let us assume that we have a signal sequence v(n) that

Transmitter
filter

Receiver
filter

Decision
device

Hybrid
Adaptive
algorithm

Adaptive
algorithm

a(n)

b(n)

�

�

�

�

Echo
canceller

Error signal

SamplerEqualizer

Input data

ˆ

Figure 1.10 Modem with adaptive equalizer and echo canceller.

Adaptive Filters

Figures 1.8–1.10 leads to a set of linear equations for the coefficients of the

911



Figure 1.11
Strong narrowband
interference X(f ) in a
wideband signal W(f ).

|V( f )| � |X( f )| � |W( f )|

|W( f )|

|X( f )|

0 fw
f

consists of a desired wideband signal sequence w(n) corrupted by an additive nar-
rowband interference sequence x(n). The two sequences are uncorrelated. These
sequences result from sampling an analog signal v(t) at the Nyquist rate (or faster)

w(n) and x(n). Usually, the interference |X(f )| is much larger than |W(f )| within
the narrow frequency band that it occupies.

In digital communications and signal detection problems that fit the above model,
the desired signal sequence w(n) is often a spread-spectrum signal, while the narrow-
band interference represents a signal from another user of the frequency band, or
intentional interference from a jammer who is trying to disrupt the communications
or detection system.

Our objective from a filtering viewpoint is to employ a filter that suppresses the
narrowband interference. In effect, such a filter will have a notch in the frequency
band occupied by |X(f )|, and in practice, the band occupied by |X(f )| is unknown.
Moreover, if the interference is nonstationary, its frequency band occupancy may
vary with time. Hence, an adaptive filter is desired.

From another viewpoint, the narrowband characteristics of the interference al-
low us to estimate x(n) from past samples of the sequence v(n) and to subtract the
estimate from v(n). Since the bandwidth of x(n) is narrow compared to the band-
width of the sequence w(n), the samples x(n) are highly correlated due to the high
sampling rate. On the other hand, the samples w(n) are not highly correlated, since
the samples are taken at the Nyquist rate of w(n). By exploiting the high correla-
tion between x(n) and past samples of the sequence v(n), it is possible to obtain an
estimate of x(n), which can be subtracted from v(n).

layed by D samples, whereD is selected sufficiently large so that the wideband signal
components w(n) and w(n − D) contained in v(n) and v(n − D), respectively, are
uncorrelated. Usually, a choice of D = 1 or 2 is adequate. The delayed signal se-
quence v(n−D) is passed through an FIR filter, which is best characterized as a linear
predictor of the value x(n) based on M samples v(n−D − k), k = 0, 1, . . . ,M − 1.
The output of the linear predictor is

Adaptive Filters

of the wideband signal w(t). Figure 1.11 illustrates the spectral characteristics of

The general configuration is illustrated in Figure 1.12. The signal v(n) is de-

912



z�D
FIR

linear
predictor

Adaptive
algorithm

�(n) � w(n) � x(n)

�(n�D)

e(n) � ŵ(n)

x̂ (n) Error signal

Decorrelation
delay

�

�

(a)

(b)

�(n�D)

x̂ (n)

z�1 z�1 z
�1 z�1

h(0) h(1) h(2)
h(M � 1)

�

Figure 1.12 Adaptive filter for estimating and suppressing
a narrowband interference in a wideband signal.

x̂(n) =
M−1∑
k=0

h(k)v(n−D − k)

This predicted value of x(n) is subtracted from v(n) to yield an estimate of w(n), as

well the narrowband interference is suppressed. It is also apparent that the delay D
must be kept as small as possible in order to obtain a good estimate of x(n), but must
be sufficiently large so that w(n) and w(n−D) are uncorrelated.

Let us define the error sequence

e(n) = v(n)− x̂(n)

= v(n)−
M−1∑
k=0

h(k)v(n−D − k)

If we apply the least-squares criterion to optimally select the prediction coefficients,

(1.16)

(1.17)

Adaptive Filters

illustrated in Figure 1.12. Clearly, the quality of the estimate x̂(n) determines how

913



we obtain the set of linear equations

M−1∑
k=0

h(k)rvv(l − k) = rvv(l +D), l = 0, 1, . . . ,M − 1

where rvv(l) is the autocorrelation sequence of v(n). Note, however, that the right-

rvv(l +D) =
N∑
n=0

v(n)v(n− l −D)

=
N∑
n=0

[w(n)+ x(n)][w(n− l −D)+ x(n− l −D)]

= rww(l +D)+ rxx(l +D)+ rwx(l +D)+ rxw(l +D)

The ex -pect
ed value of rww(l +D) is

E[rww(l +D)] = 0, l = 0, 1, . . . ,M − 1

because w(n) is wideband and D is large enough that w(n) and w(n−D) are uncor-
related. Also,

E[rxw(l +D)] = E[rwx(l +D)] = 0
by assumption. Finally,

E[rxx(l +D)] = γxx(l +D)
Therefore, the expected value of rvv(l + D) is simply the statistical autocorrelation
of the narrowband signal x(n). Furthermore, if the wideband signal is weak relative
to the interference, the autocorrelation rvv
proximately rxx(l). The major influence of w(n) is to the diagonal elements of rvv(l).
Consequently, the values of the filter coefficients determined from the linear equa-

x(n).

filter with coefficients

h′(k) =
{ 1, k = 0
−h(k −D), k = D,D + 1, . . . , D +M − 1
0, otherwise

and a frequency response

H(ω) =
M−1∑
k=0

h′(k +D)e−jωk

(1.18)

(1.19)

(1.20)

(1.21)

(1.22)

(1.23)

(1.24)

Adaptive Filters

hand side of (1.18) may be expressed as

The correlations in (1.19) are time-average correlation sequences.

(l) in the left-hand side of (1.18) is ap-

tions in (1.18) are a function of the statistical characteristics of the interference

The overall filter structure in Figure 1.12 is an adaptive FIR prediction-error

914



0.00

�5.00

�10.00

�15.00

�20.00

�25.00

�30.00

�35.00
0.00 0.06 0.12 0.19 0.25 0.31 0.37 0.44 0.50

f

Frequency (cycles/sample)

Fi
lte

r 
re

sp
on

se
 (

dB
)

Frequency response characteristics of an adaptive notch filter.

This overall filter acts as a notch filter for the interference. For example, Fig-

with M = 15 coefficients, which attempts to suppress a narrowband interference that
occupies 20 percent of the frequency band of a desired spread-spectrum signal se-
quence. The data were generated pseudorandomly by adding a narrowband interfer-
ence consisting of 100 randomly phased, equal-amplitude sinusoids to a pseudonoise
spread-spectrum signal. The coefficients of the filter were obtained by solving the

vv(l) was obtained from the
data. We observe that the overall interference suppression filter has the characteris-
tics of a notch filter. The depth of the notch depends on the power of the interference
relative to the wideband signal. The stronger the interference, the deeper the notch.

the predictor coefficients continuously, in order to track a nonstationary narrowband
interference signal.

1.5 Adaptive Line Enhancer

In the preceding example, the adaptive linear predictor was used to estimate the
narrowband interference for the purpose of suppressing the interference from the
input sequence v(n). An adaptive line enhancer (ALE) has the same configuration

different.
In the adaptive line enhancer, x(n) is the desired signal and w(n) represents

Adaptive Filters

Figure 1.13

ure 1.13 illustrates the magnitude of the frequency response of an adaptive filter

equations in (1.18), with D = 1, where the correlation r

The algorithms presented in Sections 2 and 3 are appropriate for estimating

as the interference suppression filter in Figure 1.12, except that the objective is

915



a wideband noise component that masks x(n). The desired signal x(n) is either
a spectral line or a relatively narrowband signal. The linear predictor shown in

and
provides an estimate of the narrowband signal x(n). It is apparent that the ALE(i.e.,
the FIR prediction filter) is a self-tuning filter that has a peak in its frequency res
ponse at the frequency of the sinusoid or, equivalently, in the frequency band of the
narrowband signal x(n). By having a narrow bandwidth, the noise w(n) outside of
the band is suppressed and, thus, the spectral line is enhanced in amplitude relative to
the noise power in w(n). This explains why the FIR predictor is called an ALE. Its

1.6 Adaptive Noise Cancelling

Echo cancellation, the suppression of narrowband interference in a wideband signal,
and the ALE are related to another form of adaptive filtering called adaptive noise

The primary input signal consists of a desired signal sequence x(n) corrupted
by an additive noise sequence w1(n) and an additive interference (noise) w2(n).
The additive interference (noise) is also observable after it has been filtered by some
unknown linear system that yields v2(n) and is further corrupted by an additive noise
sequence w3(n). Thus, we have available a secondary signal sequence, which may
be expressed as v(n) = v2(n) + w3(n). The sequences w1(n), w2(n), and w3(n) are
assumed to be mutually uncorrelated and zero mean.

ference sequence w2(n) from the secondary signal v(n) and subtract the estimate
w2(n) from the primary signal. The output sequence, which represents an estimate
of the desired signal x(n), is the error signal

e(n) = y(n)− ŵ2(n)

= y(n)−
M−1∑
k=0

h(k)v(n− k)

Adaptive
algorithm

Unknown
linear

system H(z)

Adaptive
FIR
filter

x(n) � w1(n)

w2(n)

w3(n)

ŵ2(n)

Observed signal y(n)

Observed signal �(n)

Output

Error
signal

�2(n)

�

�

Figure 1.14 Example of an adaptive noise-cancelling system.

(1.25)

Adaptive Filters

Figure 1.12(b) operates in exactly the same fashion as that in Figure 1.12(a),

coefficients are determined by the solution of (1.18).

cancelling. A model for the adaptive noise canceller is illustrated in Figure 1.14.

As shown in Figure 1.14, an adaptive FIR filter is used to estimate the inter-

916



This error sequence is used to adaptively adjust the coefficients of the FIR filter.
If the least-squares criterion is used to determine the filter coefficients, the result

of the optimization is the set of linear equations

M−1∑
k=0

h(k)rvv(l − k) = ryv(l), l = 0, 1, . . . ,M − 1

where rvv(l) is the sample (time-average) autocorrelation of the sequence v(n) and
ryv(l) is the sample crosscorrelation of the sequences y(n) and v(n). Clearly, the
noise cancelling problem is similar to the last three adaptive filtering applications
described above.

1.7 Linear Predictive Coding of Speech Signals

Various methods have been developed over the past four decades for digital encoding
of speech signals. In the telephone system, for example, two commonly used meth-
ods for speech encoding are pulse code modulation (PCM) and differential PCM
(DPCM). These are examples of waveform coding methods. Other waveform cod-
ing methods have also been developed, such as delta modulation (DM) and adaptive
DPCM.

Since the digital speech signal is ultimately transmitted from the source to a des-
tination, a primary objective in devising speech encoders is to minimize the number
of bits required to represent the speech signal, while maintaining speech intelligibil-
ity. This objective has led to the development of a class of low bit-rate (10,000 bits
per second and below) speech encoding methods, which are based on constructing a
model of the speech source and transmitting the model parameters. Adaptive filter-
ing finds application in these model-based speech coding systems. We describe one
very effective method called linear predictive coding (LPC).

In LPC, the vocal tract is modeled as a linear all-pole filter having the system
function

H(z) = G
1−∑p

k=1 akz
−k

where p is the number of poles, G is the filter gain, and ak are the parameters that
determine the poles. There are two mutually exclusive excitation functions, used to
model voiced and unvoiced speech sounds. On a short-time basis, voiced speech is
periodic with a fundamental frequency F0 , or a pitch period 1/F0 , which depends on
the speaker. Thus, voiced speech is generated by exciting the all-pole filter model by
a periodic impulse train with a period equal to the desired pitch period. Unvoiced
speech sounds are generated by exciting the all-pole filter model by the output of a

Given a short-time segment of a speech signal, the speech encoder at the trans-
mitter must determine the proper excitation function, the pitch period for voiced
speech, the gain parameter G, and the coefficients {ak}. A block diagram that il-

The parameters of
the model are determined adaptively from the data. Then, the speech samples are

(1.26)

(1.27)

Adaptive Filters

random-noise generator. This model is shown in Figure 1.15.

lustrates the source encoding system is given in Figure 1.16.

917



White
noise

generator

Periodic
impulse

generator

All-pole
filter

Voiced and
unvoiced switch Speech

signal

Figure 1.15 Block diagram model for the generation of a speech signal.

synthesized by using the model, and an error signal sequence is generated (as shown

sequence. The error signal and the model parameters are encoded into a binary
sequence and transmitted to the destination. At the receiver, the speech signal is
synthesized from the model and the error signal.

The parameters of the all-pole filter model are easily determined from the speech
samples by means of linear prediction. To be specific, consider the system shown in

The output of the FIR
filter is

x̂(n) =
p∑
k=1

akx(n− k)

and the corresponding error between the observed sample x(n) and the estimate
x̂(n) is

e(n) = x(n)−
p∑
k=1

akx(n− k)

�

�

Determine
parameters
of model

and
excitation

E
n
c
o
d
e
r

All-pole
model

Channel

Sampled

output from
speech
source

Excitation

e(n)x(n)

x̂(n)

x̂(n)

Parameter {ak, G}

Figure 1.16 Source encoder for a speech signal.

(1.28)

(1.29)

Adaptive Filters

in Figure 1.16) by taking the difference between the actual and the synthesized

Figure 1.17 and assume that we have N signal samples.

918



Adaptive
algorithm

Adaptive
FIR

predictor
z�1

Speech samples
Error signalx(n)

x̂(n)

�

�

Figure 1.17 Estimation of pole parameters in LPC.

By applying the least-squares criterion, we can determine the model parameters ak .
The result of this optimization is a set of linear equations

p∑
k=1

akrxx(l − k) = rxx(l), l = 1, 2, . . . , p

where rxx(l) is the time-average autocorrelation of the sequence x(n). The gain
parameter for the filter can be obtained by noting that its input–output equation is

x(n) =
p∑
k=1

akx(n− k)+Gv(n)

where v(n) is the input sequence. Clearly,

Gv(n) = x(n)−
p∑
k=1

akx(n− k)

= e(n)

Then,

G2
N−1∑
n=0

v2(n) =
N−1∑
n=0

e2(n)

If the input excitation is normalized to unit energy by design, then

G2 =
N−1∑
n=0

e2(n)

= rxx(0)−
p∑
k=1

akrxx(k)

Thus, G2 is set equal to the residual energy resulting from the least-squares opti-
mization.

(1.30)

(1.31)

(1.32)

(1.33)

Adaptive Filters

919



In this development, we have described the use of linear prediction to adaptively
determine the pole parameters and the gain of an all-pole filter model for speech gen-
eration. In practice, due to the nonstationary character of speech signals, this model
is applied to short-time segments (10 to 20 milliseconds) of a speech signal. Usually,
a new set of parameters is determined for each short-time segment. However, it is
often advantageous to use the model parameters measured from previous segments
to smooth out sharp discontinuities that usually exist in estimates of model param-
eters obtained from segment to segment. Although our discussion was totally in
terms of the FIR filter structure, we should mention that speech synthesis is usually
performed by using the FIR lattice structure and the reflection coefficients Ki . Since
the dynamic range of the Ki is significantly smaller than that of the ak , the reflection
coefficients require fewer bits to represent them. Hence, the Ki are transmitted over
the channel. Consequently, it is natural to synthesize the speech at the destination
using the all-pole lattice structure.

In our treatment of LPC for speech coding, we have not considered algorithms
for the estimation of the excitation and the pitch period. A discussion of appropriate
algorithms for these parameters of the model would take us too far afield and, hence,
is omitted. The interested reader is referred to Rabiner and Schafer (1978) and
Deller, Hansen and Proakis (2000) for a detailed treatment of speech analysis and
synthesis methods.

1.8 Adaptive Arrays

In the previous examples, we considered adaptive filtering performed on a single data
sequence. However, adaptive filtering has also been widely applied to multiple data
sequences that result from antenna, hydrophone, and seismometer arrays, where
the sensors (antennas, hydrophones, or seismometers) are arranged in some spatial
configuration. Each element of the array of sensors provides a signal sequence. By
properly combining the signals from the various sensors, it is possible to change
the directivity pattern of the array. For example, consider a linear antenna array

If the signals are simply
linearly summed, we obtain the sequence

x(n) =
5∑
k=1

xk(n)

Now,
suppose that an interference signal is received from a direction corresponding to one
of the sidelobes in the array. By properly weighting the sequences xk(n) prior to
combining, it is possible to alter the sidelobe pattern such that the array contains

obtain

x(n) =
5∑
k=1

hkxk(n)

where the hk are the weights.

(1.34)

(1.35)

Adaptive Filters

consisting of five elements, as shown in Figure 1.18(a).

which results in the antenna directivity pattern shown in Figure 1.18(a).

Thus, wea null in the direction of the interference, as shown in Figure 1.18(b).

920



Look direction

x1(n)

x2(n)

x3(n)

x4(n)

x5(n)

To combiner

Look direction

x1(n)

x2(n)

x3(n)

x4(n)

x5(n)

To combiner

Interference

Main lobe

Side lobes

(a)

(b)

Interference

Figure 1.18 Linear antenna array: (a) linear antenna array with antenna pattern;
(b) linear antenna array with a null placed in the direction of the interference.

We may also change or steer the direction of the main antenna lobe by simply
introducing delays in the output of the sensor signals prior to combining. Hence,
from K sensors we have a combined signal of the form

x(n) =
K∑
k=1

hkxk(n− nk) (1.36)

Adaptive Filters

921



where the hk are the weights and nk corresponds to an nk -sample delay in the signal
x(n). The choice of weights may be used to place nulls in specific directions.

More generally, we may simply filter each sequence prior to combining. In such
a case, the output sequence has the general form

y(n) =
K∑
k=1

yk(n)

=
K∑
k=1

M−1∑
l=0

hk(l)xk(n− nk − l)

where hk(l) is the impulse response of the filter for processing the kth sensor output
and the nk are the delays that steer the beam pattern.

selecting the weights hk or the impulse responses hk(l). The more powerful recur-

multisensor (multichannel) data problem.

2 Adaptive Direct-Form FIR Filters—The LMS Algorithm

From the examples of the previous section, we observed that there is a common
framework in all the adaptive filtering applications. The least-squares criterion that
we have adopted leads to a set of linear equations for the filter coefficients, which
may be expressed as

M−1∑
k=0

h(k)rxx(l − k) = rdx(l +D), l = 0, 1, 2, . . . ,M − 1

where rxx(l) is the autocorrelation of the sequence x(n) and rdx(l) is the crosscorre-
lation of the sequences d(n) and x(n). The delay parameter D is zero in some cases
and nonzero in others.

We observe that the autocorrelation rxx(l) and the crosscorrelation rdx(l) are
obtained from the data and, hence, represent estimates of the true (statistical) auto-
correlation and crosscorrelation sequences. As a result, the coefficients h(k) obtained

pends on the length of the data record that is available for estimating rxx(l) and
rdx(l). This is one problem that must be considered in the implementation of an
adaptive filter.

A second problem that must be considered is that the underlying random pro-
cess x(n) is usually nonstationary. For example, in channel equalization, the fre-
quency response characteristics of the channel may vary with time. As a conse-
quence, the statistical autocorrelation and crosscorrelation sequences—and, hence,
their estimates—vary with time. This implies that the coefficients of the adaptive
filter must change with time to incorporate the time-variant statistical characteristics

(1.37)

(2.1)

Adaptive Filters

The LMS algorithm described in Section 2.2 is frequently used in adaptively

sive least-squares algorithms described in Section 3 can also be applied to the

from (2.1) are estimates of the true coefficients. The quality of the estimates de-

922



of the signal into the filter. This also implies that the quality of the estimates cannot
be made arbitrarily high by simply increasing the number of signal samples used in
the estimation of the autocorrelation and crosscorrelation sequences.

There are several ways by which the coefficients of the adaptive filter can be
varied with time to track the time-variant statistical characteristics of the signal. The
most popular method is to adapt the filter recursively on a sample-by-sample basis, as
each new signal sample is received. A second approach is to estimate rxx(l) and rdx(l)
on a block-by-block basis, with no attempt to maintain continuity in the values of the
filter coefficients from one block of data to another. In such a scheme, the block size
must be relatively small, encompassing a time interval that is short compared to the
time interval over which the statistical characteristics of the data change significantly.
In addition to this block processing method, other block processing schemes can be
devised that incorporate some block-to-block continuity in the filter coefficients.

In our treatment of adaptive filtering algorithms, we consider only time-recursive
algorithms that update the filter coefficients on a sample-by-sample basis. In partic-
ular, we consider two types of algorithms, the LMS algorithm, which is based on a
gradient-type search for tracking the time-variant signal characteristics, and the class
of recursive least-squares algorithms, which are significantly more complex than the
LMS algorithm, but which provide faster convergence to changes in signal statistics.

2.1 Minimum Mean-Square-Error Criterion

The LMS algorithm that is described in the following subsection is most easily ob-
tained by formulating the optimization of the FIR filter coefficients as an estimation
problem based on the minimization of the mean-square error. Let us assume that
we have available the (possibly complex-valued) data sequence x(n), which consists
of samples from a stationary random process with autocorrelation sequence

γxx(m) = E[x(n)x∗(n−m)]

From these samples, we form an estimate of the desired sequence d(n) by passing

The filter output may be expressed as

d̂(n) =
M−1∑
k=0

h(k)x(n− k)

where d̂(n) represents an estimate of d(n). The estimation error is defined as

e(n) = d(n)− d̂(n)

= d(n)−
M−1∑
k=0

h(k)x(n− k)

(2.2)

the observed data x(n) through an FIR filter with coefficients h(n), 0 ≤ n ≤ M − 1.

(2.3)

(2.4)

Adaptive Filters

923



The mean-square error as a function of the filter coefficients is

�M = E[|e(n)|2]

= E


∣∣∣∣∣d(n)−

M−1∑
k=0

h(k)x(n− k)
∣∣∣∣∣
2

= E
{
|d(n)|2 − 2Re

[
M−1∑
k=0

h∗(l)d(n)x∗(n− l)
]
+
M−1∑
k=0

M−1∑
l=0

h∗(l)h(k)x∗(n− l)x(n− k)
}

= σ 2d − 2Re
[
M−1∑
l=0

h∗(l)γdx(l)

]
+
M−1∑
l=0

M−1∑
k=0

h∗(l)h(k)γxx(l − k)

where, by definition, σ 2d = E[|d(n)|2].
We observe that the MSE is a quadratic function of the filter coefficients. Con-

sequently, the minimization of �M with respect to the coefficients leads to the set of
M linear equations,

M−1∑
k=0

h(k)γxx(l − k) = γdx(l), l = 0, 1, . . . ,M − 1

and
crosscorrelation are employed.

coeffi
cients.

�MhM = γd
where hM denotes the vector of coefficients, �M is an M ×M (Hermitian) Toeplitz
matrix with elements �lk = γxx(l − k), and γd is an M × 1 crosscorrelation vector
with elements γdx(l), l = 0, 1, . . . ,M − 1. The complex-conjugate of hM is denoted
as h∗M and the transpose as h

t
M. The solution for the optimum filter coefficients is

hopt = �−1M γd
and the resulting minimum MSE achieved with the optimum coefficients given by

�M min = σ 2d −
M−1∑
k=0

hopt(k)γ
∗
dx(k)

= σ 2d − γHd �−1M γd

(2.5)

(2.6)

(2.7)

(2.8)

(2.9)

Adaptive Filters

The filter with coefficients obtained from (2.6), which is the Wiener–Hopf equation

If we compare (2.6) with (2.1), it is apparent that these equations are similar in
form. In (2.1), we use estimates of the autocorrelation and crosscorrelation to dete
rmine the filter coefficients, whereas in (2.6) the statistical autocorrelation

Hence, (2.6) yields the optimum (Wiener) filter
coefficients in the MSE sense, whereas (2.1) yields estimates of the optimum

The equations in (2.6) may be expressed in matrix form as

(2.8) is

-

-

is called the Wiener filter.

924



where the exponent H denotes the conjugate transpose.

E[e(n)d̂∗(n)] = 0

E

[
M−1∑
k=0

h(k)e(n)x∗(n− k)
]
=

M−1∑
k=0

h(k)E[e(n)x∗(n− k)] = 0

or, equivalently,

E[e(n)x∗(n− l)] = 0, l = 0, 1, . . . ,M − 1

Since d̂(n) is orthogonal to e(n), the residual (minimum) mean-square error is

�M min = E[e(n)d∗(n)]

= E[|d(n)|2]−
M−1∑
k=0

hopt(k)γ
∗
dx(k)

the Levinson-Durbin algorithm. However, we shall consider the use of a gradient
method for solving for hopt , iteratively. This development leads to the LMS algorithm
for adaptive filtering.

2.2 The LMS Algorithm

There are various numerical methods that can be used to solve the set of linear

imum of a function of several variables. In our problem, the performance index is the

this function has a unique minimum, which we shall determine by an iterative search.
For the moment, let us assume that the autocorrelation matrix �M and the cross-

correlation vector γd are known. Hence, �M is a known function of the coefficients
h(n), 0 ≤ n ≤ M − 1. Algorithms for recursively computing the filter coefficients
and, thus, searching for the minimum of �M , have the form

hM(n+ 1) = hM(n)+ 12�(n)S(n),

(2.10)

(2.11)

(2.12)

(2.13)

Recall that the set of linear equations in (2.6) can also be obtained by invoking 
the orthogonality principle in mean-square estimation. According to the orthogo-
nality principle, the mean-square estimation error is minimized when the error  
is orthogonal, in the statistical sense, to the estimate  that is,

e(n)

d̂(n),

Adaptive Filters

But the condition in (2.10) implies that

If we substitute for e(n) in (2.11) using the expression for e(n) given in (2.4), and
perform the expectation operation, we obtain the equations given in (2.6).

which is the result given in (2.9).
The optimum filter coefficients given by (2.8) can be solved efficiently by using

MSE given by (2.5), which is a quadratic function of the filter coefficients. Hence,

equations given by (2.6) or (2.7) for the optimum FIR filter coefficients. In the

n = 0, 1, . . .

following, we consider recursive methods that have been devised for finding the min-

925



where hM(n) is the vector of filter coefficients at the nth iteration, �(n) is a step
size at the nth iteration, and S(n) is a direction vector for the nth iteration. The
initial vector hM(0) is chosen arbitrarily. In this treatment we exclude methods that
require the computations of �−1M , such as Newton’s method, and consider only search
methods based on the use of gradient vectors.

The simplest method for finding the minimum of �M recursively is based on a
steepest-descent search (see Murray (1972)). In the method of steepest descent, the
direction vector S(n) = −g(n), where g(n) is the gradient vector at the nth iteration,
defined as

g(n) = d�M(n)
dhM(n)

= 2[�MhM(n)− γd ], n = 0, 1, 2, . . .

Hence, we compute the gradient vector at each iteration and change the values of
hM(n) in a direction opposite the gradient. Thus, the recursive algorithm based on
the method of steepest descent is

hM(n+ 1) = hM(n)− 12�(n)g(n)

or, equivalently,

hM(n+ 1) = [I−�(n)�M ]hM(n)+�(n)γd
We state without proof that the algorithm leads to the convergence of hM(n) to hopt
in the limit as n → ∞, provided that the sequence of step sizes �(n) is absolutely
summable, with �(n)→ 0 as n→∞. It follows that as n→∞, g(n)→ 0.

Other candidate algorithms that provide faster convergence are the conjugate-
gradient algorithm and the Fletcher–Powell algorithm. In the conjugate-gradient
algorithm, the direction vectors are given as

S(n) = β(n− 1)S(n− 1)− g(n)

where β(n) is a scalar function of the gradient vectors (see Beckman (1960)). In the
Fletcher–Powell algorithm, the direction vectors are given as

S(n) = −H(n)g(n)

where H(n) is anM×M positive definite matrix, computed iteratively, that converges
to the inverse of �M (see Fletcher and Powell (1963)). Clearly, the three algorithms
differ in the manner in which the direction vectors are computed.

These three algorithms are appropriate when �M and γd are known. However,
this is not the case in adaptive filtering applications, as we have previously indicated.
In the absence of knowledge of �M and γd , we may substitute estimates Ŝ(n) of the
direction vectors in place of the actual vectors S(n). We consider this approach for
the steepest-descent algorithm.

(2.14)

(2.15)

(2.16)

(2.17)

(2.18)

Adaptive Filters

926



E[e(n)X∗M(n)] = γd − �MhM(n)

where XM(n) is the vector with elements x(n − l), l = 0, 1, . . . ,M − 1. Therefore,
the gradient vector is simply

g(n) = −2E[e(n)X∗M(n)]

Clearly, the gradient vector g(n) = 0 when the error is orthogonal to the data in the
estimate d̂(n).

An unbiased estimate of the gradient vector at the nth iteration is simply ob-

ĝ(n) = −2e(n)X∗M(n)
where e(n) = d(n) − d̂(n), and XM(n) is the set of M signal samples in the filter at
the nth iteration. Thus, with ĝ(n) substituted for g(n), we have the algorithm

hM(n+ 1) = hM(n)+�(n)e(n)X∗M(n)

variable step size.
It has become common practice in adaptive filtering to use a fixed-step-size

algorithm for two reasons. The first is that a fixed-step-size algorithm is easily im-
plemented in either hardware or software. The second is that a fixed step size is ap-
propriate for tracking time-variant signal statistics, whereas if �(n)→ 0 as n→∞,

to the algorithm
hM(n+ 1) = hM(n)+�e(n)X∗M(n)

where � is now the fixed step size. This algorithm was first proposed by Widrow and
Hoff (1960) and is now widely known as the LMS (least-mean-squares) algorithm.
Clearly, it is a stochastic-gradient algorithm.

The LMS algorithm is relatively simple to implement. For this reason, it has been
widely used in many adaptive filtering applications. Its properties and limitations
have also been thoroughly investigated. In the following section, we provide a brief
treatment of its important properties concerning convergence, stability, and the noise
resulting from the use of estimates of the gradient vectors. Subsequently, we compare
its properties with the more complex recursive least-squares algorithms.

2.3 Related Stochastic Gradient Algorithms

Several variations of the basic LMS algorithm have been proposed in the literature
and implemented in adaptive filtering applications. One variation is obtained if we

(2.19)

(2.20)

(2.21)

(2.22)

(2.23)

Adaptive Filters

First, we note that the gradient vector given by (2.14) may also be expressed
in terms of the orthogonality conditions given by (2.11). In fact, the conditions in
(2.11) are equivalent to the expression

tained from (2.20) as

This is called a stochastic-gradient-descent algorithm. As given by (2.22), it has a

adaptation to signal variations cannot occur. For these reasons, (2.22) is modified

927



average the gradient vectors over several iterations prior to making adjustments of
the filter coefficients. For example, the average over K gradient vectors is

ĝ(nK) = − 2
K

K−1∑
k=0

e(nK + k)X∗M(nK + k)

and the corresponding recursive equation for updating the filter coefficients once
every K iterations is

hM((n+ 1)K) = hM(nK)− 12�ĝ(nK)

estimate of the gradient vector, as shown by Gardner (1984).
An alternative approach is to filter the gradient vectors by a lowpass filter and

use the output of the filter as an estimate of the gradient vector. For example, a
simple lowpass filter for the gradients yields as an output

Ŝ(n) = βŜ(n− 1)− ĝ(n), S(0) = −ĝ(0)

where the choice of 0 ≤ β < 1 determines the bandwidth of the lowpass filter.
When β is close to unity, the filter bandwidth is small and the effective averaging
is performed over many gradient vectors. On the other hand, when β is small, the
lowpass filter has a large bandwidth and, hence, it provides little averaging of the
gradient vectors.
ĝ(n), we obtain the filtered version of the LMS algorithm given by

hM(n+ 1) = hM(n)+ 12�Ŝ(n)

An analysis of the filtered-gradient LMS algorithm is given in Proakis (1974).

by using sign information contained in the error signal sequence e(n) and/or in the
components of the signal vector XM(n). Hence, the three possible variations are

hM(n+ 1) = hM(n)+�csgn[e(n)]X∗M(n)
hM(n+ 1) = hM(n)+�e(n)csgn[X∗M(n)]
hM(n+ 1) = hM(n)+�csgn[e(n)]csgn[X∗M(n)]

where csgn[x] is the complex sign function defined as

csgn[x] =




1+ j, if Re(x) > 0 and Im(x) > 0
1− j, if Re(x) > 0 and Im(x) < 0
−1+ j, if Re(x) < 0 and Im(x) > 0
−1− j, if Re(x) < 0 and Im(x) < 0

(2.24)

(2.25)

(2.26)

(2.27)

(2.28)

(2.29)

(2.30)

Adaptive Filters

In effect, the averaging operation performed in (2.24) reduces the noise in the

With the filtered gradient vectors given by (2.26) in place of

Three other variations of the basic LMS algorithm given in (2.23) are obtained

928



and csgn[X] denotes the complex sign function applied to each element of the vector
X. These three variations of the LMS algorithm may be called reduced complexity

The
price paid for the reduction in computational complexity is a slower convergence of
the filter coefficients to their optimum values.

Another version of the LMS algorithms, called a normalized LMS(NLMS) al-
gorithm, that is frequently used in practice is given as

hM(n+ 1) = hM(n)+ �||XM(n)||2 e(n)X
∗
M(n)

By dividing the step size by the norm of the data vector XM(n), the NLMS algorithm
is equivalent to employing a variable step size of the form

�(n) = �||XM(n)||2

Thus, the step size at each iteration is inversely proportional to the energy in the
received data vector XM(n). This scaling is advantageous in adaptive filtering appli-
cations where the dynamic range of the input to the adaptive filter is large, as would
be the case, for example, in the implementation of adaptive equalizers for slowly
fading communication channels. In such applications, it may be advantageous to add

bilities that may result when the norm of XM(n) is small. Thus, another version of
the NLMS algorithm may employ a variable step size of the form

�(n) = �
δ + ||XM(n)||2

where δ is a small positive number.

2.4 Properties of the LMS Algorithm

In this section, we consider the basic properties of the LMS algorithm given by

excess noise generated as a result of using noisy gradient vectors in place of the actual
gradient vectors. The use of noisy estimates of the gradient vectors implies that the
filter coefficients will fluctuate randomly and, hence, an analysis of the characteristics
of the algorithm should be performed in statistical terms.

The convergence and stability of the LMS algorithm may be investigated by
determining how the mean value of hM(n) converges to the optimum coefficients
hopt

h̄M(n+ 1) = h̄M(n)+�E[e(n)X∗M(n)]
= h̄M(n)+�[γd − �M h̄M(n)]
= (I−��M)h̄M(n)+�γd

(2.31)

(2.32)

(2.33)

(2.34)

Adaptive Filters

LMS algorithms, since multiplications are completely avoided in (2.30), and can
be completely avoided in (2.28) and (2.29) by selecting � to be a power of 1/2.

a small positive constant to the denominator of (2.32) to avoid numerical insta-

(2.23). In particular, we focus on its convergence properties, its stability, and the

. If we take the expected value of (2.23), we obtain

929



where h̄M(n) = E[hM(n)], and I is the identity matrix.

closed-loop system are governed by our choice of the step-size parameter �. To de-
termine the convergence behavior, it is convenient to decouple the M simultaneous

mean coefficient vector h̄M(n). The appropriate transformation is obtained by not-
ing that the autocorrelation matrix �M is Hermitian and, hence, it can be represented
(see Gantmacher (1960)) as

�M = U�UH

where U is the normalized modal matrix of �M and � is a diagonal matrix with
diagonal elements λk, 0 ≤ k ≤ M − 1, equal to the eigenvalues of �M .

h̄0M(n+ 1) = (I−��)h̄0M(n)+�γ0d
where the transformed (orthogonalized) vectors are h̄0M(n) = UH h̄M(n) and γ0d =
UHγd . pled.
Their convergence and their stability are determined from the homogeneous equ
ation

h̄0M(n+ 1) = (I−��)h̄0M(n)

observe that

h̄0(k, n) = C(1−�λk)nu(n), k = 0, 1, 2, . . . ,M − 1

where C is an arbitrary constant and u(n) is the unit step sequence. Clearly, h̄0(k, n)
converges to zero exponentially, provided that

|1−�λk| < 1

or, equivalently,

0 < � <
2
λk
, k = 0, 1, . . . ,M − 1

Figure 2.1
Closed-loop control system
representation of recursive

�

�

Filter
�

z � 1
H(z) �

h̄ M(n � 1)

�M h̄ M(n)

h̄ M (n)
z�1�M

�g(n) 
�d

(2.35)

(2.36)

(2.37)

(2.38)

(2.39)

Adaptive Filters

The recursive relation in (2.34) may be represented as a closed-loop control
The convergence rate and the stability of thissystem, as shown in Figure 2.1.

difference equations given in (2.34), by performing a linear transformation of the

When (2.35) is substituted into (2.34), the latter may be expressed as

The set of M first-order difference equations in (2.36) are now decou
-

If we focus our attention on the solution of the kth equation in (2.37), we

Equation (2.34).

930



ence equation for the kth normalized filter coefficient (kth mode of the closed-loop
system) must be satisfied for all k = 0, 1, . . . ,M − 1. Therefore, the range of values
of � that ensures the convergence of the mean of the coefficient vector in the LMS
algorithm is

0 < � <
2

λmax

where λmax is the largest eigenvalue of �M .
Since �M is an autocorrelation matrix, its eigenvalues are nonnegative. Hence,

an upper bound on λmax is

λmax <

M−1∑
k=0

λk = trace �M = Mγxx(0)

where γxx(0) is the input signal power, which is easily estimated from the received
signal. Therefore, an upper bound on the step size � is 2/Mγxx(0).

curs
when |1 − �λk| is small, that is, when the poles of the closed-loop system in Fig

between the largest and smallest eigenvalues of �M . In other words, even if we select
� to be 1/λmax , the convergence rate of the LMS algorithm will be determined by
the decay of the mode corresponding to the smallest eigenvalue λmin . For this mode,
with � = 1/λmax

h0M(k, n) = C
(

1− λmin
λmax

)n
u(n)

Consequently, the ratio λmin/λmax ultimately determines the convergence rate. If
λmin/λmax is small (much smaller than unity), the convergence rate will be slow. On
the other hand, if λmin/λmax is close to unity, the convergence rate of the algorithm
is fast.

The other important characteristic of the LMS algorithm is the noise resulting
from the use of estimates of the gradient vectors. The noise in the gradient-vector
estimates causes random fluctuations in the coefficients about their optimal values
and, thus, leads to an increase in the MMSE at the output of the adaptive filter. Hence,
the total MSE is �Mmin + �� , where �� is called the excess mean-square error.

For any given set of filter coefficients hM(n), the total MSE at the output of the
adaptive filter may be expressed as

�t (n) = �Mmin + (hM(n)− hopt)t�M(hM(n)− hopt)∗

where hopt A plot
of �t (n) as a function of the iteration n is called a learning curve. If we substitute

(2.40)

(2.41)

(2.42)

(2.43)

Adaptive Filters

The condition given by (2.39) for convergence of the homogeneous differ-

From (2.38), we observe that rapid convergence of the LMS algorithm oc
-

condition and still satisfy the upper bound in (2.39) when there is a large difference

substituted in (2.38), we have

represents the optimum filter coefficients defined by (2.8).

ure 2.1 are far from the unit circle. However, we cannot achieve this desirable

931



M and perform the linear orthogonal transformation used previously, we
obtain

�t (n) = �M min +
M−1∑
k=0

λk
∣∣h0(k, n)− h0opt(k)∣∣2

where the term h0(k, n)− h0opt(k) represents the error in the k th filter coefficient (in
the orthogonal coordinate system). The excess MSE is defined as the expected value

�� =
M−1∑
k=0

λkE
[∣∣h0(k, n)− h0opt(k)∣∣2]

To derive an expression for the excess MSE �� , we assume that the mean values
of the filter coefficients hM(n) have converged to their optimum values hopt
the term �e(n)X∗M
vector. Its covariance is

cov[�e(n)X∗M(n)] = �2E
[|e(n)|2XM(n)XHM(n)]

To a first approximation, we assume that |e(n)|2 is uncorrelated with the signal vector.
Although this assumption is not strictly true, it simplifies the derivation and yields
useful results. (The reader may refer to Mazo (1979), Jones, Cavin, and Reed (1982),
and Gardner (1984) for further discussion on this assumption). Then,

cov[�e(n)X∗M(n)] = �2E[|e(n)|2]E
[
XM(n)XHM(n)

]
= �2�M min�M

For the orthogonalized coefficient vector h0M(n) with additive noise, we have the
equation

h0M(n+ 1) = (I−��)h0M(n)+�γ0d + w0(n)
where w0(n) is the additive noise vector, which is related to the noise vector �e(n)
X∗M(n) through the transformation

w0(n) = UH [�e(n)X∗M(n)]
= �e(n)UHX∗M(n)

It is easily seen that the covariance matrix of the noise vector is

cov[w0(n)] = �2�MminUH�MU
= �2�Mmin�

(2.44)

(2.45)

. Then,

(2.46)

(2.47)

(2.48)

(2.49)

(2.50)

Adaptive Filters

(2.35) for �

of the second term in (2.44),

(n) in the LMS algorithm given by (2.23) is a zero-mean noise

932



Therefore, the M components of w0(n) are uncorrelated and each component
has the variance σ 2k = �2�M minλk, k = 0, 1, . . . ,M − 1.

Since the noise components of w0(n) are uncorrelated, we may consider the M
Each first-order difference

equation represents a filter with impulse response (1−�λk)n . When such a filter is
excited with a noise sequence w0k (n), the variance of the noise at the output of the
filter is

E
[∣∣h0(k, n)− h0opt(k)∣∣2] =

∞∑
n=0

∞∑
m=0

(1−�λk)n(1−�λk)mE
[
w0k (n)w

0∗
k (m)

]

We make the simplifying assumption that the noise sequence w0k (n) is white. Then,

E
[∣∣h0(k, n)− h0opt(k)∣∣2] = σ 2k1− (1−�λk)2 =

�2�M minλk
1− (1−�λk)2

for
the excess MSE as

�� = �2�Mmin
M−1∑
k=0

λ2k

1− (1−�λk)2

This expression can be simplified if we assume that � is selected such that �λk � 1
for all k . Then,

�� ≈ �2�M min
M−1∑
k=0

λ2k

2�λk

≈ 1
2
��M min

M−1∑
k=0

λk

≈ �M�Mminγxx(0)
2

where γxx(0) is the power of the input signal.
The expression for �� indicates that the excess MSE is proportional to the

step-size parameter �. Hence, our choice of � must be based on a compromise
between fast convergence and a small excess MSE. In practice, it is desirable to have
�� < �Mmin . Hence,

��
�M min

≈ �Mγxx(0)
2

< 1

(2.51)

(2.52)

(2.53)

(2.54)

Adaptive Filters

uncoupled difference equations in (2.48) separately.

(2.51) reduces to

If we substitute the result of (2.52) into (2.45), we obtain the expression

933



or, equivalently,

� <
2

Mγxx(0)

But, this is just the upper bound that we had obtained previously for λmax. In steady-

MSE causes significant degradation in the performance of the adaptive filter.
The preceding analysis of the excess MSE is based on the assumption that the

mean values of the filter coefficients have converged to the optimum solution hopt .
On

the other hand, we have determined that convergence of the mean coefficient vector
requires that� < 2/λmax . While a choice of� near the upper bound 2/λmax may lead
to initial convergence of the deterministic (known) gradient algorithm, such a large
value of � will usually result in instability of the stochastic-gradient LMS algorithm.

The initial convergence or transient behavior of the LMS algorithm has been
investigated by several researchers. Their results clearly indicate that the step size

the stochastic-gradient LMS algorithm. In practice, a choice of � < 1/Mγxx(0) is
usually made. Sayed (2003), Gitlin and Weinstein (1979), and Ungerboeck (1972)
have given an analysis of the transient behavior and the convergence properties of
the LMS algorithm.

In a digital implementation of the LMS algorithm, the choice of the step-size
parameter becomes even more critical. In an attempt to reduce the excess MSE, it is
possible to reduce the step-size parameter to the point at which the total output MSE
actually increases. This condition occurs when the estimated gradient components
e(n)x∗(n−l), l = 0, 1,M−1, after multiplication by the small step-size parameter �,
are smaller than one-half of the least significant bit in the fixed-point representation of
the filter coefficients. In such a case, adaptation ceases. Consequently, it is important
for the step size to be large enough to bring the filter coefficients in the vicinity of
hopt . If it is desired to decrease the step size significantly, it is necessary to increase
the precision in the filter coefficients. Typically, sixteen bits of precision may be used
for the filter coefficients, with the twelve most significant bits used for arithmetic
operations in the filtering of the data. The four least significant bits are required
to provide the necessary precision for the adaptation process. Thus, the scaled,
estimated gradient components�e(n)x∗(n−l) usually affect only the least significant
bits. In effect, the added precision also allows for the noise to be averaged out,
since several incremental changes in the least significant bits are required before any
change occurs in the upper, more significant bits used in arithmetic operations for
filtering of the data. For an analysis of round-off errors in a digital implementation
of the LMS algorithm, the reader is referred to Gitlin and Weinstein (1979), Gitlin,
Meadors, and Weinstein (1982), and Caraiscos and Liu (1984).

As a final point, we should indicate that the LMS algorithm is appropriate for
tracking slowly time-variant signal statistics. In such a case, the minimum MSE
and the optimum coefficient vector will be time variant. In other words, �M min is
a function of time, and the M -dimensional error surface is moving with the time

(2.55)

Adaptive Filters

state operation, � should satisfy the upper bound in (2.55); otherwise the excess

Under this condition, the step size � should satisfy the bound in (2.55).

must be reduced in direct proportion to the length of the adaptive filter, as in (2.55).
The upper bound given in (2.55) is necessary to ensure the initial convergence of

934



Figure 2.2
Excess mean-square error
�� and lag error �l as
a function of the step
size �.

M
ea

n-
sq

ua
re

 e
rr

or

�l error due to lag

�� error due to
noisy gradients

�� � �l

�opt

�

index n. The LMS algorithm attempts to follow the moving minimum �Mmin in the
M -dimensional space, but it is always lagging behind due to its use of (estimated)
gradient vectors. As a consequence, the LMS algorithm incurs another form of error,
called the lag error, whose mean-square value decreases with an increase in the step
size �. The total MSE can now be expressed as

�total = �M min + �� + �l
where �l denotes the MSE due to the lag.

In any given nonstationary adaptive filtering problem, if we plot the �� and �l

We observe that �� increases with an increase in �, whereas �l decreases with an
increase in �. The total error will exhibit a minimum, which will determine the
optimum choice of the step-size parameter.

When the statistical time variations of the signals occur rapidly, the lag error will
dominate the performance of the adaptive filter. In such a case, �l 
 �M min + �� ,
even when the largest possible value of � is used.

Learning curves for the LMS algorithm, when used to adaptively equalize a communication

a length M = 11. The autocorrelation matrix �M has an eigenvalue spread of λmax/λmin = 11.
These three learning curves have been obtained with step sizes � = 0.045, 0.09, and 0.115, by
averaging the (estimated) MSE in 200 simulation runs. The input signal power was normalized

we divide � by 2 to obtain 0.045, the convergence rate is reduced but the excess MSE is also

we note that a choice of � = 0.115 causes large undesirable fluctuations in the output MSE
of the algorithm. Note that � = 0.115 is significantly lower than the upper bound given in

(2.56)

Adaptive Filters

as a function of �, we expect these errors to behave as illustrated in Figure 2.2.

EXAMPLE 2.1

channel, are illustrated in Figure 2.3. The FIR equalizer was realized in direct form and had

to unity. Hence, the upper bound in (2.55) is equal to 0.18. By selecting � = 0.09 (one-half

reduced, so the algorithm performs better in the time-invariant signal environment. Finally,

of the upper bound), we obtain a rapidly decaying learning curve, as shown in Figure 2.3. If

(2.55).

935



1

10�1

10�2

0 100 200 300 400 500
10�3

Number of iterations

O
ut

pu
t M

SE

� � 0.045 � � 0.115

� � 0.09

Figure 2.3 Learning curves for the LMS algorithm applied to an
adaptive equalizer of length M = 11 and a channel with eigenvalue
spread λmax/λmin = 11.

3 Adaptive Direct-Form Filters—RLS Algorithms

The major advantage of the LMS algorithm lies in its computational simplicity. How-
ever, the price paid for this simplicity is slow convergence, especially when the
eigenvalues of the autocorrelation matrix �M have a large spread, that is, when
λmax/λmin 
 1. From another point of view, the LMS algorithm has only a single
adjustable parameter for controlling the convergence rate, namely, the step-size pa-
rameter �. Since � is limited for purposes of stability to be less than the upper

very slowly.
To obtain faster convergence, it is necessary to devise more complex algorithms,

which involve additional parameters. In particular, if the correlation matrix �M has
unequal eigenvalues λ0, λ1, . . . , λM−1 , we should use an algorithm that contains M
parameters, one for each of the eigenvalues. In deriving more rapidly converging
adaptive filtering algorithms, we adopt the least-squares criterion instead of the sta-
tistical approach based on the MSE criterion. Thus, we deal directly with the data
sequence x(n) and obtain estimates of correlations from the data.

It is convenient to express the least-squares algorithms in matrix form, in order to
simplify the notation. Since the algorithms will be recursive in time, it is also necessary
to introduce a time index in the filter-coefficient vector and in the error sequence.

Adaptive Filters

bound in (2.55), the modes corresponding to the smaller eigenvalues converge

RLS Algorithm3.1

936



Hence, we define the filter-coefficient vector at time n as

hM(n) =




h(0, n)
h(1, n)
h(2, n)
...

h(M − 1, n)




where the subscript M denotes the length of the filter. Similarly, the input signal
vector to the filter at time n is denoted as

XM(n) =




x(n)

x(n− 1)
x(n− 2)

...

x(n−M + 1)




We assume that x(n) = 0 for n < 0. This is usually called prewindowing of the input
data.

The recursive least-squares problem may now be formulated as follows. Suppose
that we have observed the vectors XM(l), l = 0, 1, . . . , n, and we wish to determine
the filter-coefficient vector hM(n) that minimizes the weighted sum of magnitude-
squared errors

�M =
n∑
l=0

wn−l |eM(l, n)|2

where the error is defined as the difference between the desired sequence d(l) and
the estimate d̂(l, n),

eM(l, n) = d(l)− d̂(l, n)
= d(l)− htM(n)XM(l)

and w is a weighting factor in the range 0 < w < 1.
The purpose of the factor w is to weight the most recent data points more heavily

and, thus, allow the filter coefficients to adapt to time-varying statistical character-
istics of the data. This is accomplished by using the exponential weighting factor
with the past data. Alternatively, we may use a finite-duration sliding window with
uniform weighting over the window length. We find the exponential weighting factor
more convenient, both mathematically and practically. For comparison, an exponen-
tially weighted window sequence has an effective memory of

N̄ =
∑∞

n=0 nw
n∑∞

n=0 wn
= w

1− w

and, hence, should be approximately equivalent to a sliding window of length N̄ .

(3.1)

(3.2)

(3.3)

(3.4)

(3.5)

Adaptive Filters

937



The minimization of �M with respect to the filter-coefficient vector hM(n) yields
the set of linear equations

RM(n)hM(n) = DM(n)
where RM(n) is the signal (estimated) correlation matrix defined as

RM(n) =
n∑
l=0

wn−lX∗M(l)X
t
M(l)

and DM(n) is the (estimated) crosscorrelation vector

DM(n) =
n∑
l=0

wn−lX∗M(l)d(l)

hM(n) = R−1M (n)DM(n)
Clearly, the matrix RM(n) is akin to the statistical autocorrelation matrix �M ,

and the vector DM(n) is akin to the crosscorrelation vector γd , defined previously.
We emphasize, however, that RM(n) is not a Toeplitz matrix, whereas �M is. We
should also mention that for small values of n, RM(n) may be ill conditioned, so that
its inverse is not computable. In such a case, it is customary to initially add the matrix
δIM to RM(n), where IM is an identity matrix and δ is a small positive constant. With
exponential weighting into the past, the effect of adding δIM dissipates with time.

hM(n − 1), and we wish to compute hM(n). It is inefficient and, hence, impractical
to solve the set of M linear equations for each new signal component. Instead, we
may compute the matrix and vectors recursively. First, RM(n) may be computed
recursively as

RM(n) = wRM(n− 1)+ X∗M(n)XtM(n)

M(n).
Since the inverse of RM(n) is needed, we use the matrix inversion lemma (see

Householder (1964)),

R−1M (n) =
1
w

[
R−1M (n− 1)−

R−1M (n− 1)X∗M(n)XtM(n)R−1M (n− 1)
w + XtM(n)R−1M (n− 1)X∗M(n)

]

Thus, R−1M (n) may be computed recursively.
For convenience, we define PM(n) = R−1M (n). It is also convenient to define an

M -dimensional vector KM(n), sometimes called the Kalman gain vector, as

KM(n) = 1
w + µM(n)PM(n− 1)X

∗
M(n)

(3.6)

(3.7)

(3.8)

(3.9)

(3.10)

(3.11)

(3.12)

Adaptive Filters

The solution of (3.6) is

Now, suppose that we have the solution of (3.9) at time n−1—that is, we have

We call (3.10) the time-update equation for R

938



where µM(n) is a scalar defined as

µM(n) = XtM(n)PM(n− 1)X∗M(n)

PM(n) = 1
w

[
PM(n− 1)−KM(n)XtM(n)PM(n− 1)

]

∗
M(n). Then,

PM(n)X∗M(n) =
1
w

[
PM(n− 1)X∗M(n)−KM(n)XtM(n)PM(n− 1)X∗M(n)

]

= 1
w
{[w + µM(n)]KM(n)−KM(n)µM(n)} = KM(n)

Therefore, the Kalman gain vector may also be defined as PM(n)X∗M(n).
Now, we may use the matrix inversion lemma to derive an equation for computing

the filter coefficients recursively. Since

hM(n) = PM(n)DM(n)

and

DM(n) = wDM(n− 1)+ d(n)X∗M(n)

hM(n) = 1
w

[
PM(n− 1)−KM(n)XtM(n)PM(n− 1)

]
× [wDM(n− 1)+ d(n)X∗M(n)]

= PM(n− 1)DM(n− 1)+ 1
w
d(n)PM(n− 1)X∗M(n)

−KM(n)XtM(n)PM(n− 1)DM(n− 1)

− 1
w
d(n)KM(n)XtM(n)PM(n− 1)X∗M(n)

= hM(n− 1)+KM(n)
[
d(n)−XtM(n)hM(n− 1)

]

(3.13)

(3.14)

(3.15)

(3.16)

(3.17)

(3.18)

Adaptive Filters

With these definitions, (3.11) becomes

Let us postmultiply (3.14) by X

we have, upon substitution of (3.14) and (3.17) into (3.9),

939



We observe that XtM(n)hM(n − 1) is the output of the adaptive filter at time n
based on use of the filter coefficients at time n− 1. Since

XtM(n)hM(n− 1) = d̂(n, n− 1) ≡ d̂(n)

and

eM(n, n− 1) = d(n)− d̂(n, n− 1) ≡ eM(n)

it follows that the time-update equation for hM(n) may be expressed as

hM(n) = hM(n− 1)+KM(n)eM(n)

or, equivalently,

hM(n) = hM(n− 1)+ PM(n)X∗M(n)eM(n)

To summarize, suppose we have the optimum filter coefficients hM(n − 1), the
matrix PM(n−1), and the vector XM(n−1). When the new signal component x(n) is
obtained, we form the vector XM(n) by dropping the term x(n−M) from XM(n− 1)
and adding the term x(n) as the first element. Then, the recursive computation for
the filter coefficients proceeds as follows:

1. Compute the filter output:

d̂(n) = XtM(n)hM(n− 1)

2. Compute the error:

eM(n) = d(n)− d̂(n)

3. Compute the Kalman gain vector:

KM(n) = PM(n− 1)X
∗
M(n)

w + XtM(n)PM(n− 1)X∗M(n)

4. Update the inverse of the correlation matrix

PM(n) = 1
w

[
PM(n− 1)−KM(n)XtM(n)PM(n− 1)

]

5. Update the coefficient vector of the filter

hM(n) = hM(n− 1)+KM(n)eM(n)

(3.19)

(3.20)

(3.21)

(3.22)

(3.23)

(3.24)

(3.25)

(3.26)

(3.27)

Adaptive Filters

940



form
recursive least-squares (RLS) algorithm. It is initialized by setting hM(−1) = 0 and
PM(−1) = 1/δIM , where δ is a small positive number.

The residual MSE resulting from the preceding optimization is

�Mmin =
n∑
l=0

wn−l |d(l)|2 − htM(n)D∗M(n)

equal to the error eM(n)multiplied by the Kalman gain vector KM(n). Since KM(n) is
an M -dimensional vector, each filter coefficient is controlled by one of the elements
of KM(n). Consequently, rapid convergence is obtained. In contrast, the time-update
equation for the coefficients of the filter adjusted by use of the LMS algorithm is

hM(n) = hM(n− 1)+�X∗(n)eM(n)

which has only the single parameter � for controlling the adjustment rate of the
coefficients.

3.2 The LDU Factorization and Square-Root Algorithms

The RLS algorithm is very susceptible to round-off noise in an implementation of
the algorithm with finite-precision arithmetic. The major problem with round-off
errors occurs in the updating of PM(n). To remedy this problem, we may perform a
decomposition of either the correlation matrix RM(n) or its inverse PM(n).

decomposition of PM(n). We may write

PM(n) = LM(n)D̄M(n)LHM(n)

where LM(n) is a lower-triangular matrix with elements lik, D̄M(n) is a diagonal
matrix with elements δk, and LHM(n) is an upper-triangular matrix. The diagonal
elements of LM(n) are set to unity (i.e., lii = 1). Now, instead of computing PM(n)
recursively, we can determine a formula for updating the factors LM(n) and D̄M(n)
directly, thus avoiding the computation of PM(n).

The desired update formula is obtained by substituting the factored form of
PM

LM(n)D̄M(n)LHM(n)

= 1
w

LM(n− 1)
[

D̄M(n− 1)− 1
w + µM(n)VM(n− 1)V

H
M(n− 1)

]
LHM(n− 1)

(3.28)

(3.29)

To be specific, let us consider an LDU (lower-triangular/ diagonal/upper-triangular)

(3.30)

(3.31)

Adaptive Filters

The recursive algorithm specified by (3.23) through (3.27) is called the direct-

From (3.27), we observe that the filter coefficients vary with time by an amount

(n) into (3.26). Thus, we have

941



where, by definition,

VM(n− 1) = D̄M(n− 1)LHM(n− 1)X∗M(n)

in an LDU factored form as

L̂M(n− 1)D̂M(n− 1)L̂HM(n− 1)

= D̄M(n− 1)− 1
w + µM(n)VM(n− 1)V

H
M(n− 1)

LM(n)D̄M(n)L̂HM(n) =
1
w

[
LM(n− 1)L̂M(n− 1)D̂M(n− 1)L̂HM(n− 1)LHM(n− 1)

]
Consequently, the desired update relations are

LM(n) = LM(n− 1)L̂M(n− 1)

D̄M(n) = 1
w

D̂M(n− 1)

To determine the factors L̂M(n− 1) and D̂M(n− 1), we need to factor the matrix on

set of linear equations

j∑
k=1

likdkl
∗
jk = pij , 1 ≤ j ≤ i − 1, i ≥ 2

where {dk} are the elements of D̂M(n− 1), {lik} are elements of L̂M(n− 1) and {pij }
Then, {lik} and {dk } are

determined as follows:

d1 = p11

lij dj = pij −
j−1∑
k=1

likdkl
∗
jk, 1 ≤ j ≤ i − 1, 2 ≤ i ≤ M

di = pii −
i−1∑
k=1
|lik|2dk, 2 ≤ i ≤M

(3.32)

(3.33)

(3.34)

(3.35)

(3.36)

(3.37)

Adaptive Filters

The term inside the brackets in (3.31) is a Hermitian matrix and may be expressed

Then, if we substitute (3.33) into (3.31), we obtain

the right-hand side (RHS) of (3.33). This factorization may be expressed by the

are the elements of the matrix on the RHS of (3.33).

942



TABLE 1 LDU Form of Square-Root RLS Algorithm

for j = 1, . . . , 2, . . . ,M do
fj = x∗j (n)

end loop j
for j = 1, 2, . . . ,M − 1 do

for i = j + 1, j + 2, . . . ,M do
fj = fj + lij (n− 1)fi

end loop j
for j = 1, 2, . . . ,M do

d̄j (n) = dj (n− 1)/w
vj = d̄j (n)fj

end loop j
αM = 1+ vMf ∗M
dM(n) = d̄M(n)/αM
k̄M = vM
for j = M − 1,M − 2, . . . , 1 do

k̄j = vj
αj = αj+1 + vjf ∗j
λj = fj /αj+1
dj (n) = d̄j (n)αj+1/α1
for i = M, M − 1, . . . , j + 1 do

lij (n) = lij (n− 1)+ k̄∗i λj
k̄i = k̄i + vj l∗ij (n− 1) down to j = 2)

end loop i
end loop j

K̄M(n) = [k̄1, k̄2, . . . , k̄M ]t
eM(n) = d(n)− d̄(n)
hM(n) = hM(n− 1)+ [eM(n)/α1]K̄M(n)

depends directly on the data vector XM(n) and not on the “square” of the data
vector. Thus, the squaring operation of the data vector is avoided and, consequently,
the effect of round-off errors is significantly reduced.

The RLS algorithms obtained from an LDU decomposition of either RM(n) or
PM(n) are called square-root RLS algorithms. Bierman (1977), Carlson and Culmone
(1979), and Hsu (1982) treat these types of algorithms. A square-root RLS algorithm
based on the LDU decomposition of PM Its
computational complexity is proportional to M2 .

3.3 Fast RLS Algorithms

The RLS direct-form algorithm and the square-root algorithms have a computational
complexity proportional to M2 , as indicated. On the other hand, the RLS lattice

to M . Basically, the lattice algorithms avoid the matrix multiplications involved in
computing the Kalman gain vector KM(n).

Adaptive Filters

The resulting algorithm, obtained from the time-update equations in (3.35),

(n), as just described, is given in Table 1.

algorithms derived in Section 4 have a computational complexity proportional

943



TABLE 2 Fast RLS Algorithm: Version A

fM−1(n) = x(n)+ atM−1(n− 1)XM−1(n− 1)
gM−1(n) = x(n−M + 1)+ btM−1(n− 1)XM−1(n)
aM−1(n) = aM−1(n− 1)−KM−1(n− 1)fM−1(n)

fM−1(n, n) = x(n)+ atM−1(n)XM−1(n− 1)
E
f

M−1(n) = wEfM−1(n− 1)+ fM−1(n)f ∗M−1(n, n)[
CM−1(n)
cMM(n)

]
≡ KM(n) =

[
0

KM−1(n− 1)
]
+ f

∗
M−1(n, n)

E
f

M−1(n)

[
1

aM−1(n)

]

KM−1(n) = CM−1(n)− cMM(n)bM−1(n− 1)1− cMM(n)gM−1(n)
bM−1(n) = bM−1(n− 1)−KM−1(n)gM−1(n)

d̂(n) = htM(n− 1)XM(n)
eM(n) = d(n)− d̂(n)
hM(n) = hM(n− 1)+KM(n)eM(n)

Initialization
aM−1(−1) = bM−1(−1) = 0

KM−1(−1) = 0
hM−1(−1) = 0
E
f

M−1(−1) = 
, 
 > 0

for the RLS lattice, it is possible to obtain time-update equations for the Kalman gain
vector that completely avoid matrix multiplications. The resulting algorithms have
a complexity that is proportional to M (multiplications and divisions) and, hence,
they are called fast RLS algorithms for direct-form FIR filters.

There are several versions of fast algorithms, which differ in minor ways. Two ver-
in

tional
complexity for Version A is 10M − 4 (complex) multiplications and divisions,where
as Version B has a complexity of 9M+1 multiplications and divisions. Further redu
ction of computational complexity to 7M is possible. For example, Carayannis,
Manolakis, and Kalouptsidis (1983) describe a fast RLS algorithm, termed the
FAEST (fast a posteriori error sequential technique) algorithm, with a computa-

these algorithms with a complexity of 7M have been proposed, but many of these al-
gorithms are extremely sensitive to round-off noise and exhibit instability problems
(Falconer and Ljung (1978), Carayannis, Manolakis, and Kalouptsidis (1983; 1986)
and Cioffi and Kailath (1984)). Slock and Kailath (1988; 1991) have shown how to

Adaptive Filters

By using the forward and backward prediction formulas derived in Section 4

sions are given in Tables 2 and 3 for complex-valued signals. The variables used
the fast algorithms listed in these tables are defined in Section 4. The computa

-

tional complexity of 7M ; this algorithm is given in Section 4. Other versions of

944



TABLE 3 Fast RLS Algorithm: Version B

fM−1(n) = x(n)+ atM−1(n− 1)XM−1(n− 1)
gM−1(n) = x(n−M + 1)+ btM−1(n− 1)XM−1(n)
aM−1(n) = aM−1(n− 1)−KM−1(n− 1)fM−1(n)

fM−1(n, n) = αM−1(n− 1)fM−1(n)
E
f

M−1(n) = wEfM−1(n− 1)+ αM−1(n− 1)|fM−1(n)|2[
CM−1(n)
cMM(n)

]
≡ KM(n) =

[
0

KM−1(n− 1)
]
+ f

∗
M−1(n, n)

E
f

M−1(n)

[
1

aM−1(n)

]

KM−1(n)= CM−1(n) − cMM(n)bM−1(n − 1)1 − cMM(n)gM−1(n)

bM−1(n) = bM−1(n− 1)−KM−1(n)gM−1(n)

αM−1(n) = αM−1(n− 1)


 1−

fM−1(n)f ∗M−1(n,n)
E
f

M−1(n)

1− cMM(n)gM−1(n)




d̂(n) = htM(n− 1)XM(n)
eM(n) = d(n)− d̂(n)
hM(n) = hM(n− 1)+KM(n)eM(n)

Initialization
aM−1(−1) = bM−1(−1) = 0

KM−1(−1) = 0, hM−1(−1) = 0
E
f

M−1(−1) = 
 > 0

stabilize these fast (7M ) algorithms with a relatively small increase in the number of

3.4 Properties of the Direct-Form RLS Algorithms

A major advantage of the direct-form RLS algorithms over the LMS algorithm is their
faster convergence rate.
which shows the convergence rate of the LMS and direct-form RLS algorithms for
an adaptive FIR channel equalizer of length M = 11. The statistical autocorrelation
matrix �M for the received signal has an eigenvalue ratio of λmax/λmin = 11. All the
equalizer coefficients were initially set to zero. The step size for the LMS algorithm
was selected as� = 0.02, which represents a good compromise between convergence
rate and excess MSE.

The superiority of the RLS algorithm in achieving faster convergence is clearly
evident. The algorithm converges in less than 70 iterations (70 signal samples) while

Adaptive Filters

computations; two stabilized fast RLS algorithms are given in Section 4.

This characteristic behavior is illustrated in Figure 3.1,

945



10�1

100

10�2
00 100 200 300 400 500 600 700

Number of iterations

M
ea

n-
sq

ua
re

 e
rr

or

RLS algorithm
w � 0.999

Gradient algorithm

Figure 3.1 Learning curves for RLS and LMS algorithms for adaptive equalizer
of length M = 11. The eigenvalue spread of the channel is λmax/λmin = 11. The
step size for the LMS algorithm is � = 0.02. (From Digital Communication by John
G. Proakis, © 1983 by McGraw-Hill Book Company. Reprinted with permission of
the publisher.)

the LMS algorithm has not converged in over 600 iterations. This rapid rate of con-
vergence of the RLS algorithm is extremely important in applications in which the
signal statistics vary rapidly with time. For example, the time variations of the char-
acteristics of an ionospheric high-frequency (HF) radio channel result in frequent
fading of the signal to the point where the signal strength is comparable to or even
lower than the additive noise. During a signal fade, both the LMS and RLS algo-
rithms are unable to track the channel characteristics. As the signal emerges from the
fade, the channel characteristics are generally different than those prior to the fade.
In such a case, the LMS algorithm is slow to adapt to the new channel characteristics.
On the other hand, the RLS algorithm adapts sufficiently fast to track such rapid
variations (Hsu (1982)).

Despite their superior convergence rate, the RLS algorithms for FIR adaptive
filtering described in the previous section have two important disadvantages. One
is their computational complexity. The square-root algorithms have a complexity
proportional to M2 . The fast RLS algorithms have a computational complexity
proportional to M , but the proportionality factor is four to five times that of the
LMS algorithm.

The second disadvantage of the algorithms is their sensitivity to round-off errors
that accumulate as a result of the recursive computations. In some cases, the round-
off errors cause these algorithms to become unstable.

The numerical properties of the RLS algorithms have been investigated by sev-
eral researchers, including Ling and Proakis (1984a), Ljung and Ljung (1985), and
Cioffi (1987b).
the steady-state (time-averaged) square error for the RLS square-root algorithm, the

Adaptive Filters

For illustrative purposes, Table 4 includes simulation results on

fast RLS algorithm in Table 2, and the LMS algorithm, for different word lengths.

946



TABLE 4 Numerical Accuracy of FIR Adaptive Filtering
Algorithms (Least-Squares Error × 10−3 )

Algorithm

Number of bits RLS
(including sign) square root Fast RLS LMS

16 2.17 2.17 2.30

13 2.33 2.21 2.30

11 6.14 3.34 19.0

10 17.6 a 77.2

9 75.3 a 311.0

8 a a 1170.0
a Algorithm does not converge to optimum coefficients.

The simulation was performed with a linear adaptive equalizer having M = 11 co-
efficients. The channel had an eigenvalue ratio of λmax/λmin = 11. The exponential
weighting factor used in the RLS algorithms was w = 0.975 and the step size for
the LMS algorithm was � = 0.025. The additive noise has a variance of 0.001. The
output MSE with infinite precision is 2.1× 10−3 .

We should indicate that the direct-form RLS algorithm becomes unstable and,
hence, does not work properly with 16-bit fixed-point arithmetic. For this algorithm,
we found experimentally that approximately 20–24 bits of precision are needed for
the algorithm to work properly. On the other hand, the square-root algorithm works
down to about 9 bits, but the degradation in performance below 11 bits is significant.
The fast RLS algorithm works well down to 11 bits for short durations of the order
of 500 iterations. For a much larger number of iterations, the algorithm becomes un-
stable due to the accumulation of round-off errors. In such a case, several methods
have been proposed to restart the algorithm in order to prevent overflow in the coef-
ficients. The interested reader may refer to Eleftheriou and Falconer (1987), Cioffi
and Kailath (1984), and Hsu (1982). Alternatively, one may modify the algorithm as
proposed by Slock and Kailath (1988; 1991) and, thus, stabilize it.

robust to round-off noise. It deteriorates as expected with a decrease in the precision
of the filter coefficients, but no catastrophic failure (instability) occurs with 8 or 9 bits
of precision. However, the degradation in performance below 12 bits is significant.

4 Adaptive Lattice-Ladder Filters

In this section, we derive adaptive filtering algorithms in which the filter struc-
ture is a lattice or a lattice-ladder. These adaptive lattice-ladder filter algorithms,

An FIR filter may also be realized as a lattice structure in which the lattice parame-
ters, called the reflection coefficients, are related to the filter coefficients in the direct-
form FIR structure. There is a method for converting the FIR filter coefficients into 
the reflection coefficients (and vice versa).

Adaptive Filters

We also observe from the results of Table 4 that the LMS algorithm is quite

947



based on the method of least squares, have several desirable properties, including
computational efficiency and robustness to round-off errors. From the development
of the RLS lattice-ladder algorithms, we obtain the fast RLS algorithms that were

4.1 Recursive Least-Squares Lattice-Ladder Algorithms

The recursive least-squares algorithms for the direct-form FIR structures de-

A change (increase or decrease) in the filter length results in a new set of filter
coefficients that are totally different from the previous set.

In contrast, the lattice filter is order recursive. As a consequence, the number
of sections that it contains can be easily increased or decreased without affecting the
reflection coefficients of the remaining sections. This and several other advantages
(described in this and subsequent sections) make the lattice filter very attractive for
adaptive filtering applications.

To begin, suppose that we observe the signal x(n− l), l = 1, 2, . . . , m, and let us
consider the linear prediction of x(n). Let fm(l, n) denote the forward prediction
error for an mth-order predictor, defined as

fm(l, n) = x(l)+ atm(n)Xm(l − 1)

where the vector −am(n) consists of the forward prediction coefficients, that is,

atm(n) = [am(1, n) am(2, n) · · · am(m, n)]

and the data vector Xm(l − 1) is

Xtm(l − 1) = [x(l − 1) x(l − 2) · · · x(l −m)]

The predictor coefficients am(n) are selected to minimize the time-averaged weighted
squared error

�fm(n) =
n∑
l=0

wn−l|fm(l, n)|2

The minimization of �fm(n) with respect to am(n) leads to the set of linear equa-
tions

Rm(n− 1)am(n) = −Qm(n)

(4.1)

(4.2)

(4.3)

(4.4)

(4.5)

described in Section 3.3.

There is a relationship between the lattice filter structure and a linear predictor, and 
equations that relate the predictor coefficients to the reflection coefficients of the 
lattice (and vice versa). There is a relationship between the Levinson-Durbin recur-
sions for the linear predictor coefficients and the reflection coefficients in the lattice 
filter. From these assertions, we would expect to obtain the recursive least-squares 
lattice filter by formulating the least-squares estimation problem in terms of linear 
prediction. This is the approach that we take.

Adaptive Filters

scribed in Section 3.1 are recursive in time only. The length of the filter is fixed.

948



where Rm m(n) is defined as

Qm(n) =
n∑
l=0

wn−lx(l)X∗m(l − 1)

am(n) = −R−1m (n− 1)Qm(n)

The minimum value of �fm
is denoted as Efm(n) and is given by

Efm(n) =
n∑
l=0

wn−lx∗(l)
[
x(l)+ atm(n)Xm(l − 1)

]

= q(n)+ atm(n)Q∗m(n)

where q(n) is defined as

q(n) =
n∑
l=0

wn−l|x(l)|2

f
m

combined in a single matrix equation of the form

[
q(n) QHm (n)

Qm(n) Rm(n− 1)
] [

1
am(n)

]
=
[
E
f
m(n)

Om

]

where Om is the m-dimensional null vector. It is interesting to note that

Rm+1(n) =
n∑
l=0

wn−lX∗m+1(l)X
t
m+1(l)

=
n∑
l=0

wn−l
[

x∗(l)
X∗m(l − 1)

] [
x(l)Xtm(l − 1)

]

=
[
q(n) QHm (n)

Qm(n) Rm(n− 1)
]

the
backward time-averaged weighted squared error for an mth-order backward
predictor defined as

�bm(n) =
n∑
l=0

wn−l|gm(l, n)|2

(4.6)

(4.7)

(4.8)

(4.9)

(4.10)

(4.11)

(4.12)

Adaptive Filters

(n) is defined by (3.7) and Q

The solution of (4.5) is

(n), obtained with the linear predictor specified by (4.7),

(n) in (4.8) can beThe linear equations in (4.5) and the equation for E

which is the matrix in (4.10).
In a completely parallel development to (4.1) through (4.11), we minimize

949



where the backward error is defined as

gm(l, n) = x(l −m)+ btm(n)Xm(l)

and btm(n) = [bm(1, n) bm(2, n). . .bm(m, n)] is the vector of coefficients for the back-
ward predictor. The minimization of �bm(n) leads to the equation

Rm(n)bm(n) = −Vm(n)

and, hence, to the solution

bm(n) = −R−1m (n)Vm(n)

where

Vm(n) =
n∑
l=0

wn−lx(l −m)X∗m(l)

The minimum value of �bm(n), denoted as E
b
m(n), is

Ebm(n) =
n∑
l=0

wn−l
[
x(l −m)+ btm(n)Xm(l)

]
x∗(l −m)

= v(n)+ btm(n)V∗m(n)

where the scalar quantity v(n) is defined as

v(n) =
n∑
l=0

wn−l|x(l −m)|2

[
Rm(n) Vm(n)
VHm (n) v(n)

] [
bm(n)

1

]
=
[

Om
Ebm(n)

]

We also note that the (estimated) autocorrelation matrix Rm+1(n) can be expressed as

Rm+1(n) =
n∑
l=0

wn−l
[

X∗m(l)
x∗(l −m)

] [
Xtm(l)x(l −m)

]

=
[

Rm(n) Vm(n)
VHm (n) v(n)

]

Thus, we have obtained the equations for the forward and backward least-squares
predictors of order m.

Next, we derive the order-update equations for these predictors, which will lead
us to the lattice filter structure. In deriving the order-update equations for am(n) and

(4.13)

(4.14)

(4.15)

(4.16)

(4.17)

(4.18)

(4.19)

(4.20)

Adaptive Filters

If we combine (4.14) and (4.17) into a single equation, we obtain

950



bm(n), we will make use of the two matrix inversion identities for a matrix of the
form

A =
[

A11 A12
A21 A22

]
where A,A11 , and A22 are square matrices. The inverse of A is expressible in two
different forms, namely,

A−1 =
[

A−111 + A−111 A12Ã−122 A21A−111 −A−111 A12Ã−122
−Ã−122 A21A−111 Ã−122

]
and

A−1 =
[

Ã−111 −Ã−111 A12A−122
−A−122 A21Ã−111 A−122 A21Ã−111 A12A−122 + A−122

]

where Ã11 and Ã12 are defined as

Ã11 = A11 − A12A−122 A21
Ã22 = A22 − A21A−111 A12

Order-Update Recursions.
inverse of Rm+1

Ã22 = v(n)− VHm (n)R−1m (n)Vm(n)
= v(n)+ btm(n)V∗m(n) = Ebm(n)

and
A−111 A12 = R−1m (n)Vm(n) = −bm(n)

Hence,

R−1
m+1(n) ≡ Pm+1(n) =




Pm(n)+ bm(n)b
H
m (n)

Ebm(n)

bm(n)
Ebm(n)

bHm (n)
Ebm(n)

1
Ebm(n)




or, equivalently,

Pm+1(n) =
[

Pm(n) 0
0 0

]
+ 1
Ebm(n)

[
bm(n)

1

] [
bHm (n) 1

]
m+1(n),

we obtain the order update for am(n). Thus,

am+1(n) = −Pm+1(n− 1)Qm+1(n)

=
[

Pm(n− 1) 0
0 0

] [−Q(n)m
. . .

]

− 1
Ebm(n− 1)

[
bm(n− 1)

1

] [
bHm (n− 1) 1

]
Qm+1(n)

=
[

am(n)
0

]
− km+1(n)
Ebm(n− 1)

[
bm(n− 1)

1

]

(4.21)

(4.22)

(4.23)

(4.24)

(4.25)

(4.26)

(4.27)

(4.28)

Adaptive Filters

Now, let us use the formula in (4.22) to obtain the
(n) by using the form in (4.20). First, we have

By substituting n− 1 for n in (4.27) and postmultiplying the result by −Q

951



where the scalar quantity km+1(n) is defined as

km+1(n) =
[
bHm (n− 1) 1

]
Qm+1(n)

tor coefficients.
To obtain the corresponding order update for bm(n), we use the matrix inversion

m+1
this case, we have

Ã11 = q(n)−QHm (n)R−1m (n− 1)Qm(n)
= q(n)+ atm(n)Q∗m(n) = Efm(n)

and
A−122 A21 = R−1m (n− 1)Qm(n) = −am(n)

Hence,

Pm+1(n) =




1

E
f
m(n)

aHm (n)

E
f
m(n)

am(n)

E
f
m(n)

Pm(n− 1)+ am(n)a
H
m (n)

E
f
m(n)




or, equivalently,

Pm+1(n) =
[

0 0
0 Pm(n− 1)

]
+ 1
E
f
m(n)

[
1

am(n)

] [
1 aHm (n)

]

m+1(n), we obtain

bm+1(n) =
[

0 0
0 Pm(n− 1)

] [ · · ·
−Vm(n− 1)

]

− 1
E
f
m(n)

[
1

am(n)

] [
1 aHm (n)

]
Vm+1(n)

=
[

0
bm(n− 1)

]
− k

∗
m+1(n)

E
f
m(n)

[
1

am(n)

]

where [
1 aHm (n)

]
Vm+1(n) =

[
btm(n− 1) 1

]
Q∗m+1(n) = k∗m+1(n)

reader. m(n) and
bm(n), respectively.

The order-update equations for Efm(n) and Ebm(n) may now be obtained. From
the definition of Efm

E
f

m+1(n) = q(n)+ atm+1(n)Q∗m+1(n)

(4.29)

(4.30)

(4.31)

(4.32)

(4.33)

(4.34)

(4.35)

Adaptive Filters

The reader should observe that (4.28) is a Levinson-type recursion for the predic-

formula in (4.23) for the inverse of R (n), along with the form in (4.11). In

Now, if we postmultiply (4.32) by −V

The proof of (4.34) and its relation to (4.29) is left as an excercise for the
Thus, (4.28) and (4.33) specify the order-update equations for a

(n) given by (4.8), we have

952



By substituting for am+1

E
f

m+1(n) = q(n)+ [a′m(n) 0]
[

Q∗m(n)
· · ·

]
− km+1(n)
Ebm(n− 1)

[b′m(n− 1) 1]Q∗m+1

= Efm(n)−
|km+1(n)|2
Ebm(n− 1)

b
m+1(n), in

the form

Ebm+1(n) = Ebm(n− 1)−
|km+1(n)|2
E
f
m(n)

The lattice filter is specified by two coupled equations involving the forward and
backward errors fm(n, n − 1) and gm(n, n − 1), respectively. From the definition of

fm+1(n, n− 1) = x(n)+ atm+1(n− 1)Xm+1(n− 1)

Substituting for at
m+1

fm+1(n, n− 1) = x(n)+
[
atm(n− 1) 0

] [Xm(n− 1)
· · ·

]

− km+1(n− 1)
Ebm(n− 2)

[
btm(n− 2) 1

]
Xm+1(n− 1)

= fm(n, n− 1)− km+1(n− 1)
Ebm(n− 2)

× [x(n−m− 1)+ btm(n− 2)Xm(n− 1)]

= fm(n, n− 1)− km+1(n− 1)
Ebm(n− 2)

gm(n− 1, n− 2)

To simplify the notation, we define

fm(n) = fm(n, n− 1)
gm(n) = gm(n, n− 1)

fm+1(n) = fm(n)− km+1(n− 1)
Ebm(n− 2)

gm(n− 1)

we
have

gm+1(n, n− 1) = x(n−m− 1)+ btm+1(n− 1)Xm+1(n)

(n)

(4.36)

(4.37)

(4.38)

(4.39)

(4.40)

(4.41)

(4.42)

Adaptive Filters

(n) from (4.28) into (4.35), we obtain

Similarly, by using (4.17) and (4.33), we obtain the order update for E

the forward error in (4.1), we have

(n− 1) from (4.28) into (4.38) yields

Then, (4.39) may be expressed as

Similarly, beginning with the definition of the backward error given by (4.13),

953



Substituting for bm+1

gm+1(n, n− 1) = gm(n− 1, n− 2)−
k∗
m+1(n− 1)
E
f
m(n− 1)

fm(n, n− 1)

or, equivalently,

gm+1(n) = gm(n− 1)−
k∗
m+1(n− 1)
E
f
m(n− 1)

fm(n)

reflection coefficients for the lattice as

�fm(n) =
−km(n)

Eb
m−1(n− 1)

�bm(n) =
−k∗m(n)
E
f

m−1(n)

The initial conditions on the order updates are

f0(n) = g0(n) = x(n)

E
f

0 (n) = Eb0 (n) =
n∑
l=0

wn−l |x(l)|2 = wEf0 (n− 1)+ |x(n)|2

f

0 (n) and E
b
0(n).

z�1gm�1(n)

fm�1(n)

gm(n)

fm(n)

�bm (n�1)

�fm (n�1)

Stage
1

Stage
2

Stage
3

Stage
M

gM(n)

fM(n)

g0(n)

x(n)

f0(n)

g1(n)

f1(n)

g2(n)

f2(n)

(a)

(b)

�
�

Figure 4.1 Least-squares lattice filter.

(4.43)

(4.44)

(4.45)

(4.46)

Adaptive Filters

(n− 1) from (4.33) and simplifying the result, we obtain

The two recursive equations in (4.41) and (4.44) specify the lattice filter
illustrated in Figure 4.1 where, for notational convenience, we have defined the

We note that (4.46) is also a time-update equation for E

954



Time-Update Recursions. Our goal is to determine a time-update equation for km(n),
which is necessary if the lattice filter is to be adaptive. This derivation will require
time-update equations for the prediction coefficients. We begin with the form

km+1(n) = −VHm+1(n)
[

1
am(n)

]

The time-update equation for Vm+1(n) is

Vm+1(n) = wVm+1(n− 1)+ x(n−m− 1)X∗m+1(n)
The time-update equations for the prediction coefficients are determined as follows.

am(n) = −Pm(n− 1)Qm(n)

= − 1
w

[
Pm(n− 2)−Km(n− 1)Xtm(n− 1)Pm(n− 2)

]
× [wQm(n− 1)+ x(n)X∗m(n− 1)]
= am(n− 1)−Km(n− 1)

[
x(n)+ atm(n− 1)Xm(n− 1)

]
where Km
we have

x(n)+ atm(n− 1)Xm(n− 1) = fm(n, n− 1) ≡ fm(n)
Therefore, the time-update equation for am(n) is

am(n) = am(n− 1)−Km(n− 1)fm(n)
time-

update equations for the coefficients of the backward predictor, in the form

bm(n) = bm(n− 1)−Km(n)gm(n)

m+1(n) is

km+1(n) = −
[
wVHm+1(n− 1)+ x∗(n−m− 1)Xtm+1(n)

]
×
([

1
am(n− 1)

]
−
[

0
Km(n− 1)fm(n)

])

= wkm+1(n− 1)− wVHm+1(n− 1)
[

0
Km(n− 1)

]
fm(n)

+ x∗(n−m− 1)Xtm+1(n)
[

1
am(n− 1)

]

− x∗(n−m− 1)Xtm+1(n)
[

0
Km(n− 1)

]
fm(n)

(4.47)

(4.48)

(4.49)

(4.50)

(4.51)

(4.52)

Adaptive Filters

From (4.6), (4.7), and (3.14), we have

(n − 1) is the Kalman gain vector at iteration n − 1. But, from (4.38),

In a parallel development, using (4.15), (4.16), and (3.14), we obtain the

Now, from (4.48) and (4.50), the time-update equation for k

955



But

Xtm+1(n)
[

1
am(n− 1)

]
= [x(n) Xtm(n− 1)]

[
1

am(n− 1)
]
= fm(n)

and

VHm+1(n− 1)
[

0
Km(n− 1)

]
= VHm (n− 2)Km(n− 1)

= V
H
m (n− 2)Pm(n− 2)X∗m(n− 1)

w + µm(n− 1)

= −b
H
m (n− 2)X∗m(n− 1)
w + µm(n− 1)

= −g
∗
m(n− 1)− x∗(n−m− 1)

w + µm(n− 1)

where µm

Xtm+1(n)
[

0
Km(n− 1)

]
= X

t
m(n− 1)Pm(n− 2)X∗m(n− 1)

w + µm(n− 1) =
µm(n− 1)

w + µm(n− 1)

the desired
time-update equation in the form

km+1(n) = wkm+1(n− 1)+ w
w + µm(n− 1)fm(n)g

∗
m(n− 1)

It is convenient to define a new variable

αm(n) = w
w + µm(n)

Clearly, αm(n) is real-valued and has a range 0 < αm(n) < 1. Then, the time-update

km+1(n) = wkm+1(n− 1)+ αm(n− 1)fm(n)g∗m(n− 1)

Order Update for αm(n). Although αm(n) can be computed directly for each value
of m and for each n, it is more efficient to use an order-update equation, which is
determined as follows. First, from the definition of Km
easily seen that

αm(n) = 1− Xtm(n)Km(n)

(4.53)

(4.54)

(4.55)

(4.56)

(4.57)

(4.58)

(4.59)

Adaptive Filters

(n) is as previously defined in (3.13). Finally,

Substituting the results of (4.53), (4.54), and (4.55) into (4.52) we obtain

equation (4.56) becomes

(n) given in (3.12), it is

956



To obtain an order-update equation for αm(n), we need an order-update equation
for the Kalman gain vector Km(n). But Km+1(n) may be expressed as

Km+1(n) = Pm+1(n)X∗m+1(n)

=
([

Pm(n) 0
0 0

]
+ 1
Ebm(n)

[
bm(n)

1

] [
bHm (n) 1

]) [ X∗m(n)
x∗(n−m)

]

=
[

Km(n)
0

]
+ g

∗
m(n, n)

Ebm(n)

[
bm(n)

1

]

The term gm(n, n) may also be expressed as

gm(n, n) = x(n−m)+ btm(n)Xm(n)
= x(n−m)+ [btm(n− 1)−Ktm(n)gm(n)]Xm(n)
= x(n−m)+ btm(n− 1)Xm(n)− gm(n)Ktm(n)Xm(n)
= gm(n)

[
1−Ktm(n)Xm(n)

]
= αm(n)gm(n)

Hence, the order-update equation for Km

Km+1(n) =
[

Km(n)
0

]
+ αm(n)g

∗
m(n)

Ebm(n)

[
bm(n)

1

]

tion
for αm(n) as follows:

αm+1(n) = 1− Xtm+1(n)Km+1(n) = 1−
[
Xtm(n)x(n−m)

]
×
([

Km(n)
0

]
+ αm(n)g

∗
m(n)

Ebm(n)

[
bm(n)

1

])

= αm(n)− αm(n)g
∗
m(n)

Ebm(n)

[
Xtm(n) x(n−m)

] [ bm(n)
1

]

= αm(n)− αm(n)g
∗
m(n)

Ebm(n)
gm(n, n)

= αm(n)− α
2
m(n)|gm(n)|2
Ebm(n)

Thus, we have obtained both the order-update and time-update equations for the

(4.60)

(4.61)

(4.62)

(4.63)

Adaptive Filters

(n) in (4.60) may also be written as

By using (4.62) and the relation in (4.59), we obtain the order-update equa

basic least-squares lattice shown in Figure 4.1. The basic equations are (4.41)
and (4.44) for the forward and backward errors, usually called the residuals,
(4.36) and (4.37) for the corresponding least-squares errors, the time-update

957



m rame
ter αm(n). Initially, we have

Efm(−1) = Ebm(−1) = Ebm(−2) = 
 > 0
fm(−1) = gm(−1) = km(−1) = 0
αm(−1) = 1, α−1(n) = α−1(n− 1) = 1

Joint Process Estimation. The last step in the derivation is to obtain the least-squares
estimate of the desired signal d(n) from the lattice. Suppose that the adaptive filter
has m + 1 coefficients, which are determined to minimize the average weighted
squared error

�m+1 =
n∑
l=0

wn−l |em+1(l, n)|2

where
em+1(l, n) = d(l)− htm+1(n)Xm+1(l)

The linear estimate
d̂(l, n) = htm+1(n)Xm+1(l)

which will be obtained from the lattice by using the residuals gm(n), is called the joint
process estimate.

hm+1(n) = Pm+1(n)Dm+1(n)

We have also established that hm(n) satisfies the time-update equation given in

Now, let us obtain an order-update equation for hm(n).

hm+1(n) =
[

Pm(n) 0
0 0

] [
Dm(n)
· · ·

]
+ 1
Ebm(n)

[
bm(n)

1

] [
bHm (n) 1

]
Dm+1

We define a complex-valued scalar quantity δm(n) as

δm(n) =
[
bHm (n) 1

]
Dm+1(n)

hm+1(n) =
[

hm(n)
0

]
+ δm(n)
Ebm(n)

[
bm(n)

1

]

(4.64)

(4.65)

(4.66)

(4.67)

(4.68)

(n) (4.69)

(4.70)

(4.71)

Adaptive Filters

Equation (4.58) for k (n), and the order-update Equation (4.63) for the pa -

From the results of Section 3.1, we have already established that the coeffi-
cients of the adaptive filter that minimize (4.65) are given by the equation

(3.27).
From (4.68) and

(4.27), we have

Then, (4.69) may be expressed as

958



The scalar δm(n) satisfies a time-update equation that is obtained from the time-
update equations for bm(n) and Dm tively.
Thus,

δm(n) =
[
bHm (n− 1)−KHm (n)g∗m(n) 1

]
[wDm+1(n− 1)+ d(n)X∗m+1(n)]

= wδm(n− 1)+
[
bHm (n− 1) 1

]
X∗m+1(n)d(n)

− wg∗m(n)
[
KHm (n) 0

]
Dm+1(n− 1)− g∗m(n)d(n)

[
KHm (n) 0

]
X∗m+1(n)

But

[
bHm (n− 1) 1

]
X∗m+1(n) = x∗(n−m)+ bHm (n− 1)X∗m(n) = g∗m(n)

Also,

[
KHm (n) 0

]
Dm+1(n− 1) = 1

w + µm(n)
[
Xtm(n)Pm(n− 1) 0

] [Dm(n− 1)
· · ·

]

= 1
w + µm(n)X

t
m(n)hm(n− 1)

[
KHm (n) 0

] [X∗m(n)
· · ·

]
= 1
w + µm(n)X

t
m(n)Pm(n− 1)X∗m(n)

= µm(n)
w + µm(n)

time-
update equation for δm(n) as

δm(n) = wδm(n− 1)+ αm(n)g∗m(n)em(n)

Order-update equations for αm(n) and gm(n) have already been derived. With
e0(n) = d(n), the order-update equation for em(n) is obtained as follows:

em(n) = em(n, n− 1) = d(n)− htm(n− 1)Xm(n)

= d(n)− [htm−1(n− 1) 0]
[

Xm−1(n)
· · ·

]

− δm−1(n− 1)
Eb
m−1(n− 1)

[
btm−1(n− 1) 1

]
Xm(n)

= em−1(n)− δm−1(n− 1)gm−1(n)
Eb
m−1(n− 1)

(4.72)

(4.73)

(4.74)

(4.75)

(4.76)

(4.77)

Adaptive Filters

(n), given by (4.51) and (3.17), respec

The last term in (4.72) may be expressed as

Upon substituting the results in (4.73–4.75) into (4.72), we obtain the desired

959



Stage
1

Stage
2

Stage
M�1

g0(n)

e1(n) e2(n)

x(n)

d(n) eM(n)

d̂(n)

f0(n)

g1(n)

f1(n)

g2(n)

f2(n)

gM�2(n)

fM�2(n)

gM�1(n)

fM�1(n)

eM�1(n)e3(n)�

�

�

�

�

�

�

�

�

�

�M�1(n�1)

EbM�1(n�1)

�M�2(n�1)

EbM�2(n�1)

�2(n�1)

Eb2(n�1)

�1(n�1)

Eb1(n�1)

�0(n�1)

Eb0(n�1)

�
� �

� �

Figure 4.2 Adaptive RLS lattice-ladder filter.

Finally, the output estimate d(n) of the least-squares lattice is

d̂(n) = htm+1(n− 1)Xm+1(n)

But ht
m+1(n − 1) is not computed explicitly. By repeated substitution of the order-

update equation for hm+1
expression for d̂(n) in the form

d̂(n) =
M−1∑
k=0

δk(n− 1)
Ebk (n− 1)

gk(n)

In other words, the output estimate d̂(n) is a linear weighted sum of the backward
residuals gk(n).

The adaptive least-squares lattice/joint-process (ladder) estimator is illustrated

called the a priori form of the RLS lattice-ladder algorithm in order to distinguish
it from another form of the algorithm, called the a posteriori form, in which the
coefficient vector hM(n) is used in place of hM(n−1) to compute the estimate d(n). In
many adaptive filtering problems, such as channel equalization and echo cancellation,
the a posteriori form cannot be used, because hM(n) cannot be computed prior to
the computation of d(n).

We now describe a number of modifications that can be made to the “conven-

(4.78)

(4.79)

Adaptive Filters

(n) given by (4.71) into (4.78), we obtain the desired

in Figure 4.2. This lattice-ladder structure is mathematically equivalent to the RLS
direct-form FIR filter. The recursive equations are sumarized in Table 5. This is

tional” RLS lattice-ladder algorithm given in Table 5.

960



TABLE 5 A Priori Form of the RLS Lattice-Ladder Algorithm

Lattice predictor: Begin with n = 1 and compute the order updates for m = 0, 1, . . . ,M − 2
km+1(n− 1) = wkm+1(n− 2)+ αm(n− 2)fm(n− 1)g∗m(n− 2)

�f
m+1(n− 1) = −

km+1(n− 1)
Ebm(n− 2)

�bm+1(n− 1) = −
k∗m+1(n− 1)
E
f
m(n− 1)

fm+1(n) = fm(n)+�fm+1(n− 1)gm(n− 1)
gm+1(n) = gm(n− 1)+�bm+1(n− 1)fm(n)

E
f

m+1(n− 1) = Efm(n− 1)−
|km+1(n− 1)|2
Ebm(n− 2)

Eb
m+1(n− 1) = Ebm(n− 2)−

|km+1(n− 1)|2
E
f
m(n− 1)

αm+1(n− 1) = αm(n− 1)− α
2
m(n− 1)|gm(n− 1)|2

Ebm(n− 1)
Ladder filter: Begin with n = 1 and compute the order updates for m = 0, 1, . . . ,M − 1

δm(n− 1) = wδm(n− 2)+ αm(n− 1)g∗m(n− 1)em(n− 1)

ξm(n− 1) = − δm(n− 1)
Ebm(n− 1)

em+1(n) = em(n)+ ξm(n− 1)gm(n)
Initialization

α0(n− 1) = 1, e0(n) = d(n), f0(n) = g0(n) = x(n)
E
f

0 (n) = Eb0 (n) = wEf0 (n− 1)+ |x(n)|2
αm(−1) = 1, km(−1) = 0
Ebm(−1) = Efm(0) = 
 > 0; δm(−1) = 0

Modified RLS Lattice Algorithms. The recursive equations in the RLS lattice algo-
Modifications can be made to

some of the equations without affecting the optimality of the algorithm. However,
some modifications result in algorithms that are more numerically robust when fixed-
point arithmetic is used in the implementation of the algorithms. We give a number
of basic relationships that are easily established from the above developments.

First, we have a relationship between the a priori and a posteriori error residuals.
A priori errors:

fm(n, n− 1) ≡ fm(n) = x(n)+ atm(n− 1)Xm(n− 1)
gm(n, n− 1) ≡ gm(n) = x(n−m)+ btm(n− 1)Xm(n)

(4.80)

Adaptive Filters

rithm given in Table 5 are by no means unique.

961



A posteriori errors:

fm(n, n) = x(n)+ atm(n)Xm(n− 1)
gm(n, n) = x(n−m)+ btm(n)Xm(n)

fm(n, n) = αm(n− 1)fm(n)
gm(n, n) = αm(n)gm(n)

Second, we may obtain time-update equations for the least-squares forward and

Efm(n) = q(n)+ atm(n)Q∗m(n)
= q(n)+ [atm(n− 1)−Ktm(n− 1)fm(n)] [wQ∗m(n− 1)+ x∗(n)Xm(n− 1)]
= wEfm(n− 1)+ αm(n− 1)|fm(n)|2

Ebm(n) = wEbm(n− 1)+ αm(n)|gm(n)|2

Third, we obtain a time-update equation for the Kalman gain vector, which is
not explicitly used in the lattice algorithm, but which is used in the fast FIR filter
algorithms. For this derivation, we also use the time-update equations for the forward

Km(n) = Pm(n)X∗m(n)

=
[

0 0
0 Pm−1(n− 1)

] [
x∗(n)

X∗
m−1(n− 1)

]

+ 1
E
f

m−1(n)

[
1

am−1(n)

] [
1 aHm−1(n)

] [ x∗(n)
X∗
m−1(n− 1)

]

=
[

0
Km−1(n− 1)

]
+ f

∗
m−1(n, n)

E
f

m−1(n)

[
1

am−1(n)

]

≡
[

Cm−1(n)
cmm(n)

]

(4.81)

(4.82)

(4.83)

(4.84)

(4.85)

Adaptive Filters

The basic relations between (4.80) and (4.81) are

These relations follow easily by using (4.50) and (4.51) in (4.81).

backward errors. For example, from (4.8) and (4.50) we obtain

Similarly, from (4.17) and (4.51) we obtain

Usually, (4.83) and (4.84) are used in place of the sixth and seventh equations
in Table 5.

and backward prediction coefficients given by (4.50) and (4.51). Thus, we have

962



where, by definition, Cm−1(n) consists of the first (m − 1) elements of Km(n) and
cmm
for Km(n) as

Km(n) =
[

Km−1(n)
0

]
+ g

∗
m−1(n, n)

Eb
m−1(n)

[
bm−1(n)

1

]

cmm(n) =
g∗
m−1(n, n)

Eb
m−1(n)

and, hence,

Km−1(n)+ cmm(n)bm−1(n) = Cm−1(n)

By substituting for bm−1

Km−1(n) = Cm−1(n)− cmm(n)bm−1(n− 1)1− cmm(n)gm−1(n)

There is also a time-update equation for the scalar αm(n).
have

αm(n) = αm−1(n)−
α2
m−1(n)|gm−1(n)|2

Eb
m−1(n)

= αm−1(n)[1− cmm(n)gm−1(n)]

m−1(n) in the expres-
sion for αm(n). Then,

αm(n) = 1− Xtm(n)Km(n)

= αm−1(n− 1)
[

1− f
∗
m−1(n, n)fm−1(n)

E
f

m−1(n)

]

αm(n)

as

αm−1(n) = αm−1(n− 1)




1− f
∗
m−1(n,n)fm−1(n)

E
f

m−1(n)

1− cmm(n)gm−1(n)




(4.86)

(4.87)

(4.88)

(4.89)

(4.90)

(4.91)

(4.92)

Adaptive Filters

(n) is the last element. From (4.60), we also have the order-update equation

By equating (4.85) to (4.86), we obtain the result

(n) from (4.51) into (4.88), we obtain the time-update
equation for the Kalman gain vector in (4.85) as

From (4.63), we

A second relation is obtained by using (4.85) to eliminate K

By equating (4.90) to (4.91), we obtain the desired time-update equation for

963



Finally, we wish to distinguish between two different methods for updating the
reflection coefficients in the lattice filter and the ladder part: the conventional (indi-
rect) method and the direct method. In the conventional (indirect) method,

�f
m+1(n) = −

km+1(n)
Ebm(n− 1)

�bm+1(n) = −
k∗
m+1(n)

E
f
m(n)

ξm(n) = − δm(n)
Ebm(n)

where km+1 m and
E
f
m(n) and Ebm tuting for

km+1 equation in

�f
m+1(n) = −

km+1(n− 1)
Ebm(n− 2)

(
wEbm(n− 2)
Ebm(n− 1)

)
− αm(n− 1)fm(n)g

∗
m(n− 1)

Ebm(n− 1)

= �f
m+1(n− 1)

(
1− αm(n− 1)|gm(n− 1)|

2

Ebm(n− 1)
)

− αm(n− 1)fm(n)g
∗
m(n− 1)

Ebm(n− 1)

= �f
m+1(n− 1)−

αm(n− 1)fm+1(n)g∗m(n− 1)
Ebm(n− 1)

which is a formula for directly updating the reflection coefficients in the lattice. Sim-
equation

�bm+1(n) = �bm+1(n− 1)−
αm(n− 1)f ∗m(n)gm+1(n)

E
f
m(n)

Finally, the ladder gain can also be updated directly according to the relation

ξm(n) = ξm(n− 1)− αm(n)g
∗
m(n)em+1(n)
Ebm(n)

and backward residuals are fed back to time-update the reflection coefficients in the
lattice stage, and em+1(n) is fed back to update the ladder gain ξm(n). For this reason,
this RLS lattice-ladder algorithm has been called the error-feedback form. A similar
form can be obtained for the a posteriori RLS lattice-ladder algorithm. For more
details on the error-feedback form of RLS lattice-ladder algorithms, the interested
reader is referred to Ling, Manolakis, and Proakis (1986).

(4.93)

(4.94)

(4.95)

(4.96)

(4.97)

(4.98)

Adaptive Filters

(n) is time-updated from (4.58), δ (n) is updated according to (4.76),
(n) are updated according to (4.83) and (4.84). By substi

(n) from (4.58) into (4.93), and using (4.84) and the eighth
Table 5, we obtain

ilarly, by substituting (4.58) into (4.94), and using (4.83) and the eighth
in Table 5, we obtain

The RLS lattice-ladder algorithm that uses the direct update relations in (4.96–
4.98) and (4.83–4.84) is listed in Table 6.

An important characteristic of the algorithm in Table 6 is that the forward

964



TABLE 6 Direct Update (Error-Feedback) Form of the A Priori RLS Lattice-Ladder
Algorithm

Lattice predictor: Begin with n = 1 and compute the order updates for m = 0, 1, . . . ,M − 2

�fm+1(n− 1) = �fm+1(n− 2)−
αm(n− 2)fm+1(n− 1)g∗m(n− 2)

Ebm(n− 2)

�bm+1(n− 1) = �bm+1(n− 2)−
αm(n− 2)f ∗m(n− 1)gm+1(n− 1)

E
f
m(n− 1)

fm+1(n) = fm(n)+�fm+1(n− 1)gm(n− 1)
gm+1(n) = gm(n− 1)+�bm+1(n− 1)fm(n)

E
f

m+1(n− 1) = wEfm+1(n− 2)+ αm+1(n− 2)|fm+1(n− 1)|2

αm+1(n− 1) = αm(n− 1)− α
2
m(n− 1)|gm(n− 1)|2

Ebm(n− 1)
Eb
m+1(n− 1) = wEbm+1(n− 2)+ αm+1(n− 1)|gm+1(n− 1)|2

Ladder filter: Begin with n = 1 and compute the order updates m = 0, 1, . . . ,M − 1

ξm(n− 1) = ξm(n− 2)− αm(n− 1)g
∗
m(n− 1)em+1(n− 1)
Ebm(n− 1)

em+1(n) = em(n)+ ξm(n− 1)gm(n)
Initialization

α0(n− 1) = 1, e0(n) = d(n), f0(n) = g0(n) = x(n)
E
f

0 (n) = Eb0 (n) = wEf0 (n− 1)+ |x(n)|2

αm(−1) = 1, �fm(−1) = �bm(−1) = 0
Ebm(−1) = Efm(0) = 
 > 0

Fast RLS Algorithms. The two versions of the fast RLS algorithms given in Sec-

tion. In particular, we fix the size of the lattice and the associated forward and
backward predictors at M − 1 stages. Thus, we obtain the first seven recursive equa-
tions in the two versions of the algorithm. The remaining problem is to determine the

m (n)

to reduce the computations from 10M to 9M . Version A of the algorithm, given in

updating of the Kalman gain vector, they have been called fast Kalman algorithms
(for reference, see Falconer and Ljung (1978) and Proakis (1989)).

Further reduction of computational complexity to 7M is possible by directly
updating the following alternative (Kalman) gain vector (see Carayannis, Manolakis,
and Kalouptsidis (1983)) defined as

K̃M(n) = 1
w

PM(n− 1)X∗M(n) (4.99)

Adaptive Filters

tion 3.3 follow directly from the relationships that we have obtained in this sec-

time-update equation for the Kalman gain vector, which was determined in (4.85–
4.89). In version B of the algorithm, given in Table 3, we used the scalar α

Table 2, avoids the use of this parameter. Since these algorithms provide a direct

965



Several fast algorithms using this gain vector have been proposed, with complexi-

Sequential Technique) algorithm with a computational complexity 7M (for a deriva-

In general, the 7M fast RLS algorithms and some variations are very sensi-
tive to round-off noise and exhibit instability problems (Falconer and Ljung (1978),
Carayannis, Manolakis, and Kalouptsidis (1983; 1986), and Cioffi and Kailath (1984)).
The instability problem in the 7M algorithms has been addressed by Slock and
Kailath (1988; 1991), and modifications have been proposed that stabilize these algo-
rithms. The resulting stabilized algorithms have a computational complexity ranging
from 8M to 9M . Thus, their computational complexity is increased by a relatively
small amount compared to the unstable 7M algorithms.

To understand the stabilized fast RLS algorithms, we begin by comparing the

TABLE 7 FAEST Algorithm

fM−1(n) = x(n)+ atM−1(n− 1)XM−1(n− 1)

f̄ M−1(n, n) = fM−1(n)
ᾱM−1(n− 1)

aM−1(n) = aM−1(n− 1)− K̄M−1(n− 1)f̄ M−1(n, n)
E
f

M−1(n) = wEfM−1(n− 1)+ fM−1(n)f ∗M−1(n, n)

K̄M(n) ≡
[

C̄M−1(n)
c̄MM(n)

]
=
[

0
K̄M−1(n− 1)

]
+ f

∗
M−1(n)

wE
f

M−1(n− 1)

[
1

aM−1(n− 1)
]

gM−1(n) = −wEbM−1(n− 1)c̄∗MM(n)
K̄M−1(n) = C̄M−1(n)− bM−1(n− 1)c̄MM(n)

ᾱM(n) = ᾱM−1(n− 1)+ |fM−1(n)|
2

wE
f

M−1(n− 1)
ᾱM−1(n) = ᾱM(n)+ gM−1(n)c̄MM(n)

ḡM−1(n, n) = gM−1(n)
ᾱM−1(n)

Eb
M−1(n) = wEbM−1(n− 1)+ gM−1(n)ḡ∗M−1(n, n)

bM−1(n) = bM−1(n− 1)+ K̄M−1(n)ḡM−1(n, n)
eM(n) = d(n)− htM(n− 1)XM(n)

ēM(n, n) = eM(n)
ᾱM(n)

hM(n) = hM(n− 1)+ K̄M(n)ēM(n, n)
Initialization: Set all vectors to zero

E
f

M−1(−1) = EbM−1(−1) = 
 > 0
ᾱM−1(−1) = 1

Adaptive Filters

ties ranging from 7M to 10M . Table 7 lists the FAEST (Fast A Posteriori Error

tion, see Carayannis, Manolakis, and Kalouptsidis (1983; 1986) and Problem 7).

fast RLS algorithm given in Table 3 and the FAEST algorithm in Table 7. As

966



indicated, there are two major differences between these two algorithms. First, the
FAEST algorithm uses the alternative (Kalman) gain vector instead of the Kalman
gain vector. Second, the fast RLS algorithm computes the a priori backward predic-
tion error gM−1(n) through FIR filtering using the backward prediction coefficient
vector bm−1(n − 1), whereas the FAEST algorithm computes the same quantity
through a scalar operation by noticing that the last element of the alternative gain
vector, c̃MM(n), is equal to −wEbM−1gM−1(n). Since these two algorithms are al-
gebraically equivalent, the backward prediction errors calculated in different ways
should be identical if infinite precision is used in the computation. Practically, when
finite-precision arithmetic is used, the backward prediction errors computed using
different formulas are only approximately equal. In what follows, we denote them
by g(f )

M−1 and g
(s)

M−1(n), respectively. The superscripts (f ) and (s) indicate that they
are computed using the filtering approach and scalar operation, respectively.

There are other quantities in the algorithms that can also be computed in dif-
ferent ways. In particular, the parameter αM−1(n) can be computed from the vector
quantities K̃M−1(n) and XM−1(n) as

αM−1(n) = 1+ K̃tM−1(n)XM−1(n)

or from scalar quantities. We denote these values as α̃(f )
M−1(n) and α̃

(s)

M−1(n), respec-
tively. Finally, the last element of K̃M(n), denoted as c̃

(f )

MM(n), may be computed from
the relation

c̃
(f )

MM(n) =
−g(f )

M−1(n)

wEb
M−1(n− 1)

The two quantities in each of the three pairs [g(f )
M−1(n), g

(s)

M−1(n)], [α
(f )

M−1(n),
α
(s)

M−1(n)], and [c̃
(f )

MM(n), c̃
(s)
MM(n)] are algebraically equivalent. Hence, either of the

two quantities or their linear combination (of the form kβ(s) + (1− k)β(f ) , where β
represents any of the three parameters) are algebraically equivalent to the original
quantities, and may be used in the algorithm. Slock and Kailath (1988; 1991) found
that by using the appropriate quantity or its linear combination in the fast RLS
algorithm, it was sufficient to correct for the positive feedback inherent in the fast
RLS algorithms. Implementation of this basic notion leads to the stabilized fast RLS

stants ki, i = 1, 2, . . . , 5, to form five linear combinations of the three pairs of quanti-
ties just described. The best values of the ki found by Slock and Kailath resulted from
computer search, and are given as k1 = 1.5, k2 = 2.5, k3 = 1, k4 = 0, k5 = 1. When
ki = 0 or 1, we use only one of the quantities in the linear combination. Hence, some
of the parameters in the three pairs need not be computed. It was also found that
the stability of the algorithm is only slightly affected if α(f )

M−1(n) is not used. These

8M and is numerically stable.
The performance of the stabilized fast RLS algorithms depends highly on proper

initialization. On the other hand, an algorithm that uses g(f )
M−1(n) in its computations

(4.100)

(4.101)

Adaptive Filters

algorithm given in Table 8.

simplifications result in the algorithm given in Table 9, which has a complexity of

We observe from Table 8 that the stabilized fast RLS algorithm employs con-

967



The Stabilized Fast RLS Algorithm

fM−1(n) = x(n)+ atM−1(n− 1)XM−1(n− 1)

fM−1(n, n) = fM−1(n)
ᾱM−1(n− 1)

aM−1(n) = aM−1(n− 1)− K̄M−1(n− 1)fM−1(n, n)

c̄M1(n) =
f ∗
M−1(n)

wE
f

M−1(n− 1)[
C̄M−1(n)
c̄
(s)
MM(n)

]
=
[

0
K̄M−1(n− 1)

]
+ c̄M1(n)

[
1

aM−1(n− 1)
]

g
(f )

M−1(n) = x(n−M + 1)+ btM−1(n− 1)XM−1(n)

c̄
(f )

MM(n) = −
g
(f )∗
M−1(n)

wEb
M−1(n− 1)

c̄MM(n) = k4c̄(f )MM(n)+ (1− k4)c̄(s)MM(n)

K̄M(n) =
[

C̄M−1(n)
c̄MM(n)

]

g
(s)

M−1(n) = −wEbM−1(n− 1)c̄(s)∗MM(n)
g
(i)

M−1(n) = kig(f )M−1(n)+ (1− ki)g(s)M−1(n), i = 1, 2, 5
K̄M−1(n) = C̄M−1(n)− bM−1(n− 1)c̄MM(n)
ᾱM(n) = ᾱM−1(n− 1)+ c̄M1(n)fM−1(n)

ᾱ
(s)

M−1(n) = ᾱM(n)+ g(s)M−1(n)c̄(s)MM(n)
ᾱ
(f )

M−1(n) = 1+ K̄tM−1(n)XM−1(n)
ᾱM−1(n) = k3ᾱ(f )M−1(n)+ (1− k3)ᾱ(s)M−1(n)
E
f

M−1(n) = wEfM−1(n− 1)+ fM−1(n)f ∗M−1(n, n)[
or, 1

E
f

M−1(n)
= 1
w

1

E
f

M−1(n− 1)
− |c̄M1(n)|

2

ᾱ
(s)

M−1(n)

]

g
(i)

M−1(n, n) =
g
(i)

M−1(n)
ᾱM−1(n)

, i = 1, 2

bM−1(n) = bM−1(n− 1)+ K̄M−1(n)g(1)M−1(n, n)
Eb
M−1(n) = wEbM−1(n− 1)+ g(2)M−1(n)g(2)∗M−1(n, n)
eM(n) = d(n)− htM(n− 1)XM(n)

eM(n, n) = eM(n)
ᾱM(n)

hM(n) = hM(n− 1)+ K̄M(n)eM(n, n)

Adaptive Filters

TABLE 8

968



A Simplified Stabilized Fast RLS Algorithm

fM−1(n) = x(n)+ atM−1(n− 1)XM−1(n− 1)

fM−1(n, n) = fM−1(n)
ᾱM−1(n− 1)

aM−1(n) = aM−1(n− 1)− K̄M−1(n− 1)fM−1(n, n)

c̄M1(n) =
f ∗
M−1(n)

wE
f

M−1(n− 1)

K̄M(n) ≡
[

C̄M−1(n)
c̄MM(n)

]
=
[

0
K̄M−1(n− 1)

]
+ f

∗
M−1(n)

wE
f

M−1(n− 1)

[
1

aM−1(n− 1)
]

g
(f )

M−1(n) = x(n−M + 1)+ btM−1(n− 1)XM−1(n)
g
(s)

M−1(n) = −wEbM−1(n− 1)c̄∗MM(n)
g
(i)

M−1(n) = kig(f )M−1(n)+ (1− ki)g(s)M−1(n), i = 1, 2
K̄M−1(n) = C̄M−1(n)− bM−1(n− 1)c̄MM(n)
ᾱM(n) = ᾱM−1(n− 1)+ c̄M1(n)fM−1(n)

ᾱM−1(n) = ᾱM(n)+ g(f )M−1(n)c̄MM(n)
E
f

M−1(n) = wEfM−1(n− 1)+ fM−1(n)f ∗M−1(n, n)

g
(i)

M−1(n, n) =
g
(i)

M−1(n)
ᾱM−1(n)

, i = 1, 2

bM−1(n) = bM−1(n− 1)+ K̄M−1(n)g(1)M−1(n, n)
Eb
M−1(n) = wEbM−1(n− 1)+ g(2)M−1(n)g(2)∗M−1(n, n)
eM(n) = d(n)− htM(n− 1)XM(n)

eM(n, n) = eM(n)
ᾱM(n)

hM(n) = hM(n− 1)+ K̄M(n)eM(n, n)

is not critically affected by proper initialization (although, enventually, it will di-
verge). Consequently, we may initially use g(f )

M−1(n) in place of g
(s)

M−1(n) (or their
linear combination) for the first few hundred iterations, and then switch to the form
for the stabilized fast RLS algorithm. By doing so, we obtain a stabilized fast RLS
algorithm that is also insensitive to initial conditions.

4.2 Other Lattice Algorithms

Another type of RLS lattice algorithm is obtained by normalizing the forward and

backward prediction errors through division of the errors by
√
E
f
m(n) and

√
E
b
m(n),

respectively, and multiplication by
√
αm(n− 1) and

√
αm(n), respectively. The re-

sulting lattice algorithm is called a square-root or angle-and-power normalized RLS

Adaptive Filters

TABLE 9

969



lattice algorithm. This algorithm has a more compact form than the other forms
of RLS lattice algorithms. However, the algorithm requires many square-root op-
erations, which can be computationally complex. This problem can be solved by
using CORDIC processors, which compute a square root in N clock cycles, where
N is the number of bits of the computer word length. A description of the square-
root/normalized RLS lattice algorithm and the CORDIC algorithm is given in the
book by Proakis et al. (2002).

It is also possible to simplify the computational complexity of the RLS algorithms
described in the previous section at the expense of compromising the convergence
rate. One such algorithm is called the gradient-lattice algorithm. In this algorithm
each stage of the lattice filter is characterized by the output–input relations

fm(n) = fm−1(n)− km(n)gm−1(n− 1)
gm(n) = gm−1(n− 1)− k∗m(n)fm−1(n)

where km(n) is the reflection coefficient in the mth stage of the lattice and fm(n) and
gm(n) are the forward and backward residuals.

This form of the lattice filter is identical to the Levinson-Durbin algorithm, except
that now km(n) is allowed to vary with time so that the lattice filter adapts to the time

m(n)} may be optimized
by employing the method of least squares, which results in the solution

km(n) =
2
∑n

l=0w
n−lfm−1(l)g∗m−1(l − 1)∑n

l=0wn−l[|fm−1(l)|2 + |gm−1(l − 1)|2]
, m = 1, 2, . . .M − 1

These coefficients can also be updated recursively in time. The ladder coefficients are
computed recursively in time by employing an LMS-type algorithm that is obtained
by applying the mean-square-error criterion. A description of this algorithm is given
in the paper by Griffiths (1978) and in the book by Proakis et al. (2002).

4.3 Properties of Lattice-Ladder Algorithms

The lattice algorithms that we have derived in the two previous subsections have
a number of desirable properties. In this subsection, we consider the properties of
these algorithms and compare them with the corresponding properties of the LMS
algorithm and the RLS direct-form FIR filtering algorithms.

Convergence Rate. The RLS lattice-ladder algorithms basically have the same con-
vergence rate as the RLS direct-form FIR filter structures. This characteristic be-
havior is not surprising, since both filter structures are optimum in the least-squares
sense. Although the gradient lattice algorithm retains some of the optimal character-
istics of the RLS lattice, nevertheless the former is not optimum in the least-squares
sense and, hence, its convergence rate is slower.

(4.102)

(4.103)

Adaptive Filters

variations in the signal statistics. The reflection coefficients {k

970



1000�3.0

�2.0

�1.0

0.0

200 300 400 500 600 700 800 900

L
og

 o
f 

ou
tp

ut
 m

ea
n-

sq
ua

re
 e

rr
or

Number of iterations

Gradient algorithm (LMS)

Optimum

Least-squares lattice algorithm

Gradient lattice algorithm

Channel-correlation matrix
Eigenvalue ratio � 11

11-tap equalizer, noise variance � 0.001

Figure 4.3 Learning curves for RLS lattice, gradient lattice, and
LMS algorithms for adaptive equalizer of length M = 11. (From
Digital Communications by John G. Proakis. ©1989 by McGraw-
Hill Book Company. Reprinted with permission of the publisher.)

for an adaptive equalizer of length M = 11, implemented as an RLS lattice-ladder
filter, a gradient lattice-ladder filter, and a direct-form FIR filter using the LMS algo-
rithm, for a channel autocorrelation matrix that has eigenvalue ratios of λmax/λmin =
11 and λmax/λmin = 21, respectively. From these learning curves, we observe that the
gradient lattice algorithm takes about twice as many iterations to converge as the op-
timum RLS lattice algorithm. Furthermore, the gradient lattice algorithm provides
significantly faster convergence than the LMS algorithm. For both lattice structures,
the convergence rate does not depend on the eigenvalue spread of the correlation
matrix.

Computational Requirements. The RLS lattice algorithms described in the previous
subsection have a computational complexity that is proportional to M . In contrast,
the computational complexity of the RLS square-root algorithms is proportional to
M2 . On the other hand, the direct-form fast algorithms, which are a derivative of the
lattice algorithm, have a complexity proportional to M , and they are a little more
efficient than the lattice-ladder algorithms.

multiplications and divisions) of the various adaptive filtering algorithms that we
have described. Clearly, the LMS algorithm requires the fewest computations. The

rithms shown, closely followed by the gradient lattice algorithm, then the RLS lattice
algorithms and, finally, the square-root algorithms. Note that for small values of M ,
there is little difference in complexity among the rapidly convergent algorithms.

Numerical Properties. In addition to providing fast convergence, the RLS and gra-
dient lattice algorithms are numerically robust. First, these lattice algorithms are

Adaptive Filters

For comparison purposes, Figures 4.3 and 4.4 illustrate the learning curves

In Figure 4.5 we illustrate the computational complexity (number of complex

fast RLS algorithms in Tables 3 and 9 are the most efficient of the RLS algo-

971



1000
�3.0

�2.0

�1.0

0.0

200 300 400 500 600 700 800 900L
og

 o
f 

ou
tp

ut
 m

ea
n-

sq
ua

re
 e

rr
or

Number of iterations

Gradient algorithm (LMS)

Optimum

Least-squares lattice algorithm

Gradient lattice algorithm

Channel-correlation matrix
Eigenvalue ratio � 21

11-tap equalizer, noise variance � 0.001

Figure 4.4 Learning curves for RLS lattice, gradient lattice,
and LMS algorithms for adaptive equalizer of length M = 11.
(From Digital Communications by John G. Proakis. ©1989 by
McGraw-Hill Book Company. Reprinted with permission of the
publisher.)

Direct-form
RLS square-root
algorithm

RLS
lattice-ladder
algorithm

Gradient
lattice-ladder

Fast RLS
algorithm

LMS algorithm

5 10 15 20 250

100

200

300

400

500

600

700

M

M ~ Length of filter

N
um

be
r 

of
 c

om
pl

ex
 m

ul
tip

lic
at

io
ns

 a
nd

 d
iv

is
io

ns

Figure 4.5 Computational complexity of adaptive filter algo-
rithms.

Adaptive Filters

972



TABLE 10 Numerical Accuracy, in Terms of Output MSE for Channel with λmax/λmin = 11
and w = 0.975, MSE × 10−3

Algorithm

Number of bits RLS Fast Conventional Error feedback

(including sign) square root RLS RLS lattice RLS lattice LMS

16 2.17 2.17 2.16 2.16 2.30

13 2.33 2.21 3.09 2.22 2.30

11 6.14 3.34 25.2 3.09 19.0

9 75.3 a 365 31.6 311
aAlgorithm did not converge.

numerically stable, which means that the output estimation error from the computa-
tional procedure is bounded when a bounded error signal is introduced at the input.
Second, the numerical accuracy of the optimum solution is also relatively good when
compared to the LMS and the RLS direct-form FIR algorithms.

squared error or (estimated) minimum MSE obtained through computer simulation
from the two RLS lattice algorithms and the direct-form FIR filter algorithms de-

obtained with the RLS lattice-ladder algorithm, in which the reflection coefficients

error-feedback form of the RLS lattice algorithm. It is clear that the direct updat-
ing of these coefficients is significantly more robust to round-off errors than all the
other adaptive algorithms, including the LMS algorithm. It is also apparent that
the two-step process used in the conventional RLS lattice algorithm to estimate the
reflection coefficients is not as accurate. Furthermore, the estimation errors that are
generated in the coefficients at each stage propagate from stage to stage, causing
additional errors.

The effect of changing the weighting factor w is illustrated in the numerical
In this table, we give the minimum (estimated) MSE

TABLE 11 Numerical Accuracy, in Terms of Output MSE, of A Priori Least-Squares Lattice Al-
gorithm with Different Values of the Weighting Factor w, MSE, × 10−3

Algorithm

w = 0.99 w = 0.975 w = 0.95
Number of bits Error Error Error

with sign Conventional feedback Conventional feedback Conventional feedback

16 2.14 2.08 2.18 2.16 2.66 2.62

13 7.08 2.11 3.09 2.22 3.65 2.66

11 39.8 3.88 25.2 3.09 15.7 2.78

9 750 44.1 365 31.6 120 15.2

Adaptive Filters

For purposes of comparison, we illustrate in Table 10 the steady-state average

scribed in Section 2. The striking result in Table 10 is the superior performance

and the ladder gain are updated directly according to (4.96–4.98). This is the

results given in Table 11.

973



obtained with the conventional and error-feedback forms of the RLS lattice algo-
rithm. We observe that the output MSE decreases with an increase in the weighting
factor when the precision is high (13 bits and 16 bits). This reflects the improvement
in performance obtained by increasing the observation interval. As the number of
bits of precision is decreased, we observe that the weighting factor should also be
decreased in order to maintain good performance. In effect, with low precision, the
effect of a longer averaging time results in a larger round-off noise. Of course, these
results were obtained with time-invariant signal statistics. If the signal statistics are
time-variant, the rate of the time variations will also influence the choice of w .

In the gradient lattice algorithm, the reflection coefficients and the ladder gains
are also updated directly. Consequently, the numerical accuracy of the gradient
lattice algorithm is comparable to that obtained with the direct update form of the
RLS lattice.

Analytical and simulation results on numerical stability and numerical accuracy
in fixed-point implementation of these algorithms can be found in Ling and Proakis
(1984), Ling, Manolakis, and Proakis (1985; 1986), Ljung and Ljung (1985), and
Gardner (1984).

Implementation Considerations. As we have observed, the lattice filter structure is
highly modular and allows for the computations to be pipelined. Because of the
high degree of modularity, the RLS and gradient lattice algorithms are particularly
suitable for implementation in VLSI. As a result of this advantage in implementa-
tion and the desirable properties of stability, excellent numerical accuracy, and fast
convergence, we anticipate that adaptive filters will be increasingly implemented as
lattice-ladder structures in the near future.

5 Summary and References

We have presented adaptive algorithms for direct-form FIR and lattice filter struc-
tures. The algorithms for the direct-form FIR filter consisted of the simple LMS
algorithm due to Widrow and Hoff (1960) and the direct-form, time-recursive least-

the square-root RLS forms described by Bierman (1977), Carlson and Culmone
(1979), and Hsu (1982), and the RLS fast Kalman algorithms, one form of which was
described by Falconer and Ljung (1978), and other forms later derived by Carayannis,
Manolakis, and Kalouptsidis (1983), Proakis (1989), and Cioffi and Kailath (1984).

Of these algorithms, the LMS algorithm is the simplest. It is used in many appli-
cations where its slow convergence is adequate. Of the direct-form RLS algorithms,
the square-root algorithms have been used in applications where fast convergence
is required. The algorithms have good numerical properties. The family of stabi-
lized fast RLS algorithms is very attractive from the viewpoint of computational
efficiency. Methods to avoid instability due to round-off errors have been proposed
by Hsu (1982), Cioffi and Kailath (1984), Lin (1984), Eleftheriou and Falconer (1987),
and Slock and Kailath (1988; 1991).

The adaptive lattice-ladder filter algorithm derived in this chapter is the optimum
RLS lattice-ladder algorithms (in both conventional and error-feedback form). Only
the a priori form of the lattice-ladder algorithm was derived, which is the form most

Adaptive Filters

squares algorithms, including the conventional RLS form given by (3.23–3.27),

974



often used in applications. In addition, there is an a posteriori form of the RLS
lattice-ladder algorithms (both conventional and error-feedback), as described by
Ling, Manolakis, and Proakis (1986). The error-feedback form of the RLS lattice-
ladder algorithm has excellent numerical properties, and is particularly suitable for
implementation in fixed-point arithmetic and in VLSI.

In the direct-form and lattice RLS algorithms, we used exponential weighting
into the past in order to reduce the effective memory in the adaptation process. As an
alternative to exponential weighting, we may employ finite-length uniform weighting
into the past. This approach leads to the class of finite-memory RLS direct-form and
lattice structures described in Cioffi and Kalaith (1985) and Manolakis, Ling, and
Proakis (1987).

In addition to the various algorithms that we have presented in this chapter, there
has been considerable research into efficient implementation of these algorithms
using systolic arrays and other parallel architectures. The reader is referred to Kung
(1982), and Kung, Whitehouse, and Kailath (1985).

Problems

1 Use the least-squares criterion to determine the equations for the parameters of the

noise, w(n).
2 Determine the equations for the coefficients of an adaptive echo canceler based on

the least-squares criterion.
presence of a near-end echo only.

3 If the sequences w1(n), w2(n), and w3(n) in the adaptive noise-canceling system

the estimated correlation sequences rvv(k) and ryv
4
5
6 Derive the equation for the reflection coefficients in a gradient lattice-algorithm

7
gain vector

K̃M(n) = 1
w

PM(n− 1)X∗M(n)

instead of the Kalman gain vector KM(n).
8 The tap-leaky LMS algorithm proposed by Gitlin, Meadors, and Weinstein (1982)

may be expressed as

hM(n+ 1) = whM(n)+�e(n)X∗M(n)

where 0 < w < 1,� is the step size, and XM(n) is the data vector at time n. Deter-
mine the condition for the convergence of the mean value of hM(n).

Adaptive Filters

FIR filter model in Figure 1.2, when the plant output is corrupted by additive

Use the configuration in Figure 1.8 and assume the

shown in Figure 1.14 are mutually uncorrelated, determine the expected value of
(k) contained in (1.26).

Prove the result in (4.34).
Derive the equation for the direct update of the ladder gain given by (4.98).

given in (4.103).
Derive the FAEST algorithm given in Table 7 by using the alternative Kalman

975



9
cost function

E(n) = |e(n)|2 + c||hM(n)||2
where c is a constant and e(n) is the error between the desired filter output and the
actual filter output. Show that the minimization of E(n) with respect to the filter
coefficient vector hM(n) leads to the following tap-leaky LMS algorithm.

hM(n+ 1) = (1−�c)hM(n)+�e(n)X∗M(n)
10

of step size � to ensure the stability of the algorithm in the mean-square-error sense.
11

number of computations.
12 Consider the random process

x(n) = gv(n)+ w(n), n = 0, 1, . . . ,M − 1
where v(n) is a known sequence, g is a random variable with E[g] = 0, and
E[g2] = G. The process w(n) is a white noise sequence with

γww(m) = σ 2wδ(m)
Determine the coefficients of the linear estimator for g , that is,

ĝ =
M−1∑
n=0

h(n)x(n)

that minimizes the mean-square error

E = e[(g − ĝ)2]
13 Recall that an FIR filter can be realized in the frequency-sampling form with system

function

H(z) = 1− z
−M

M

M−1∑
k=0

Hk

1− ej2πk/Mz−1

= H1(z)H2(z)
where H1(z) is the comb filter and H2(z) is the parallel bank of resonators.
(a) Suppose that this structure is implemented as an adaptive filter using the LMS

algorithm to adjust the filter (DFT) parameters Hk . Give the time-update equa-
tion for these parameters. Sketch the adaptive filter structure.

(b) Suppose that this structure is used as an adaptive channel equalizer, in which the
desired signal is

d(n) =
M−1∑
k=0

Ak cosωkn, ωk = 2πk
M

Adaptive Filters

The tap-leaky LMS algorithm in Problem 8 may be obtained by minimizing the

For the normalized LMS algorithm given by (2.31), determine the range of values

By using the alternative Kalman gain vector given in Problem 8, modify the a
priori fast least-squares algorithms given in Tables 2 and 3, and thus reduce the

976



With this form for the desired signal, what advantages are there in the LMS adaptive
algorithm for the DFT coefficientsHk over the direct-form structure with coefficients
h(n)? (Hint: Refer to Proakis (1970).)

14 Consider the performance index

J = h2 − 40h+ 28

Suppose that we search for the minimum of J by using the steepest-descent
algorithm

h(n+ 1) = h(n)− 1
2
�g(n)

where g(n) is the gradient.

(a) Determine the range of values of � that provides an overdamped system for the
adjustment process.

(b) Plot the expression for J as a function of n for a value of � in this range.

15
additive noise processes are white and mutually uncorrelated, with equal variances
σ 2w . Suppose that the linear system has a known system function

H(z) = 1
1− 12z−1

Determine the optimum weights of a three-tap noise canceler that minimizes the
MSE.

16 Determine the coefficients a1 and a2
given that the autocorrelation γxx(n) of the input signal is

γxx(m) = a|m|, 0 < a < 1

Figure P16

Z�1 Z�1

e(n)x(n)
�

�

a2a1

Adaptive Filters

Consider the noise-canceling adaptive filter shown in Figure 1.14. Assume that the

for the linear predictor shown in Figure P16,

977



17 Determine the lattice filter and its optimum reflection coefficients corresponding to

18
terized by the system function

C(z) = 1
1+ 0.9z−1

Determine the optimum coefficients of the adaptive FIR filter B(z) = b0 + b1z−1
that minimize the MSE. The additive noise is white with variance σ 2w = 0.1.

C(z)
Adaptive

FIR
filter

w(n)

x(n) e(n)
�

�

19 In the gradient lattice algorithm, the forward and backward prediction errors are

(a) Show that the minimization of the least-squares error

ELSm =
n∑
l=0

wn−l[|f 2 + |gm(n)|2]

with respect to the reflection coefficients {km(n)} results in the equation given

(b) To determine an equation for the recursive computation of the reflection coeffi-

um(n) = wum(n− 1)+ 2fm−1(n)g∗m−1(n− 1)
υm(n) = wvm(n− 1)+ |fm−1(n)|2 + |gm−1(n− 1)|2

so that km(n) = um(n)/υm(n). Then, show that km(n) can be computed recur-
sively by the relation

km(n) = km(n− 1)+
fm(n)g

∗
m−1(n− 1)+ g∗m(n)fm−1(n)

wυm(n− 1)
20

(a) Determine the quadratic performance index and the optimum parameters for
the signal

x(n) = sin nπ
4
+ w(n)

where w(n) is white noise with variance σ 2w = 0.1.

m (n)|

Adaptive Filters

the linear predictor in Problem 16.
Consider the adaptive FIR filter shown in Figure P18. The system C(z) is charac-

Figure P18

given by (4.102).

by (4.103).

cients given by (4.103) define

Consider the adaptive predictor shown in Figure P16.

978



(b) Generate a sequence of 1000 samples of x(n), and use the LMS algorithm to
adaptively obtain the predictor coefficients. Compare the experimental results
with the theoretical values obtained in part (a). Use a step size of � ≤ 110�max.

(c) Repeat the experiment in part (b) for N = 10 trials with different noise se-
quences, and compute the average values of the predictor coefficients. Comment
on how these results compare with the theoretical values in part (a).

21 An autoregressive process is described by the difference equation

x(n) = 1.26x(n− 1)− 0.81x(n− 2)+ w(n)
(a) Generate a sequence of N = 1000 samples of x(n), where w(n) is a white noise

sequence with variance σ 2w = 0.1. Use the LMS algorithm to determine the
parameters of a second-order (p = 2) linear predictor. Begin with a1(0) =
a2(0) = 0. Plot the coefficients a1(n) and a2(n) as a function of the iteration
number.

(b) Repeat part (a) for 10 trials, using different noise sequences, and superimpose
the 10 plots of a1(n) and a2(n).

(c) Plot the learning curve for the average (over the 10 trials) MSE for the data in
part (b).

22 A random process x(n) is given as

x(n) = s(n)+ w(n)
= sin(ω0n+ φ)+ w(n), ω0 = π/4, φ = 0

where w(n) is an additive white noise sequence with variance σ 2w = 0.1.
(a) Generate N = 1000 samples of x(n) and simulate an adaptive line enhancer of

length L = 4. Use the LMS algorithm to adapt the ALE.
(b) Plot the output of the ALE.

(c) Compute the autocorrelation γxx(m) of the sequence x(n).

(d) Determine the theoretical values of the ALE coefficients and compare them
with the experimental values.

(e) Compute and plot the frequency response of the linear predictor (ALE).

(f) Compute and plot the frequency response of the prediction-error filter.

(g) Compute and plot the experimental values of the autocorrelation ree(m) of the
output error sequence for 0 ≤ m < 10.

(h) Repeat the experiment for 10 trials, using different noise sequences, and super-
impose the frequency response plots on the same graph.

(i) Comment on the result in parts (a) through (h).

Adaptive Filters

979



This page intentionally left blank 



Random Number
Generators

In practice, random number generators are often used to simulate the effect of
noiselike signals and other random phenomena encountered in the physical world.
Such noise is present in electronic devices and systems and usually limits our ability
to communicate over large distances and to be able to detect relatively weak signals.
By generating such noise on a computer, we are able to study its effects through
simulation of communication systems, radar detection systems, and the like and to
assess the performance of such systems in the presence of noise.

Most computer software libraries include a uniform random number generator.
Such a random number generator generates a number between zero and 1 with equal
probability. We call the output of the random number generator a random variable.
If A denotes such a random variable, its range is the interval 0 ≤ A ≤ 1.

We know that the numerical output of a digital computer has limited precision,
and as a consequence, it is impossible to represent the continuum of numbers in the
interval 0 ≤ A ≤ 1. However, we can assume that our computer represents each
output by a large number of bits in either fixed point or floating point. Consequently,
for all practical purposes, the number of outputs in the interval 0 ≤ A ≤ 1 is suffi-
ciently large, so that we are justified in assuming that any value in the interval is a
possible output from the generator.

The uniform probability density function for the random variable A, denoted as
p(A), is illustrated in Fig. 1(a). We note that the average value or mean value of
A, denoted as mA , is mA = 12 . The integral of the probability density function, which

In some of the examples, random numbers are generated to simulate the effect of 
noise on signals and to illustrate how the method of correlation can be used to  
detect the presence of a signal buried in noise. In the case of periodic signals, the 
correlation technique also allowed us to estimate the period of the signal.

John G. Proakis, Dimitris G. Manolakis. Copyright © 2007 by Pearson Education, Inc. All rights reserved.
From Appendix   A of Digital Signal Processing: Principles, Algorithms, and Applications, Fourth Edition.

A P P E N D I X

981



1
2

0

p(A)

A
1

(a)

0

F(A)

A
1

1

1

(b)

represents the area under p(A), is called the probability distribution function of the
random variable A and is defined as

F(A) =
∫ A
−∞

p(x)dx

that can be achieved by a distribution function. Hence

F(1) =
∫ 1
−∞

p(x)dx = 1

and the range of F(A) is 0 ≤ F(A) ≤ 1 for 0 ≤ A ≤ 1.
If we wish to generate uniformly distributed noise in an interval (b, b+ 1) it can

simply be accomplished by using the output A of the random number generator and
shifting it by an amount b . Thus a new random variable B can be defined as

B = A+ b
which now has a mean value mB = b + 12 . For example, if b = −12 , the random
variable B is uniformly distributed in the interval (− 12 , 12

p(B)

B

(a)

1
2

1
2

0

F(B)

B

1

1

(b)

−

1
2

1
2

0−

Figure 1

(1)

(2)

For any random variable, this area must always be unity, which is the maximum value

(3)

), as shown in Fig 2(a).
Its probability distribution function F(B) is shown in Fig 2(b).

Figure 2

Appendix: Random Number Generators

982



F(C)

C

1

0C

A = F(C)

F(C) = A
C = F−1(A)

A uniformly distributed random variable in the range (0, 1) can be used to gen-
erate random variables with other probability distribution functions. For example,
suppose that we wish to generate a random variable C with probability distribution

we begin by generating a uniformly distributed random variable A in the range (0, 1).
If we set

F(C) = A
then

C = F−1(A)

Generate a random variable C that has the linear probability density function shown in

p(C) =
{
C

2
, 0 ≤ C ≤ 2

0, otherwise

p(C)

C

1

0 2

(a)

C
2

F(C)

C

1

0 2

(b)

1
2

C

(4)

(5)

which F(C) = A. By this means we obtain a new random variable C with probability
distribution F(C). This inverse mapping from A to C is illustrated in Fig 3.

Fig 4(a), that is,

Thus we solve (4) for C , and the solution in (5) provides the value of C for

Figure 4

Figure 3

function F(C), as illustrated in Fig 3. Since the range of F(C) is the interval (0, 1),

EXAMPLE 1

Appendix: Random Number Generators

983



Solution. This random variable has a probability distribution function

F(C) =



0, C < 0
1
4C

2, 0 ≤ C ≤ 2
1, C > 2

set F(C) = A. Hence
F(C) = 14C2 = A

Upon solving for C , we obtain
C = 2

√
A

−1(A) was simple. In some cases it
is not. This problem arises in trying to generate random numbers that have a normal
distribution function.

Noise encountered in physical systems is often characterized by the normal or
The probability

density function is given by

p(C) = 1√
2πσ

e−C
2/2σ2 , −∞ < C <∞

where σ 2 is the variance of C , which is a measure of the spread of the probability
density function p(C). The probability distribution function F(C) is the area under
p(C) over the range (−∞, C). Thus

F(C) =
∫ C
−∞

p(x)dx

F(C)

C

1

0

1
2

p(C)

C
0

(a)

(b)

(7)

(6)

which is illustrated in Fig 4(b). We generate a uniformly distributed random variable A and

Thus we generate a random variable C with probability function F(C), as shown in Fig 4(b).

In Example 1 the inverse mapping C = F

Gaussian probability distribution, which is illustrated in Fig 5.

Figure 5

Appendix: Random Number Generators

984



Consequently, the inverse mapping is difficult to achieve.
A way has been found to circumvent this problem. From probability theory it is

known that a (Rayleigh distributed) random variable R , with probability distribution
function

F(R) =
{

0, R < 0
1− e−R2/2σ 2, R ≥ 0

tion

C = R cos�
D = R sin�

where � is a uniformly distributed variable in the interval (0, 2π). The parameter
σ 2

F(R) = 1− e−R2/2σ 2 = A
and hence

R =
√

2σ 2 ln[1/(1− A)]

generate a second uniformly distributed random variable B and define

� = 2πB

ted random variables C and D .

C SUBROUTINE GAUSS CONVERTS A UNIFORM RANDOM
C SEQUENCE XIN IN [0,1] TO A GAUSSIAN RANDOM
C SEQUENCE WITH G(0,SIGMA**2)
C PARAMETERS :
C XIN :UNIFORM IN [0,1] RANDOM NUMBER
C B :UNIFORM IN [0,1] RANDOM NUMBER
C SIGMA :STANDARD DEVIATION OF THE GAUSSIAN
C YOUT :OUTPUT FROM THE GENERATOR
C

SUBROUTINE GAUSS 9XIN,B,SIGMA,YOUT)
PI=4.0*ATAN (1.0)
B=2.0*PI*B
R=SQRT (2.0*(SIGMA**2)*ALOG(1.0/(1.0-XIN)))
YOUT=R*COS(B)
RETURN
END

C NOTE: TO USE THE ABOVE SUBROUTINE FOR A
C GAUSSIAN RANDOM NUMBER GENERATOR
C YOU MUST PROVIDE AS INPUT TWO UNIFORM RANDOM NUMBERS
C XIN AND B
C XIN AND B MUST BE STATISTICALLY INDEPENDENT
C

Subroutine for generating Gaussian random variables

(8)

(9)

is related to a pair of Gaussian random variables C and D , through the transforma-

(10)

where A is a uniformly distributed random variable in the interval (0, 1). Now if we

(11)

(12)

Figure 6

then from (9) and (10), we obtain two statistically independent Gaussian distribu-

Unfortunately, the integral in (7) cannot be expressed in terms of simple functions.

is the variance of C and D . Since (8) is easily inverted, we have

Appendix: Random Number Generators

985



The method described above is often used in practice to generate Gaussian

mean value of zero and a variance σ 2 . If a nonzero mean Gaussian random variable
is desired, then C and D can be translated by the addition of the mean value.

A subroutine implementing this method for generating Gaussian distributed
random variables is given in Fig 6.

distributed random variables. As shown in Fig 5, these random variables have a

Appendix: Random Number Generators

986



Tables of Transition
Coefficients for the
Design of Linear-Phase
FIR Filters

ation of Hr (ω) at a set of equally spaced frequencies ωk = 2π (k + α)/M , where
α = 0 o r α = 12 , k = 0, 1, . . . , (M − 1)/2 for M odd and k = 0, 1, 2, . . . , (M/2)−1 for M even, where M is the length of the filter. Within the passband of the filter,
we select Hr(ωk) = 1, and in the stopband, Hr (ωk) = 0. For frequencies in the
transition band, the values of Hr(ωk) are optimized to minimize the maximum
sidelobe in the stopband. This is called a minimax optimization criterion.

The optimization of the values of Hr(ω) in the transition band has been per-
formed by Rabiner et al. (1970) and tables of transition values have been provided
in the published paper. A selected number of the tables for lowpass FIR filters are
included in this appendix.

Four tables are given. Table 1 lists the transition coefficients for the case α = 0
and one coefficient in the transition band for both M odd and M even. Table 2 lists
the transition coefficients for the case α = 0, and two coefficients in the transition

α = 12 , M even and one coefficient in the transition band.
the transition coefficients for the case α = 12 , M even, and two coefficients in the
transition band. The tables also include the level of the maximum sidelobe and a
bandwidth parameter, denoted as BW.

To use the tables, we begin with a set of specifications, including (1) the band-
width of the filter, which can be defined as (2π/M)(BW+α), where BW is the number
of consecutive frequencies at which H(ωk) = 1, (2) the width of the transition re-
gion, which is roughly 2π/M times the number of transition coefficients, and (3) the
maximum tolerable sidelobe in the stopband. The length of the filter can be selected
from the tables to satisfy the specifications.

John G. Proakis, Dimitris G. Manolakis. Copyright © 2007 by Pearson Education, Inc. All rights reserved.
From Appendix   B   of Digital Signal Processing: Principles, Algorithms, and Applications, Fourth Edition.

Consider a design method for linear-phase FIR filters that -involves the specific

band for M odd and M even. Table 3 lists the transition coefficients for the case
Finally, Table 4 lists

A P P E N D I X

987



Transition Coefficients for α = 0
M Odd M Even

BW Minimax T1 BW Minimax T1
M = 15 M = 16

1 −42.30932283 0.43378296 1 −39.75363827 0.42631836
2 −41.26299286 0.41793823 2 −37.61346340 0.40397949
3 −41.25333786 0.41047636 3 −36.57721567 0.39454346
4 −41.94907713 0.40405884 4 −35.87249756 0.38916626
5 −44.37124538 0.39268189 5 −35.31695461 0.38840332
6 −56.01416588 0.35766525 6 −35.51951933 0.40155639

M = 33 M = 32
1 −43.03163004 0.42994995 1 −42.24728918 0.42856445
2 −42.42527962 0.41042481 2 −41.29370594 0.40773926
3 −42.40898275 0.40141601 3 −41.03810358 0.39662476
4 −42.45948601 0.39641724 4 −40.93496323 0.38925171
6 −42.52403450 0.39161377 5 −40.85183477 0.37897949
8 −42.44085121 0.39039917 8 −40.75032616 0.36990356

10 −42.11079407 0.39192505 10 −40.54562140 0.35928955
12 −41.92705250 0.39420166 12 −39.93450451 0.34487915
14 −44.69430351 0.38552246 14 −38.91993237 0.34407349
15 −56.18293285 0.35360718

M = 65 M = 64
1 −43.16935968 0.42919312 1 −42.96059322 0.42882080
2 −42.61945581 0.40903320 2 −42.30815172 0.40830689
3 −42.70906305 0.39920654 3 −42.32423735 0.39807129
4 −42.86997318 0.39335937 4 −42.43565893 0.39177246
5 −43.01999664 0.38950806 5 −42.55461407 0.38742065
6 −43.14578819 0.38679809 6 −42.66526604 0.38416748

10 −43.44808340 0.38129272 10 −43.01104736 0.37609863
14 −43.54684496 0.37946167 14 −43.28309965 0.37089233
18 −43.48173618 0.37955322 18 −43.56508827 0.36605225
22 −43.19538212 0.38162842 22 −43.96245098 0.35977783
26 −42.44725609 0.38746948 26 −44.60516977 0.34813232
30 −44.76228619 0.38417358 30 −43.81448936 0.29973144
31 −59.21673775 0.35282745

M = 125 M = 128
1 −43.20501566 0.42899170 1 −43.15302420 0.42889404
2 −42.66971111 0.40867310 2 −42.59092569 0.40847778
3 −42.77438974 0.39868774 3 −42.67634487 0.39838257
4 −42.95051050 0.39268189 4 −42.84038544 0.39226685
6 −43.25854683 0.38579101 5 −42.99805641 0.38812256
8 −43.47917461 0.38195801 7 −43.25537014 0.38281250

10 −43.63750410 0.37954102 10 −43.52547789 0.3782638
18 −43.95589399 0.37518311 18 −43.93180990 0.37251587
26 −44.05913115 0.37384033 26 −44.18097305 0.36941528
34 −44.05672455 0.37371826 34 −44.40153408 0.36686401
42 −43.94708776 0.37470093 42 −44.67161417 0.36394653
50 −43.58473492 0.37797851 50 −45.17186594 0.35902100
58 −42.14925432 0.39086304 58 −46.92415667 0.34273681
59 −42.60623264 0.39063110 62 −49.46298973 0.28751221
60 −44.78062010 0.38383713
61 −56.22547865 0.35263062

Source: Rabiner et al. (1970); © 1970 IEEE; reprinted with permission.

TABLE 1

Appendix : Tables   of Transition Coefficients for the Design of Linear-Phase FIR Filters

988



TABLE 2 Transition Coefficients for α = 0
M Odd M Even

BW Minimax T1 T2 BW Minimax T1 T2
M = 15 M = 16

1 −70.60540585 0.09500122 0.58995418 1 −65.27693653 0.10703125 0.60559357
2 −69.26168156 0.10319824 0.59357118 2 −62.85937929 0.12384644 0.62201631
3 −69.91973495 0.10083618 0.58594327 3 −62.96594906 0.12827148 0.62855407
4 −75.51172256 0.08407953 0.55715312 4 −66.03942485 0.12130127 0.61952704
5 −103.45078300 0.05180206 0.49917424 5 −71.73997498 0.11066284 0.60979204

M = 33 M = 32
1 −70.60967541 0.09497070 0.58985167 1 −67.37020397 0.09610596 0.59045212
2 −68.16726971 0.10585937 0.59743846 2 −63.93104696 0.11263428 0.60560235
3 −67.13149548 0.10937500 0.59911696 3 −62.49787903 0.11931763 0.61192546
5 −66.53917217 0.10965576 0.59674101 5 −61.28204536 0.12541504 0.61824023
7 −67.23387909 0.10902100 0.59417456 7 −60.82049131 0.12907715 0.62307031
9 −67.85412312 0.10502930 0.58771575 9 −59.74928167 0.12068481 0.60685586

11 −69.08597469 0.10219727 0.58216391 11 −62.48683357 0.13004150 0.62821502
13 −75.86953640 0.08137207 0.54712777 13 −70.64571857 0.11017914 0.60670943
14 −104.04059029 0.05029373 0.49149549

M = 65 M = 64
1 −70.66014957 0.09472656 0.58945943 1 −70.26372528 0.09376831 0.58789222
2 −68.89622307 0.10404663 059.476127 2 −67.20729542 0.10411987 0.59421778
3 −67.90234470 0.10720215 0.59577449 3 −65.80684280 0.10850220 0.59666158
4 −67.24003792 0.10726929 0.59415763 4 −64.95227051 0.11038818 0.59730067
5 −66.86065960 0.10689087 0.59253047 5 −64.42742348 0.11113281 0.59698496
9 −66.27561188 0.10548706 0.58845983 9 −63.41714096 0.10936890 0.59088884

13 −65.96417046 0.10466309 0.58660485 13 −62.72142410 0.10828857 0.58738641
17 −66.16404629 0.10649414 0.58862042 17 −62.37051868 0.11031494 0.58968142
21 −66.76456833 0.10701904 0.58894575 21 −62.04848146 0.11254273 0.59249461
25 −68.13407993 0.10327148 0.58320831 25 −61.88074064 0.11994629 0.60564501
29 −75.98313046 0.08069458 0.54500379 29 −70.05681992 0.10717773 0.59842159
30 −104.92083740 0.04978485 0.48965181

M = 125 M = 128
1 −70.68010235 0.09464722 0.58933268 1 −70.58992958 0.09445190 0.58900996
2 −68.94157696 0.10390015 0.59450024 2 −68.62421608 0.10349731 0.59379058
3 −68.19352627 0.10682373 0.59508549 3 −67.66701698 0.10701294 0.59506081
5 −67.34261131 0.10668945 0.59187505 4 −66.95196629 0.10685425 0.59298926
7 −67.09767151 0.10587158 0.59821869 6 −66.32718945 0.10596924 0.58953845
9 −67.05801296 0.10523682 0.58738706 9 −66.01315498 0.10471191 0.58593906

17 −67.17504501 0.10372925 0.58358265 17 −65.89422417 0.10288086 0.58097354
25 −67.22918987 0.10316772 0.58224835 25 −65.92644215 0.10182495 0.57812308
33 −67.11609936 0.10303955 0.58198956 33 −65.95577812 0.10096436 0.57576437
41 −66.71271324 0.10313721 0.58245499 41 −65.97698021 0.10094604 0.57451694
49 −66.62364197 0.10561523 0.58629534 49 −65.67919827 0.09865112 0.56927420
57 −69.28378487 0.10061646 0.57812192 57 −64.61514568 0.09845581 0.56604486
58 −70.35782337 0.09663696 0.57121235 61 −71.76589394 0.10496826 0.59452277
59 −75.94707718 0.08054886 0.54451285
60 −104.09012318 0.04991760 0.48963264

Source: Rabiner et al. (1970); © 1970 IEEE; reprinted with permission.

Appendix : Tables   of Transition Coefficients for the Design of Linear-Phase FIR Filters

989



TABLE 3 Transition Coefficients for α = 12
BW Minimax T1

M = 16
1 −51.60668707 0.26674805
2 −47.48000240 0.32149048
3 −45.19746828 0.34810181
4 −44.32862616 0.36308594
5 −45.68347692 0.36661987
6 −56.63700199 0.34327393

M = 32
1 −52.64991188 0.26073609
2 −49.39390278 0.30878296
3 −47.72596645 0.32984619
4 −46.68811989 0.34217529
6 −45.33436489 0.35704956
8 −44.30730963 0.36750488

10 −43.11168003 0.37810669
12 −42.97900438 0.38465576
14 −56.32780266 0.35030518

M = 64
1 −52.90375662 0.25923462
2 −49.74046421 0.30603638
3 −48.38088989 0.32510986
4 −47.47863007 0.33595581
5 −46.88655186 0.34287720
6 −46.46230555 0.34774170

10 −45.46141434 0.35859375
14 −44.85988188 0.36470337
18 −44.34302616 0.36983643
22 −43.69835377 0.37586059
26 −42.45641375 0.38624268
30 −56.25024033 0.35200195

M = 128
1 −52.96778202 0.25885620
2 −49.82771969 0.30534668
3 −48.51341629 0.32404785
4 −47.67455149 0.33443604
5 −47.11462021 0.34100952
7 −46.43420267 0.34880371

10 −45.88529110 0.35493774
18 −45.21660566 0.36182251
26 −44.87959814 0.36521607
34 −44.61497784 0.36784058
42 −44.32706451 0.37066040
50 −43.87646437 0.37500000
58 −42.30969715 0.38807373
62 −56.23294735 0.35241699

Source: Rabiner et al. (1970); © 1970 IEEE; reprinted with permission.

Appendix : Tables   of Transition Coefficients for the Design of Linear-Phase FIR Filters

990



TABLE 4 Transition Coefficients for α = 12
BW Minimax T1 T2

M = 16
1 −77.26126766 0.05309448 0.41784180
2 −73.81026745 0.07175293 0.49369211
3 −73.02352142 0.07862549 0.51966134
4 −77.95156193 0.07042847 0.51158076
5 −105.23953247 0.04587402 0.46967784

M = 32
1 −80.49464130 0.04725342 0.40357383
2 −73.92513466 0.07094727 0.49129255
3 −72.40863037 0.08012695 0.52153983
5 −70.95047379 0.08935547 0.54805908
7 −70.22383976 0.09403687 0.56031410
9 −69.94402790 0.09628906 0.56637987

11 −70.82423878 0.09323731 0.56226952
13 −104.85642624 0.04882812 0.48479068

M = 64
1 −80.80974960 0.04658203 0.40168723
2 −75.11772251 0.06759644 0.48390015
3 −72.66662025 0.07886963 0.51850058
4 −71.85610867 0.08393555 0.53379876
5 −71.34401417 0.08721924 0.54311474
9 −70.32861614 0.09371948 0.56020256

13 −69.34809303 0.09761963 0.56903714
17 −68.06440258 0.10051880 0.57543691
21 −67.99149132 0.10289307 0.58007699
25 −69.32065105 0.10068359 0.57729656
29 −105.72862339 0.04923706 0.48767025

M = 128
1 −80.89347839 0.04639893 0.40117195
2 −77.22580583 0.06295776 0.47399521
3 −73.43786240 0.07648926 0.51361278
4 −71.93675232 0.08345947 0.53266251
6 −71.10850430 0.08880615 0.54769675
9 −70.53600121 0.09255371 0.55752959

17 −69.95890045 0.09628906 0.56676912
25 −69.29977322 0.09834595 0.57137301
33 −68.75139713 0.10077515 0.57594641
41 −67.89687920 0.10183716 0.57863142
49 −66.76120186 0.10264282 0.58123560
57 −69.21525860 0.10157471 0.57946395
61 −104.57432938 0.04970703 0.48900685

Source: Rabiner et al. (1970); © 1970 IEEE; reprinted with permission.

Appendix : Tables   of Transition Coefficients for the Design of Linear-Phase FIR Filters

991



As an illustration, the filter design for which M = 15 and

Hr

(
2πk
M

)
=
{ 1, k = 0, 1, 2, 3
T1, k = 4
0, k = 5, 6, 7

corresponds to α = 0, BW = 4, since Hr(ωk) = 1 at the four consecutive frequencies
ωk = 2πk/15, k = 0, 1, 2, 3, and the transition coefficient is T1 at the frequency ωk =
8π/15. The value given in Table 1 for M = 15 and BW = 4 is T1 = 0.40405884.
The maximum sidelobe is at −41.9 dB, according to Table 1.

Appendix : Tables   of Transition Coefficients for the Design of Linear-Phase FIR Filters

992



References and Bibliography

AKAIKE, H. 1969. “Power Spectrum Estimation Through Autoregression Model Fitting,” Ann. Inst.
Stat. Math., Vol. 21, pp. 407–149.

AKAIKE, H. 1974. “A New Look at the Statistical Model Identification,” IEEE Trans. Automatic
Control, Vol. AC-19, pp. 716–723, December.

ANDERSEN, N. O. 1978. “Comments on the Performance of Maximum Entropy Algorithm,” Proc. IEEE,
Vol. 66, pp. 1581–1582, November.

ANTONIOU, A. 1979. Digital Filters: Analysis and Design, McGraw-Hill, New York.
AUER, E. 1987. “A Digital Filter Structure Free of Limit Cycles,” Proc. 1987 ICASSP, pp. 21.11.1–

21.11.4, Dallas, TX, April.
AVENHAUS, E., and SCHUESSLER, H. W. 1970. “On the Approximation Problem in the Design of Digital

Filters with Limited Wordlength,” Arch. Elek. Ubertragung, Vol. 24, pp. 571–572.
BAGGEROER, A. B. 1976. “Confidence Intervals for Regression (MEM) Spectral Estimates,” IEEE

Trans. Information Theory, Vol. IT-22, pp. 534–545, September.
BANDLER, J. W., and BARDAKJIAN, B. J. 1973. “Least p th Optimization of Recursive Digital Filters,”

IEEE Trans. Audio and Electroacoustics, Vol. AU-21, pp. 460–470, October.
BARNES, C. W., and FAM, A. T. 1977. “Minimum Norm Recursive Digital Filters That Are Free of

Overflow Limit Cycles,” IEEE Trans. Circuits and Systems, Vol. CAS-24, pp. 569–574, October.
BARTLETT, M. S. 1948. “Smoothing Periodograms from Time Series with Continuous Spectra,” Nature

(London), Vol. 161, pp. 686–687, May.
BARTLETT, M. S. 1961. Stochastic Processes, Cambridge University Press, Cambridge, UK.
BECKMAN, F. S. 1960 The Solution of Linear Equations by the conjugate Gradient Method” in Mathe-

matical Methods for Digital Computers, A. Ralston and H.S. Wilf, eds., Wiley, New York
BERGLAND, G. D. 1969. “A Guided Tour of the Fast Fourier Transform,” IEEE Spectrum, Vol. 6,

pp. 41–52, July.
BERK, K. N. 1974. “Consistent Autoregressive Spectral Estimates,” Ann. Stat., Vol. 2, pp. 489–502.
BERNHARDT, P. A., ANTONIADIS, D. A., and DA ROSA, A. V. 1976. “Lunar Perturbations in Columnar

Electron Content and Their Interpretation in Terms of Dynamo Electrostatic Fields,” J. Geophys.
Res., Vol. 81, pp. 5957–5963, December.

BERRYMAN, J. G. 1978. “Choice of Operator Length for Maximum Entropy Spectral Analysis,” Geo-
physics, Vol. 43, pp. 1384–1391, December.

BIERMAN, G. J. 1977. Factorization Methods for Discrete Sequential Estimation, Academic, New York.
BLACKMAN, R. B., and TUKEY, J. W. 1958. The Measurement of Power Spectra, Dover, New York.
BLAHUT, R. E. 1985. Fast Algorithms for Digital Signal Processing, Addison-Wesley, Reading, MA.
BLUESTEIN, L. I. 1970. “A Linear Filtering Approach to the Computation of the Discrete Fourier

Transform,” IEEE Trans. Audio and Electroacoustics, Vol. AU-18, pp. 451–455, December.
BOLT, B. A. 1988. Earthquakes, W. H. Freeman and Co., New York.
BOMAR, B. W. 1985. “New Second-Order State-Space Structures for Realizing Low Roundoff Noise

Digital Filters,” IEEE Trans. Acoustics, Speech, and Signal Processing, Vol. ASSP-33, pp. 106–110,
February.

BRACEWELL, R. N. 1978. The Fourier Transform and Its Applications, 2d ed., McGraw-Hill, New York.

John G. Proakis, Dimitris G. Manolakis. Copyright © 2007 by Pearson Education, Inc. All rights reserved.
From Digital Signal Processing: Principles, Algorithms, and Applications, Fourth Edition.

993



BRIGHAM, E. O. 1988. The Fast Fourier Transform and Its Applications, Prentice Hall, Upper Saddle
River, NJ.

BRIGHAM, E. O., and MORROW, R. E. 1967. “The Fast Fourier Transform,” IEEE Spectrum, Vol. 4,
pp. 63–70, December.

BRILLINGER, D. R. 1974. “Fourier Analysis of Stationary Processes,” Proc. IEEE, Vol. 62, pp. 1628–1643,
December.

BROPHY, F., and SALAZAR, A. C. 1973. “Considerations of the Padé Approximant Technique in the Syn-
thesis of Recursive Digital Filters,” IEEE Trans. Audio and Electroacoustics, Vol. AU-21, pp. 500–
505, December.

BROWN, J. L., JR., 1980. “First-Order Sampling of Bandpass Signals—A New Approach,” IEEE Trans.
Information Theory, Vol. IT-26, pp. 613–615, September.

BROWN, R. C. 1983. Introduction to Random Signal Analysis and Kalman Filtering, Wiley, New York.
BRUBAKER, T. A., and GOWDY, J. N. 1972. “Limit Cycles in Digital Filters,” IEEE Trans. Automatic

Control, Vol. AC-17, pp. 675–677, October.
BRUZZONE, S. P., and KAVEH, M. 1980. “On Some Suboptimum ARMA Spectral Estimators,” IEEE

Trans. Acoustics, Speech, and Signal Processing, Vol. ASSP-28, pp. 753–755, December.
BURG, J. P. 1967. “Maximum Entropy Spectral Analysis,” Proc. 37th Meeting of the Society of Ex-

ploration Geophysicists, Oklahoma City, OK, October. Reprinted in Modern Spectrum Analysis,
D. G. Childers, ed., IEEE Press, New York.

BURG, J. P. 1968. “A New Analysis Technique for Time Series Data,” NATO Advanced Study Institute
on Signal Processing with Emphasis on Underwater Acoustics, August 12–23. Reprinted in Modern
Spectrum Analysis, D. G. Childers, ed., IEEE Press, New York.

BURG, J. P. 1972. “The Relationship Between Maximum Entropy and Maximum Likelihood Spectra,”
Geophysics, Vol. 37, pp. 375–376, April.

BURG, J. P. 1975. “Maximum Entropy Spectral Analysis,” Ph.D. dissertation, Department of Geophysics,
Stanford University, Stanford, CA, May.

BURRUS, C. S., and PARKS, T. W. 1970. “Time–Domain Design of Recursive Digital Filters,” IEEE Trans.
Audio and Electroacoustics, Vol. 18, pp. 137–141, June.

BURRUS, C. S. and PARKS, T. W. 1985. DFT/FFT and Convolution Algorithms, Wiley, New York.
BUTTERWECK, H. J., VAN MEER, A. C. P., and VERKROOST, G. 1984. “New Second-Order Digital Filter

Sections Without Limit Cycles,” IEEE Trans. Circuits and Systems, Vol. CAS-31, pp. 141–146,
February.

CADZOW, J. A. 1979. “ARMA Spectral Estimation: An Efficient Closed-Form Procedure,” Proc. RADC
Spectrum Estimation Workshop, pp. 81–97, Rome, NY, October.

CADZOW, J. A. 1981. “Autoregressive–Moving Average Spectral Estimation: A Model Equation Error
Procedure,” IEEE Trans. Geoscience Remote Sensing, Vol. GE-19, pp. 24–28, January.

CADZOW, J. A. 1982. “Spectral Estimation: An Overdetermined Rational Model Equation Approach,”
Proc. IEEE, Vol. 70, pp. 907–938, September.

CANDY, J. C. 1986. “Decimation for Sigma Delta Modulation,” IEEE Trans. Communications, Vol. COM-
34, pp. 72–76, January.

CANDY, J. C., WOOLEY, B. A., and BENJAMIN, D. J. 1981. “A Voiceband Codec with Digital Filtering,”
IEEE Trans. Communications, Vol. COM-29, pp. 815–830, June.

CAPON, J. 1969. “High-Resolution Frequency–Wavenumber Spectrum Analysis,” Proc. IEEE, Vol. 57,
pp. 1408–1418, August.

CAPON, J. 1983. “Maximum-Likelihood Spectral Estimation,” in Nonlinear Methods of Spectral Anal-
ysis, 2d ed., S. Haykin, ed., Springer-Verlag, New York.

CAPON, J., and GOODMAN, N. R. 1971. “Probability Distribution for Estimators of the Frequency-
Wavenumber Spectrum,” Proc. IEEE, Vol. 58, pp. 1785–1786, October.

CARAISCOS, C., and LIU, B. 1984. “A Roundoff Error Analysis of the LMS Adaptive Algorithm,” IEEE
Trans. Acoustics, Speech, and Signal Processing, Vol. ASSP-32, pp. 34–41, January.

CARAYANNIS, G., MANOLAKIS, D. G., and KALOUPTSIDIS, N. 1983. “A Fast Sequential Algorithm for
Least-Squares Filtering and Prediction,” IEEE Trans. Acoustics, Speech, and Signal Processing,
Vol. ASSP-31, pp. 1394–1402, December.

References and Bibliography

994



CARAYANNIS, G., MANOLAKIS, D. G., and KALOUPTSIDIS, N. 1986. “A Unified View of Parametric
Processing Algorithms for Prewindowed Signals,” Signal Processing, Vol. 10, pp. 335–368, June.

CARLSON, N. A., and CULMONE, A. F. 1979. “Efficient Algorithms for On-Board Array Processing,”
Record 1979 International Conference on Communications, pp. 58.1.1–58.1.5, Boston, June 10–14.

CHAN, D. S. K., and RABINER, L. R. 1973a. “Theory of Roundoff Noise in Cascade Realizations of Finite
Impulse Response Digital Filters,” Bell Syst. Tech. J., Vol. 52, pp. 329–345, March.

CHAN, D. S. K., and RABINER, L. R. 1973b. “An Algorithm for Minimizing Roundoff Noise in Cascade
Realizations of Finite Impulse Response Digital Filters,” Bell Sys. Tech. J., Vol. 52, pp. 347–385,
March.

CHAN, D. S. K., and RABINER, L. R. 1973c. “Analysis of Quantization Errors in the Direct Form for
Finite Impulse Response Digital Filters,” IEEE Trans. Audio and Electroacoustics, Vol. AU-21,
pp. 354–366, August.

CHANG, T. 1981. “Suppression of Limit Cycles in Digital Filters Designed with One Magnitude-Truncation
Quantizer,” IEEE Trans. Circuits and Systems, Vol. CAS-28, pp. 107–111, February.

CHEN, C. T. 1970. Introduction to Linear System Theory, Holt, Rinehart and Winston, New York.
CHEN, W. Y., and STEGEN, G. R. 1974. “Experiments with Maximum Entropy Power Spectra of Sinu-

soids,” J. Geophys. Res., Vol. 79, pp. 3019–3022, July.
CHILDERS, D. G., ed. 1978. Modern Spectrum Analysis, IEEE Press, New York.
CHOW, J. C. 1972a. “On the Estimation of the Order of a Moving-Average Process,” IEEE Trans.

Automatic Control, Vol. AC-17, pp. 386–387, June.
CHOW, J. C. 1972b. “On Estimating the Orders of an Autoregressive-Moving Average Process with

Uncertain Observations,” IEEE Trans. Automatic Control, Vol. AC-17, pp. 707–709, October.
CHOW, Y., and CASSIGNOL, E. 1962. Linear Signal Flow Graphs and Applications, Wiley, New York.
CHUI, C. K., and CHEN, G. 1987. Kalman Filtering, Springer-Verlag, New York.
CIOFFI, J. M. 1987, “Limited Precision Effects in Adaptive Filtering,” IEEE Trans. Circuits and Systems,

Vol. CAS-34, pp. 821–833, July.
CIOFFI, J. M., and KAILATH, T. 1984. “Fast Recursive-Least-Squares Transversal Filters for Adaptive

Filtering,” IEEE Trans. Acoustics, Speech, and Signal Processing, Vol. ASSP-32, pp. 304–337, April.
CIOFFI, J. M., and KAILATH, T. 1985. “Windowed Fast Transversal Filters Adaptive Algorithms with

Normalization,” IEEE Trans. Acoustics, Speech, and Signal Processing, Vol. ASSP-33, pp. 607–625,
June.

CLAASEN, T. A. C. M., MECKLENBRAUKER, W. F. G., and PEEK, J. B. H. 1973. “Second-Order Digital
Filter with Only One Magnitude-Truncation Quantizer and Having Practically No Limit Cycles,”
Electron. Lett., Vol. 9, November.

CLARKE, R. J. 1985. Transform Coding of Images. Academic Press, London, UK.
COCHRAN, W. T., COOLEY, J. W., FAVIN, D. L., HELMS, H. D., KAENEL, R. A., LANG, W. W., MALING,

G. C., NELSON, D. E., RADER, C. E., and WELCH, P. D. 1967. “What Is the Fast Fourier Transform,”
IEEE Trans. Audio and Electroacoustics, Vol. AU-15, pp. 45–55, June.

CONSTANTINIDES, A. G. 1967. “Frequency Transformations for Digital Filters,” Electron. Lett., Vol. 3,
pp. 487–489, November.

CONSTANTINIDES, A. G. 1968. “Frequency Transformations for Digital Filters,” Electron. Lett., Vol. 4,
pp. 115–116, April.

CONSTANTINIDES, A. G. 1970. “Spectral Transformations for Digital Filters,” Proc. IEEE, Vol. 117,
pp. 1585–1590, August.

COOLEY, J. W., and TUKEY, J. W. 1965. “An Algorithm for the Machine Computation of Complex Fourier
Series,” Math. Comp., Vol. 19, pp. 297–301, April.

COOLEY, J. W., LEWIS, P., and WELCH, P. D. 1967. “Historical Notes on the Fast Fourier Transform,”
IEEE Trans. Audio and Electroacoustics, Vol. AU-15, pp. 76–79, June.

COOLEY, J. W., LEWIS, P., and WELCH, P. D. 1969. “The Fast Fourier Transform and Its Applications,”
IEEE Trans. Education, Vol. E-12, pp. 27–34, March.

COULSON, A. 1995. “A Generalization of Nonuniform Bandpass Sampling.” IEEE Trans. on Signal
Processing, Vol. 43(3), pp. 694–704, March.

COULSON, A., VAUGHAM, R., and POULETTI, M, 1994. “Frequency Shifting Using Bandpass Sampling.”
IEEE Trans. on Signal Processing, Vol 42(6), pp. 1556–1559, June.

References and Bibliography

995



CROCHIERE, R. E. 1977. “On the Design of Sub-Band Coders for Low Bit Rate Speech Communication,”
Bell Syst. Tech. J., Vol. 56, pp. 747–711, May–June.

CROCHIERE, R. E. 1981. “Sub-Band Coding,” Bell Syst. Tech. J., Vol. 60, pp. 1633–1654, September.
CROCHIERE, R. E., and RABINER, L. R. 1975. “Optimum FIR Digital Filter Implementations for Dec-

imation, Interpolation, and Narrowband Filtering,” IEEE Trans. Acoustics, Speech and Signal
Processing, Vol. ASSP-23, pp. 444–456, October.

CROCHIERE, R. E., and RABINER, L. R. 1976. “Further Considerations in the Design of Decimators and
Interpolators,” IEEE Trans. Acoustics, Speech, and Signal Processing, Vol. ASSP-24, pp. 296–311,
August.

CROCHIERE, R. E., and RABINER, L. R. 1981. “Interpolation and Decimation of Digital Signals—A
Tutorial Review,” Proc. IEEE, Vol. 69, pp. 300–331, March.

CROCHIERE, R. E., and RABINER, L. R. 1983. Multirate Digital Signal Processing, Prentice Hall, Upper
Saddle River, NJ.

DANIELS, R. W. 1974. Approximation Methods for the Design of Passive, Active and Digital Filters,
McGraw-Hill, New York.

DAVENPORT, W. B., JR. 1970. Probability and Random Processes: An Introduction for Applied Scientists
and Engineers, McGraw-Hill, New York.

DAVIS, H. F. 1963. Fourier Series and Orthogonal Functions, Allyn and Bacon, Boston.
DECZKY, A. G. 1972. “Synthesis of Recursive Digital Filters Using the Minimum p -Error Criterion,”

IEEE Trans. Audio and Electroacoustics, Vol. AU-20, pp. 257–263, October.
DELLER, J. R. Jr., HANSEN, J. H. L., and PROAKIS, J. G. 2000. Discrete-Time Processing of Speech Signals,

Wiley, New York.
DELSARTE, P., and GENIN, Y. 1986. “The Split Levinson Algorithm,” IEEE Trans. Acoustics, Speech,

and Signal Processing, Vol. ASSP-34, pp. 470–478, June.
DELSARTE, P., GENIN, Y., and KAMP, Y. 1978. “Orthogonal Polynomial Matrices on the Unit Circle,”

IEEE Trans. Circuits and Systems, Vol. CAS-25, pp. 149–160. January.
DERUSSO, P. M., ROY, R. J., and CLOSE, C. M. 1965. State Variables for Engineers, Wiley, New York.
DUHAMEL, P. 1986. “Implementation of Split-Radix FFT Algorithms for Complex, Real, and Real-

Symmetric Data,” IEEE Trans. Acoustics, Speech, and Signal Processing, Vol. ASSP-34, pp. 285–
295, April.

DUHAMEL, P., and HOLLMANN, H. 1984. “Split-Radix FFT Algorithm,” Electron. Lett., Vol. 20, pp. 14–
16, January.

DURBIN, J. 1959. “Efficient Estimation of Parameters in Moving-Average Models,” Biometrika, Vol. 46,
pp. 306–316.

DWIGHT, H. B. 1957. Tables of Integrals and Other Mathematical Data, 3d ed., Macmillan, New York.
DYM, H., and MCKEAN, H. P. 1972. Fourier Series and Integrals, Academic, New York.
EBERT, P. M., MAZO, J. E., and TAYLOR, M. G. 1969. “Overflow Oscillations in Digital Filters,” Bell Syst.

Tech. J., Vol. 48, pp. 2999–3020, November.
ELEFTHERIOU, E., and FALCONER, D. D. 1987. “Adaptive Equalization Techniques for HF Channels,”

IEEE J. Selected Areas in Communications, Vol. SAC-5, pp. 238–247, February.
ELUP, L., GARDNER, F. M., and HARRIS F. A., 1993 “Interpolation in digital Modems, Part II: Funda-

mentals and performance.” IEEE trans. on Communications, Vol, 41(6), pp. 998–1008, June.
FALCONER, D. D., and LJUNG, L. 1978. “Application of Fast Kalman Estimation to Adaptive Equaliza-

tion,” IEEE Trans. Communications, Vol. COM-26, pp. 1439–1446, October.
FAM, A. T., and BARNES, C. W. 1979. “Non-minimal Realizations of Fixed-Point Digital Filters That Are

Free of All Finite Wordlength Limit Cycles,” IEEE Trans. Acoustics, Speech, and Signal Processing,
Vol. ASSP-27, pp. 149–153, April.

FARROW, C. W. 1998. “ A Continiously Variable Digital Delay Element.” Proc. IEEE Intern. Sympo-
sium on Circuits and Systems, pp. 2641–2645.

FETTWEIS, A. 1971. “Some Principles of Designing Digital Filters Imitating Classical Filter Structures,”
IEEE Trans. Circuit Theory, Vol. CT-18, pp. 314–316, March.

FLETCHER, R., and POWELL, M. J. D. 1963. “A Rapidly Convergent Descent Method for Minimization,”
Comput. J., Vol. 6, pp. 163–168.

References and Bibliography

996



FOUGERE, P. F., ZAWALICK, E. J., and RADOSKI, H. R. 1976. “Spontaneous Line Splitting in Maximum
Entropy Power Spectrum Analysis,” Phys. Earth Planet. Inter., Vol. 12, 201–207, August.

FRERKING, M. E. 1994. Digital Signal Processing in Communication Systems, Kluwer Academic Pub-
lishers, Boston.

FRIEDLANDER, B. 1982a. “Lattice Filters for Adaptive Processing,” Proc. IEEE, Vol. 70, pp. 829–867,
August.

FRIEDLANDER, B. 1982b. “Lattice Methods for Spectral Estimation,” Proc. IEEE, Vol. 70, pp. 990–1017,
September.

FUCHS, J. J. 1988. “Estimating the Number of Sinusoids in Additive White Noise,” IEEE Trans. Acous-
tics, Speech, and Signal Processing, Vol. ASSP-36, pp. 1846–1853, December.

GANTMACHER, F. R. 1960. The Theory of Matrices, Vol. I., Chelsea, New York.
GARDNER, F. M. 1993. “Interpolation in Digital Modems, Part I: Fundamentals.” IEEE Trans. on

Communications, Vol. 41(3), pp. 502–508, March.
GARDNER, W. A. 1984. “Learning Characteristics of Stochastic-Gradient-Descent Algorithms: A Gen-

eral Study, Analysis and Critique,” Signal Processing, Vol. 6, pp. 113–133, April.
GARDNER, W. A. 1987. Statistical Spectral Analysis: A Nonprobabilistic Theory, Prentice Hall, Upper

Saddle River, NJ.
GARLAN, C., and ESTEBAN, D. 1980. “16 Kbps Real-Time QMF Sub-Band Coding Implementation,”

Proc. 1980 International Conference on Acoustics, Speech, and Signal Processing, pp. 332–335,
April.

GEORGE, D. A., BOWEN, R. R., and STOREY, J. R. 1971. “An Adaptive Decision-Feedback Equalizer,”
IEEE Trans. Communication Technology, Vol. COM-19, pp. 281–293, June.

GERONIMUS, L. Y. 1958. Orthogonal Polynomials (in Russian) (English translation by Consultants
Bureau, New York, 1961).

GERSCH, W., and SHARPE, D. R. 1973. “Estimation of Power Spectra with Finite-Order Autoregressive
Models,” IEEE Trans. Automatic Control, Vol. AC-18, pp. 367–369, August.

GERSHO, A. 1969. “Adaptive Equalization of Highly Dispersive Channels for Data Transmission,” Bell
Syst. Tech. J., Vol. 48, pp. 55–70, January.

GIBBS, A. J. 1969. “An Introduction to Digital Filters,” Aust. Telecommun. Res., Vol. 3, pp. 3–14,
November.

GIBBS, A. J. 1970. “The Design of Digital Filters,” Aust. Telecommun. Res., Vol. 4, pp. 29–34, May.
GITLIN, R. D., and WEINSTEIN, S. B. 1979. “On the Required Tap-Weight Precision for Digitally Imple-

mented Mean-Squared Equalizers,” Bell Syst. Tech. J., Vol. 58, pp. 301–321, February.
GITLIN, R. D., MEADORS, H. C., and WEINSTEIN, S. B. 1982. “The Tap-Leakage Algorithm: An Algorithm

for the Stable Operation of a Digitally Implemented Fractionally Spaced, Adaptive Equalizer,” Bell
Syst. Tech. J., Vol. 61, pp. 1817–1839, October.

GOERTZEL, G. 1968. “An Algorithm for the Evaluation of Finite Trigonometric Series,” Am. Math.
Monthly, Vol. 65, pp. 34–35, January.

GOHBERG, I., ed. 1986. I. Schür Methods in Operator Theory and Signal Processing, Birkhauser Verlag,
Stuttgart, Germany.

GOLD, B., and JORDAN, K. L., JR. 1986. “A Note on Digital Filter Synthesis,” Proc. IEEE, Vol. 56,
pp. 1717–1718, October.

GOLD, B., and JORDAN, K. L., JR. 1969. “A Direct Search Procedure for Designing Finite Duration
Impulse Response Filters,” IEEE Trans. Audio and Electroacoustics, Vol. AU-17, pp. 33–36, March.

GOLD, B., and RADER, C. M. 1966. “Effects of Quantization Noise in Digital Filters.” Proc. AFIPS
1966 Spring Joint Computer Conference, Vol. 28, pp. 213–219.

GOLD, B., and RADER, C. M. 1969. Digital Processing of Signals, McGraw-Hill, New York.
GOLDEN, R. M., and KAISER, J. F. 1964. “Design of Wideband Sampled Data Filters,” Bell Syst. Tech.

J., Vol. 43, pp. 1533–1546, July.
GOOD, I. J. 1971. “The Relationship Between Two Fast Fourier Transforms,” IEEE Trans. Computers,

Vol. C-20, pp. 310–317.
GORSKI-POPIEL, J., ed. 1975. Frequency Synthesis: Techniques and Applications, IEEE Press, New

York.

References and Bibliography

997



GOYAL, V. 2001. “Theoretical Foundations of Transform Coding.” IEEE Signal Processing Magazine,
pp 9-21, September.

GRACE, O. D., and PITT, S. P. 1969. “Sampling and Interpolation of Bandlimited Signals by Quadrature
Methods.” J. Acoust. Soc. Amer., Vol. 48(6), pp. 1311–1318, November.

GRAUPE, D., KRAUSE, D. J., and MOORE, J. B. 1975. “Identification of Autoregressive–Moving Average
Parameters of Time Series,” IEEE Trans. Automatic Control, Vol. AC-20, pp. 104–107, February.

GRAY, A. H., and MARKEL, J. D. 1973. “Digital Lattice and Ladder Filter Synthesis,” IEEE Trans.
Acoustics, Speech, and Signal Processing, Vol. ASSP-21, pp. 491–500, December.

GRAY, A. H., and MARKEL, J. D. 1976. “A Computer Program for Designing Digital Elliptic Filters,”
IEEE Trans. Acoustics, Speech, and Signal Processing, Vol. ASSP-24, pp. 529–538, December.

GRAY, R. M. 1990. Source Coding Theory, Kluwer, Boston, MA.
GRENANDER, O., and SZEGO, G. 1958. Toeplitz Forms and Their Applications, University of California

Press, Berkeley, CA.
GRIFFITHS, L. J. 1975. “Rapid Measurements of Digital Instantaneous Frequency,” IEEE Trans. Acous-

tics, Speech, and Signal Processing, Vol. ASSP-23, pp. 207–222, April.
GRIFFITHS, L. J. 1978. “An Adaptive Lattice Structure for Noise Cancelling Applications,” Proc. ICASSP-

78, pp. 87–90. Tulsa, OK, April.
GUILLEMIN, E. A. 1957. Synthesis of Passive Networks, Wiley, New York.
GUPTA, S. C. 1966. Transform and State Variable Methods in Linear Systems, Wiley, New York.
HAMMING, R. W. 1962. Numerical Methods for Scientists and Engineers, McGraw-Hill, New York.
HARRIS, F. 1997. Performance and Design of Farrow Filter Used for Arbitrary Resampling. 31st

Conference on Signals, Systems, and Computers, Pacific Grove, CA, pp. 595–599.
HAYKIN, S. 1991. Adaptive Filter Theory, 2d ed., Prentice Hall, Upper Saddle River, NJ.
HELME, B., and NIKIAS, C. S. 1985. “Improved Spectrum Performance via a Data-Adaptive Weighted

Burg Technique,” IEEE Trans. Acoustics, Speech, and Signal Processing, Vol. ASSP-33, pp. 903–910,
August.

HELMS, H. D. 1967. “Fast Fourier Transforms Method of Computing Difference Equations and Simu-
lating Filters,” IEEE Trans. Audio and Electroacoustics, Vol. AU-15, pp. 85–90, June.

HELMS, H. D. 1968. “Nonrecursive Digital Filters: Design Methods for Achieving Specifications on Fre-
quency Response,” IEEE Trans. Audio and Electroacoustics, Vol. AU-16, pp. 336–342, September.

HELSTROM, C. W. 1990. Probability and Stochastic Processes for Engineers, 2d ed., Macmillan, New
York.

HERRING, R. W. 1980. “The Cause of Line Splitting in Burg Maximum-Entropy Spectral Analysis,”
IEEE Trans. Acoustics, Speech, and Signal Processing, Vol. ASSP-28, pp. 692–701, December.

HERMANN, O. 1970. “Design of Nonrecursive Digital Filters with Linear Phase,” Electron. Lett., Vol. 6,
pp. 328–329, November.

HERMANN, O., and SCHüESSLER, H. W. 1970a. “Design of Nonrecursive Digital Filters with Minimum
Phase,” Electron. Lett., Vol. 6, pp. 329–330, November.

HERRMANN, O., and SCHüESSLER, H. W. 1970b. “On the Accuracy Problem in the Design of Nonrecursive
Digital Filters,” Arch. Elek. Ubertragung, Vol. 24, pp. 525–526.

HERRMANN, O., RABINER, L. R., and CHAN, D. S. K. 1973. “Practical Design Rules for Optimum Finite
Impulse Response Lowpass Digital Filters,” Bell Syst. Tech. J., Vol. 52, pp. 769–799, July–August.

HILDEBRAND, F. B. 1952. Methods of Applied Mathematics, Prentice Hall, Upper Saddle River, NJ.
HOFSTETTER, E., OPPENHEIM, A. V., and SIEGEL, J. 1971. “A New Technique for the Design of Nonrecur-

sive Digital Filters,” Proc. 5th Annual Princeton Conference on Information Sciences and Systems,
pp. 64–72.

HOGENAUER, E.B. 1981. “An Economical Class of Digital Filters for Decimation and Interpolation”
IEEE Trans. on ASSP, Vol. 29(2), pp. 155–162, April.

HOUSEHOLDER, A. S. 1964. The Theory of Matrices in Numerical Analysis, Blaisdell, Waltham, MA.
HSU, F. M. 1982. “Square-Root Kalman Filtering for High-Speed Data Received Over Fading Dispersive

HF Channels,” IEEE Trans. Information Theory, Vol. IT-28, pp. 753–763, September.
HSU, F. M., and GIORDANO, A. A. 1978. “Digital Whitening Techniques for Improving Spread Spectrum

Communications Performance in the Presence of Narrowband Jamming and Interference,” IEEE
Trans. Communications, Vol. COM-26, pp. 209–216, February.

References and Bibliography

998



HWANG, S. Y. 1977. “Minimum Uncorrelated Unit Noise in State Space Digital Filtering,” IEEE Trans.
Acoustics, Speech, and Signal Processing, Vol. ASSP-25, pp. 273–281, August.

INCINBONO, B.1978. “ adaptive Signal Processing for Detection and Communication,” in communication
Systems and Random Process Theory, J.K. Skwirzynski, ed., Sijthoff en Noordhoff, Alphen aan den
Rijn, The Netherlands.

JAYANT, N. S., and NOLL, P. 1984. Digital Coding of waveforms, Prentice Hall, Upper Saddle River, NJ.
JACKSON, L. B. 1969. “An Analysis of Limit Cycles Due to Multiplication Rounding in Recursive Digital

(Sub) Filters,” Proc. 7th Annual Allerton Conference on Circuit and System Theory, pp. 69–78.
JACKSON, L. B. 1970a. “On the Interaction of Roundoff Noise and Dynamic Range in Digital Filters,”

Bell Syst. Tech. J., Vol. 49, pp. 159–184, February.
JACKSON, L. B. 1970b. “Roundoff Noise Analysis for Fixed-Point Digital Filters Realized in Cascade or

Parallel Form,” IEEE Trans. Audio and Electroacoustics, Vol. AU-18, pp. 107–122, June.
JACKSON, L. B. 1976. “Roundoff Noise Bounds Derived from Coefficients Sensitivities in Digital Filters,”

IEEE Trans. Circuits and Systems, Vol. CAS-23, pp. 481–485, August.
JACKSON, L. B. 1979. “Limit Cycles on State-Space Structures for Digital Filters,” IEEE Trans. Circuits

and Systems, Vol. CAS-26, pp. 67–68, January.
JACKSON, L. B., LINDGREN, A. G., and KIM, Y. 1979. “Optimal Synthesis of Second-Order State-Space

Structures for Digital Filters,” IEEE Trans. Circuits and Systems, Vol. CAS-26, pp. 149–153, March.
JACKSON, M., and MATTHEWSON, P. 1986. “Digital Processing of Bandpass Signals.” GEC Journal of

Research, Vol. 4(1), pp. 32–41.
JAHNKE, E., and EMDE, F. 1945. Tables of Functions, 4th ed., Dover, New York.
JAIN, V. K., and CROCHIERE, R. E. 1984. “Quadrature Mirror Filter Design in the Time Domain,” IEEE

Trans. Acoustics, Speech, and Signal Processing, Vol. ASSP-32, pp. 353–361, April.
JANG, Y., and YANG, S. 2001. Recursive Cascaded Integrator-Comb Decimation Filters with Integer

Multiple factors, 44th IEEE Midwest Symposium on circuits and Systems, Daytona, OH, August.
JENKINS, G. M., and WATTS, D. G. 1968. Spectral Analysis and Its Applications, Holden-Day, San

Francisco.
JOHNSON, D. H. 1982. “The Application of Spectral Estimation Methods to Bearing Estimation Prob-

lems,” Proc. IEEE, Vol. 70, pp. 1018–1028, September.
JOHNSTON, J. D. 1980. “A Filter Family Designed for Use in Quadrature Mirror Filter Banks,” IEEE

International Conference on Acoustics, Speech, and Signal Processing, pp. 291–294, April.
JONES, R. H. 1976. “Autoregression Order Selection,” Geophysics, Vol. 41, pp. 771–773, August.
JONES, S. K., CAVIN, R. K., and REED, W. M. 1982. “Analysis of Error-Gradient Adaptive Linear Equal-

izers for a Class of Stationary-Dependent Processes,” IEEE Trans. Information Theory, Vol. IT-28,
pp. 318–329, March.

JURY, E. I. 1964. Theory and Applications of the z-Transform Method, Wiley, New York.
KAILATH, T. 1974. “A View of Three Decades of Linear Filter Theory,” IEEE Trans. Information

Theory, Vol. IT-20, pp. 146–181, March.
KAILATH, T. 1981. Lectures on Wiener and Kalman Filtering, 2d printing, Springer-Verlag, New York.
KAILATH, T. 1985. “Linear Estimation for Stationary and Near-Stationary Processes,” in Modern Signal

Processing, T. Kailath, ed., Hemisphere Publishing Corp., Washington, DC.
KAILATH, T. 1986. “A Theorem of I. Schür and Its Impact on Modern Signal Processing,” in Gohberg

1986.
KAILATH, T., VIEIRA, A. C. G, and MORF, M. 1978. “Inverses of Toeplitz Operators, Innovations, and

Orthogonal Polynomials,” SIAM Rev., Vol. 20, pp. 1006–1019.
KAISER, J. F. 1963. “Design Methods for Sampled Data Filters,” Proc. First Allerton Conference on

Circuit System Theory, pp. 221–236, November.
KAISER, J. F. 1966. “Digital Filters,” in System Analysis by Digital Computer, F. F. Kuo and J. F. Kaiser,

eds., Wiley, New York.
KALOUPTSIDIS, N., and THEODORIDIS, S. 1987. “Fast Adaptive Least-Squares Algorithms for Power

Spectral Estimation,” IEEE Trans. Acoustics, Speech, and Signal Processing, Vol. ASSP-35, pp. 661–
670, May.

KALMAN, R. E. 1960. “A New Approach to Linear Filtering and Prediction Problems,” Trans. ASME,
J. Basic Eng., Vol. 82D, pp. 35–45, March.

References and Bibliography

999



KALMAN, R. E., and BUCY, R. S. 1961. “New Results in Linear Filtering Theory,” Trans. ASME, J. Basic
Eng., Vol. 83, pp. 95–108.

KASHYAP, R. L. 1980. “Inconsistency of the AIC Rule for Estimating the Order of Autoregressive
Models,” IEEE Trans. Automatic Control, Vol. AC-25, pp. 996–998, October.

KAVEH, J., and BRUZZONE, S. P. 1979. “Order Determination for Autoregressive Spectral Estimation,”
Record of the 1979 RADC Spectral Estimation Workshop, pp. 139–145, Griffin Air Force Base,
Rome, NY.

KAVEH, M., and LIPPERT, G. A. 1983. “An Optimum Tapered Burg Algorithm for Linear Prediction and
Spectral Analysis,” IEEE Trans. Acoustics, Speech, and Signal Processing, Vol. ASSP-31, pp. 438–
444, April.

KAY, S. M. 1980. “A New ARMA Spectral Estimator,” IEEE Trans. Acoustics, Speech, and Signal
Processing, Vol. ASSP-28, pp. 585–588, October.

KAY, S. M. 1988. Modern Spectral Estimation, Prentice Hall, Upper Saddle River, NJ.
KAY, S. M., and MARPLE, S. L., JR. 1979. “Sources of and Remedies for Spectral Line Splitting in

Autoregressive Spectrum Analysis,” Proc. 1979 ICASSP, pp. 151–154.
KAY, S. M., and MARPLE, S. L., JR. 1981. “Spectrum Analysis: A Modern Perspective,” Proc. IEEE,

Vol. 69, pp. 1380–1419, November.
KESLER, S. B., ed. 1986. Modern Spectrum Analysis II, IEEE Press, New York.
KETCHUM, J. W., and PROAKIS, J. G. 1982. “Adaptive Algorithms for Estimating and Suppressing Narrow-

Band Interference in PN Spread-Spectrum Systems,” IEEE Trans. Communications, Vol. COM-30,
pp. 913–923, May.

KNOWLES, J. B., and OLCAYTO, E. M. 1968. “Coefficient Accuracy and Digital Filter Response,” IEEE
Trans. Circuit Theory, Vol. CT-15, pp. 31–41, March.

KOHLENBURG, A. 1953. “Exact Interpolation of Bandlimited Functions.” Journal of Applied Physics,
Vol. 24(12), pp. 1432–1436, May.

KRISHNA, H. 1988. “New Split Levinson, Schür, and Lattice Algorithms for Digital Signal Processing,”
Proc. 1988 International Conference on Acoustics, Speech, and Signal Processing, pp. 1640–1642,
New York, April.

KROMER, R. E. 1969. “Asymptotic Properties of the Autoregressive Spectral Estimator,” Ph.D. disser-
tation, Department of Statistics, Stanford University, Stanford, CA.

KUNG, H. T. 1982. “Why Systolic Architectures”? IEEE Computer, Vol. 15, pp. 37–46.
KUNG, S. Y., and HU, Y. H. 1983. “A Highly Concurrent Algorithm and Pipelined Architecture for

Solving Toeplitz Systems,” IEEE Trans. Acoustics, Speech, and Signal Processing, Vol. ASSP-31,
pp. 66–76, January.

KUNG, S. Y., WHITEHOUSE, H. J., and KAILATH, T., eds. 1985. VLSI and Modern Signal Processing,
Prentice Hall, Upper Saddle River, NJ.

LAAKSO, T., VALIMAKI, V., KARJALAINEN, M., and LAINE, U. 1996. “Splitting the Unit Delay.” IEEE
Signal Processing Magazine, No. 1, pp. 30–54, January.

LACOSS, R. T. 1971. “Data Adaptive Spectral Analysis Methods,” Geophysics, Vol. 36, pp. 661–675,
August.

LANG, S. W., and MCCLELLAN, J. H. 1980. “Frequency Estimation with Maximum Entropy Spectral
Estimators,” IEEE Trans. Acoustics, Speech, and Signal Processing, Vol. ASSP-28, pp. 716–724,
December.

LEVINSON, N. 1947. “The Wiener RMS Error Criterion in Filter Design and Prediction,” J. Math. Phys.,
Vol. 25, pp. 261–278.

LEVY, H., and LESSMAN, F. 1961. Finite Difference Equations, Macmillan, New York.
LIN, D. W. 1984. “On Digital Implementation of the Fast Kalman Algorithm,” IEEE Trans. Acoustics,

Speech, and Signal Processing, Vol. ASSP-32, pp. 998–1005, October.
LINDEN, D. A. 1959. “ A Discussion of Sampling Theorems.” Proc. of the IRE, Vol. 47(11), pp. 1219–

1226, November.
LING, F., and PROAKIS, J. G. (1984a). “Numerical Accuracy and Stability: Two Problems of Adaptive

Estimation Algorithms Caused by Round-Off Error,” Proc. ICASSP-84, pp. 30.3.l–30.3.4, San Diego,
CA, March.

References and Bibliography

1000



LING, F., and PROAKIS, J. G. (1984b). “Nonstationary Learning Characteristics of Least-Squares Adap-
tive Estimation Algorithms,” Proc. ICASSP-84, pp. 3.7.1–3.7.4, San Diego, CA, March.

LING, F., MANOLAKIS, D., and PROAKIS, J. G. 1985, “New Forms of LS Lattice Algorithms and Analysis
of Their Round-Off Error Characteristics,” Proc. ICASSP-85, pp. 1739–1742, Tampa, FL, April.

LING, F., MANOLAKIS, D., and PROAKIS, J. G. 1986. “Numerically Robust Least-Squares Lattice-Ladder
Algorithms with Direct Updating of the Reflection Coefficients,” IEEE Trans. Acoustics, Speech,
and Signal Processing, Vol. ASSP-34, pp. 837–845, August.

LIU, B. 1971. “Effect of Finite Word Length on the Accuracy of Digital Filters—A Review,” IEEE Trans.
Circuit Theory, Vol. CT-18, pp. 670–677, November.

LJUNG, S., and LJUNG, L. 1985. “Error Propagation Properties of Recursive Least-Squares Adaptation
Algorithms,” Automatica, Vol. 21, pp. 157–167.

LUCKY, R. W. 1965. “Automatic Equalization for Digital Communications,” Bell Syst. Tech. J., Vol. 44,
pp. 547–588, April.

MAGEE, F. R. and PROAKIS, J. G. 1973. “Adaptive Maximum-Likelihood Sequence Estimation for Digital
Signaling in the Presence of Intersymbol Interference,” IEEE Trans. Information Theory, Vol. IT-19,
pp. 120–124, January.

MAKHOUL, J. 1975. “Linear Prediction: A Tutorial Review,” Proc. IEEE, Vol. 63, pp. 561–580, April.
MAKHOUL, J. 1978. “A Class of All-Zero Lattice Digital Filters: Properties and Applications,” IEEE

Trans. Acoustics, Speech, and Signal Processing, Vol. ASSP-26, pp. 304–314, August.
MAKHOUL, J. 1980. “A Fast Cosine Transform In One and Two Dimentions.” IEEE Trans. on ASSP,

Vol. 28(1), pp. 27–34, February.
MARKEL, J. D., and GRAY, A. H., JR. 1976. Linear Prediction of Speech, Springer-Verlag, New York.
MANOLAKIS, D., LING, F., and PROAKIS, J. G. 1987. “Efficient Time-Recursive Least-Squares Algorithms

for Finite-Memory Adaptive Filtering,” IEEE Trans. Circuits and Systems, Vol. CAS-34, pp. 400–408,
April.

MARPLE, S. L., JR. 1980. “A New Autoregressive Spectrum Analysis Algorithm,” IEEE Trans. Acoustics,
Speech, and Signal Processing, Vol. ASSP-28, pp. 441–454, August.

MARPLE, S. L., JR. 1987. Digital Spectral Analysis with Applications, Prentice Hall, Upper Saddle River,
NJ.

MARTUCCI, S. A. 1994. “Symmetric Convolution and the Discrete Sine and Cosine Transforms.” IEEE
Trans. on Signal Processing, Vol. 42(5), pp. 1038–1051, May.

MARZETTA, T. L. 1983. “A New Interpretation for Capon’s Maximum Likelihood Method of Frequency–
Wavenumber Spectral Estimation,” IEEE Trans. Acoustics, Speech, and Signal Processing, Vol. ASSP-
31, pp. 445–449, April.

MARZETTA, T. L., and LANG, S. W. 1983. “New Interpretations for the MLM and DASE Spectral
Estimators,” Proc. 1983 ICASSP, pp. 844–846, Boston, April.

MARZETTA, T. L., and LANG, S. W. 1984. “Power Spectral Density Bounds,” IEEE Trans. Information
Theory, Vol. IT-30, pp. 117–122, January.

MASON, S. J., and ZIMMERMAN, H. J. 1960. Electronic Circuits, Signals and Systems, Wiley, New York.
MAZO, J. E. 1979. “On the Independence Theory of Equalizer Convergence,” Bell Syst. Tech. J., Vol. 58,

pp. 963–993, May.
MCCLELLAN, J. H. 1982. “Multidimensional Spectral Estimation,” Proc. IEEE, Vol. 70, pp. 1029–1039,

September.
MCDONOUGH, R. N. 1983. “Application of the Maximum-Likelihood Method and the Maximum Entropy

Method to Array Processing,” in Nonlinear Methods of Spectral Analysis, 2d ed., S. Haykin, ed.,
Springer-Verlag, New York.

MCGILLEM, C. D., and COOPER, G. R. 1984. Continuous and Discrete Signal and System Analysis, 2d
ed., Holt Rinehart and Winston, New York.

MEDITCH, J. E. 1969. Stochastic Optimal Linear Estimation and Control, McGraw-Hill, New York.
MEYER, R.,and BURRUS, S. 1975. “A Unified Analysis of Multirate and Periodically Time-Varying Digital

Filters.” IEEE Trans. on Circuits and Systems, Vol. 22(3), pp. 162–168, March.
MILLS, W. L., MULLIS, C. T., and ROBERTS, R. A. 1981. “Low Roundoff Noise and Normal Realizations of

Fixed-Point IIR Digital Filters,” IEEE Trans. Acoustics, Speech, and Signal Processing, Vol. ASSP-
29, pp. 893–903, August.

References and Bibliography

1001



MOORER, J. A. 1977. “Signal Aspects of Computer Music; A Survey,” Proc. IEEE, Vol. 65, pp. 1108–
1137, August.

MORF, M., VIEIRA, A., and LEE, D. T. 1977. “Ladder Forms for Identification and Speech Processing,”
Proc. 1977 IEEE Conference Decision and Control, pp. 1074–1078, New Orleans, LA, December.

MULLIS, C. T., and ROBERTS, R. A. 1976a. “Synthesis of Minimum Roundoff Noise Fixed-Point Digital
Filters,” IEEE Trans. Circuits and Systems, Vol. CAS-23, pp. 551–561, September.

MULLIS, C. T., and ROBERTS, R. A. 1976b. “Roundoff Noise in Digital Filters: Frequency Transfor-
mations and Invariants,” IEEE Trans. Acoustics, Speech and Signal Processing, Vol. ASSP-24,
pp. 538–549, December.

MURRAY, W., ed. 1972 Numerical Methods for Unconstrained Minimization, Acedemic, New York.
MUSICUS, B. 1985. “Fast MLM Power Spectrum Estimation from Uniformly Spaced Correlations,” IEEE

Trans. Acoustics, Speech, and Signal Proc., Vol. ASSP-33, pp. 1333–1335, October.
NEWMAN, W. I. 1981. “Extension to the Maximum Entropy Method III,” Proc. 1st ASSP Workshop on

Spectral Estimation, pp. 1.7.1–1.7.6, Hamilton, ON, August.
NICHOLS, H. E., GIORDANO, A. A., and PROAKIS, J. G. 1977. “MLD and MSE Algorithms for Adaptive

Detection of Digital Signals in the Presence of Interchannel Interference,” IEEE Trans. Information
Theory, Vol. IT-23, pp. 563–575, September.

NIKIAS, C. L., and RAGHUVEER, M. R. 1987. “Bispectrum Estimation: A Digital Signal Processing
Framework,” Proc. IEEE, Vol. 75, pp. 869–891, July.

NIKIAS, C. L., and SCOTT, P. D. 1982. “Energy-Weighted Linear Predictive Spectral Estimation: A New
Method Combining Robustness and High Resolution,” IEEE Trans. Acoustics, Speech, and Signal
Processing, Vol. ASSP-30, pp. 287–292, April.

NUTTALL, A. H. 1976. “Spectral Analysis of a Univariate Process with Bad Data Points, via Maximum
Entropy and Linear Predictive Techniques,” NUSC Technical Report TR-5303, New London, CT,
March.

NYQUIST, H. 1928. “Certain Topics in Telegraph Transmission Theory,” Trans. AIEE, Vol. 47, pp. 617–
644, April.

OPPENHEIM, A. V. 1978. Applications of Digital Signal Processing, Prentice Hall, Upper Saddle River,
NJ.

OPPENHEIM, A. V., and SCHAFER, R. W. 1989. Discrete-Time Signal Processing, Prentice Hall, Upper
Saddle River, NJ.

OPPENHEIM, A. V., and WEINSTEIN, C. W. 1972. “Effects of Finite Register Length in Digital Filters and
the Fast Fourier Transform,” Proc. IEEE, Vol. 60, pp. 957–976. August.

OPPENHEIM, A. V., and WILLSKY, A. S. 1983. Signals and Systems, Prentice Hall, Upper Saddle River,
NJ.

PAPOULIS, A. 1962 The Fourier Integral and Its Applications, McGraw-Hall, New York.
PAPOULIS, A. 1984. Probability, Random Variables, and Stochastic Processes, 2d ed., McGraw-Hill,

New York.
PARKER, S. R., and HESS, S. F. 1971. “Limit-Cycle Oscillations in Digital Filters,” IEEE Trans. Circuit

Theory, Vol. CT-18, pp. 687–696, November.
PARKS, T. W., and MCCLELLAN, J. H. 1972a. “Chebyshev-Approximation for Nonrecursive Digital

Filters with Linear Phase,” IEEE Trans. Circuit Theory, Vol. CT-19, pp. 189–194, March.
PARKS, T. W., and MCCLELLAN, J. H. 1972b. “A Program for the Design of Linear Phase Finite Im-

pulse Response Digital Filters,” IEEE Trans. Audio and Electroacoustics, Vol. AU-20, pp. 195–199,
August.

PARZEN, E. 1957. “On Consistent Estimates of the Spectrum of a Stationary Time Series,” Am. Math.
Stat., Vol. 28, pp. 329–348.

PARZEN, E. 1974. “Some Recent Advances in Time Series Modeling,” IEEE Trans. Automatic Control,
Vol. AC-19, pp. 723–730, December.

PEACOCK, K. L, and TREITEL, S. 1969. “Predictive Deconvolution—Theory and Practice,” Geophysics,
Vol. 34, pp. 155–169.

PEEBLES, P. Z., JR. 1987. Probability, Random Variables, and Random Signal Principles, 2d ed.,
McGraw-Hill, New York.

References and Bibliography

1002



PICINBONO, B. 1978. “Adaptive Signal Processing for Detection and Communication,” in Communica-
tion Systems and Random Process Theory, J. K. Skwirzynski, ed., Sijthoff en Noordhoff, Alphen aan
den Rijn, The Netherlands.

PISARENKO, V. F. 1973. “The Retrieval of Harmonics from a Convariance Function,” Geophys. J. R. As-
tron. Soc., Vol. 33, pp. 347–366.

POOLE, M. A. 1981. Autoregressive Methods of Spectral Analysis, E.E. degree thesis, Department of
Electrical and Computer Engineering, Northeastern University, Boston, May.

PRICE, R. 1990. “Mirror FFT and Phase FFT Algorithms,” unpublished work, Raytheon Research
Division, May.

PROAKIS, J. G. 1970. “Adaptive Digital Filters for Equalization of Telephone Channels,” IEEE Trans.
Audio and Electroacoustics, Vol. AU-18, pp. 195–200, June.

PROAKIS, J. G. 1974, “Channel Identification for High Speed Digital Communications,” IEEE Trans.
Automatic Control, Vol. AC-19, pp. 916–922, December.

PROAKIS, J. G. 1975. “Advances in Equalization for Intersymbol Interference,” in Advances in Commu-
nication Systems, Vol. 4, A. J. Viterbi, ed., Academic, New York.

PROAKIS, J. G. 1989. Digital Communications, 2nd ed., McGraw-Hill, New York.
PROAKIS, J. G., and MILLER, J. H. 1969. “Adaptive Receiver for Digital Signaling through Channels with

Intersymbol Interference,” IEEE Trans. Information Theory, Vol. IT-15, pp. 484–497, July.
PROAKIS, J. G., RADER, C. M., LING, F., NIKIAS, C. L., MOONEN, M. and PROUDLER, I. K. 2002. Algo-

rithms for Statistical Signal Processing, Prentice-Hall, Upper Saddle River, NJ.
PROAKIS, J. G. 2001. Digital Communications, 4th ed., McGraw-Hill, New York.
QI, R., COAKLEY, F., and EVANS, B. 1996. “Practical Consideration for Bandpass Sampling.” Electronics

Letters, Vol. 32(20), pp. 1861–1862, September.
RABINER, L. R., and SCHAEFER, R. W. 1974a. “On the Behavior of Minimax Relative Error FIR Digital

Differentiators,” Bell Syst. Tech. J., Vol. 53, pp. 333–362, February.
RABINER, L. R., and SCHAFER, R. W. 1974b. “On the Behavior of Minimax FIR Digital Hilbert Trans-

formers,” Bell Syst. Tech. J., Vol. 53, pp. 363–394, February.
RABINER, L. R., and SCHAFER, R. W. 1978. Digital Processing of Speech Signals, Prentice Hall, Upper

Saddle River, NJ.
RABINER, L. R., SCHAFER, R. W., and RADER, C. M. 1969. “The Chirp z-Transform Algorithm and Its

Applications,” Bell Syst. Tech. J., Vol. 48, pp. 1249–1292, May–June.
RABINER, L. R., GOLD, B., and MCGONEGAL, C. A. 1970. “An Approach to the Approximation Problem

for Nonrecursive Digital Filters,” IEEE Trans. Audio and Electroacoustics, Vol. AU-18, pp. 83–106,
June.

RABINER, L. R., MCCLELLAN, J. H., and PARKS, T. W. 1975. “FIR Digital Filter Design Techniques
Using Weighted Chebyshev Approximation,” Proc. IEEE, Vol. 63, pp. 595–610, April.

RADER, C. M. 1970. “An Improved Algorithm for High-Speed Auto-correlation with Applications to
Spectral Estimation,” IEEE Trans. Audio and Electroacoustics, Vol. AU-18, pp. 439–441, December.

RADER, C. M., and BRENNER, N. M. 1976. “A New Principle for Fast Fourier Transformation,” IEEE
Trans. Acoustics, Speech and Signal Processing, Vol. ASSP-24, pp. 264–266, June.

RADER, C. M., and GOLD, B. 1967a. “Digital Filter Design Techniques in the Frequency Domain,” Proc.
IEEE, Vol. 55, pp. 149–171, February.

RADER, C. M., and GOLD, B. 1967b. “Effects of Parameter Quantization on the Poles of a Digital Filter,”
Proc. IEEE, Vol. 55, pp. 688–689, May.

RAMSTAD, T. A. 1984. “Digital Methods for Conversion Between Arbitrary Sampling Frequencies,”
IEEE Trans. Acoustics, Speech, and Signal Processing, Vol. ASSP-32, pp. 577–591, June.

RAO, K., and HUANG, J. 1996. Techniques and Standards for Image, Video, and Audio Coding, Prentice
Hall, Upper Saddle River, NJ.

RAO, K. R., and YIP, P 1990. Discrete Cosine Transform, Academic Press, Boston.
REMEZ, E. YA. 1957. General Computational Methods of Chebyshev Approximation, Atomic Energy

Translation 4491, Kiev, USSR.
RICE, D., and WU, K. 1982. “Quatature Sampling with High Dynamic Range.”IEEE Trans. Aerospace

and Electronic Systems, Vol. 18(4), pp. 736–739, November.

References and Bibliography

1003



RISSANEN, J. 1983. “A Universal Prior for the Integers and Estimation by Minimum Description Length,”
Ann. Stat., Vol. 11, pp. 417–431.

ROBERTS, R. A., and MULLIS, C. T. 1987. Digital Signal Processing, Addison-Wesley, Reading, MA.
ROBINSON, E. A. 1962. Random Wavelets and Cybernetic Systems, Charles Griffin, London.
ROBINSON, E. A. 1982. “A Historical Perspective of Spectrum Estimation,” Proc. IEEE, Vol. 70,

pp. 885–907, September.
ROBINSON, E. A., and TREITEL, S. 1978. “Digital Signal Processing in Geophysics,” in Applications of

Digital Signal Processing, A. V. Oppenheim, ed., Prentice Hall, Upper Saddle River, NJ.
ROBINSON, E. A., and TREITEL, S. 1980. Geophysical Signal Analysis, Prentice Hall, Upper Saddle

River, NJ.
ROY, R., PAULRAJ, A., and KAILATH, T. 1986. “ESPRIT: A Subspace Rotation Approach to Estima-

tion of Parameters of Cisoids in Noise,” IEEE Trans. Acoustics, Speech, and Signal Processing,
Vol. ASSP-34, pp. 1340–1342, October.

SAFRANEK, R. J., MACKAY, K. JAYANT, N. W., and KIM, T. 1988. “Image Coding Based on Selective
Quantization of the Reconstruction Noise in the Dominant Sub-Band,” Proc. 1988 IEEE Interna-
tional Conference on Acoustics, Speech, and Signal Processing, pp. 765–768, April.

SAKAI, H. 1979. “Statistical Properties of AR Spectral Analysis,” IEEE Trans. Acoustics, Speech, and
Signal Processing, Vol. ASSP-27, pp. 402–409, August.

SANDBERG, I. W., and KAISER, J. F. 1972. “A Bound on Limit Cycles in Fixed-Point Implementations of
Digital Filters,” IEEE Trans. Audio and Electroacoustics, Vol. AU-20, pp. 110–112, June.

SATORIUS, E. H., and ALEXANDER J. T. 1978. “High Resolution Spectral Analysis of Sinusoids in
Correlated Noise,” Proc. 1978 ICASSP, pp. 349–351, Tulsa, OK, April 10–12.

SAYED, A. H. 2003 Adaptive Filters, Wiley, New York.
SCHAFER, R. W., and RABINER, L. R. 1973. “A Digital Signal Processing Approach to Interpolation,”

Proc. IEEE, Vol. 61, pp. 692–702, June.
SCHEUERMANN, H., and GOCKLER, H. 1981. “A Comprehensive Survey of Digital Transmultiplexing

Methods,” Proc. IEEE, Vol. 69, pp. 1419–1450.
SCHMIDT, R. D. 1981. “A Signal Subspace Approach to Multiple Emitter Location and Spectral Estima-

tion,” Ph.D. dissertation, Department of Electrical Engineering, Stanford University, Stanford, CA,
November.

SCHMIDT, R. D. 1986. “Multiple Emitter Location and Signal Parameter Estimation,” IEEE Trans.
Antennas and Propagation, Vol. AP 34, pp. 276–280, March.

SCHOTT, J. P., and MCCLELLAN, J. H. 1984. “Maximum Entropy Power Spectrum Estimation with
Uncertainty in Correlation Measurements,” IEEE Trans. Acoustics, Speech, and Signal Processing,
Vol. ASSP-32, pp. 410–418, April.

SCHUR, I. 1917. “On Power Series Which Are Bounded in the Interior of the Unit Circle,” J. Reine
Angew. Math., Vol. 147, pp. 205–232, Berlin. For an English translation of the paper, see Gohberg
1986.

SCHUSTER, SIR ARTHUR. 1898. “On the Investigation of Hidden Periodicities with Application to a
Supposed Twenty-Six-Day Period of Meteorological Phenomena,” Terr. Mag., Vol. 3, pp. 13–41,
March.

SEDLMEYER, A., and FETTWEIS, A. 1973. “Digital Filters with True Ladder Configuration,” Int. J. Circuit
Theory Appl., Vol. 1, pp. 5–10, March.

SHANKS, J. L. 1967. “Recursion Filters for Digital Processing,” Geophysics, Vol. 32, pp. 33–51, February.
SHANNON, C. E. 1949. “Communication in the Presence of Noise,” Proc. IRE, pp. 10–21, January.
SHEINGOLD, D. H., ed. 1986. Analog-Digital Conversion Handbook, Prentice Hall, Upper Saddle

River, NJ.
SIEBERT, W. M. 1986. Circuits, Signals and Systems, McGraw-Hill, New York.
SINGLETON, R. C. 1967. “A Method for Computing the Fast Fourier Transform with Auxiliary Memory

and Limit High Speed Storage,” IEEE Trans. Audio and Electroacoustics, Vol. AU-15, pp. 91–98,
June.

SINGLETON, R. C. 1969. “An Algorithm for Computing the Mixed Radix Fast Fourier Transform,” IEEE
Trans. Audio and Electroacoustics, Vol. AU-17, pp. 93–103, June.

References and Bibliography

1004



SLOCK, D. T. M., and KAILATH, T 1988. “Numerically Stable Fast Recursive Least Squares Transversal
Filters,” Proc. 1988 Int. Conf. Acoustics, Speech, and Signal Processing, pp. 1364–1368, NY, April.

SLOCK, D. T. M., and KAILATH, T 1991. “Numerically Stable Fast Transversal Filters for Recursive Least
Squares Adaptive Filtering,” IEEE Trans. Signal Processing, Vol. 39, pp. 92–114, January.

SMITH, M. J. T., and BARWELL, T. P. 1984. “A Procedure for Designing Exact Reconstruction Filter Banks
for Tree Structured Subband Coders,” Proc. 1984 IEEE International Conference on Acoustics,
Speech, and Signal Processing, pp. 27.1.1–27.1.4, San Diego, March.

SMITH, M. J. T., and EDDINS, S. L. 1988. “Subband Coding of Images with Octave Band Tree Structures,”
Proc. 1987 IEEE International Conference on Acoustics, Speech, and Signal Processing, pp. 1382–
1385, Dallas, April.

STARK, H, and WOODS, J. W. 1994. Probability, Random Processes, and Estimation Theory for Engi-
neers, 2nd Ed., Prentice Hall, Upper Saddle River, NJ.

STEIGLITZ, K. 1965. “The Equivalence of Digital and Analog Signal Processing,” Inf. Control, Vol. 8,
pp. 455–467, October.

STEIGLITZ, K. 1970. “Computer-Aided Design of Recursive Digital Filters,” IEEE Trans. Audio and
Electroacoustics, Vol. AU-18, pp. 123–129, June.

STOCKHAM, T. G. 1966. “High Speed Convolution and Correlation,” 1966 Spring Joint Computer Con-
ference, AFIPS Proc., Vol. 28, pp. 229–233.

STORER, J. E. 1957. Passive Network Synthesis, McGraw-Hill, New York.
STRANG, G. 1999. “The Discrete Cosine Transform.” SIAM Review, Vol. 41(1), pp. 135–137.
SWARZTRAUBER, P. 1986. “Symmetric FFT’s,” Mathematics of Computation, Vol. 47, pp. 323–346, July.
SWINGLER, D. N. 1979a. “A Comparison Between Burg’s Maximum Entropy Method and a Nonrecursive

Technique for the Spectral Analysis of Deterministic Signals,” J. Geophys. Res., Vol. 84, pp. 679–685,
February.

SWINGLER, D. N. 1979b. “A Modified Burg Algorithm for Maximum Entropy Spectral Analysis,” Proc.
IEEE, Vol. 67, pp. 1368–1369, September.

SWINGLER, D. N. 1980. “Frequency Errors in MEM Processing,” IEEE Trans. Acoustics, Speech, and
Signal Processing, Vol. ASSP-28, pp. 257–259, April.

SZEGO, G. 1967. Orthogonal Polynomials, 3d ed., Colloquium Publishers, No. 23, American Mathemat-
ical Society, Providence, RI.

THORVALDSEN, T. 1981. “A Comparison of the Least-Squares Method and the Burg Method for Au-
toregressive Spectral Analysis,” IEEE Trans. Antennas and Propagation, Vol. AP-29, pp. 675–679,
July.

TONG, H. 1975. “Autoregressive Model Fitting with Noisy Data by Akaike’s Information Criterion,”
IEEE Trans. Information Theory, Vol. IT-21, pp. 476–480, July.

TONG, H. 1977. “More on Autoregressive Model Fitting with Noise Data by Akaike’s Information
Criterion,” IEEE Trans. Information Theory, Vol. IT-23, pp. 409–410, May.

TRETTER, S. A. 1976. Introduction to Discrete-Time Signal Processing, Wiley, New York.
TUFTS, D. W., and KUMARESAN, R. 1982. “Estimation of Frequencies of Multiple Sinusoids: Making

Linear Prediction Perform Like Maximum Likelihood,” Proc. IEEE, Vol. 70, pp. 975–989, Septem-
ber.

ULRYCH, T. J., and BISHOP, T. N. 1975. “Maximum Entropy Spectral Analysis and Autoregressive
Decomposition,” Rev. Geophys. Space Phys., Vol. 13, pp. 183–200, February.

ULRYCH, T. J., and CLAYTON, R. W. 1976. “Time Series Modeling and Maximum Entropy,” Phys. Earth
Planet. Inter., Vol. 12, pp. 188–200, August.

UNGERBOECK, G. 1972. “Theory on the Speed of Convergence in Adaptive Equalizers for Digital
Communication,” IBM J. Res. Devel., Vol. 16, pp. 546–555, November.

VAIDYANATHAN, P. P. 1990. “Multirate Digital Filters, Filter Banks, Polyphase Networks, and Applica-
tions: A Tutorial,” Proc. IEEE, Vol. 78, pp. 56–93, January.

VAIDYANATHAN, P. P. 1993. Multirate Systems and Filter Banks, Prentice Hall, Upper Saddle River, NJ.
VAUGHAN, R., SCOTT, N., and WHITE, D. 1991. “The Theory of Bandpass Sampling.” IEEE Trans. on

signal Procesing, Vol. 39(9), pp. 1973–1984.
VETTERLI, J. 1984. “Multi-dimensional Sub-Band Coding: Some Theory and Algorithms,” Signal Pro-

cessing, Vol. 6, pp. 97–112, April.

References and Bibliography

1005



VETTERLI, J. 1987. “A Theory of Multirate Filter Banks,” IEEE Trans. Acoustics, Speech, and Signal
Processing, Vol. ASSP-35, pp. 356–372, March.

VIEIRA, A. C. G. 1977. “Matrix Orthogonal Polynomials with Applications to Autoregressive Modeling
and Ladder Forms,” Ph.D. dissertation, Department of Electrical Engineering, Stanford University,
Stanford, CA, December.

WALKER, G. 1931. “On Periodicity in Series of Related Terms,” Proc. R. Soc., Ser. A, Vol. 313,
pp. 518–532.

WANG, Z. 1984. “Fast Algorithms for the Discrete W Transform for the Discrete foruier Transform.”
IEEE Trans. on ASSP, Vol. 32(4), pp. 803–816, August.

WATERS, W„ and JARRETT, B. 1982. “Bandpass Signal Sampling and Coherent Dection.” IEEE Trans.
on Aerospace and Electronic Systems, Vol. 18(4), pp. 731–736, November.

WAX, M., and KAILATH, T. 1985. “Detection of Signals by Information Theoretic Criteria,” IEEE Trans.
Acoustics, Speech, and Signal Processing, Vol. ASSP-32, pp. 387–392, April.

WEINBERG, L. 1962. Network Analysis and Synthesis, McGraw-Hill, New York.
WELCH, P. D. 1967. “The Use of Fast Fourier Transform for the Estimation of Power Spectra: A

Method Based on Time Averaging over Short Modified Periodograms,” IEEE Trans. Audio and
Electroacoustics, Vol. AU-15, pp. 70–73, June.

WIDROW, B., and HOFF, M. E., Jr. 1960. “Adaptive Switching Circuits,” IRE WESCON Conv. Rec., pt.
4, pp. 96–104.

WIDROW, B., MANTEY, P., and GRIFFITHS, L. J. 1967. “Adaptive Antenna Systems,” Proc. IEEE, Vol.
55, pp. 2143–2159, December.

WIDROW, B. et al. 1975. “Adaptive Noise Cancelling Principles and Applications,” Proc. IEEE, Vol. 63,
pp. 1692–1716, December.

WIDROW, B., MANTEY, P., and GRIFFITHS, L. J. 1967. “Adaptive Antenna syatems.” Proc. IEEE, Vol. 55,
pp. 2143–2159, December.

WIENER, N. 1949. Extrapolation, Interpolation and Smoothing of Stationary Time Series with Engi-
neering Applications, Wiley, New York.

WIENER, N., and PALEY, R. E. A. C. 1934. Fourier Transforms in the Complex Domain, American
Mathematical Society, Providence, RI.

WINOGRAD, S. 1976. “On Computing the Discrete Fourier Transform,” Proc. Natl. Acad. Sci., Vol. 73,
pp. 105–106.

WINOGRAD, S. 1978. “On Computing the Discrete Fourier Transform,” Math. Comp., Vol. 32, pp. 177–
199.

WOLD, H. 1938. A Study in the Analysis of Stationary Time Series, reprinted by Almqvist & Wiksell,
Stockholm, 1954.

WOOD, L. C., and TREITEL, S. 1975. “Seismic Signal Processing,” Proc. IEEE, Vol. 63, pp. 649–661,
April.

WOODS, J. W., and O’NEIL, S. D. 1986. “Subband Coding of Images,” IEEE Trans. Acoustics, Speech,
and Signal Processing, Vol. ASSP-34, pp. 1278–1288, October.

YOULA, D., and KAZANJIAN, N. 1978. “Bauer-Type Factorization of Positive Matrices and the Theory of
Matrices Orthogonal on the Unit Circle,” IEEE Trans. Circuits and Systems, Vol. CAS-25, pp. 57–69,
January.

YULE, G. U. 1927. “On a Method of Investigating Periodicities in Disturbed Series with Special Refer-
ences to Wolfer’s Sunspot Numbers,” Philos. Trans. R. Soc. London, Ser. A, Vol. 226, pp. 267–298,
July.

ZADEH, L. A., and DESOER, C. A. 1963. Linear System Theory: The State-Space Approach, McGraw-
Hill, New York.

ZVEREV, A. I. 1967. Handbook of Filter Synthesis, Wiley, New York.

References and Bibliography

1006



Index

Index
Page references followed by "f" indicate illustrated
figures or photographs; followed by "t" indicates a
table.

3
3M, 526

A
Acceleration, 7
Accumulator, 57-59, 447, 563

system, 57-59, 447
Accuracy, 3, 6, 11, 20-21, 33, 35, 361, 420, 451, 549,

627, 947, 973-974, 1000-1001
mapping, 33

Acoustic, 560
Acoustics, 993-1006
Activity, 3, 274
Adaptive DPCM, 917
adder, 59, 146-147, 563, 600, 643, 648

full, 643
power, 563, 648

Addition, 43, 54, 59, 76, 92, 114, 157, 178, 198, 272,
280-281, 338, 343, 346, 349, 452, 517, 532,
562-563, 568, 580, 594, 596, 605, 617-618,
638, 642-643, 645, 651, 654, 662, 672-673,
698, 701, 762, 789, 805, 846, 891, 923, 971,
975

Additivity property, 66, 72
Adjustment, 901, 907, 941, 977
Advanced, 52, 994
Air, 3, 334, 842, 1000
Aircraft, 5
Algorithms, 1, 4, 6, 9, 473, 493, 510, 513, 523-575,

579-580, 611, 856, 864, 891-892, 899-900,
915, 920, 922-923, 925-927, 929, 936, 941,
943-948, 961-962, 964-967, 969-976,
993-997, 999-1003, 1005-1006

graphical, 9
testing, 6
written, 510

All-pole lattice structure, 611-613, 877-878, 920
All-zero path, 611, 877
Alternation theorem, 700-701
Amplitude, 11-14, 25, 27, 29, 31, 34, 36-37, 47, 49, 52,

54-55, 74, 89, 93, 99, 126, 180, 193-194,
202, 230, 248, 265, 269, 304, 309-311, 320,
383, 415, 437, 520, 641, 648, 685, 747, 756,
821, 904, 915-916

Amplitude modulation, 304
Analog, 1-2, 4-6, 9-14, 19-21, 23-34, 36-39, 41, 44-45,

54, 118, 254, 380, 396-403, 405-408,
410-411, 418, 420-422, 438, 445, 447-448,
450-452, 454-455, 457-458, 505-507, 518,
623, 645, 670, 707, 717-719, 721-728,
731-733, 743-744, 746-748, 750-758, 760,
803, 814-815, 831-832, 907, 912, 1004-1005

instruments, 9, 31
Analog Input, 5, 414, 418
Analog signals, 2, 6, 9, 19, 21, 27, 33, 452

A/D conversion, 6, 19, 21, 452
D/A conversion, 452

Analog-to-digital (A/D) conversion, 19, 396
quantizer, 19

Analog-to-digital conversion, 11
Analytical solution, 366
and, 1-25, 27-41, 43-150, 151-228, 229-254, 256-275,

277-285, 287-303, 305, 307-321, 323-392,
395-460, 461-522, 523-557, 559-575, 593,
595-649, 651-668, 669-685, 687-688,
690-696, 698-744, 746-758, 760-766,
767-776, 778-781, 783-806, 808-840,
841-897, 910-918, 920, 922-967, 969-979,
987, 992, 993-1006

Angle, 47, 194, 209, 320, 326, 347, 437, 457, 969
Angles, 328, 345, 740

phase, 328, 345

Angular motion, 13
Antennas, 920, 1004-1005

gain, 920
Arc, 556-557, 630
architecture, 549, 871-872, 1000
area, 1, 45, 269, 427, 681, 685, 831, 846, 982
Arguments, 94, 132, 242, 379-380, 383
Arrays, 526, 528-529, 920, 975
Artificial, 367, 390
Assumptions, 419, 562, 646, 881
Atomic, 1003
Attenuation, 118, 314, 317, 422, 449, 664, 694,

705-706, 733-735, 739, 743, 755-756, 830
Attribute, 232, 334
Attributes, 6
Autocorrelation, 43, 120-131, 146, 150, 170, 223, 291,

294, 323, 330-333, 368, 389, 445, 491, 514,
647, 664, 762, 844-852, 855-856, 864-866,
868-869, 873, 880, 882, 884, 886, 889,
893-897, 903, 906, 914, 917, 919, 930-931,
935-936, 945, 950, 977, 979

Automatic, 905, 993-995, 997-998, 1000-1003
Automatic control, 993-995, 997-998, 1000,

1002-1003
Availability, 11, 670
Average, 35, 48-50, 57, 92-93, 114, 117-118, 182, 234,

236-239, 251-252, 332, 349-350, 380, 433,
512, 626, 761, 844, 901-902, 914, 917, 919,
928, 958, 973, 979, 981, 994-996

Average value, 238, 626, 981
Axis, 52, 178, 194, 212-213, 269, 280, 326, 328, 339,

401-402, 430, 718, 721, 725, 728, 736-738,
740, 798, 843

B
Back, 94, 231, 500, 619, 908, 964
Backward, 592, 611, 615, 719, 721, 858-860, 862-863,

874-875, 880, 893-894, 944, 949-950, 953,
955, 957, 960, 962, 964-965, 967, 969-970,
978

Backward difference, 719, 721
Backward prediction error, 858, 860, 862, 967
Band, 31, 245, 340, 349, 358, 399, 407, 423-428, 430,

433-434, 438, 448-452, 456-457, 639-641,
672, 675-676, 687-688, 693, 701, 704-706,
708-709, 714, 716-717, 733, 736, 740-741,
745-749, 795-798, 804-805, 821-823,
836-839, 905, 912, 915-916, 987, 996-997,
1004-1005

Bandpass filters, 337, 340, 703, 721, 728, 750, 811
Bandwidth, 20, 31, 271-273, 345-347, 349, 382-383,

386, 391-392, 401, 407, 410-411, 422-423,
427-428, 430, 438-439, 444-445, 452, 454,
456, 501, 676, 691, 708-709, 713, 732, 734,
740-741, 743, 749, 769, 783, 795, 798,
805-806, 814-815, 912, 916, 928, 987

aliasing, 20, 31, 401, 407, 410, 423, 428, 430,
438-439, 445, 454, 769, 783, 798, 806

base, 53-54, 616, 1000
Basic, 4, 7, 12, 17, 19, 37, 43, 45, 59-60, 69, 71,

94-95, 104, 111, 115, 186, 197, 205,
232-233, 254, 256, 308, 317, 321, 337, 340,
355, 371, 414-415, 420-421, 445, 448,
450-451, 457, 500, 524-525, 534-535,
537-538, 540, 542, 545, 572, 581-582,
599-600, 606, 618-619, 814-816, 832, 878,
881, 927-929, 957, 961-962, 999-1000

size, 111, 448, 715, 927, 929
Basis sequences, 511
Bias, 626, 641
Bilateral z-transform, 159
Bilinear transformation, 728, 730-732, 743-744, 746,

750, 753, 755-758
frequency warping, 730

Binary numbers, 35, 416, 642, 662
Biological, 19, 273-274
Bipolar, 416, 418, 421

Bit-reversed order, 535-536, 538, 548, 552
Black, 8, 56, 61
Black box, 56, 61
Blackman windows, 682, 752
block diagram, 5, 56, 59-61, 63, 95-96, 141, 355, 383,

414, 560, 578, 586-587, 599-600, 645, 649,
785, 802, 806, 815-816, 858, 907, 917-918

functions, 917
nodes, 599-600, 645
reduction, 806
structures, 586, 599-600, 785
terminals, 56, 907

Block processing, 923
Blocking, 231
Bottom, 545, 547
Bounds, 999, 1001
buffer, 414, 457
Building Blocks, 43, 59-61, 115, 197, 205, 233, 606,

627, 655
Bus, 414

circuit, 414
Butterworth filters, 733-735, 742

bandpass, 733

C
Calculations, 282
Calculators, 32, 93
Carrier, 275, 304, 356, 423, 427, 520, 814

signal, 275, 304, 423, 427, 520, 814
Carriers, 437
carry, 13, 169, 187, 189, 192, 211, 419, 612, 617-618,

662, 717, 878
Cascade-form structures, 581, 596, 603
Catastrophic failure, 947
Cation, 588
Cauchy integral theorem, 160-161
Causal sequences, 122, 266
Causal systems, 218, 223
Causality, 71, 85-86, 91, 200-201, 218, 671-672,

674-675, 677
Center, 302, 340, 422-423, 434, 508, 678, 720
Center of gravity, 302
changing, 4-5, 211, 342, 390, 418, 435, 448, 457, 486,

549, 768, 771, 799, 973
Channel, 8-9, 41, 357-358, 371, 374, 387, 391, 407,

414, 806, 814-816, 819-821, 824-830,
838-839, 900-901, 903-908, 910, 920, 922,
935-936, 945-947, 960, 971-973, 976, 1003

Channels, 6, 334, 357, 445-446, 814, 825, 907-908,
929, 996-998, 1003

Characteristic equation, 102-103, 107
Characteristic polynomial, 101, 110
Chemical, 231-232

synthesis, 231-232
CIC, 787-789
Circles, 175, 737
circuit parameters, 452
Circuit theory, 996, 1000-1002, 1004
Circuits, 1-2, 4, 451-452, 616, 993-996, 999,

1001-1002, 1004, 1006
integrated, 1, 451
short, 1006

Circular, 457, 477-479, 483-494, 496-497, 514-515,
557, 559, 630

Circular autocorrelation, 491, 514
Circular frequency, 490, 492
Clipping, 419, 643, 645
CLOCK, 447-448, 803-804, 872, 970

frequency, 448, 804
clocks, 799, 803
Closed-loop control, 930
Closed-loop control system, 930
Closed-loop system, 930-931
Closing, 37, 831
Clutter, 349
Coding systems, 917
Coefficient, 95, 97-100, 105, 109-111, 114, 131, 153,

1007



164, 166, 173, 182, 190, 192, 197-198, 206,
218, 240, 256, 297, 510-513, 571, 577, 589,
598, 628, 630, 633-638, 654, 666, 718-719,
801, 861-863, 865-867, 869-872, 895-896,
930-932, 934, 936-938, 940-941, 992, 1000

Collection, 43, 178
Columnar, 993
Columns, 511, 528-529, 800
Communication, 39, 119, 334, 358, 391, 414, 445,

549, 712, 841, 900-903, 905, 907-908, 929,
935, 946, 996-997, 1003-1005

and engineers, 996
characteristics of, 358, 900-901, 903, 929, 946
feedback, 997
reception, 908
transmission, 903, 907-908

Communications, 5, 19, 118-119, 131, 275, 334, 358,
371, 423, 438, 445, 908, 911-912, 971-972,
994-998, 1000-1001, 1003

network, 908
standards, 1003

Complex cepstrum, 267-268, 368-369
homomorphic deconvolution, 268, 368

Complex function, 177
Complex logarithm, 369
Complex numbers, 9, 534-535, 552, 605

graphical representation of, 9
notation, 9

component, 27, 29, 39, 51, 72-73, 105-106, 119, 132,
193-194, 216-217, 231, 236, 238, 248-249,
268, 270, 336, 351, 355, 380, 430, 432, 437,
545, 591, 649-650, 672, 785, 826, 834-835,
881, 888, 897, 916, 933

type, 835
Compressed, 247
Compression, 507, 730, 806

images, 806
quantization, 806

Computations, 1, 4, 36, 108, 120, 334, 417, 442, 493,
500, 524, 530-531, 533, 535-536, 538, 542,
544, 547-548, 560, 569, 571, 578-579, 584,
590, 615-616, 622, 631, 687, 693, 790, 797,
858, 945-946, 965, 967, 971, 974

Computer, 1, 4-6, 32, 53, 60, 121, 148, 337, 361, 370,
375, 391, 414, 452, 493, 497, 573, 578-579,
616, 621, 638, 643, 670, 687, 703, 741,
743-744, 751, 754, 868, 970, 973, 997-1000,
1002-1003, 1005

simulation, 973, 981
Computer software, 670, 981
Computer-aided, 1005
computer-aided design, 1005
Computers, 1, 36, 93, 549, 622, 997-998
Concentration, 349, 351
Conduction, 232
Conformal mapping, 728
Conservation of, 244

energy, 244
Constants, 17-18, 65, 99, 102, 105, 107, 122, 161,

203, 336, 477
exponential, 18, 102, 107
representation of, 65

Construction, 56, 428
Continuity, 13, 275, 923
Continuous, 4, 9-25, 31, 37-38, 40-41, 151, 218,

230-232, 235, 237-238, 240-242, 247-248,
252-254, 256-258, 260-261, 269, 272,
274-277, 295, 333, 396, 401-405, 407,
409-413, 415, 422-423, 428-429, 439-444,
452-455, 470, 476, 493-494, 500, 624, 723,
769, 772, 797-798, 847-848, 852, 874, 1001

Continuous-time signals, 17-18, 23, 151, 218, 231,
247, 253-254, 256-257, 261, 275, 277, 295,
396, 402, 407, 410, 439, 442, 444, 452, 464

discrete-time processing of, 407
LTI, 151, 218, 410
sampling of, 396, 410, 439, 442, 452

Contours, 554, 556-557
Contrast, 8, 15, 18, 32, 45, 76, 91, 93-95, 112, 118,

178, 232, 247, 249, 254, 328, 511-512, 592,
621, 900, 941, 948, 971

Control, 2, 6, 19, 413-414, 423, 548, 633, 715-716,
841, 903, 905, 930, 993-995, 997-998,
1000-1003, 1005

adaptive, 900, 903, 905, 930, 994-995, 997-998,
1000-1003, 1005

numerical, 998, 1000, 1002
segment, 2

Control systems, 841, 900, 903

Controller, 903
Controlling, 652, 936, 941
Controls, 19, 903
Conversion, 2, 6, 11, 19-21, 28, 34, 36-37, 40, 417,

420, 423, 442, 445, 448, 452, 593, 595, 614,
645, 718, 733, 750, 753, 761, 768-772,
780-781, 784, 787-788, 792-794, 797-800,
802-804, 814-816, 862, 1003-1004

Convolution, 43, 71, 75-88, 90-92, 94, 96-97, 108, 118,
121, 129-130, 135-138, 141-142, 144, 150,
168-170, 172-173, 181, 213, 218-219,
221-222, 289-290, 294-296, 309, 320,
330-331, 334, 357-359, 371, 389, 409,
435-436, 442, 477, 483-488, 491-494,
496-498, 503, 514-516, 554-555, 558-559,
567-568, 575, 580, 591-592, 676-677,
680-682, 684, 771-772, 779, 812, 859-860,
994, 1001

Convolution:, 85, 150
Fourier transform of, 295, 309, 334, 409, 435, 681

Convolution:
operation, 85

Convolution sum, 71, 75-77, 79-80, 94, 309, 484,
591-592, 677, 859-860

Coordinates, 2-3, 357, 436-437, 737
Corners, 426
Correlation, 4, 43, 118-119, 125-127, 131, 144-145,

150, 170, 173, 222, 290, 296, 323, 329-331,
445, 448, 490-492, 550, 567, 850-851, 856,
871, 873, 875, 883, 892, 912, 914-915, 936,
938, 940-941, 971-972, 975, 1003-1005

cost, 6, 21, 111, 452, 549, 662, 908, 976
Covariance, 513, 932
Covers, 35
Cross, 175, 291, 330, 553-554, 847, 903
crosstalk, 908
Curing, 643
Curves, 457, 711, 716, 935-936, 971-972
Cutoff, 385, 391, 422, 450-451, 453, 675, 704, 709,

711, 713, 733, 739, 749, 758, 776, 780,
831-832, 839

Cycles, 12-14, 23, 247, 255, 384, 638-643, 654-655,
915, 970, 993-996, 999, 1004

D
Damping, 47, 356, 693-694

critical, 694
data, 11, 126, 312, 352, 357, 414, 418, 451, 458,

497-501, 503-504, 523-526, 528-531,
535-536, 538-539, 547-556, 567, 571-572,
592, 750, 792-793, 806, 858-859, 875,
881-882, 884-885, 903, 906-911, 915, 917,
920, 922-923, 927, 929, 934, 936-937, 948,
975, 979, 994, 996-1000, 1002, 1005

delays, 792, 910, 922
processing of, 11, 793, 996-997, 999

Decibels, 35, 127, 329, 382, 419, 633, 636, 676
Decisions, 907
Decomposition, 72, 74, 85, 132, 188, 196-197, 230,

232, 254, 365-366, 391, 517, 525, 528,
544-545, 785, 788, 834-835, 837, 943

Degree, 2-3, 11, 118, 443, 548-549, 588, 595, 633,
635, 676-677, 700, 801, 830, 862, 880

delay, 36, 52-54, 57, 60, 62-64, 73-74, 76, 94, 96, 112,
118-119, 131, 146-147, 211, 302, 336,
353-354, 361, 364, 366, 381-382, 413, 422,
578, 580, 598-599, 662, 671, 684, 709, 718,
802-803, 819-820, 822, 871, 906, 922

minimum, 364, 366, 662, 996
Delays, 57, 60-61, 73-75, 94, 111-112, 114, 146, 377,

384, 792, 802, 910, 921-922
Demodulation, 437-438, 445
density, 236-240, 244-245, 247, 251-255, 260-264,

271-272, 277, 291, 294, 298, 321-322,
329-330, 333, 351-352, 375, 420, 449-450,
665, 704, 843, 846-848, 852-855, 874, 880,
883, 886, 892-893, 896, 981, 983-984, 1001

Dependent variable, 6, 52
Depth, 131, 654, 915
Design, 2, 12, 37, 55, 61, 71, 111, 130, 141, 172, 224,

260, 295, 317, 334, 336-337, 340, 357-358,
370, 379, 382-385, 391, 417, 438, 452, 663,
669-766, 772, 776, 780, 783, 799, 802, 804,
806, 814, 820-822, 829-830, 835-836, 841,
881, 883, 891, 897, 919, 987-992, 993-994,
996-1000, 1002-1003

Design for, 757, 992
Design process, 670
Deterioration, 6, 568

Deviation, 445, 637, 985
standard, 445, 637, 985

Diagonal matrix, 930, 941
Diagrams, 569, 754
Dies, 89, 106
Digital, 1-2, 4-6, 10-11, 18-20, 27-28, 31-32, 36-37,

39-41, 43, 55, 71, 91, 118, 130-131, 178,
205, 289, 317, 328, 336-337, 341-346,
357-358, 370, 374, 376, 379, 382-384,
390-391, 413-417, 420, 422-423, 434, 439,
445, 449-452, 457, 493, 497, 513, 549, 561,
577-579, 588, 615-616, 618, 622, 626,
637-638, 641-642, 645, 654-655, 660, 662,
664, 669-766, 767-840, 856, 899-904, 907,
917, 934, 946, 971-972, 993-1005

imaging, 784
oscilloscope, 31
signals, 1-2, 4-6, 10-11, 18-20, 27, 31-32, 36-37,

40, 43, 55, 71, 91, 118, 130-131, 178,
289, 358, 376, 379, 413-417, 420,
422-423, 439, 445, 449-452, 457, 497,
513, 712, 750-751, 761, 768, 772, 777,
805-807, 809-810, 813-816, 832-833,
835, 907, 917, 994-999, 1001-1005

video, 19, 27, 768, 1003
Digital frequency, 355
Digital signal processing, 1-2, 4-6, 36-37, 43, 55,

130-131, 289, 334, 413, 434, 549, 561, 579,
615-616, 654, 767-840, 856, 996-997, 1000,
1002, 1004

Digital signal processors, 6, 417, 439, 473, 549, 571,
618

Digital signals, 6, 20, 27, 36, 417, 452, 513, 768, 996,
1002

Digital-to-analog (D/A) conversion, 20, 396
zero-order hold, 20

Digits, 31-32, 541, 568, 615-616, 633-636
Dimension, 12
Direct, 71, 87, 100, 111-115, 130, 141, 145, 152, 170,

184, 187, 242, 275, 278, 292, 359, 379, 383,
405, 434, 528, 532-534, 553-556, 560-561,
564, 566, 577, 580-581, 585-586, 588-590,
593-598, 600-602, 604-606, 608, 612-614,
618-619, 627, 633-634, 636, 654-655,
658-660, 662, 666-667, 685, 751, 754-755,
757, 797, 857-858, 860-862, 864, 867, 878,
900-902, 943-945, 947-948, 960, 964-965,
970-975, 977, 995

Discontinuities, 234, 243, 422, 512, 682, 920
Discrete cosine transform (DCT), 514
Discrete Fourier transform (DFT), 461, 466, 468, 808

Goertzel algorithm, 523
Discrete-time signal processing, 1002, 1005
Discrete-time signals, 9-10, 12, 17, 21-24, 36, 43-150,

151, 161, 218, 247, 252, 257, 262, 275, 277,
295-296, 398, 402, 439, 442, 772, 797, 847

discrete-time systems, 43, 55, 59, 61, 69, 91-92,
94, 111, 133, 218

sampling frequency, 21-22, 439
sampling period, 21, 44, 277

Discrete-time systems, 43, 55, 59, 61, 69, 91-92, 94,
111, 133, 196, 213, 218, 577-668

all-pole lattice structure, 611-613
convolution sum, 94, 591-592
FIR, 91, 94, 577, 579-597, 600, 602-603, 606,

608-612, 627, 634, 636, 638, 644,
654-655, 659-661, 666

linear time-invariant systems, 111, 358, 577
random process, 664
structures for, 111, 577, 579, 596, 600, 608, 658
white noise, 646-648, 665, 668

Discretization, 277
Discrimination, 308, 334
Distances, 273, 981
Distribution, 127, 232, 236, 245, 252, 261, 631, 846,

982-985, 994
probability, 848, 982-985, 994

Disturbance, 881, 910
divide-and-conquer, 523, 525, 531, 536, 539
Double, 178-181, 191-192, 194, 205, 207, 378, 380,

450, 456, 458, 527, 548, 588
Downsampling, 771, 774-776, 780, 782, 784-787, 789
Draw, 134, 221, 227, 326, 426, 570-572, 659,

662-663, 672, 754
Drop, 587, 675, 687, 772, 843, 890
dual, 277, 294, 490-491
Dummy, 79
Dust, 334
Dynamic systems, 62

1008



E
Ear, 99
Earth, 358, 612, 997, 1005
Earthquakes, 7, 993
Efficiency, 500, 544, 550, 553, 789, 812, 892, 948, 974
Eigenvalues, 930-931, 936
Eigenvectors, 513
Electrical engineering, 1004, 1006

circuit theory, 1004
Electromagnetic, 273-275
electron, 349, 351-352, 993, 995-996, 998
Elements, 4, 37, 62, 112, 450-451, 473, 510, 526-528,

578, 599, 662, 832, 866, 868, 870-872, 882,
894, 914, 920, 927, 930, 941-942, 963

frame, 37
quadratic, 882, 924

Emitter, 1004
Encoder, 446, 448, 806, 824, 917-918

binary, 918
energy, 47-50, 120, 122, 125, 130, 132, 134, 230, 234,

236, 239, 243-247, 252, 254, 257-264, 267,
271-273, 275, 277, 291, 294, 297, 320-322,
329-330, 366, 390, 397, 455, 461, 474, 492,
512, 515, 672, 805-806, 919, 929,
1002-1003

coefficient, 297, 322, 512
ideal, 455, 672, 821
limited, 262, 272-273
power and, 49, 848
specific, 262
work, 262, 1003

Engineering, 1, 4, 12, 118, 247, 1003-1004, 1006
aerospace, 1003, 1006
computer, 1, 4, 1003

Engineers, 523, 622, 996, 998, 1005
Ensemble averages, 849
Equalization, 334, 371, 900-901, 903-904, 922, 960,

996-997, 1001, 1003
Equations, 43, 62-63, 66, 69, 91, 95, 97-98, 100, 102,

109-112, 130-131, 141, 145, 173, 197, 209,
213-214, 225, 283, 322, 357, 379, 382-383,
432, 442, 444, 555, 578, 590, 593, 595,
601-606, 609-612, 630, 646, 655, 659, 665,
688, 756, 762-764, 827, 863-869, 875,
877-878, 882-885, 891-894, 897, 903,
906-907, 911, 914-915, 919, 924-925, 930,
938, 942-944, 948-950, 952-955, 959-962,
965, 975, 998, 1000

Ergodic, 848-851
Error, 20, 31, 33-35, 40, 258, 349, 391, 416-420,

445-446, 448, 456, 466, 512-513, 565-566,
572, 575, 589, 622-626, 636-637, 640-641,
645-646, 648-649, 694, 698-704, 708-710,
712, 714, 716, 761, 763, 821, 841, 857-860,
862-864, 872-875, 881-882, 884-885, 891,
893-896, 901-904, 906-907, 910-911,
913-914, 916-919, 923-925, 927-928,
931-932, 934-937, 940-941, 946-950, 958,
961, 964-967, 970, 973-976, 978-979, 994,
996, 999-1001, 1003

scale, 416, 418-419, 566
Errors, 357, 361, 413, 415, 418, 421, 446-447, 512,

548, 561-566, 568, 575, 615, 619, 622-630,
632-633, 638, 645, 649-650, 655, 698, 761,
763-765, 822, 863, 875, 907, 934-935, 943,
946-948, 957, 961-962, 969, 973-974, 978,
995, 1005

propagation, 1005
residuals, 957, 961

Estimating, 717, 913, 915, 922, 995, 997, 1000
Estimation, 131, 146, 232, 473, 841, 849, 881, 884,

891, 900, 919-920, 923, 925, 948, 958, 973,
993-997, 999-1006

Estimators, 994, 1000-1001
Evaluation, 81, 161, 170, 184-185, 187, 209, 328, 556,

560, 662, 871, 997
Even function, 123, 237, 312, 844, 852
Even symmetry, 245, 252, 261, 507
event, 643
Expectation, 127, 849, 875, 925
Expected value, 331, 333, 843, 849-851, 914, 929,

932, 975
Experiments, 995
Exponent, 153, 257, 277, 619-622, 661, 925

F
Factors, 6, 15, 21, 28, 137, 189, 198-199, 202, 216,

326, 329, 363, 510, 525, 527-530, 533,
544-545, 548-550, 554-555, 565, 567-568,
578-579, 603, 637-638, 645, 654, 831,
941-942, 999

combined, 216, 638, 831
grid, 637

Fading, 929, 946, 998
fast, 946, 998
slow, 946

Failure, 947
Fast Fourier transform (FFT), 473, 493-494, 523-524,

580
Feedback loop, 94-95, 446
Field, 717, 733
Figures, 137, 283, 406, 512, 788-789, 828, 911, 971
Film, 3, 334
Filtering, 3-4, 27, 308, 334, 343, 370-371, 389, 410,

443, 449-450, 452, 473, 492-494, 496-497,
499-500, 514, 523, 552-554, 561, 568, 626,
645, 654-655, 774, 781, 784, 787, 789-790,
792, 797-798, 801, 814, 832, 857, 881, 883,
891-892, 900-904, 912, 916-917, 920,
922-923, 925-927, 929, 934-936, 946-948,
960, 970-971, 993-996, 998-1001

windowed, 995
Filters, 4, 71, 91, 308, 328, 334-342, 347-349, 351,

353, 370, 380, 382, 384, 390, 407, 413, 422,
445, 451-452, 555, 560-561, 577, 581,
584-585, 588, 593-594, 615, 630-632, 634,
637-638, 645, 654-655, 659-661, 666,
669-766, 769, 780, 784-785, 787-789,
792-796, 800, 804-806, 808-814, 816,
818-821, 823-827, 829-831, 837-839,
841-897, 899-979, 987-992, 993-999,
1001-1005

Final value theorem, 213, 220-221
Fine, 621
First-order system, 100-101, 106, 111
Fit, 27, 425, 579
Fitting, 20, 498, 500, 993, 1005
Flat, 331, 368, 391, 448-449, 647, 762, 821
Flexibility, 2, 5-6
Floating-point arithmetic, 548, 568, 579
Floating-point numbers, 620, 622
Floating-point representations, 620, 622
Force, 3, 94, 1000
Forcing function, 103-104, 108, 138, 199

general, 104, 108
Format, 417, 615-618, 622, 661
Forming, 79, 91, 550, 581, 603, 612, 617, 878, 890,

910
Forward prediction error, 857, 863-864, 948
Foundations, 998
Fourier coefficients, 18, 234-235, 238-239, 241-242,

245, 247-249, 256, 265, 272, 298-299, 463,
476, 852

Fourier series, 18, 230-238, 240, 242, 245, 247-250,
253, 256-257, 260, 265, 274-275, 277, 297,
304, 308, 311, 319-320, 453, 460, 463,
476-477, 520, 567, 852-853, 995-996

fundamental frequency, 18, 233, 240
phase spectrum, 237, 245
steady-state response, 308, 319
symmetry and, 567
waveforms, 231-232

Fourier transform, 230-231, 240-247, 254, 256-260,
262-270, 272, 274-275, 277-278, 282-283,
285-288, 291-292, 295-297, 300-305,
308-311, 321-322, 324, 326-328, 330,
332-334, 336, 342, 368, 370-371, 397, 399,
403, 405, 422-423, 429, 433-436, 440-442,
453-454, 460, 461-522, 523-575, 580, 644,
673-674, 680-681, 808, 832-833, 846-848,
993-995, 1002, 1004, 1006

Fourier transform:, 304, 461-522
inverse, 240, 242, 257-258, 267-268, 275, 278,

295, 308, 321, 370-371, 397, 399, 410,
423, 433-435, 468, 493, 509-512, 514,
516, 518, 552, 846

Fourier transform:
magnitude of, 521
pairs, 514
phase of, 469-470
symmetry properties of, 480, 482, 517
unit step function, 435

framework, 11, 698, 902, 922, 1002
Frequency, 4, 12-28, 30-32, 35-36, 38-40, 47, 73, 111,

194, 209, 229-265, 267-305, 307-393,
396-399, 401-413, 422-424, 426-428, 430,

433-436, 439-440, 442, 444, 448-451,
455-459, 461-462, 464-468, 470, 472-473,
476, 490-494, 500-503, 511-514, 517-521,
536, 538-539, 541-545, 549, 552, 555, 557,
559, 569-573, 583-588, 627, 630, 641, 644,
647, 651-652, 654, 661, 663, 670-676,
678-685, 687-688, 690-694, 696-700,
703-716, 718-725, 727-728, 730-736,
738-741, 743-754, 756-758, 760-761,
763-764, 766, 769, 773-774, 778, 780-781,
783, 788, 795, 797-799, 808-810, 812,
814-819, 821-824, 830-832, 839, 846, 848,
889, 904-905, 914-917, 922, 979, 994-995,
997-998, 1000-1003, 1005

carrier, 275, 304, 356, 423, 427, 520, 814
cutoff, 385, 391, 422, 450-451, 453, 675, 704, 709,

711, 713, 733, 739, 749, 758, 776, 780,
831-832, 839

damped, 47
fading, 946, 998
modulation, 292-293, 296, 304, 356, 396, 436,

448-449, 799, 917
reference, 274, 296-297, 371, 672, 904

Frequency bands, 334, 698, 700, 704, 795, 805, 812,
814

Frequency domain, 111, 230, 248-249, 253, 272, 274,
277-278, 283, 288-289, 295-297, 314, 333,
362, 368, 370-371, 385, 406, 408-409, 433,
458, 470, 476, 492-494, 500, 503, 513-514,
716, 719, 802, 889, 1003

convolution in, 295
Fourier transform and, 277, 370
transformers, 716, 1003

Frequency regions, 426, 434
Frequency response, 308-309, 312, 314-317,

320-322, 325-326, 328, 331, 333-334,
336-337, 339-343, 345-352, 354, 358,
362-365, 368, 370-371, 378-381, 384-386,
389, 391, 410-411, 413, 422, 436, 444, 449,
453, 455, 492, 517, 571, 577-578, 583-584,
627, 630, 633-634, 636-637, 644, 647,
670-676, 678-682, 690-694, 696-700,
705-710, 712-715, 722-724, 727-728,
735-736, 738, 740, 743-745, 747, 750-754,
756-758, 763-764, 773, 776, 778, 780-781,
806, 809, 818-819, 821-823, 922, 979

Frequency response plots, 316, 979
Frequency shifting, 292, 296, 434, 995
Full, 138, 149, 298, 416, 643, 708, 868, 907
Full-duplex, 907
Functions, 1-4, 8-9, 12, 39, 43, 76, 173, 183, 188, 198,

231, 247, 249, 269, 275, 278, 280-281, 295,
308, 311-313, 322, 324-325, 329-331, 333,
362, 364-365, 370, 387-388, 403, 431, 436,
449, 457, 459, 503, 582, 594, 625, 652, 665,
682-685, 697, 741, 801, 838-839, 841-843,
868-869, 895-896, 985, 996, 999-1000

sinusoidal, 12, 39, 231, 308, 370, 403, 457

G
Gain, 77, 326, 332, 335, 337-338, 344, 380, 383-384,

391, 418, 421, 569, 574, 581, 584, 587, 599,
633, 635-636, 638, 644, 652, 667, 732, 745,
754, 757, 766, 905, 917, 919-920, 938-941,
943-944, 957, 962-967, 973

Gains, 584, 754-755, 974
Gamma rays, 275
Gaussian, 420, 574, 763, 984-986
General, 1-2, 4, 21, 24, 26-27, 31, 36, 43, 56, 61-63,

65-66, 70-73, 76-77, 91-92, 94, 97-99,
101-102, 104, 108, 112, 115-118, 130, 133,
156, 177-178, 182-184, 187, 189-190, 197,
222, 235, 244, 261, 273, 311, 317, 319, 332,
334, 350, 354, 361, 365, 413, 417, 419, 423,
433, 452, 490-492, 503, 521, 524, 534, 549,
560-561, 564, 577-579, 598, 602-605, 611,
619-620, 622, 627, 629, 641, 645, 652,
670-671, 679-680, 725, 746, 748, 750, 806,
834, 843, 849-850, 894, 906

Generation, 3, 343, 437, 618, 712, 918, 920
Generator, 11, 355, 374, 458, 870-872, 917-918,

981-982, 985
Generator matrix, 870-872
Geometric, 89-90, 153, 248, 253, 262, 287, 326-328,

380, 383, 885
Gibbs phenomenon, 260, 671, 674, 685
Glass, 231
glitch, 421
Gold, 10, 655, 751, 997, 1003

1009



Gradient method, 925, 993
Granularity, 800
graph, 10, 48-49, 80, 137, 156, 178, 207-209,

238-239, 284, 320, 391, 438, 484, 506, 534,
541, 545, 549, 569-572, 599-600, 602, 642,
676, 709, 979

Graphs, 47, 77, 79, 285, 312, 569, 599-600, 654, 709,
995

Gravity, 302
Greater than, 27, 35, 186, 399, 416
Green, 8, 231
Group, 336, 354, 366, 381-382, 604
Groups, 814

H
Hamming window, 683, 686, 752-753, 756, 758, 761,

765
Hand, 5-6, 8, 20, 24, 26, 49-50, 52-53, 59-63, 67-69,

73-74, 80, 82-84, 86, 96-97, 99, 101-103,
106-107, 109-110, 116-118, 126-127, 130,
154, 161, 183-184, 192, 199, 218, 232-233,
244, 256, 271, 289, 291, 294, 313, 318,
320-321, 449, 463, 482, 507, 518, 536, 549,
567, 581, 594, 620-621, 643, 652, 687, 701,
721, 740, 796, 820, 843-844, 861, 867-868,
876, 946-947, 967, 971

Handling, 622
Hanning window, 503-504, 570, 683
Harmonic, 12, 18, 236, 297, 374-375, 459, 517

order, 459
Harmony, 275
Heart, 3
heat, 232
Height, 37, 269, 421, 681
Help, 509, 785
Hermitian matrix, 942
Hertz, 12, 21-22, 262, 401, 451, 752
Hertz (Hz), 12
High-frequency, 31, 271, 316, 512, 805, 946
High-speed, 998, 1003
Hilbert transformer, 436, 443-444, 704, 709, 712-715
History, 98, 891
Homogeneous difference equation, 100-101, 107, 356
Homogeneous solution, 100-101, 103, 105, 107
HP, 381-382, 746-747
Human, 612
Hybrid, 908-911

I
IBM, 1005
Identity matrix, 828, 930, 938
IEEE, 391, 546, 622, 988-991, 993-1006
Impact, 567, 999
Improper rational function, 188-189
Impulse response, 76-77, 79-91, 94, 96-97, 108-110,

115, 117, 128-130, 134, 137-145, 148-149,
172, 181-183, 213, 222-227, 308, 310-311,
315, 321, 325, 330-333, 336, 342, 355,
358-362, 366-368, 370, 372, 376-378,
385-388, 390-391, 410-413, 436, 443-444,
455, 484, 493-494, 498, 516, 553, 555, 575,
577, 583, 647, 656, 658-661, 667, 670-671,
673, 688, 705-707, 715, 718, 723, 727, 745,
750, 757-758, 762-763, 765, 770-773, 780,
782-783, 789, 801, 803, 809, 811-813, 836,
881, 904, 995, 997-998, 1002

function, 76, 80, 94, 108-109, 138, 181-183,
224-227, 308, 311, 325, 331, 336, 355,
359-362, 367-368, 370, 377, 387, 391,
410-411, 436, 455, 516, 555, 656,
659-660, 667, 705-706, 718, 727,
757-758, 763, 765, 770-771, 809, 821,
855, 893

Independent variable, 2-3, 8-9, 36, 44, 52-54
Index, 9, 19, 75-77, 79, 89, 120, 211-212, 238,

287-289, 291, 428, 462, 468, 484, 486, 488,
508, 525-527, 535, 695, 697, 770-771, 843,
901-902, 935-936, 977-978

Indirect method, 100, 500
Induction, 66, 85, 303, 590, 873
Infinite frequency, 22, 397, 440
Information, 3-5, 20, 26-27, 33, 58, 93, 118-120, 134,

153, 210, 232, 237, 245, 253, 273-274, 278,
291, 325, 468, 512, 599, 762, 833, 839,
903-904, 906-907, 928, 993-994, 998-999,
1001-1003, 1005-1006

Initial conditions, 96-98, 100-102, 105, 107-110, 145,
151, 197-199, 209, 213-218, 356-357, 383,

555, 742, 954, 969
Initial value theorem, 172-173, 227
initialization, 870, 944-945, 961, 965-967, 969
Input, 4-5, 19, 39, 43, 55-58, 60-69, 71-73, 75-77,

79-83, 85-104, 106-109, 111-112, 114-117,
127-130, 133-135, 137-142, 145, 149, 181,
198-200, 202-205, 209, 213, 215-218,
223-225, 230, 308-315, 317-322, 329-334,
336, 356-358, 366-373, 376-380, 382-383,
385, 388, 408-411, 414-421, 436, 447-448,
450-451, 456, 484, 492-494, 497-500, 513,
517, 529-530, 534-536, 538-539, 541-544,
548-550, 552-556, 562-563, 566, 569, 571,
574, 588, 598-601, 608-609, 611-612,
638-639, 641-650, 654-655, 658-660,
663-665, 676, 704, 706, 709, 718, 752-754,
760-764, 769-774, 776, 780, 782, 784,
786-787, 789, 803, 810-812, 814, 825-826,
831, 833, 839, 853-855, 857, 875-882,
892-893, 896, 901-903, 909-911, 915-916,
919, 931, 933, 970, 973, 985

Input filter, 704
Inputs, 58, 62, 68-69, 75, 85-86, 94, 121, 319, 333,

372, 376-377, 571, 579-580, 808, 817, 819,
825

Inserts, 459
Instability, 203, 694, 934, 944, 947, 966, 974
Instantaneous frequency, 998
Institute of Electrical and Electronic Engineers (IEEE),

622
Instruments, 9, 31, 232
integer, 9-11, 14-15, 17, 19, 24, 30, 35-36, 38, 44, 52,

54, 97, 118-119, 133, 135-136, 214, 221,
233, 272, 303, 350-351, 390, 423-426, 431,
434, 442, 460, 521, 603, 615-616, 619,
621-622, 661, 709, 725, 770-773, 782,
797-799, 831, 836, 843, 847

Integrated, 1, 414, 451
circuits, 1, 451

Integrated circuits, 451
Integration, 1, 161, 171, 184, 227, 233, 238, 242, 256,

261, 278, 295, 398, 674, 728
Integrator, 410-411, 447-450, 753-754, 787-788, 999
Interconnection, 69-71, 82, 85, 139, 141, 391, 578,

627, 652
Interconnections, 69-70, 611
Interest, 4, 19-20, 28, 165, 198, 218, 230, 234, 343,

423, 434, 561, 567, 634, 715, 773, 799, 812
Interference, 3-4, 119, 126-127, 131, 357, 407, 881,

900-901, 904-906, 911-916, 920-921, 998,
1000-1003

Intermediate, 566, 569, 579
Internal, 56, 61, 643, 654-655
Interpolation, 20, 28-29, 36, 401, 405-406, 409, 413,

431, 433-434, 438-443, 450-453, 455,
458-460, 464-465, 470, 769, 772, 778,
780-781, 783, 787-789, 791-794, 797-798,
800, 804-805, 811-813, 831-832, 837,
996-998, 1000, 1006

complex, 438, 452, 811, 996, 1006
Intervals, 9, 398, 415, 993
Inverse Fourier transform, 242, 268, 278, 321, 397,

399, 410, 423, 433-435, 493, 846
Inverse problem, 325
Inverse systems, 357-358, 360
Inverse z-transform, 151-152, 160, 168-169, 181,

183-186, 188-189, 193-194, 202, 205, 218,
220, 369-370

Isolation, 908
Iterative process, 596, 693, 862

J
Joint probability, 843
Joint probability density function, 843

K
Kaiser window, 686

L
Label, 131
Lag, 120, 123, 128, 850, 935
Laplace transform, 151, 218, 718, 723

one-sided, 151, 218
Large errors, 629
Laurent series, 267, 852
Lead, 8, 11, 427, 569, 640, 950
Leading, 574
Leakage, 501-503

Learning curves, 935-936, 946, 971-972
Least squares, 762, 948, 970, 1005
Least-squares method, 765, 1005
Less than, 30, 88, 90, 110, 188, 410-411, 415, 420,

452, 463, 501, 554, 612, 637, 662, 723, 800,
885, 907, 945

Less than or equal to, 88, 518
Light, 231-232, 275, 334
Limits, 86-87, 122, 138, 233, 241, 274, 295, 501, 625,

641, 676-677, 694, 788, 905, 981
Line spectra, 240
Linear, 4, 17-18, 20, 34, 36, 43, 47, 65-73, 75-77,

80-92, 94-95, 97-100, 105, 108-112,
114-116, 118, 122, 130-131, 133-135,
137-138, 140, 146, 149, 161, 165, 173,
181-183, 188, 193, 197, 200-201, 204, 218,
230, 232-233, 240, 273, 285, 289, 308-311,
313-315, 317-321, 331, 334-336, 355,
357-359, 362, 366, 371, 380, 384, 389, 392,
402-403, 407-410, 416, 441-443, 446,
452-453, 455-456, 458-459, 471-473, 479,
484, 486, 492-494, 496-500, 511, 517-518,
550, 552-554, 559, 561, 568, 575, 577-578,
586, 589, 592, 612-613, 631, 634, 636-640,
645, 647, 649, 654-655, 674, 676-681,
687-688, 690-694, 697, 699, 701, 703,
707-709, 712, 742-743, 750-752, 755-756,
761-762, 774, 780-781, 783-785, 787,
804-805, 819-821, 832, 836, 838, 841-897,
905-906, 911-922, 924-925, 930, 938,
947-949, 960, 976-979, 983, 987-992,
998-1002, 1005-1006

array, 800, 920-921, 995, 1001
Linear algebra, 514
linear feedback shift register, 146
Linear interpolation, 20, 36, 441, 443, 455, 459, 800,

804, 832
Linear predictive coding (LPC), 917
Linear predictor, 857, 859-860, 862, 875-876, 891,

894, 912-913, 915-916, 948-949, 977-979
Linear programming, 687
Linear quantizers, 416
Linear time-invariant (LTI) systems, 43
Linear time-invariant system, 77, 80-81, 83-92, 100,

108, 114-116, 118, 137, 140, 181-182, 197,
200-201, 213, 309-311, 313, 315, 318-321,
331, 334, 357, 359, 362, 366, 517, 672, 718,
762, 785

Linearity, 65-66, 68, 71-72, 75, 98-99, 105, 134, 138,
140, 161, 163-164, 173, 188, 230, 285, 296,
418, 477, 492

Lines, 34, 231, 236, 239-240, 273, 275, 374, 403, 435,
503, 908

List, 639
Loading, 792
Logic, 4, 616
Loop, 94-95, 446, 908-909, 930-931, 943

closed, 930-931
Lower, 6, 89, 97, 210, 233, 246, 427, 450, 468, 503,

517, 594-595, 622, 651, 670, 677, 684-685,
704, 713, 745-749, 751, 789, 805, 861-862,
872, 880, 891, 935, 946

M
Machine, 622, 868, 995
Magnetic, 6, 53
Magnitude:, 327
Magnitude condition, 819
Magnitude spectrum, 237, 244, 261, 288-289, 453,

502-504, 506
Main lobe, 245-246, 503, 681, 684, 921
Mapping, 22, 25, 33, 165, 526-527, 719-722, 724-726,

728-731, 733, 746, 748, 750, 800, 983-985
Masks, 504, 916
MASON, 654, 1001
Mathematical model, 419
MATLAB, 766
Matrices, 541, 830, 892, 951, 996-998, 1006

inverse of, 951
square, 951, 998
unit, 830, 996, 1006

Matrix, 357, 472-473, 510-511, 513, 540-541, 593,
702, 827-830, 838-839, 856, 861, 864-866,
868-872, 882-883, 885, 892, 894, 896,
924-926, 930-932, 935-936, 938, 940-945,
949-952, 971-972, 1006

relationships, 473, 593, 861, 870
Matrix multiplication, 541
Maximum-phase systems, 390

1010



Mean, 35, 57, 69, 160, 257-259, 267, 331-332, 419,
456, 574, 637, 646-648, 664-665, 718,
762-763, 841, 844, 848-851, 859, 862-864,
872, 881-882, 884-885, 891, 895-896, 900,
916, 923-925, 927, 929-932, 934-935, 946,
970, 975-976, 981-982, 986, 997

Mean value, 57, 332, 419, 646-648, 844, 849-851,
929, 975, 981-982, 986

Measurement, 358, 375, 903, 993
Measurements, 9, 37, 231-232, 349, 351, 358, 612,

762, 903, 998, 1004
Mechanical, 232
memory, 18-19, 53, 60-62, 91-92, 96-97, 111-112, 114,

224, 457, 497, 536, 548, 552, 561, 571,
578-580, 597-598, 654, 670, 799, 975, 1001,
1004

Memoryless, 59-60, 62, 67, 416
Memoryless systems, 62
Method of, 111, 187, 197, 337, 500, 727, 762, 865,

926, 948, 970, 981, 998, 1001, 1006
Methods, 6, 47, 71, 111, 151, 183-184, 226, 273-274,

492, 497, 514, 544, 552, 578, 580, 655, 676,
680, 715-719, 728, 751, 794, 802-803, 831,
917, 925-926, 947, 964, 993-994, 996-1004

arbitrary, 71, 768, 802, 831, 998, 1003
direction, 920, 926
indirect, 964
least-squares, 917, 947, 994, 999, 1001

Microphone, 146
Midtread quantizer, 415-417
minimum delay, 364
Model, 11, 20, 419, 448-449, 456-457, 512, 561, 612,

625-626, 638, 645-646, 649-650, 761-763,
789, 792, 881, 902-903, 916-920, 975,
993-994

components, 448, 512, 561, 646, 649
elements, 920
mathematical, 11, 419, 881, 902, 993, 1005
stochastic, 11, 993

Modeling, 91, 762, 900-902, 1002, 1005-1006
Models, 645, 763, 856, 901, 996-997, 1000
Modulation, 292-293, 296, 304, 356, 396, 436-437,

446-449, 452, 799, 811, 917
Modulation theorem, 292-293
Modulator, 64, 451, 815-816, 907
Module, 459
Moments, 847, 849
Motion, 12-13, 37

relative, 37
Motivation, 37, 43, 232
Moving average, 57, 114, 117-118, 182, 312, 349-350,

380, 994-995
Multiple sampling, 768
multiplier, 60, 64, 615
Multirate signal processing, 768-769, 802, 831

N
Natural, 3, 97, 130, 198-200, 216, 273, 513, 536, 538,

548, 717, 800, 920
Natural response, 97, 130, 198-199, 216
Newton, 231, 926
Newton, Isaac, 231
Nichols, 901, 1002
Nodes, 599-600, 644-645
Noise, 3-4, 11, 31, 35, 40, 118-119, 127, 129, 131,

146, 274, 334, 407, 413, 419-420, 445,
447-452, 456-457, 460, 513, 561, 563, 574,
577, 625-626, 645-650, 652-654, 663-666,
668, 761, 763, 841-843, 845, 852-855, 864,
874-876, 880-881, 883, 886-887, 893-894,
896-897, 900-901, 903-906, 910, 916-918,
927-929, 931-934, 946-947, 966, 971-972,
974-979, 981-982, 984, 995, 999,
1001-1002, 1004-1006

Nonlinear quantization, 626
Nonlinear system, 69, 320, 639
Nonlinear systems, 65
Normal, 530, 538, 541-543, 548, 552, 569, 693, 859,

864-865, 868-869, 875, 882-885, 891-894,
984, 1001

Normal distribution, 984
Normal equations, 859, 864-865, 868-869, 875,

882-885, 891-894
Normalizing, 969
Not equal to, 67, 223, 418
Notation, 9, 56, 73, 82, 173, 278, 296, 433, 441, 477,

492, 532, 580, 612, 724, 744, 782, 936, 953
Notes, 995
Numbers, 9-10, 19-20, 31-32, 35-36, 69, 127-128,

135-136, 214, 374, 396, 416, 528, 534-535,
548, 552, 568, 579, 605, 615-623, 625, 642,
645, 676, 789, 981, 984-985, 1006

floating-point, 548, 568, 579, 619-620, 625, 661
Nyquist rate, 28-30, 39, 399, 403, 413, 445, 450, 815,

910-912

O
Objectives, 391
Objects, 334, 349
Observations, 127, 902
Octave, 806, 1005
Odd function, 237, 312
Odd sequence, 672
Offset, 29, 417-418, 421, 428, 431, 433-434, 548, 562,

771
One, 1-3, 5-6, 8-10, 18, 24, 27, 32-33, 36-38, 41, 51,

57, 59-60, 63, 65, 71-72, 76-77, 79, 81,
85-88, 92-93, 99, 110, 112, 119-121, 123,
127, 131, 137, 147, 151, 168-169, 175,
177-178, 181, 185, 195, 199-202, 204-205,
209-215, 221, 226, 230-231, 234, 245,
258-259, 262, 268-269, 274, 277, 289, 297,
314, 338, 351-353, 358, 364-365, 379,
413-414, 438, 442, 448, 456-457, 459, 470,
484-486, 513, 525, 529-530, 533-534, 536,
548, 551-554, 564, 572, 578, 580, 589-590,
604, 606, 609, 615-618, 637-638, 643, 663,
671-672, 714-715, 719, 743, 746, 750-751,
772-773, 788-789, 859, 864-865, 870-872,
896, 907-908, 910, 920, 927, 941, 946-947,
970

One-sided z-transform, 151, 160, 209-215, 218, 221,
226

Open, 701
Optical, 1
Optimization, 687, 692, 695, 700, 714, 821, 830, 841,

900-902, 906, 919, 923, 941
Optimum, 20, 36, 446, 513, 553, 691, 693-694,

703-704, 711, 751, 821, 841-897, 902,
924-925, 931-932, 934-935, 940, 947,
970-974, 977-978, 998, 1000

Order, 2, 10, 20, 31, 34, 36, 59, 70, 77, 82-83, 85, 95,
97-103, 105-111, 113-116, 120, 133, 138,
141, 145, 168, 171, 173, 175, 182-187, 189,
191, 197, 199, 202-206, 233, 252, 261, 289,
291, 318, 350, 353-357, 378-380, 391, 397,
423, 427-431, 446-447, 449-451, 453,
456-459, 530, 534-536, 538, 541-543, 548,
552, 560, 568-569, 574, 581-582, 589-592,
602-604, 609-611, 627, 632-634, 637-638,
641-643, 645, 649-652, 655, 659, 664, 672,
708, 721-722, 726-727, 730-731, 733-734,
736-745, 755-756, 758, 762-763, 785,
788-789, 800-801, 836, 841, 859-862,
864-868, 896, 907, 913, 930, 947-954,
956-961, 963, 965, 974-975, 979, 993-995,
999-1000

Orthogonal matrix, 510
Orthogonality, 511-512, 568, 875, 884-886, 890, 893,

896, 925, 927
Out of, 420, 599, 639, 772, 803
Output, 4-5, 11, 19-20, 31, 35-36, 39, 43, 56-73,

75-77, 79-83, 85-92, 94-97, 99-100, 111-112,
114-118, 127-130, 133-134, 137-139, 142,
145-146, 198-199, 204-205, 209, 213,
215-216, 218, 224, 230, 308-313, 315,
317-318, 320-322, 329-334, 336, 352,
357-358, 366-373, 376-377, 379-380, 383,
385, 414, 416-417, 420-422, 436, 450-451,
458, 484, 493-500, 529-530, 536, 538,
541-543, 548-549, 552-555, 558, 563,
565-566, 569, 571, 573-575, 578-581, 586,
588-592, 598-601, 608-609, 611-613, 631,
638-639, 641-643, 645-650, 652, 654-655,
660, 663-666, 676, 718-719, 752-753, 757,
761-763, 765, 769-776, 778-783, 785-789,
799, 801-804, 809, 811-813, 817, 819,
824-826, 832, 852-855, 857-858, 874-879,
881-882, 884-887, 896, 901-907, 916-919,
921-923, 960, 970, 973-976, 979, 981-982,
985

Output equation, 64-65, 68-69, 71, 95, 115-117, 312,
367, 611, 919

Outputs, 61-62, 65-68, 75, 80, 97-98, 115, 373, 382,
415, 579, 612, 641, 664, 761, 789, 792, 808,
812, 815-816, 825, 834, 878-879, 981

overdamped, 977
Overdamped system, 977

Overlap, 138, 149, 158, 399, 401, 424, 431, 498-500,
503, 552-553

P
pad, 494, 525, 549, 559, 763-764, 994
Parabola, 207
Parallel processing, 579, 868, 892
Parameter estimation, 1004
Parameters, 12, 46, 72, 105, 109-110, 114, 182-183,

208, 274, 317, 324, 334, 346, 355-356, 390,
536, 542, 578-580, 583-584, 586, 593-596,
611-615, 629, 633, 638, 642-643, 659-660,
670, 676, 691, 693, 695-699, 702-704, 706,
709, 714-716, 733-734, 739, 744, 749, 754,
762-765, 798, 849, 855-856, 861, 864,
878-880, 891, 901-903, 917-920, 947, 967,
975-976, 978-979, 985, 998

determining, 109, 183, 317, 578, 594, 861, 903
Partial-fraction expansion, 184-185, 188-195, 197-199,

208, 214-215, 218, 367, 605, 633, 673, 727
Particles, 334
Parts, 19, 40, 65, 69, 96, 100, 131-133, 141-142,

147-148, 198, 216, 278, 283, 298, 300,
374-375, 386, 435, 519-520, 544, 573-574,
659, 661, 663, 667, 674, 752, 754, 979

Passband, 335-337, 340, 342, 349, 351, 368, 407,
451, 670, 674-676, 685, 693-694, 698, 700,
704-709, 713-714, 716, 733, 735-736,
739-744, 746, 755-756, 776, 780, 804, 822,
833, 836, 987

Passband ripple, 675-676, 705-706, 736, 740-741,
744, 755, 795-796, 804

Patterns, 151, 218, 223, 338, 353, 378, 384
Peak value, 38, 127
Performance index, 901-902, 925, 977-978
Period, 10, 13, 15-18, 21, 26, 37-38, 41, 44, 50, 73,

125-127, 134, 147, 233-234, 238-241, 245,
247, 249-253, 255-256, 258, 262, 265,
272-273, 275, 277-278, 292, 295, 298-299,
311, 319-320, 351, 374-375, 401, 406, 408,
412, 440-441, 453, 455, 457-458, 462-463,
476-477, 516, 518, 639, 723, 762, 770, 772,
782, 792-793, 832-833, 907, 920, 981

Period T, 21, 26, 277, 455, 832
Periodic function, 239, 256, 852

Fourier coefficients in, 256, 852
Periodic impulse train, 917
Periodic sampling, 21-22, 25, 397-398, 402, 423
pH, 994, 1000, 1004, 1006
Phase response, 314, 325, 329, 335, 338, 341,

344-346, 353-355, 363, 372-374, 376-377,
379-382, 387-389, 391, 633-635, 659, 670,
672, 718, 741-742, 752-753

Phase shift, 311, 314, 320, 358, 709, 802
Phasors, 13-14, 568

transform, 568
Pitch, 268, 917, 920
pixel, 806
Plant, 902-903, 975
Plots, 40, 223, 254, 316, 391, 511, 573-574, 979

proper, 574
Point, 6, 8, 11, 25, 36, 45, 59, 61, 83, 90-93, 98, 111,

115, 117, 119, 147, 157, 164, 177, 181, 196,
200, 204, 206-207, 256, 259-260, 288,
325-329, 335, 345, 365, 426-428, 445,
468-470, 472-477, 479-480, 482-484,
487-488, 494-501, 503, 505-511, 514-521,
527-535, 537-548, 551-554, 556, 559,
561-566, 568-571, 573-574, 578-580, 586,
615-616, 618-626, 631, 633, 638-639,
643-645, 652, 655, 661, 663-666, 672, 680,
731-732, 757-758, 779, 788, 812-814, 840,
849, 872, 884, 936, 946-947, 974-975, 981,
1001-1002

Points, 29, 45, 153-154, 207, 216, 326, 328, 337, 427,
455, 470, 475, 484, 494-495, 497-501, 514,
522, 524, 528, 531, 534, 536, 539, 544-549,
551-552, 554, 556-559, 566-567, 569,
571-572, 584, 599-600, 631-632, 674-675,
693, 702-704, 720-721, 724-726, 728, 733,
738, 748, 758-759, 937

Polar coordinates, 437
Positioning, 423-426, 434, 797-799
Positive definite matrix, 926
Power, 27, 35, 40, 47-50, 111, 125, 127, 130, 132,

152-153, 155, 157-158, 161, 186-188, 232,
236-239, 247, 251-255, 271-273, 277,
297-298, 329, 331-334, 347, 349, 351-352,
374-375, 384, 407, 419, 445, 449-450,

1011



456-457, 460, 494, 523, 531, 539, 552,
562-563, 567, 648, 650, 652-653, 663,
758-759, 788, 846-848, 852-855, 886,
892-893, 896, 915-916, 929, 931, 935, 969,
995, 999, 1001-1002, 1004, 1006

Power density spectrum, 236-239, 251-255, 271, 277,
298, 333, 375, 846-848, 874, 880, 892-893,
896

power distribution, 236
Power series expansion, 186-188

by long division, 186
Power spectral density, 236-237, 298, 352, 449-450,

460, 852-855, 874, 883, 896, 1001
Predictive, 446, 917, 1002
Press, 993-995, 997-998, 1000, 1003

fitting, 993
Pressure, 274, 842
primary, 7, 670, 795, 916-917
primary input, 916
Prime factor algorithms, 568
Principal, 8
Principle of superposition, 65, 98
Probability, 11, 420, 625, 665, 842-843, 848, 850-851,

901-902, 981-985, 994, 996, 998, 1002,
1005

Probability density function, 420, 665, 843, 981,
983-984

Probability density function (PDF), 843
Probability distribution, 982-985, 994
Probability distribution function, 982, 984-985
Probability of error, 901-902
Probability theory, 842, 985
Procedures, 841
Process, 2, 9-11, 19-21, 28, 31-35, 39, 76-77, 93, 119,

187, 232, 268, 331-333, 358, 391, 396-397,
403, 413, 415-417, 423, 429, 438-439, 441,
443, 450, 456, 514, 532, 534, 538, 579, 590,
596, 626, 633, 664, 693, 702, 723, 743, 751,
768-773, 789, 794-796, 798-800, 832, 835,
841-857, 862, 864, 872-876, 883, 886,
892-897, 922-923, 934, 958, 975-977, 979,
999, 1002-1003

information, 20, 33, 93, 119, 232, 397, 999,
1002-1003

Processing, 1-2, 4-6, 9, 11, 20, 27, 36-37, 39, 43, 55,
68, 86, 130-131, 268, 274, 289, 334, 358,
368-369, 371, 391, 411, 413-414, 416, 450,
452, 497-498, 513-514, 570, 579, 588,
615-616, 654, 675, 712-713, 718, 767-840,
856, 864, 872, 891-892, 906, 922-923,
993-1006

Product, 55, 60, 74, 76-82, 84, 121, 154, 172, 177,
197, 202, 208, 218, 248, 273, 289, 345, 363,
365, 378, 484-487, 493, 495-496, 525, 528,
534, 541, 559-561, 619-620, 639-641, 645,
663, 666, 789, 794, 809, 830

Products, 61, 91-92, 126, 280, 641, 645, 649, 664,
868

Programmable, 1, 5, 578-579
Programming, 687
Programming Techniques, 687
Programs, 232, 667, 670, 741
Propagation, 1001, 1004-1005
Property, 15-16, 61-62, 66, 69, 72, 75-76, 82-83, 85,

87, 105, 123, 151, 161, 163-165, 168-169,
172-173, 181-182, 188, 211-214, 218-219,
224, 230, 275, 285, 287-288, 290, 292-293,
296, 313-314, 324, 342, 365, 436, 465, 473,
477, 490-492, 512, 544, 568, 634, 637, 687,
721, 820, 844, 864-865, 867-868, 873-875,
896

Prototype, 342, 386, 670, 733, 746-749, 758, 808-809,
812, 837-838

Prototype filters, 758

Q
Quadratic factor, 604
Quadrature components, 434, 437-438, 845
Quality, 20, 35, 40, 440, 763, 913, 922-923
Quantity, 2, 7, 13, 112, 244, 261, 264, 386, 446, 514,

559, 763, 770, 906, 958, 967
Quantizers, 133, 416, 638, 649

R
Radar, 5, 19, 37, 118-119, 131, 146, 273-275, 334,

423, 438, 445, 981
Radian, 243, 757
Radix, 531, 536, 539-549, 552, 554, 564, 567,

569-572, 616, 996, 1004
Random noise, 146
Random process, 331-332, 664, 842-849, 851-857,

859, 864, 873-876, 886, 895-897, 922-923,
976, 979, 999, 1003

Random signal analysis, 994
Random signals, 11, 841-842, 847
Random variables, 562, 843-845, 983, 985-986, 1002

variance of, 562, 844, 985
Range, 4, 6, 10, 13, 15-17, 22, 24-25, 27, 31-35, 38,

40, 49, 89-90, 123, 127, 138, 140, 209,
238-239, 247, 249, 252, 254-255, 262, 269,
271-272, 274-275, 277-278, 321, 335, 363,
392, 397-399, 415-420, 425-428, 433-434,
440, 445-446, 448, 450, 462, 501-502, 521,
557, 559, 562-563, 618-625, 639-640,
642-643, 646, 648, 665, 674-676, 730, 756,
760, 762, 771, 778, 797, 821, 831, 835, 931,
937, 956, 976-977, 981-984, 1003

argument, 16, 559
rank, 885
Rapid, 1, 199, 259, 262, 931, 941, 946, 998
Ratios, 173, 971
Reading, 131, 535, 561, 993, 1004
Reasonable, 11, 649, 652
Record, 126, 498, 501, 518, 882, 922, 1000
Recovery, 401, 769
Recursive computation, 940, 978
Reduction, 445, 449-450, 503, 528, 532, 541, 544,

563, 623, 651, 751, 789, 796-797, 805-806,
831, 929, 965

Redundant, 107
Region of convergence (ROC), 152
register, 146-147, 627, 1002
regression, 993
Rejection, 349, 352, 448
Relationships, 13, 99, 127, 130, 237, 242, 248, 277,

282, 290, 308, 320-321, 324, 329-330,
593-594, 611, 624, 647, 783, 855, 861, 870,
961, 965

Repeat, 38, 40, 73-74, 132, 136, 147, 247, 374-375,
386, 429, 452-453, 456, 459, 486, 519, 532,
545, 549, 563, 571-572, 574, 634, 663, 702,
752, 761, 763, 796, 799, 840, 893, 895, 979

Representation, 9, 14, 17-18, 27, 44-47, 52-54, 65,
151-153, 230, 232, 234-235, 240, 247, 265,
269, 283, 290, 293, 319, 355, 414, 436-437,
439, 442, 444-445, 461, 492, 594, 615-616,
618-626, 632-633, 639-641, 662, 664, 685,
698, 774, 801, 806, 827, 845, 852-854, 861,
886

requirements, 6, 92, 98, 111-112, 218, 380, 383, 407,
414, 445, 452, 578, 637, 654, 757, 849, 971

Resampling, 450, 769, 800, 998
Residuals, 957-958, 960-961, 964, 970
Residue, 185, 889, 891
Resistors, 842-843
Resolution, 32, 35, 39-40, 73-74, 231, 414, 416, 445,

501-503, 518, 619-621, 625, 994, 1002,
1004

A/D converter, 35, 39-40, 414, 416
Resonance, 345, 347, 349, 391, 641
Resonant frequency, 343, 345-346, 379, 651, 727,

731-732
Response:, 378, 689

step, 77, 80, 87, 93, 103, 105, 107, 137, 139-140,
142-143, 145, 148, 204, 217, 223-225,
227, 414, 667, 673-674, 855, 893, 979

unbounded, 88, 181, 204
Responses, 65, 67, 72, 75, 83-85, 95, 98, 138, 141,

144, 199, 207, 313, 316, 320, 335, 337, 373,
380, 383, 387, 391, 410, 412, 422, 453, 652,
663, 667, 673, 679, 683, 742-743, 809,
811-812, 816, 830, 838, 922

Ring, 158, 171, 195
rise, 80, 260, 361
Robustness, 577, 615, 948, 1002
Rotation, 165, 357, 512, 548, 896, 1004
Rotations, 565
Rounding, 11, 32-33, 40, 133, 375, 561-562, 622-625,

633, 640-642, 648-649, 655, 663-664, 999
Rules, 4, 998

S
Saddle, 994, 996-1005
Sample, 10, 14, 20-21, 23, 28-29, 31-32, 35-36, 38-39,

44-45, 54-55, 57, 60, 62, 71, 73-76, 80, 82,
86, 91, 108, 114, 127, 130, 181, 183, 197,
200, 202-203, 207-209, 218, 309, 331, 357,

368, 384, 401, 405, 420-422, 427-428,
438-439, 445-447, 451, 453, 455, 459, 462,
465-466, 476, 501, 552, 574, 579-580, 588,
643-644, 647, 650, 652, 666, 676-680,
684-685, 687-689, 703, 712-714, 723, 726,
752, 755, 757-758, 764, 770-772, 779, 783,
792-793, 799-804, 833, 847-848, 917-918,
922-923

mean of, 848
repeated, 82, 703, 804

Sample function, 331, 843, 848
Sampling:, 403, 430, 461
Sampling frequency, 21-22, 32, 35, 38-40, 249, 351,

380, 383-384, 396, 399, 401, 404-405,
410-411, 423-424, 426-428, 450, 453,
455-458, 745, 766, 769, 804, 824, 832

Sampling interval, 19, 54, 118, 255, 401, 407, 460,
719, 723, 800-802

Sampling period, 21, 26, 44, 277, 441, 455, 832-833
Sampling rate, 2, 20-21, 23-30, 34, 38-40, 54, 401,

407, 423, 425, 427-428, 430, 434, 438-439,
442, 445, 448-454, 518, 723, 761, 768-773,
776, 780-785, 787, 789-790, 792-795,
797-800, 802-803, 831-833, 835-836, 912

Savings, 541, 548, 568, 805
Scale, 1, 60, 104, 126-127, 137, 198-199, 216, 310,

399, 407, 416, 418-419, 566, 643-644, 650,
676, 747, 766, 778-779, 829

Scaling factor, 177
Scaling property, 66, 72, 165
Scope, 100, 232, 442
Second-order system, 115-116, 203, 206, 355-356,

380, 641
Seconds, 21, 38-40, 396, 421, 455, 457, 522, 904
Segments, 36, 426, 455, 805, 920
Selective, 334-335, 670, 675, 1004
Selectivity, 751
Sensitivity analysis, 634
Sensor, 8, 921-922
Sensors, 920-921
Separation, 268, 349, 351, 824
Sequences:, 484, 529
Shape, 123, 178, 233, 236, 245, 288, 320, 391, 778,

904
index, 288

shifter, 437, 443
Side, 74, 83, 96-97, 126-127, 158-159, 161, 184, 192,

233, 238, 248, 256, 289, 291, 294, 361,
398-399, 518, 687, 788, 867, 894, 914, 921,
942

Side lobes, 921
Sign bit, 617, 619-621, 639, 661-663
Signal generation, 3
Signal processing, 1-2, 4-6, 9, 36-37, 39, 43, 53, 55,

68, 86, 130-131, 268, 289, 334, 358,
368-369, 371, 391, 413-414, 416, 497, 549,
579, 615-616, 654, 675, 712-713, 718,
767-840, 856, 993-1006

multidimensional, 6, 9
Signals, 1-14, 16-24, 27, 31-38, 40, 43-150, 151-152,

159, 161-166, 169-170, 173, 178-181, 187,
193, 196, 198, 209-211, 213-215, 218-223,
226-227, 229-305, 308-309, 316, 318-321,
329-331, 334, 353, 356, 358, 372, 375-376,
379, 395-460, 461, 464, 473, 477, 482,
511-514, 518-520, 559-560, 612, 643-644,
712, 750-751, 761, 768, 772, 777, 797-798,
805-807, 809-810, 813-816, 832-833, 835,
847, 896, 917, 920-921, 994-999, 1001-1006

Signals:, 40, 136, 144, 271, 304-305, 309
Signal-to-noise ratio (SNR), 127, 648
Signal-to-quantization-noise ratio, 420, 664
Signs, 54, 97, 342
SIMPLE, 15, 25, 29, 39, 52-54, 58, 61, 87, 92, 94-96,

98, 100, 108, 117, 152, 163, 173, 177, 180,
185, 187, 189-192, 197, 242, 288-289,
308-309, 336-337, 339, 341-343, 349, 370,
385, 398, 418, 442, 446, 496, 581, 583, 630,
662, 694, 717, 746, 798, 831, 862, 901,
927-928, 974, 984-985

Simple pole, 192
Simple process, 413, 798
Simulation, 410, 935, 946-947, 973-974, 981

purposes, 946, 973, 981
responses, 410

Single, 3, 8-9, 17, 20, 37, 41, 50, 70, 74, 76-77, 85,
112, 125, 178-179, 233, 239, 249, 252, 279,
295, 337-338, 353-354, 381, 423, 438, 448,
467, 549, 555, 561, 568, 589, 594, 598, 602,

1012



605-606, 610, 629, 639-641, 645-647, 661,
727, 731-732, 746, 748-749, 757, 768, 772,
780, 796, 804-805, 814-815, 835-836,
848-851, 889, 902, 936, 949-950

Sinusoidal signals:, 40, 309
Sinusoids, 3, 13-19, 22, 24-25, 27, 34, 37, 230, 232,

317-318, 356, 358, 375, 380, 391, 457, 459,
644, 915, 997, 1004-1005

Slope, 447-448, 456
Smoothing, 36, 380, 420, 451, 504, 682, 684, 881,

890, 993
Software, 1-2, 4-6, 37, 232, 510, 548-549, 577-579,

615, 627, 638, 670, 703, 743, 927, 981
Sources, 3, 7, 232, 664-665, 1000
specification, 194, 278, 676, 680, 687, 698
Spectral shaping, 334, 370, 715
Spirals, 557
Spread, 239, 450, 501, 694, 912, 915, 935-936, 946,

971, 984, 998, 1000
Spreading, 716
Square, 35, 67, 93-94, 232, 237, 244, 257-260, 264,

267-269, 364, 422, 841, 851, 859, 862-864,
872, 881-882, 884-885, 891, 895-896, 900,
923-925, 931, 935, 941, 943, 946-947, 951,
969-974, 976, 998

Stability, 69, 71, 87-88, 90, 98-100, 106, 110, 130,
197, 200-201, 204-207, 224, 365, 615, 642,
657, 862, 901, 929-930, 936, 967, 974, 976,
1000

BIBO, 69, 87, 99-100, 110, 200-201, 204-206
stable, 69, 87-90, 99-100, 130, 133-134, 137, 140,

142, 149, 200-206, 218, 223-227, 267, 309,
315, 318-320, 337, 354, 360, 365, 367, 379,
385, 387, 389-391, 410, 612, 642, 658-659,
661, 673, 718, 721, 748, 854, 895, 967, 973,
1005

Standard, 445, 622, 637, 802, 985
deviation, 445, 637, 985

Standard deviation, 445, 637, 985
standards, 507, 1003
State space, 999
State variable, 998
State variables, 996
Stationary, 331, 456, 646, 665, 841, 843-849, 852-857,

859, 874, 881, 886, 892, 894, 896, 994, 999,
1002, 1006

Statistical, 37, 131, 331, 418-419, 513, 561-562, 565,
625-626, 638, 645, 666, 841-844, 847-851,
854, 900, 902, 914, 922-925, 929, 935-938,
945, 993, 997, 1003-1004

Statistics, 420, 841, 900, 902, 923, 927, 934, 946,
974, 1000

Status, 414
Steady state, 421

parameter, 421
Steady-state response, 106, 138, 199-200, 225, 308,

318-319, 377
Step function, 435
Step response, 87, 137, 139-140, 142-143, 145, 148,

204, 213, 215, 217, 223-225, 227, 667
Step size, 32-33, 416, 419, 448, 926-927, 929, 931,

934-935, 945-947, 975-976, 979
Stopband, 335, 342, 407, 670, 674-676, 685, 693-694,

698, 700, 704-708, 717, 733-734, 736,
738-741, 743, 755-756, 776, 780, 795-796,
804, 821-822, 830, 833, 987

Stopband attenuation, 705-706, 755, 830
Strength, 946
Stress, 61, 82, 100, 201, 277, 409
String, 36, 615, 618
Stroboscope, 31
Structure, 56, 111-115, 117, 142, 145, 357, 382-383,

423, 530, 549, 578-583, 585, 588-589,
593-594, 598-606, 608-609, 611-613, 615,
630, 632, 654, 656, 659-660, 662, 667, 688,
784-792, 799, 802-803, 812-816, 825,
827-829, 833, 835-837, 860-861, 871,
876-880, 891-892, 897, 901-902, 914, 920,
947-948, 950, 960, 974, 976-977, 993

sensitive, 579, 598
Structures, 111-112, 116, 118, 442, 549, 577, 579, 581,

583, 586, 588-589, 596, 598-600, 602-603,
605, 608, 611-612, 615, 654-655, 658, 662,
667, 771-772, 784-786, 788-789, 792,
800-801, 812, 831, 837, 856, 877-878, 948,
970-971, 974-975, 996, 1005

Subdivision, 805-806, 824
Substitution, 87, 102, 104-105, 107, 145, 177, 193,

242, 283, 292, 398, 400, 428-429, 507, 531,

543, 558, 629, 678, 719-720, 722, 726, 750,
939, 960

Subsystems, 71, 85, 603-605, 659
Subtraction, 47
Sunspots, 10
Superposition, 65-66, 75, 98, 273, 309, 313-314, 319
Surface, 8, 177, 528, 560, 934
Surface acoustic wave (SAW), 560
Symmetry, 50-51, 237, 245, 252-253, 261-262,

278-279, 282-283, 293, 311, 313, 324, 473,
479-480, 482, 507-509, 512, 517, 524, 548,
556, 567, 572, 580-581, 585-586, 630,
677-679, 789, 822, 841

Synthesis, 231-232, 235, 242, 248, 257, 260, 275,
277-278, 370, 806-808, 810-811, 813-814,
816-817, 820, 824-825, 827-830, 836,
838-839, 920, 994, 996-999, 1002,
1005-1006

System, 2-6, 12, 37, 39, 43, 55-73, 75-77, 79-103,
106-119, 128-130, 132-135, 137-146,
148-149, 172, 181-183, 197-209, 213,
215-218, 222-227, 230, 232, 308-326,
328-334, 336-341, 347, 349-350, 353-373,
375-392, 407-410, 413-414, 416-417, 419,
422, 425, 430, 432, 446-450, 453-454,
457-459, 484, 492, 516-518, 541, 555,
571-573, 577-582, 584, 586, 592, 594-600,
602-614, 626-627, 630, 638-646, 649, 652,
654-660, 662-667, 671-673, 675-677, 694,
717-722, 725-729, 731-732, 742, 746,
748-749, 752-758, 760-765, 769, 771,
784-791, 801-804, 808, 815, 824, 826, 831,
835, 852-855, 857, 860-862, 873-879, 886,
888, 893, 895-897, 900-908, 912, 916-918,
930-932, 975-978, 999, 1001

analogous, 634
attribute, 232, 334
of units, 391

T
Tasks, 1, 37
Taylor series expansion, 456, 853
Telecommunications, 750
temperature, 232, 842-843
Terminals, 56, 907-908
Test, 34, 63, 68, 135, 197, 862, 868
Test signals, 34
Testing, 6

algorithms, 6
thermal, 842-843
Thermal noise, 842-843
Three-dimensional, 8
Time domain, 71, 130, 157, 160, 166, 169, 173, 178,

214, 216, 218, 226, 272-273, 275, 277-278,
283, 288-289, 295-296, 309, 359, 369, 379,
385, 396-397, 408, 443, 484, 492-493, 496,
500, 514, 520, 781, 809, 818

Time invariance, 71, 98, 132, 134, 138
Time-average, 850-851, 914, 917, 919
Tolerances, 6, 413, 427
Tool, 151, 161, 213-214, 289, 368, 473, 492
Tools, 11, 111, 230-232, 273-274, 289, 473
Top, 545-546, 589
Total, 72, 97-100, 105-109, 216-217, 236-237, 245,

294, 319, 330, 366, 375, 416, 438, 459, 472,
512, 533-534, 541, 561, 563, 565-566,
574-575, 651-652, 665, 716, 763, 931,
934-935

Total energy, 245, 330, 366, 512
Trace, 931
Training, 907
Transfer, 593, 809, 820, 834-835, 837-839, 876
Transfer function, 809, 820, 834-835, 837-838, 876
Transform, 60, 100, 151-228, 230-231, 240-247, 254,

256-260, 262-270, 272, 274-275, 277-278,
280, 282-288, 291-292, 295-297, 300-305,
308-311, 321-324, 326-328, 330, 332-334,
336, 342, 359-360, 366, 368-371, 380, 383,
397, 399, 403, 405, 409-410, 422-423, 429,
433-436, 440-442, 444, 449, 453-454, 460,
461-522, 523-575, 578, 580, 583, 644,
673-674, 680-681, 688, 713, 715, 721, 729,
733, 746, 748, 778, 785, 808, 817-818, 825,
832-833, 846-848, 861, 993-995, 998-999,
1001-1006

Transformation, 52, 56, 165, 222, 268, 285, 288, 342,
471-472, 512, 530, 541, 600, 730-732,
743-744, 746-750, 753, 755-758, 896, 930,
932, 1003

Transformations, 369, 472, 670, 733, 743, 746-748,
751, 995, 1002

Transformers, 438, 703, 707, 709, 716, 1003
Transient response, 106, 138, 199-200, 318-319
Transition region, 676, 684, 692, 795, 797, 987
Transpose, 660, 785-786, 790, 797, 813, 827, 829,

835-836, 866, 924-925
Transposed form, 600, 655, 797

transposition, 600
Transposition, 511, 600
Trials, 979
Two-sided transform, 210
Two-sided z-transform, 160, 209, 211
Types, 3, 7, 31, 90, 104, 159, 233, 272, 274-275, 335,

447-448, 486, 596, 655, 698-699, 703, 708,
735, 741, 750, 768, 808, 908, 923, 943

U
Unbounded response, 204
Uncertainty, 11, 119, 247, 903, 1004
Uniform, 13, 21, 127, 256-258, 416, 423, 425, 434,

476, 621, 631, 665, 808-812, 837-838, 937,
975, 981, 985

Uniform distribution, 127, 631
Unilateral z-transform, 160, 210
Unipolar, 416
Unit circle, 178-181, 199, 201-202, 204-206, 213, 218,

222, 224, 227, 266-269, 322-323, 325-330,
337, 340, 343, 345, 347, 349, 351, 363-365,
367, 381, 475, 554-557, 569, 584, 596, 612,
629, 631-634, 641, 648-649, 651-652, 659,
673, 678, 692-694, 718, 720-722, 725, 728,
730, 748, 852-854, 862, 873-874, 891,
895-896, 931, 996, 1004, 1006

Unit step sequence, 49, 80, 103, 105, 217, 268, 674,
930

Units, 52, 54, 63-65, 69, 79, 81, 120-121, 145, 336,
391, 478-479, 485-486, 684, 718, 772, 868,
871

of time, 52, 63, 69, 486, 718
Unity, 45, 47, 73, 90, 110, 209, 238, 315, 335, 338,

345, 471, 568, 599, 612, 675, 698, 754, 793,
931, 935, 941, 982

unknown, 2, 29, 56, 126-127, 331, 334, 357-358,
366-368, 761-763, 900-903, 906, 916

Us, 2, 8, 15-16, 23, 26-27, 31-32, 43, 61, 63, 77, 79,
81, 85, 88, 91, 97-99, 101, 105, 109-111,
118-119, 121-122, 127, 154, 161, 164, 181,
198, 205, 215, 218, 240-241, 256, 258-259,
261, 275, 279, 308-309, 318, 321, 325-326,
331-332, 335, 350, 358, 362, 368-369,
399-400, 480, 496, 502-503, 525, 531, 539,
548, 552-554, 557-559, 561-562, 564,
599-600, 603, 608, 610-612, 616, 619-623,
630, 638, 642-643, 645-646, 648-649, 698,
713, 724-725, 728, 741, 761, 773, 794-799,
802-803, 811-812, 814-815, 835, 843-844,
848-850, 861-862, 866-869, 878-879,
911-913, 920, 923, 939, 948, 950-951

Utility, 37

V
Value, 6, 8-11, 13, 15, 17, 19-20, 27, 31-33, 36, 38,

44-45, 50, 57-58, 63, 73-74, 76, 88-89,
92-94, 100, 105-106, 123, 127, 147, 164,
172-173, 185, 207, 213, 220-222, 227,
233-234, 238-239, 259, 269, 299, 315-316,
326, 331-333, 340-341, 345, 379, 383-384,
417, 441, 445, 456, 470, 484, 524, 552-553,
555-556, 562, 564, 568, 572-573, 580,
592-593, 622-626, 636, 638-639, 643, 651,
664, 693, 700-701, 731-733, 736, 757, 762,
770-771, 773, 785, 843-844, 849-851,
855-857, 863-864, 872, 889, 912-914,
934-935, 949-950, 977, 981-983

added, 552, 934
Values, 4, 9-11, 16, 18-20, 22-24, 28, 32-33, 36, 38,

40, 44, 48, 50, 55, 57-58, 63, 68-69, 73,
76-77, 79-80, 84-86, 88-90, 92-96, 110, 116,
121, 123, 127, 131, 140, 142-143, 152-154,
174, 182-185, 187, 206, 210, 238-239, 246,
339, 345-346, 351, 361, 368-369, 383, 406,
413, 433, 442, 445-446, 452, 459, 465-466,
475, 486, 495, 511-513, 516-517, 524-525,
527, 529, 552-554, 556, 566-567, 569, 571,
579-580, 598, 630, 637, 639, 642, 664,
687-688, 690-691, 693, 725, 741, 744, 754,
758, 762, 769-771, 812, 857, 890, 906-907,

1013



931-932, 967, 973, 976-977, 987
Variables, 2-3, 6, 8, 21-25, 160, 165, 184, 275,

397-398, 439, 550, 562, 718, 721, 730-731,
778, 843-845, 925, 944, 983, 985-986, 996

Variance, 419, 445-446, 449, 456, 562-563, 565-567,
572, 574-575, 637, 646-648, 650, 652, 663,
665-666, 761, 763, 844, 848-849, 851, 864,
874, 880, 883, 892, 894, 897, 933, 947,
971-972, 978-979, 984-986

Variations, 262, 415, 438, 452, 507, 892, 905, 907,
927-929, 935, 946, 966, 974

vector, 7-8, 299, 326, 357, 472, 511-512, 547, 549,
806, 830, 866-867, 882, 884, 894-895,
924-932, 934, 936-941, 943-944, 948-950,
955, 957, 962-963, 965-967, 975-976

Velocity, 274
Vertical, 34, 426, 441
Vertical line, 426, 441
Vibrations, 232
Vocal tract, 3, 612, 917
VOL, 391, 993-1006
Voltage, 39, 237, 397, 415, 421
Volume, 908

W
Walls, 146
Warping, 730
Wave, 8, 253, 260, 265, 298, 512, 560, 654
Waveforms, 231-232, 842-843, 999
Web, 962, 964-969
Weight, 37, 707, 997
Weighted moving average, 114
Weighting function, 334, 698-700, 706, 708, 714
Well, 1, 5, 11, 17, 48, 55, 61, 97, 181, 185, 214, 231,

269-270, 342, 357, 375, 434, 524, 566-567,
676, 717, 733, 738, 754, 763, 795, 860,
867-868, 878, 885, 907, 913

White, 8, 127, 231-232, 332, 334, 419, 426, 449, 513,
561-562, 646-648, 665, 668, 761, 852-855,
864, 874-876, 880, 883, 886, 893-894,
896-897, 918, 933, 976-979, 997, 1005

White noise, 127, 334, 419, 449, 513, 561, 646-648,
665, 668, 761, 852-855, 864, 874-876, 880,
883, 893-894, 896-897, 918, 976, 978-979,
997

Window function, 504, 506-507, 680-681, 685
Windowing, 294, 370, 501-502, 505

theorem, 294
Wire, 907-908
Wood, 371, 1006
Word, 6, 35-36, 416, 421, 577-579, 615, 621, 627,

637, 642, 654, 664, 946, 970, 1001
Work, 88, 232, 262, 361, 452, 655, 751, 831, 892, 947,

1003

X
X rays, 275

Y
Yield, 1, 24-25, 60, 63, 70, 76, 81-82, 120, 166, 213,

216, 321, 401, 429, 470, 474, 486, 496-497,
559, 680, 691, 751, 756, 793, 812, 821, 890

Yielding, 741

Z
Zero frequency, 271, 573, 708
Zero placement, 337, 341, 370
z-transform, 60, 151-228, 265-269, 321, 323, 326,

330, 359, 366, 369-370, 380, 383, 449, 554,
556, 559-560, 569-570, 572, 578, 583,
591-592, 729, 775, 778, 785, 817-818, 825,
886-888, 893-894, 1003

bilateral, 159
convolution property, 169, 181, 213, 218-219
differentiation property, 168
linearity property, 161, 188
region of convergence (ROC), 152
time-shifting property, 163, 168, 182
two-sided, 158-160, 171, 195, 209-211, 213, 219,

887
unit circle, 178-181, 199, 201-202, 204-206, 213,

218, 222, 224, 227, 266-269, 323, 326,
330, 475, 554, 556, 569, 852, 893

1014


	Cover
	Table of Contents
	1. Introduction
	2. Discrete-Time Signals and Systems
	3. The z-Transform and Its Application to the Analysis of LTI Systems
	4. Frequency Analysis of Signals
	5. Frequency-Domain Analysis of LTI Systems
	6. Sampling and Reconstruction of Signals
	7. The Discrete Fourier Transform: Its Properties and Applications
	8. Efficient Computation of the DFT: Fast Fourier Transform Algorithms
	9. Implementation of Discrete-Time Systems
	10. Design of Digital Filters
	11. Multirate Digital Signal Processing
	12. Linear Prediction and Optimum Linear Filters
	13. Adaptive Filters
	14. Appendix: Random Number Generators
	15. Appendix: Tables of Transition Coefficients for the Design of Lnear-Phase FIR Filters
	16. References and Bibliography
	Index
	A
	B
	C
	D
	E
	F
	G
	H
	I
	J
	K
	L
	M
	N
	O
	P
	Q
	R
	S
	T
	U
	V
	W
	X
	Y
	Z


