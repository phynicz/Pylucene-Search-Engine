














































Computer Graphics, C Version (2nd Ed.)




Contents 

PREFACE xvii 

1 A Survey of Computer 2-2 Graphics 2 
Computer-Aided Design 2-3 
Presentation Graphics 'I 2-4 
Computer Art  l 3  2-5 
Entertainment 18 
Education and Training 2 1 
Visualization 25 
Image Processing 3 2 

Graphical User Interfaces 3 4 

Overview of Graphics 2 systems 35 2-6 
2-1 VideoDisplayDevices 36 2-7 

Refresh Cathode-Ray Tubes 37 
Raster-Scan Displays 40 
Random-Scan Displays 41 
Color CRT Monitors 42 
Direct-View Storage Tubes 4.5 
Flat-Panel Displays 45 
Three-Dimensional Viewing Devices 49 

Stereoscopic and Virtual-Reality 
Systems 

Raster-Scan System!; 
Video Controller 
Raster-Scan Display Processor 

Random-Scan Systems 
Graphics Monitors and Workstations 
Input Devices 
Keyboards 
Mouse 
Trackball and Spaceball 
Joysticks 
Data Glove 
Digitizers 
Image Scanners 
Touch Panels 
Light Pens 
Voice Systems 

Hard-Copy Devices 

Graphics Software 
Coordinate Representations 
Graphics Functions 
Software Standards 
PHIGS Workstations 

Summary 
References 
Exercises 

vii 



Contents 

3 Outout Primitives 83 
Points and Lines 
Line-Drawing Algorithms 
DDA Algorithm 
Bresenham's Line Algorithm 
Parallel Line Algorithms 
Loading the Frame Buffer 
Line Function 
Circle-Generating Algorithms 
Properties of Circles 
Midpoint Circle Algorithm 

Ellipse-Generating Algorithms 
Properties of Ellipses 
Midpoint Ellipse Algorithm 

Other Curves 
Conic Sections 
Polynomials and Spline Curves 

Parallel Curve Algorithms 
Curve Functions 
Pixel Addressing 
and Object Geometry 
Screen Grid Coordinates 
Maintaining Geometric Properties 
of Displayed Objects 
Filled-Area Primitives 
Scan-Line Polygon Fill Algorithm 
Inside-Outside Tests 
Scan-Line Fill of Curved Boundary 
Areas 
Boundary-Fill Algorithm 
Flood-Fill Algorithm 
Fill-Area Functions 
Cell Array 
Character Generation 

Summary 
Applications 
References 
Exercises 

Attributes of Output 
Primitives 143 

Line Attributes 
Line Type 
Line Width 
Pen and Brush Options 
Line Color 

Curve Attributes 
Color and Grayscale Levels 
Color Tables 
Grayscale 
Area-Fill Attributes 
Fill Styles 
Pattern Fill 
Soft Fill 
Character Attributes 
Text Attributes 
Marker Attributes 

Bundled Attributes 
Bundled Line Attributes 
Bundled Area-Fi Attributes 
Bundled Text Attributes 
Bundled Marker Attributes 

Inquiry Functions 
Antialiasing 
Supersampling Straight Line 
Segments 
Pixel-Weighting Masks 



Contents 

Area Sampling Straight Line 5-6 Aff ine Transformations 208 
Segments 174 5-7 Transformation Functions 208 
Filtering Techniques 174 5-8 Raster Methods for Transformations 210 
Pixel Phasing 1 75 Summary 212 
Compensating for Line lntensity 
Differences 1 75 References 21 3 

Antialiasing Area Boundaries 1 76 Exercises 213 

Summary 
References 
Exercises 

Two-Dimensional 
180 180 6 Viewing 21 6 

6-1 The Viewing Pipeline 

5 Two-Dimensional Geometric 
6-2 Viewing Coordinate Reference Frame 

183 
6-3 Window-teviewport Coordinate 

Transformations Transformation 

5-1 Basic Transformations 
Translation 
Rotation 
Scaling 

5-2 Matrix Representations 
and Homogeneous Coordinates 

5-3 Composite Transformations 
Translations 
Rotations 
Scalings 
General Pivot-Point Rotation 
General Fixed-Point Scaling 
General Scaling Directions 
Concatenation Properties 
General Composite Transformations 
and Computational Efficiency 

5-4 Other Transformations 
Reflection 
Shear 

Two-Dimensional Wewing Functions 
Clipping Operations 
Point Clipping 
Line Clipping 
Cohen-Sutherland Line Clipping 
Liang-Barsky Line Clipping 
Nicholl-Lee-Nicholl Line Clipping 
Line Clipping Using Nonrectangular 
Clip Windows 
Splitting Concave Polygons 

Polygon Clipping 
Sutherland-Hodgernan Polygon 
Clipping 
Weiler-Atherton Polygon Clipping 
Other Polygon-Clipping Algorithms 

Curve Clipping 
Text Clipping 
Exterior Clipping 

Summary 
5-5 Transformations Between Coordinate References 

Systems 205 Exercises 



7 Structures and Hierarchical Modeling 250 
7-1 Structure Concepts 250 

Basic Structure Functions 250 
Setting Structure Attributes 253 

7-2 Editing Structures 254 
Structure Lists and the Element 
Pointer 255 
Setting the Edit Mode 250 
Inserting Structure Elements 256 
Replacing Structure Elements 257 
Deleting Structure Elements 257 
Labeling Structure Elements 258 
Copying Elements from One Structure 
to Another 260 

7-3 Basic Modeling Concepts 2 60 
Mode1 Representations 261 
Symbol Hierarchies 262 
Modeling Packages. 263 

7-4 Hierarchical Modeling 
with Structures 265 
Local Coordinates and Modeling 
Transformations 265 
Modeling Transformations 266 
Structure Hierarchies 266 

Summary 268 
References 269 
Exercises 2 69 

Graphical User Interfaces 

8 and Interactive lnput Methods 271 
8-1 The User Dialogue 

Windows and Icons 

Accommodating Multiple 
Skill Levels 
Consistency 
Minimizing Memorization 
Backup and Error Handling 
Feed back 

8-2 lnput of Graphical Data 
Logical Classification of Input 
Devices 
Locator Devices 
Stroke Devices 
String Devices 
Valuator Devices 
Choice Devices 
Pick Devices 

8-3 lnput Functions 
Input Modes 
Request Mode 
Locator and Stroke Input 
in Request Mode 
String Input in Request Mode 
Valuator Input in Request Mode 
Choice lnput in Request Mode 
Pick Input in Request Mode 
Sample Mode 
Event Mode 
Concurrent Use of Input Modes 

8-4 Initial Values for Input-Device 
Parameters 

8-5 lnteractive Picture-Construction 
Techniques 
Basic Positioning Methods 
Constraints 
Grids 
Gravity Field 
Rubber-Band Methods 
Dragging 
Painting and Drawing 



8-6 Virtual-Reality Environments 292 10-4 

Summary 233 
References 294 
Exercises 294 10-5 

10-6 

9 Three-Dimensional Concepts 296 
9-1 Three-Dimensional Display Methods 

Parallel Projection 
Perspective Projection 
Depth Cueing 
Visible Line and Surface 
Identification 
Surface Rendering 
Exploded and Cutaway Views 
Three-Dimensional and Stereoscopic 
Views 

9-2 Three-Dimensional Graphics 
Packages 302 

Three-Dimensional 

10-1 Polygon Surfaces 
Polygon Tables 
Plane Equations 
Polygon Meshes 

10-2 Curved Lines and Surfaces 
10-3 Quadric Sutiaces 

Sphere 
Ellipsoid 
Torus 

Superquadrics 
Superellipse 
Superellipsoid 

Blobby Objects 
Spline Representations 
Interpolation and Approximation 
Splines 
Parametric Continuity 
Conditions 
Geometric Continuity 
Conditions 
Spline Specifications 

Cubic Spline Interpolation 
Methods 
Natural Cubic Splines 
Hermite Interpolation 
Cardinal Splines 
Kochanek-Bartels Splines 
Bezier Curves and Surfaces 
Bezier Curves 
Properties of Bezier Curves 
Design Techniques Using Bezier 
Curves 
Cubic Ezier  Curves 
Bezier Surfaces 

B-Spline Curves and Surfaces 
B-Spline Curves 
Uniform, Periodic B-Splines 
Cubic, Periodic â‚¬3-Splines 
Open, Uniform B-Splines 
Nonuniform 13-Splines 
B-Spline Surfaces 

Beta-Splines 
Beta-Spline Continuity 
Conditions 
Cubic, Periodic Beta-Spline 
Matrix Representation 

Rational Splines 



Contents 

Conversion Between Spline 
Representations 
Displaying Spline Curves 
and Surfaces 
Homer's Rule 
Forward-Difference Calculations 
Subdivision Methods 

Sweep Representations 
Constructive Solid-Geometry 
Methods 
Octrees 
BSP Trees 
Fractal-Geometry Methods 
Fractal-Generation Procedures 
Classification of Fractals 
Fractal Dimension 
Geometric Construction 
of Deterministic Self-Similar 
Fractals 
Geometric Construction 
of Statistically Self-Similar 
Fractals 
Affine Fractal-Construction 
Methods 
Random Midpoint-Displacement 
Methods 
Controlling Terrain Topography 
Self-squaring Fractals 
Self-inverse Fractals 

Shape Grammars and Other 
Procedural Methods 
Particle Systems 
Physically Based Modeling 
Visualization of Data Sets 
Visual Representations 
for Scalar Fields 
VisuaI Representations 
for Vector Fields 
Visual Representations 
for Tensor Fields 

Visual Representations 
for Multivariate Data Fields 402 

Summary 404 
References 404 
Exercises 404 

Three-Dimensional 

11 Geometric and Modeling Transformations 407 
Translation 408 
Rotation 409 
Coordinate-Axes Rotations 409 
General Three-Dimensional 
Rotations 41 3 
Rotations with Quaternions 419 

Scaling 420 
Other Transformat~ons 422 
Reflections 422 
Shears 423 

Conlposite Transformations 423 
Three-Dimens~onal Transformation 
Functions 425 
Modeling and Coordinate 
Transformations 426 
Summary 429 
References 429 
Exercises 430 

Three-Dimensional 12 Viewing 43  1 
12-1 Viewing Pipeline 432 
12-2 Viewing Coordinates 433 

Specifying the Virbw Plane 433 
Transformation from World 

- 40 1 to Viewing Coordinates 437 

xii 



Contents 

Projections 
Parallel Projections 
Perspective IJrojections 

View Volumes and General 
Projection Transformations 
General Parallel-Projection 
Transformations 
General Perspective-Projection 
Transformations 

Clipping 
Normalized View Volumes 
Viewport Clipping 
Clipping in Homogeneous 
Coordinates 

Hardware Implementations 
Three-Dimensional Viewing 
Functions 
Summary 
References 
Exercises 

1 3-1 2 Wireframe Methods 490 
13-1 3 Visibility-Detection Functions 490 

Summary 49 1 
Keferences 492 

Exercises 49 2 

lllumination Models 

14 and Surface-Rendering Methods 494 

Visi ble-Su dace Detection 
Met hods 469 

Classification of Visible-Surface 
D~tection Algorithms 
Back-Face Detection 
Depth-Buffer Method 

A-Buffer Method 
Scan-Line Method 
Depth-Sorting Method 
BSP-Tree Method 

Area-Subdivision Method 

Octree Methods 
Ray-Casting Met hod 

Curved Surfaces 
Curved-Surface Representations 
Surface Contour Plots 

Light Sources 
Basic lllumination Models 
Ambient Light 
Diffuse Reflection 
Specular Reflection 
and the Phong Model 
Combined Diffuse and Specular 
Reflections with Multiple Light 
Sources 
Warn Model 
Intensity Attenuation 
Color Considerations 
Transparency 
Shadows 

Displaying Light Intensities 
Assigning Intensity Levels 
Gamma Correction and Video 
Lookup Tables 
Displaying Continuous-Tone 
Images 

Halftone Patterns and Dithering 
Techniques 
Halftone Approximations 
Dithering Techniques 

Polygon-Rendering Methods 
Constant-Intensity Shading 
Gouraud Shading 
Phong Shading 



Contents 

Fast Phong Shading 

Ray-Tracing Methods 
Basic Ray-Tracing Algorithm 
Ray-Surface Intersection 
CaIculations 
Reducing Object-Intersection 
Calculations 
Space-Subdivision Methods 
AntiaIiased Ray Tracing 
Distributed Ray Tracing 

Radiosity Lighting Model 
Basic Radiosity Model 
Progressive Refinement 
Radiosity Method 

Environment Mapping 

Adding Surface Detail 
Modeling Surface Detail 
with Polygons 
Texture Mapping 
Procedural Texturing 
Methods 
Bump Mapping 
Frame Mapping 

Summary 
References 
Exercises 

15-6 CMY Color Model 
15-7 HSV Color Model 

15-8 Conversion Between HSV 
and RGB Models 

15-9 HLS Color Model 
1 5-1 0 Color Selection 

and Applications 

Summary 
Reierences 
Exercises 

16 Computer Animation 583 
14-1 Design of Animation Sequences 
16-2 General Computer-Animation 

Functions 
16-3 Raster Animations 
16-4 Computer-Animation Languages 
16-5 Key-Frame Systems 

Morphing 
Simulating Accelerations 

16-6 Motion Specifications 
Direct Motion Specification 
Goal-Directed Systems 
Kinematics and Dynamics 

Color Models and Color Summary 
Apd ications 564 References . , Exercises 597 

15-1 Properties of Light 565 
15-2 Standard Primaries and the 

Chromaticity Diagram 568 A Mathematics for Computer 
XYZ Color Model 569 Graphics 599 
CIE Chromaticity Diagram 

569 A-1 Coordinate-Reference Frames 600 
1 5-3 Intuitive Color Concepts 571 Two-Dimensional Cartesian 
15-4 RGB Color Model 

15-5 Y I Q  Color Model 
572 Reference Frames 600 

5 74 Polar Coordinates in the xy Plane 601 

xiv 



Contents 

Three-Dimensional Cartesian 
Reference Frames 
Three-Dimensional Curvilinear 
Coordinate Systems 
Solid Angle 

A-2 Points and Vectors 
Vector Addition and Scalar 
Multiplication 
Scalar Product of Two Vectors 
Vector Product of Two Vectors 

A-3 Basis Vectors and the Metric Tensor 
Orthonormal Basis 
Metric Tensor 

A-4 Matrices 

Matrix Transpose 
Determinant of a Matrix 
Matrix Inverse 

Complex Numbers 
Quaternions 
Nonparametric Representations 
Parametric Representations 
Numerical Methods 
Solving Sets of Linear Equations 
Finding Roots of Nonlinear 
Equations 
Evaluating Integrals 
Fitting C U N ~ S  to Data Sets 

Scalar Multiplication and Matrix BIBLIOGRAPHY 
Addition 612 
Matrix Multiplication 612 INDEX 



Graphics 
C Version 





C omputers have become a powerful tool for the rapid and economical pro- duction of pictures. There is virtually no area in which graphical displays 
cannot be used to some advantage, and so it is not surprising to find the use of 
computer graphics so widespread. Although early applications in engineering 
and science had to rely on expensive and cumbersome equipment, advances in 
computer technology have made interactive computer graphics a practical tool. 
Today, we find computer graphics used routinely in such diverse areas as science, 
engineering, medicine, business, industry, government, art, entertainment, ad- 
vertising, education, and training. Figure 1-1 summarizes the many applications 
of graphics in simulations, education, and graph presentations. Before we get 
into the details of how to do computer graphics, we first take a short tour 
through a gallery of graphics applications. 

- 

F ' I ~ ~ I I ~  1 - I 
Examples of computer graphics applications. (Courtesy of DICOMED 
Corpora! ion.) 



A major use of computer graphics is in design processes, particularly for engi- 
neering and architectural systems, but almost all products are now computer de- 
signed. Generally referred to as CAD, computer-aided design methods are now 
routinely used in the design of buildings, automobiles, aircraft, watercraft, space- 
craft, computers, textiles, and many, many other products. 

For some design applications; objeck are f&t displayed in a wireframe out- 
line form that shows the overall sham and internal features of obiects. Wireframe 
displays also allow designers to qui'ckly see the effects of interacthe adjustments 
to design shapes. Figures 1-2 and 1-3 give examples of wireframe displays in de- 
sign applications. 

Software packages for CAD applications typically provide the designer 
with a multi-window environment, as in Figs. 1-4 and 1-5. The various displayed 
windows can show enlarged sections or different views of objects. 

Circuits such as the one shown in Fig. 1-5 and networks for comrnunica- 
tions, water supply, or other utilities aR constructed with repeated placement of 
a few graphical shapes. The shapes used in a design represent the different net- 
work or circuit components. Standard shapes for electrical, electronic, and logic 
circuits are often supplied by the design package. For other applications, a de- 
signer can create personalized symbols that are to be used to constmct the net- 
work or circuit. The system is then designed by successively placing components 
into the layout, with the graphics package automatically providing the connec- 
tions between components. This allows the designer t~ quickly try out alternate 
circuit schematics for minimizing the number of components or the space re- - 
quired for the system. 

Figure 1-2 
Color-coded wireframe display for 
an automobile wheel assembly. 
(Courtesy of Emns b Sutherland.) 



Figure 1-3 
Color-coded wireframe displays of body designs for an aircraft and an automobile. 
(Courtesy of (a) Ewns 6 Suthcrhnd and (b )  Megatek Corporation.) 

Animations are often used in CAD applications. Real-time animations using 
wiseframe displays on a video monitor are useful for testing perfonuance of a ve- 
hicle or system, as demonstrated in Fig. ld. When we do not display o b j s  with 
rendered surfaces, the calculations for each segment of the animation can be per- 
formed quickly to produce a smooth real-time motion on the screen. Also, wire- 
frame displays allow the designer to see into the interior of the vehicle and to 
watch the behavior of inner components during motion. Animations in virtual- 
reality environments are used to determine how vehicle operators are affected by 

Figure 1-4 
Multiple-window, color-coded CAD workstation displays. (Courtesy of Intergraph 
Corporation.) 



Figure 1-5 
A drcuitdesign application, using 
multiple windows and colorcded 
logic components, displayed on a 
Sun workstation with attached 
speaker and microphone. (Courtesy 
of Sun Microsystems.) 

- .  - 
Figure 1-6 
Simulation of vehicle performance 
during lane changes. (Courtesy of 
Ewns 6 Sutherland and Mechanical 
Dynrrrnics, lnc.) 

certain motions. As the tractor operator in Fig. 1-7 manipulates the controls, the 
headset presents a stereoscopic view (Fig. 1-8) of the front-loader bucket or the 
backhoe, just as if the operator were in the tractor seat. This allows the designer 
to explore various positions of the bucket or backhoe that might obstruct the o p  
erator's view, which can then be taken into account in the overall hactor design. 
Figure 1-9 shows a composite, wide-angle view from the tractor seat, displayed 
on a standard video monitor instead of in a virtual threedimensional scene. And 
Fig. 1-10 shows a view of the tractor that can be displayed in a separate window 
or  on another monitor. 



- - - - - -  

Figure 1-7 
Operating a tractor In a virtual-dty envimnment. As the contFols are 
moved, the operator views the front loader, backhoe, and surroundings 
through the headset. (Courtesy of the National Center for Supercomputing 
Appl ica th ,  Univmity of Illinois at Urba~Chrrmpign ,  and Catopillnr, 
Inc.) 

Figure 1-8 
A headset view of the backhoe 
presented to the tractor operator. 
(Courtesy of the Notional Centerfor 
Supcomputing Applications, 
UniwrsifV of Illinois at Urbam- 
~ h r r m p i & n d  Caterpillnr, Inc.) 

Figure 1-9 
Operator's view of the tractor 
bucket, cornposited in several 
sections to form a wide-angle view 
on a standard monitor. (Courtesy oi 
the National Centerfor 
Supercomputing Applications, 
University of lllinois at Urhno- 
C h m p i g n ,  and Caterpillnr, Inc.) 



Chapter 1 

A Survey of Computer Graphics 

Figure 1-10 
View of the tractor displayed on a 
standad monitor. (Courtesy of tk 
National Cmter for Superwmputing 
ApplicPths, Uniwrsity of Illinois at 
U r b P ~ U w m p i g n ,  and Gterpilhr, 
Inc.) 

When obpd designs are complete, or nearly complete, realistic lighting 
models and surface rendering are applied to produce displays that wiU show the 
appearance of the final product. Examples of this are given in Fig. 1-11. Realistic 
displays are also generated for advertising of automobiles and other vehicles 
using special lighting effects and background scenes (Fig. 1-12). 

The manufaduring process is also tied in to the computer description of d e  
signed objects to automate the construction of the product. A circuit board lay- 
out, for example, can be transformed into a description of the individud 
processes needed to construct the layout. Some mechanical parts are manufac- 
tured by describing how the surfaces are to be formed with machine tools. Figure 
1-13 shows the path to be taken by machine tools over the surfaces of an object 
during its construction. Numerically controlled machine tools are then set up to 
manufacture the part according to these construction layouts. 

~ealistic renderings of design products. (Courtesy of fa) Intergraph 
Corpomtion and fb) Emns b Sutherland.) 



Figure 1-12 
Studio lighting effects and realistic 
surfacerendering techniques are 
applied to produce advertising 
pieces for finished products. The 
data for this rendering of a Chrysler 
Laser was supplied by Chrysler 
Corporation. (Courtesy of Eric 
Haines, 3DIEYE Inc. ) 

Figure 1-13 
A CAD layout for describing the 
numerically controlled machining 
of a part. The part surface is 
displayed in one mlor and the tool 
path in another color. (Courtesy of 
Los Alamm National Labomtoty.) 

Figure 1-14 
Architectural CAD layout for a building design. (Courtesy of Precision 
Visuals, Inc., Boulder, Colorado.) 



Chapter 1 

A Survey of Computer Graphics 
Architects use interactive graphics methods to lay out floor plans, such as 

Fig. 1-14, that show the positioning of rooms, doon, windows, stairs, shelves, 
counters, and other building features. Working from the display of a building 
layout on a video monitor, an electrical designer can try out arrangements for 
wiring, electrical outlets, and fire warning systems. Also, facility-layout packages 
can be applied to the layout to determine space utilization in an office or on a 
manufacturing floor. 

Realistic displays of architectural designs, as in Fig. 1-15, permit both archi- 
tects and their clients to study the appearance of a single building or a group of 
buildings, such as a campus or industrial complex. With virtual-reality systems, 
designers can even go for a simulated "walk" through the rooms or around the 
outsides of buildings to better appreciate the overall effect of a particular design. 
In addition to realistic exterior building displays, architectural CAD packages 
also provide facilities for experimenting with three-dimensional interior layouts 
and lighting (Fig. 1-16). 

Many other kinds of systems and products are designed using either gen- 
eral CAD packages or specially dweloped CAD software. Figure 1-17, for exam- 
ple, shows a rug pattern designed with a CAD system. 

- 
Figrrre 1-15 
Realistic, three-dimensional rmderings of building designs. (a) A street-level perspective 
for the World Trade Center project. (Courtesy of Skidmore, Owings & M m i l l . )  
(b) Architectural visualization of an atrium, created for a compdter animation by 
Marialine Prieur, Lyon, France. (Courtesy of Thomson Digital Imngc, Inc.) 



Figtin 1-16 
A hotel corridor providing a sense 
of movement by placing light 
fixtures along an undulating path 
and creating a sense of enhy by 
using light towers at each hotel 
room. (Courtesy of Skidmore, Owings 
B Menill .)  

Figure 1-17 
Oriental rug pattern created with 
computer graphics design methods. 
(Courtesy of Lexidnta Corporation.) 

. - 
PRESENTATION GRAPHICS 

Another major applicatidn ama is presentation graphics, used to produce illus- 
trations for reports or to generate 35-mm slides or transparencies for use with 
projectors. Presentation graphics is commonly used to summarize financial, sta- 
tistical, mathematical, scientific, and economic data for research reports, manage 
rial reports, consumer information bulletins, and other types of reports. Worksta- 
tion devices and service bureaus exist for converting screen displays into 35-mm 
slides or overhead transparencies for use in presentations. Typical examples of 
presentation graphics are bar charts, line graphs, surface graphs, pie charts, and 
other displays showing relationships between multiple parametem. 

Figure 1-18 gives examples of two-dimensional graphics combined with g e  
ographical information. This illustration shows three colorcoded bar charts com- 
bined onto one graph and a pie chart with three sections. Similar graphs and 
charts can be displayed in three dimensions to provide additional information. 
Three-dimensional graphs are sometime used simply for effect; they can provide 
a more dramatic or more attractive presentation of data relationships. The charts 
in Fig. 1-19 include a three-dimensional bar graph and an exploded pie chart. 

Additional examples of three-dimensional graphs are shown in Figs. 1-20 
and 1-21. Figure 1-20 shows one kind of surface plot, and Fig. 1-21 shows a two- 
dimensional contour plot with a height surface. 



Chapter 1 

A  SUN^^ of Computer Graph~s 

Figure 1-18 
Two-dimensional bar chart and me 
chart hked to a geographical c l h .  
(Court~sy of Computer Assocbtes, 
copyrighi 0 1992: All rights reserved.) 

Figure 1-19 
Three-dimensional bar chart. 
exploded pie chart, and line graph. 
(Courtesy of Cmnputer Associates, 
copyi'ghi 6 1992: All rights reserved.) 

Figure 1-20 
Showing relationships with a 
surface chart. (Courtesy of Computer 
Associates, copyright O 1992. All 
rights reserved.) 

Figure 1-21 
Plotting two-dimensional contours 
in the &und plane, with a height 
field plotted as a surface above the 
p u n d  plane. (Cmrtesy of Computer 
Associates, copyright 0 1992. All 
rights d . j  



kclion 1-3 

Computer Art 

Figure 1-22 
T i e  chart displaying relevant 
information about p p c t  tasks. 
(Courtesy of computer Associntes, 
copyright 0 1992. ,411 rights m d . )  

Figure 1-22 illustrates a time chart used in task planning. Tine charts and 
task network layouts are used in project management to schedule and monitor 
the progess of propcts. 

1-3 
COMPUTER ART 

Computer graphics methods are widely used in both fine art and commercial art 
applications. Artists use a variety of computer methods, including special-pur- 
p&e hardware, artist's paintbrush (such as Lumens), other paint pack- 
ages (such as Pixelpaint and Superpaint), specially developed software, symbolic 
mathematits packages (such as Mathematics), CAD paclpges, desktop publish- 
ing software, and animation packages that provide faciliHes for desigrung object 
shapes and specifiying object motions. 

Figure 1-23 illustrates the basic idea behind a paintbrush program that al- 
lows artists to "paint" pictures on the screen of a video monitor. Actually, the pic- 
ture is usually painted electronically on a graphics tablet (digitizer) using a sty- 
lus, which can simulate different brush strokes, brush widths, and colors. A 
paintbrush program was used to m t e  the characters in Fig. 1-24, who seem to 
be busy on a creation of their own. 

A paintbrush system, with a Wacom cordlek, pressure-sensitive stylus, was 
used to produce the electronic painting in Fig. 1-25 that simulates the brush 
strokes of Van Gogh. The stylus transIates changing hand presswe into variable 
line widths, brush sizes, and color gradations. Figure 1-26 shows a watercolor 
painting produced with this stylus and with software that allows the artist to cre- 
ate watercolor, pastel, or oil brush effects that simulate different drying out times, 
wetness, and footprint. Figure 1-27 gives an example of paintbrush methods 
combined with scanned images. 

Fine artists use a variety of other computer technologies to produce images. 
To create pictures such as the one shown in Fig. 1-28, the artist uses a combina- 
tion of three-dimensional modeling packages, texture mapping, drawing pro- 
grams, and CAD software. In Fig. 1-29, we have a painting produced on a pen 



Figure 1-23 
Cartoon drawing produced with a paintbrush program, 
symbolically illustrating an artist at work on a video monitor. 
(Courtesy of Gould Inc., Imaging 6 Graphics Division and Aurora 
Imaging.) 

plotter with specially designed software that can m a t e  "automatic art" without 
intervention from the artist. 

Figure 1-30 shows an example of "mathematical" art. This artist uses a corn- 
biat ion of mathematical fundions, fractal procedures, Mathematics software, 
ink-jet printers, and other systems to create a variety of three-dimensional and 
two-dimensional shapes and stereoscopic image pairs. Another example of elm- 

Figure 1-24 
Cartoon demonstrations of an "artist" mating a picture with a paintbrush system. The picture, drawn on a 
graphics tablet, is displayed on the video monitor as the elves look on. In (b), the cartoon is superimposed 
on the famous Thomas Nast drawing of Saint Nicholas, which was input to the system with a video 
camera, then scaled and positioned. (Courtesy Gould Inc., Imaging & Gmphics Division and Aurora Imaging.) 



Figure 1-25 
A Van Gogh look-alike created by 
graphcs artist E&abeth O'Rourke 
with a cordless, pressuresensitive 
stylus. (Courtesy of Wacom 
Technology Corpomtion.) 

Figure 1-26 
An elechPnic watercolor, painted 
by John Derry of Tune Arts, Inc. 
using a cordless, pressure-sensitive 
stylus and Lwnena gouache-brush 
&ware. (Courtesy of Wacom 
Technology Corporation.) 

Figure 1-27 
The artist of this picture, called Electrunic Awlnnche, makes a statement 
about our entanglement with technology using a personal computer 
with a graphics tablet and Lumena software to combine renderings of 
leaves, Bower petals, and electronics componenb with scanned images. 
(Courtesy of the Williams Gallery. w g h t  0 1991 by Imn Tnrckenbrod, Tke 
School of the Arf Instituie of Chicago.) 



Figwe 1-28 Figure 1-29 
From a series called Sphnrs  of Inpumce, this electronic painting Electronic art output to a pen 
(entitled, WhigmLaree) was awted with a combination of plotter from software specially 
methods using a graphics tablet, three-dimensional modeling, designed by the artist to emulate 
texture mapping, and a series of transformations. (Courtesy of the his style. The pen plotter includes 
Williams Gallery. Copyn'sht (b 1992 by w n e  RPgland,]r.) multiple pens and painting 

inshuments, including Chinese 
brushes. (Courtesy of the Williams 
Gallery. Copyright 8 by Roman 
Vmtko, Minneapolis College of Art 6 
Design.) 

Figure 1-30 
This creation is based on a visualization of Fermat's Last 
Theorem, I" + y" = z", with n = 5, by Andrew Hanson, 
Department of Computer Science, Indiana University. The image 
was rendered using Mathematics and Wavefront software. 
(Courtesy of the Williams Gallery. Copyright 8 1991 by Stcmrt 
Dirkson.) 

Figure 1-31 
Using mathematical hlnctiow, 
fractal procedures, and 
supermmpu ters, this artist- 
composer experiments with various 
designs to synthesii form and color 
with musical composition. (Courtesy 
of Brian Ewns, Vanderbilt University.) 



tronic art created with the aid of mathematical relationships is shown in Fig. 1-31. seaion 1-3 
The artwork of this composer is often designed in relation to frequency varia- Computer Art 
tions and other parameters in a musical composition to produce a video that inte- 
grates visual and aural patterns. 

Although we have spent some time discussing current techniques for gen- 
erating electronic images in the fine arts, these methods are also applied in com- 
mercial art for logos and other designs, page layouts combining text and graph- 
ics, TV advertising spots, and other areas. A workstation for producing page 
layouts that combine text and graphics is ihstrated in Fig. 1-32. 

For many applications of commercial art (and in motion pictures and other 
applications), photorealistic techniques are used to render images of a product. 
Figure 1-33 shows an example of logo design, and Fig. 1-34 gives three computer 
graphics images for product advertising. Animations are also usxi frequently in 
advertising, and television commercials are produced frame by frame, where 

l i p r t .  1-32 
Page-layout workstation. (Courtesy 
oj Visunl Technology.) 

Figure 1-33 
Three-dimens~onal rendering for a 
logo. (Courtesy of Vertigo Technology, 
Inc.) 

- . 
Fi<yuru 1 - 34 
Product advertising. (Courtesy oj la) Audrey Fleisherand lb) and lc) SOFTIMAGE, Inc.) 



Chapter 1 each frame of the motion is rendered and saved as an image file. In each succes- 
A Survey of Computer Graphics sive frame, the motion is simulated by moving o w  positions slightly from their 

positions in the previous frame. When all frames in the animation sequence have 
been mdered, the frames are t rans fed  to film or stored in a video buffer for 
playback. Film animations require 24 frames for each second in the animation se- 
quence. If the animation is to be played back on a video monitor, 30 frames per 
second are required. 

A common graphics method employed in many commercials is rnorphing, 
where one obiect is transformed (metamomhosed) into another. This method has 
been used in h commercials to an oii can into an automobile engine, an au- 
tomobile into a tiger, a puddle of water into a tk, and one person's face into an- 
other face. An example of rnorphing is given in Fig. 1-40. 

1-4 
ENTERTAINMENT 

Computer graphics methods am now commonly used in making motion pic- 
tures, music videos, and television shows. Sometimes the graphics scenes are dis- 
played by themselves, and sometimes graphics objects are combined with the ac- 
tors and live scenes. 

A graphics scene generated for the movie Star Trek-% Wrath of Khan is 
shown in Fig. 1-35. The planet and spaceship are drawn in wirefame form and 
will be shaded with rendering methods to produce solid surfaces. Figure 1-36 
shows scenes generated with advanced modeling and surfacerendering meth- 
ods for two awardwinning short h. 

Many TV series regularly employ computer graphics methods. Figure 1-37 
shows a scene p d u c e d  for the seriff Deep Space Nine. And Fig. 1-38 shows a 
wireframe person combined with actors in a live scene for the series Stay lhned. 

~ i a ~ h i a  developed for the 
Paramount Pictures movie Stnr 
Trek-The Wllrrh of Khan. (Courtesy of 
Ewns & Sutherland.) 



In Fig. 1-39, we have a highly realistic image taken from a reconstruction of thir- M i o n  1-4 
teenth-century Dadu (now Beijing) for a Japanese broadcast. Enterlainrnent 

Music videos use graphin in several ways. Graphics objects can be com- 
bined with the live action, as in Fig.1-38, or graphics and image processing tech- 
niques can be used to produce a transformation of one person or object into an- 
other (morphing). An example of morphing is shown in the sequence of scenes in 
Fig. 1-40, produced for the David Byme video She's Mad. 

F i p r c  1-36 
(a) A computer-generated scene from the film M s  D mm, copyright O Pixar 1987. (b) A 
computer-generated scene from the film K n i c M ,  copyright O Pixar 1989. (Courfesy of 
Pixar.) 

- - - . - - -. - - - 
I i p r c  1 -  17 
A graphics scene in the TV series Dwp Space Nine. (Courtesy of Rhythm b. 
Hues Studios.) 



A Survey of Computer Graphics 

Figurp 1-38 
Graphics combined with a Live scene in the TV series Stay 7bned. 
(Courtesy of Rhythm 6 Hues Studios.) 

Figure 1-39 
An image from a &owhuction of 
thirteenth-centwy Dadu (Beijmg 
today), created by Taisei 
Corporation (Tokyo) and rendered 
with TDI software. (Courtesy of 
Thompson Digital Image, lnc.) 



St*ion 1-5 
Education and Training 

F i p w  I -413 
Examples of morphing from 
the David Byrne video Slw's 
Mnd. (Courtcsv of Dnvid Bvrne, 
I& video. o h  ~ a c i f i c  Dota 
Images.) 

1-5 
EDUCATION AND TRAINING 

Computer-generated models of physical, financial, and economic systems are 
often used as educational aids. Models of physical systems, physiological sys- 
tems, population trends, or equipment, such as the colorcoded diagram in Fig. 1- 
41, can help trainees to understand the operation of the system. 

For some training applications, special systems are designed. Examples of 
such specialized systems are the simulators for practice sessions or training of 
ship captains, aircraft pilots, heavy-equipment operators, and air trafficcontrol 
personnel. Some simulators have no video screens; for example, a flight simula- 
tor with only a control panel for instrument flying. But most simulators provide 
graphics screens for visual operation. Two examples of large simulators with in- 
ternal viewing systems are shown in Figs. 1-42 and 1-43. Another type of viewing 
system is shown in Fig. 1 4 4 .  Here a viewing screen with multiple panels is 
mounted in front of the simulator. and color projectors display the flight m e  on 
the screen panels. Similar viewing systems are used in simulators for training air- 
craft control-tower personnel. Figure 1-45 gives an example of the inshuctor's 
area in a flight simulator. The keyboard is used to input parameters affeding the 
airplane performance or the environment, and the pen plotter is used to chart the 
path of the aircraft during a training session. 

Scenes generated for various simulators are shown in Figs. 1-46 through 1- 
48. An output from an automobile-driving simulator is given in Fig. 1-49. This 
simulator is used to investigate the behavior of drivers in critical situations. The 
drivers' reactions are then used as a basis for optimizing vehicle design to maxi- 
mize traffic safety. 



Figure 1 -4 1 
Color-coded diagram used to 
explain the operation of a nuclear 
reactor. (Courtesy of Las Almnos 
National laboratory.) 

Figure 1-42 
A Me, enclosed tlight simulator 
with a full-color visual system and 
six degrees of freedom in its 
motion. (Courtesy of Fmxm 
Intematwml.) 

- -- 

Figure 1 4 3  
A military tank simulator with a visual imagery system. (Courtesy of 
Mediatech and GE Aerospace.) 



kction 1-5 
Edwtion and Training 

Figure 1-44 
A fight simulator with an external full-zulor viewing system. (Courtay a f F m  
InternafiomI.) 

Figure 1-45 
An instructor's area in a flight sunulator. The equipment allows the 
instructor to monitor flight conditions and to set airphne and 
environment parameters. (Courtesy of Frasur Infermtionol.) 



F i p  1-46 
Flightsimulator imagery. ((Courtesy 4 Emns 6 Sutherfund.) 

- 

Figure 1-47 
Imagery generated for a naval 
simulator. (Courtesy of Ewns 6 
Sutherlrmd.) 

Figlire 1-48 
Space shuttle imagery. (Courtesy of 
Mediatech and GE Aerospce.) 



Figure 1-49 
Imagery from an automobile 
simulator used to test driver 
reaction. (Courtesy of Evans 6 
Sutherlrmd.) 

1-6 
VISUALIZATION 

Scientists, engineers, medical personnel, business analysts, and others often need 
to analyze large amounts of information or to study the behavior of certain 
processes. Numerical simulations carried out on supercomputers frequently pro- 
duce data files containing thousands and even millions of data values. Similarly, 
satellite cameras and other sources are amassing large data files faster than they 
can be interpreted. Scanning these large sets of n u m b a  to determine trends and 
relationships is a tedious and ineffective process. But if the data are converted to 
a visual form, the trends and patterns are often immediately apparent. Figure 1- 
50 shows an example of a large data set that has been converted to a color-coded 
display of relative heights above a ground plane. Once we have plotted the den- 
sity values in this way, we can see easily the overall pattern of the data. Produc- 
ing graphical representations for scientific, engineering, and medical data sets 
and processes is generally referred to as scientific visualization. And the tenn busi- 
ness visualization is used in connection with data sets related to commerce, indus- 
try, and other nonscientific areas. 

There are many different kinds of data sets, and effective visualization 
schemes depend on the characteristics of the data. A collection of data can con- 
tain scalar values, vectors, higher-order tensors, or any combiytion of these data 
types. And data sets can be two-dimensional or threedimensional. Color coding 
is just one way to visualize a data set. Additional techniques include contour 
plots, graphs and charts, surface renderings, and visualizations of volume interi- 
ors. In addition, image processing techniques are combined with computer 
graphics to produce many of the data visualizations. 

Mathematicians, physical scientists, and others use visual techniques to an- 
alyze mathematical functions and processes or simply to produce interesting 
graphical representations. A color plot of mathematical curve functions is shown 
in Fig. 1-51, and a surface plot of a function is shown in Fig. 1-52. Fractal proce- 

W i o n  16 
Visualization 



Chapter 1 

A Survey of Computer Graphics 

- .- 

Figure 1-50 
A color-coded plot with 16 million density points of relative brightness 
ob~t?~ed  for the Whirlpool Nebula reveals two distinct galaxies. 
(Courtesy of Lar AIam National Laboratory.) 

- - 

Figure 1-51 Figurn 1-52 
Mathematical curve functiow Lighting effects and surface- 
plotted in various color rendering techniqws were applied 
combinations. (Courtesy ofMeluin L. to produce this surface 
Prun'tt, Los Alamos National representation for a three- 
Laboratory.) dimensional funhon. (Courtesy of 

Wfmm h m h ,  Inc, The hfaker of 
Mathmurtica.) 



dures using quaternions generated the object shown in Fig. 1-53, and a topologi- 1-6 
cal shucture is displayed in Fig. 1-54. Scientists are also developing methods for wsualization 
visualizing general classes of data. Figure 1-55 shows a general technique for 
graphing and modeling data distributed over a spherical surface. 

A few of the many other visualization applications are shown in Figs. 1-56 
through 149. These f i g k  show airflow ove? ihe surface of a space shuttle, nu- 
merical modeling of thunderstorms, study of aack propagation in metals, a 
colorcoded plot of fluid density over an airfoil, a cross-sectional slicer for data 
sets, protein modeling, stereoscopic viewing of molecular structure, a model of 
the ocean floor, a Kuwaiti oil-fire simulation, an air-pollution study, a com-grow- 
ing study, rrconstruction of Arizona's Cham CanY& tuins, and a-graph ofauto- 
mobile accident statistics. 

-- 
Figure 1-53 
A four-dimensional object 
projected into three- 
dimensional space, then 
projected to a video monitor, 
and color coded. The obpct 
was generated using 
quaternions and fractal 
squaring procedm, with an 
Want subtracted to show the 
complex Julia set. (Crmrtrsy of 
Iohn C.  Ifart, School of 
Electrical Enginem'ng d 
Computer Science, Washingfon 
State Uniwrsity.) 

Figure 1 -54 
Four views from a real-time, 
interactive computer-animation 
study of minimal surface ("snails") 
in the 3- sphere projected to three- 
dimensional Euclidean space. 
(Courtesy of George Francis, 
Deprtmmt of M a t h t i c s  ad the 
Natwnal Colter for Sup~rromputing 
Applications, University of Illinois at 
UrhnaChampaign. Copyright O 
1993.) 

- 

F+pre 1-55 
A method for graphing and 
modeling data distributed over a 
spherical surface. (Courfesy of Greg 
Nielson. Computer Science 
Department, Arizona State 
University.) 



Cjlapter 1 

A Survey of Computer Graphics 

Figure 1-56 
A visualization of &eam surfaces 
flowing past a space shuttle by Jeff 
Hdtquist and Eric Raible, NASA 
Ames. (Courtlsy of Sam W t o n ,  
NASA Amcs Raaadr Cnrtlr.) 

Figure 1-57 
Numerical model of airflow inside 
a thunderstorm. (Cmtrtsv of Bob 

Figure 2-58 
Numerical model of the surface of a 
thunderstorm. (Courtsy of Sob 
Wilklmsbn, Lkprhnmt of  
Atmospheric Sciences and t k  NatiaMl 
Center lor Supercomputing 
Applications, Unimmity ofnlinois at 
Urbana-Champrip.) 



-- -- 
Figure 1-59 
C o l o r d e d  visualization of stress 
energy density in a crack- 
propagation study for metal plates, 
modeled by Bob Haber. (Courfesy of 
tk Natioml Cinter for 
Supercaputmg Applicutions, 
U n m i t y  of n l i ~ i s  at Urbrmn- 
Chnmpa~gn.) 

Figure 1-60 
A fluid dynamic simulation, 
showing a color-coded plot of fluid 
density over a span of grid planes 
around an aircraft wing, developed 
by Lee-Hian Quek, John 
Eickerneyer, and Jeffery Tan. 
(Courtesy of the Infinnation 
Technology Institute, Republic of 
Singapore.) 

F@w 1-61 
Commercial slicer-dicer software, 
showing color-coded data values 
over awsedional  slices of a data 
set. (Courtesy of Spyglnss, Im.) 

F i p m  1-62 
Visualization of a protein structure 
by Jay Siege1 and Kim Baldridge, 
SDSC. (Courfesy of Stephnnie Sides, 
San Diego Supercomputer Cmter.) 



Figure 1 -63 
Stereoscopic viewing of a molecular strumup using a "boom" device. 
(Courtesy of the Nafiaal  Centerfir Supermputing Applhtions, Univmity  
of Illinois at UrbomChnmprign.) 

Figure 1-64 
One image from a s t e n d q n c  pair, 
showing a visualization of the 
ocean floor obtained from mteltik 
data, by David Sandwell and Chris 
Small, Scripps Institution of Ocean- 
ography, and Jim Mdeod, SDSC. 
(Courtesy of Stephanie Sids, Sun 
Diego Supramrputer Center.) 

Figvne 1 6 5  
A simulation of the e f f d s  of the 
Kuwaiti oil fire, by Gary 
Glatpneier, Chuck Hanson, and 
Paul Hinker. ((Courtesy of Mike 
Kmzh, Adrnnced Computing 
lnboratwy 41 Los Alrrmos Nafionul 
hbomtwy.)  



Section 1-6 

Visualization 
-1 -----7 - I '1 
Figure 1-66 
A visualization of pollution over 
the earth's surface by Tom Palmer, 
Cray Research Inc./NCSC; Chris 
Landreth, NCSC; and Dave W, 
NCSC. Pollutant SO, is plotted as a 
blue surface, acid-rain deposition is 
a color plane on the map surface, 
and rain concentration is shown as 
clear cylinders. (Courtesy of the 
North Cnmlim Supercomputing 
Center/MCNC.) 

- - .  - 
Figure 1-68 
A visualization of the 
reconstruction of the ruins at Cham 
Canyon, Arizona. (Courtesy of 
Melvin L. Pnceitt, Los Alamos 
Nationul lnboratory. Data supplied by 
Stephen If. Lekson.) 

Figure 1-67 
One frame of an animation 
sequence showing the development 
of a corn ear. (Couitcsy of tk 
National Center for Supmomputing 
Applimhs, U n i m i t y  ofnlinois at 
UrhnaChampaign.) 

Figure 1-69 
A prototype technique, called 
WinVi,  for visualizing tabular 
multidimensional data is used here 
to correlate statistical information 
on pedestrians involved in 
automobile accidents, developed by 
a visuahzation team at ITT. 
(Courtesy of Lee-Hian Quek, 
Infonnatwn Technology Institute. 
Republic of Singapore.) 



Although methods used in computer graphics and Image processing overlap, the 
amas am concerned with fundamentally different operations. In computer 

graphics, a computer is used to create a pichue. Image processing, on the other 
hand. applies techniques to modify or interpret existing pibures, such as p h e  
tographs and TV scans. Two principal applications of image pmcessing are (1) 
improving picture quality and (2) machine perception of visual information, as 
used in robotics. 

To apply imageprocessing methods, we first digitize a photograph or other 
picture into an image file. Then digital methods can be applied to rearrange pic- 
ture parts, to enhance color separations, or to improve the quality of shading. An 
example of the application of imageprocessing methods to enhance the quality 
of a picture is shown in Fig. 1-70. These techniques are used extensively in com- 
mercial art applications that involve the retouching and rearranging of sections 
of photographs and other artwork. Similar methods are used to analyze satellite 
photos of the earth and photos of galaxies. 

Medical applications also make extensive use of imageprocessing tech- 
niques for picture enhancements, in tomography and in simulations of opera- 
tions. Tomography is a technique of X-ray photography that allows cross-sec- 
tional views of physiological systems to be displayed. Both computed X-rav 
tomography (CT) and position emission tomography (PET) use propchon methods to 
reconstruct cross sections from digital data. These techniques are also used to 

.- -. 

figure 1-70 
A blurred photograph of a license plate becomes legible after 
the application of imageprocessing techniques. (Courtesy of 
Los Alamos National Laboratory.) 



monitor internal functions and show crcss sections during surgery. Other me& ~~ 1-7 
ical imaging techniques include ultrasonics and nudear medicine scanners. With Image Pm&ng 
ultrasonics, high-frequency sound waves, instead of X-rays, are used to generate 
digital data. Nuclear medicine scanners colled di@tal data from radiation emit- 
ted from ingested radionuclides and plot colorcoded images. 

lmage processing and computer graphics are typically combined in 
many applications. Medicine, for example, uses these techniques to model and 
study physical functions, to design artificial limbs, and to plan and practice 
surgery. The last application is generally referred to as computer-aided surgery. 
Two-dimensional cross sections of the body are obtained using imaging tech- 
niques. Then the slices are viewed and manipulated using graphics methods to 
simulate actual surgical procedures and to try out different surgical cuts. Exam- 
ples of these medical applications are shown in Figs. 1-71 and 1-72. 

Figure 1-71 
One frame from a computer 
animation visualizing cardiac 
activation levels within 
regions of a semitransparent 
volume rendered dog heart. 
Medical data provided by 
Wiiam Smith, Ed Simpson, 
and G. Allan Johnson, Duke 
University. Image-rendering 
software by Tom Palmer, 
Cray Research, Inc./NCSC. 
(Courtesy of Dave Bock, North 
Carolina Supercomputing 
CenterlMCNC.) Figure 1-72 

One image from a stereoscopic pair 
showing the bones of a human 
hand. The images were rendered by 
lnmo Yoon, D. E. Thompson, and 
W. N. Waggempack, Jr;, LSU, from 
a data set obtained with CT scans 
by Rehabilitation Research, 
GWLNHDC. These images show a 
possible tendon path for 
reconstructive surgery. (Courtesy of 
IMRLAB, Mechnniwl Engineering, 
Louisiow State Un iws i t y . )  



Chapter 1 1 4  

ASuNe~ofComputerCraphics GRAPHICAL USER INTERFACES 

I t  is common now for software packages to provide a graphical interface. A 
major component of a graphical interface is a window manager that allows a user 

. to display multiple-window areas. Each window can contain a different process 
that can contain graphical or nongraphical displays. To make a particular win- 
dow active, we simply click in that window using an interactive pointing dcvicc. 

Interfaces also display menus and icons for fast selection of processing op- 
tions or parameter values. An icon is a graphical symbol that is designed to look 
like the processing option it represents. The advantages of icons are that they 
take up  less screen space than corresponding textual descriptions and they can be 
understood more quickly if well designed. Menbs contain lists of textual descrip- 
tions and icons. 

Figure 1-73 illustrates a typical graphical mterface, containing a window 
manager, menu displays, and icons. In this example, the menus allow selection of 
processing options, color values, and graphics parameters. The icons represent 
options for painting, drawing, zooming, typing text strings, and other operations 
connected with picture construction. 

-- 

Figure 1-73 
A graphical user interface, showing 
multiple window areas, menus, and 
icons. (Courtmy of Image-ln 
Grponrtion.) 





D ue to the widespread recognition of the power and utility of computer graphics in virtually all fields, a broad range of graphics hardware and 
software systems is now available. Graphics capabilities for both two-dimen- 
sional and three-dimensional applications a x  now common on general-purpose 
computers, including many hand-held calculators. With personal computers, we 
can use a wide variety of interactive input devices and graphics software pack- 
ages. For higherquality applications, we can choose from a number of sophisti- 
cated special-purpose graphics hardware systems and technologies. In this chap 
ter, we explore the basic features of graphics hardwa~e components and graphics 
software packages. 

2-1 
VIDEO DISPLAY DEVICES 

Typically, the primary output device in a graphics system is a video monitor (Fig. 
2-1). The operation of most video monitors is based on the standard cathode-ray 
tube (CRT) design, but several other technologies exist and solid-state monitors 
may eventually predominate. 

- -- - 
rig~rrr 2-1 
A computer graphics workstation. (Courtrsyof T h i r .  Inc.) 



Refresh Cathode-Ray Tubes 

F i p m  2-2 illustrates the basic operation of,a CRT. A beam of electrons (cathode 
rays), emitted by an electron gun, passes through focusing and deflection systems 
that direct the beam toward specified positions on the phosphomted screen. 
The phosphor then emits a small spot of light at each position contacted by the 
electron beam. Because the light emitted by the phosphor fades very rapidly, 
some method is needed for maintaining the screen picture. One way to keep the 
phosphor glowing is to redraw the picture repeatedly by quickly directing the 
electron beam back over the same points. This type of display is called a refresh 
CRT. 

The primary components of an electron gun in a CRT are the heated metal 
cathode and a control grid (Fig. 2-31. Heat is supplied to the cathode by direding 
a current through a coil of wire, called the filament, inside the cylindrical cathode 
structure. This causes electrons to be 'k i led  off" the hot cathode surface. In the 
vacuum inside the CRT envelope, the free, negatively charged electrons are then 
accelerated toward the phosphor coating by a high positive voltage. The acceler- 

Magnetic 
Deflection Coils Phosphor- 

Focusina Coated 
Screen 

Electron 
Beam Connector Elrnron a !. .'k 

- -- 

Figure 2-2 
Basic design of a magneticdeflection CRT. 

Focusing 
Cathode Anode 

I 

Electron 
Beam 
Path 

kction 2-1 
V k h  Display Devices 

- 

Figure 2-3 
Operation of an electron gun with an accelerating anode. 



Chapter 2 ating voltage can be generated with a positively charged metal coating on the in- 
overview of Graphics Systems side of the CRT envelope near the phosphor screen, or an accelerating anode can 

be used, as in Fig. 2-3. Sometimes the electron gun is built to contain the acceler- 
ating anode and focusing system within the same unit. 

Intensity of the electron beam is controlled by setting voltage levels on the 
control grid, which is a metal cylinder that fits over the cathode. A high negative 
voltage applied to the control grid will shut off the beam by repelling eledrons 
and stopping them from passing through the small hole at the end of the control 
grid structure. A smaller negative voltage on the control grid simply decreases 
the number of electrons passing through. Since the amount of light emitted by 
the phosphor coating depends on the number of electrons striking the screen, we  
control the brightness of a display by varying the voltage on the control grid. We 
specify the intensity level for individual screen positions with graphics software 
commands, as discussed in Chapter 3. 

The focusing system in a CRT is needed to force the electron beam to con- 
verge into a small spot as it strikes the phosphor. Otherwise, the electrons would 
repel each other, and the beam would spread out as it approaches the screen. Fo- 
cusing is accomplished with either electric or magnetic fields. Electrostatic focus- 
ing is commonly used in television and computer graphics monitors. With elec- 
trostatic focusing, the elwtron beam passes through a positively charged metal 
cylinder that forms an electrostatic lens, as shown in Fig. 2-3. The action of the 
electrostatic lens fdcuses the electron beam at the center of the screen, in exactly 
the same way that an optical lens focuses a beam of hght at a particular focal dis- 
tance. Similar lens focusing effects can be accomplished with a magnetic field set 
up by a coil mounted around the outside of the CRT envelope. Magnetic lens fc- 
cusing produces the smallest spot size on the screen and is used in special- 
purpose devices. 

Additional focusing hardware is used in high-precision systems to keep the 
beam in focus at all m n  positions. The distance that thc electron beam must 
travel to different points on the screen varies because thc radius of curvature for 
most CRTs is greater than the distance from the focusing system to the screen 
center. Therefore, the electron beam will be focused properly only at the center o t  
the screen. As the beam moves to the outer edges of the screen, displayed images 
become blurred. To compensate for this, the system can adjust the focusing ac- 
cording to the screen position of the beam. 

As with focusing, deflection of the electron beam can be controlled either 
with electric fields or with magnetic fields. Cathode-ray tubes are now commonl!. 
constructed with magnetic deflection coils mounted on the outside of the CRT 
envelope, as illustrated in Fig. 2-2. Two pairs of coils are used, with the coils in 
each pair mounted on opposite sides of the neck of the CRT envelope. One pair is 
mounted on the top and bottom of the neck, and the other pair is mounted on 
opposite sides of the neck. The magnetic, field produced by each pair of coils re- 
sults in a transverse deflection force that is perpendicular both to the direction of 
the magnetic field and to the direction of travel of the electron beam. Horizontal 
deflection is accomplished with one pair of coils, and vertical deflection by the 
other pair. The proper deflection amounts are attained by adjusting the current 
through the coils. When electrostatic deflection is used, two pairs of parallel 
plates are mounted inside the CRT envelope. One pair oi plates is mounted hori- 
zontally to control the vertical deflection, and the other pair is mounted verticall!. 
to control horizontal deflection (Fig. 2-4). 

Spots of light are produced on the screen by the transfer of the CRT beam 
energy to the phosphor. When the electrons in the beam collide with the phos- 



Ven~cal 
Focusing Deflection 

Connector Elr-,:wn ticr~zontal Beam 
Pins Gun De!lection 

Plates 

Figure 2-4 
Electmstatic deflection of the electron beam in a CRT. 

Phosphor- 
Coated 
Screen 

Electron 

phor coating, they are stopped and thek kinetic energy is absorbed by the phos- 
phor. Part of the beam energy is converted by friction into heat energy, and the 
remainder causes electrons in the phosphor atoms to move up to higher quan- 
tum-energy levels. After a short time, the "excited phosphor electrons begin 
dropping back to their stable ground state, giving up their extra energy as small 
quantums of Light energy. What we see on the screen is the combined effect of all 
the electron light emissions: a glowing spot that quickly fades after all the excited 
phosphor electrons have returned to their ground energy level. The frequency (or 
color) of the light emitted by the phosphor is proportional to the energy differ- 
ence between the excited quantum state and the ground state. 

Different hnds  of phosphors are available for use in a CRT. Besides color, a 
mapr difference between phosphors is their persistence: how long they continue 
to emit light (that is, have excited electrons returning to the ground state) after 
the CRT beam is removed. Persistence is defined as the time it takes the emitted 
light from the screen to decay to one-tenth of its original intensity. Lower- 
persistence phosphors require higher refresh rates to maintain a picture on the 
screen without flicker. A phosphor with low persistence is useful for animation; a 
high-persistence phosphor is useful for displaying highly complex, static pic- 
tures. Although some phosphors have a persistence greater than 1 second, graph- 
ics monitors are usually constructed with a persistence in the range from 10 to 60 
microseconds. 

Figure 2-5 shows the intensity distribution of a spot on the screen. The in- 
tensity is greatest at the center of the spot, and decreaws with a Gaussian distrib- 
ution out to the edges of the spot. This distribution corresponds to the mss-  
sectional electron density distribution of the CRT beam. ' 

The maximum number of points that can be displayed without overlap on a 
CRT is referred to as the resolution. A more precise definition of m!ution is the 
number of points per centimeter that can be plotted horizontally and vertically, 
although it is often simply stated as the total number of points in each direction. 
Spot intensity has a Gaussian distribution (Fig. 2-5), so two adjacent spok will 
appear distinct as long as their separation is greater than the diameter at which 
each spot has an intensity of about 60 percent of that at the center of the spot. 
This overlap position is illustrated in Fig. 2-6. Spot size also depends on intensity. 
As more electrons are accelerated toward the phospher per second, the CRT 
beam diameter and the illuminated spot increase. In addition, the increased exci- 
tation energy tends to spread to neighboring phosphor atoms not directly in the 

Fipn 2-5 
Intensity distribution of an 
illuminated phosphor spot on 
a CRT screen. 



Chrpcer 2 

Overview of Graphics Sptems 

Figure 2-6 
Two illuminated phosphor 
spots are distinguishable 
when their separation is 
greater than the diameter at 
which a spot intensity has 
fallen to 60 percent of 
maximum. 

path of the beam, which further increases the spot diameter. Thus, resolution of a 
CRT is dependent on the type of phosphor, the intensity to be displayed, and the 
focusing and deflection systems. Typical resolution on high-quality systems is 
1280 by 1024, with higher resolutions available on many systems. High- 
resolution systems are often referred to as high-definition systems. The physical 
size of a graphics monitor is given as  the length of the screen diagonal, with sizes 
varying from about 12 inches to 27 inches or more. A CRT monitor can be at- 
tached to a variety of computer systems, so the number of screen points that can 
actually be plotted depends on the capabilities of the system to which it is at- 
tached. 

Another property of video monitors is aspect ratio. This number gives the 
ratio of vertical points to horizontal points necessary to produce equal-length 
lines in both directions on the screen. (Sometimes aspect ratio is stated in terms of 
the ratio of horizontal to vertical points.) An aspect ratio of 3 / 4  means that a ver- 
tical line plotted with three points has the same length as a horizontal line plot- 
ted with four points. 

Raster-Scan Displays 

The most common type of graphics monitor employing a CRT is the raster-scan 
display, based on television technology. In a raster-scan system, the electron 
beam is swept across the screen, one row at a time from top to bottom. As the 
eledron beam moves across each row, the beam intensity is turned on and off to 
create a pattern of illuminated spots. Picture definition is stored in a memory 
area called the refresh buffer or frame buffer. This memory area holds the set of 
intensity values for all the screen points. Stored intensity values are then re- 
trieved from the refresh buffer and "painted" on the screen one row (scan line) a t  
a time (Fig. 2-7). Each screen point is referred to as a pixel or pel (shortened 
fonns of picture element). The capability of a raster-scan system to store inten- 
sity information for each screen point makes it well suited for the realistic displav 
of scenes containing subtle shading and color patterns. Home television sets and 
printers are examples of other systems using raster-scan methods. 

intensity range for pixel positions depends on the capability of the raster 
system. In a simple black-and-white system, each screen point is either on or off, 
so only one bit per pixel is needed to control the intensity of screen positions. For 
a bilevel system, a bit value of 1 indicates that the electron beam is to be t u r n 4  
on at that position, and a value of 0 indicates that the beam intensity is to be off. 
Additional bits are needed when color and intensity variations can be displayed. 
Up to 24 bits per pixel are included in high-quality systems, which can require 
severaI megabytes of storage for the frame buffer, depending on the resolution of 
the system. A system with 24 bits per pixel and a screen resolution of 1024 bv 
1024 requires 3 megabytes of storage for the frame buffer. On a black-and-white 
system with one bit per pixeI, the frame buffer is commonly called a bitmap. For 
systems with multiple bits per pixel, the frame buffer is Aten referred to as a 
pixmap. 

Refreshing on raster-scan displays is carried out at the rate of 60 to 80 
frames per second, although some systems are designed for higher refresh rates. 
Sometimes, refresh rates are described in units of cycles per second, or Hertz 
(Hz), where a cycle corresponds to one frame. Using these units, we would de- 
scribe a refresh rate of 60 frames per second as simply 60 Hz. At the end of each 
scan line, the electron beam returns to the left side of the screen to begin displav- 
ing the next scan line. The return to the left of the screen, after refreshing each 



Figure 2-7 
A raster-scan system displays an object as a set of dismte points across 
each scan line. 

scan line, is called the horizontal retrace of the electron beam. And at the end of 
each frame (displayed in 1/80th to 1/60th of a second), the electron beam returns 
(vertical retrace) to the top left comer of the screen to begin the next frame. 

On some raster-scan systems (and in TV sets), each frame is displayed in 
two passes using an interlaced refresh pmedure. In the first pass, the beam 
sweeps across every other scan line fmm top to bottom. Then after the vertical re- 
trace, the beam sweeps out the remaining scan lines (Fig. 2-8). Interlacing of the 
scan lines in this way allows us to see the entire s m n  displayed in one-half the 
time it would have taken to sweep a m s s  all the lines at once fmm top to bottom. 
Interlacing is primarily used with slower refreshing rates. On an older, 30 frame- 
per-second, noninterlaced display, for instance, some flicker is noticeable. But 
with interlacing, each of the two passes can be accomplished in 1/60th of a sec- 
ond, which brings the refresh rate nearer to 60 frames per second. This is an effec- 
tive technique for avoiding flicker, providing that adjacent scan lines contain sim- 
ilar display information. 

Random-Scan Displays 

When operated as a random-scan display unit, a CRT has the electron beam di- 
rected only to the parts of the screen where a picture is to be drawn. Random- 
scan monitors draw a picture one line at a time and for this reason are also re- 
ferred to as vector displays (or stroke-writing or calligraphic diisplays). The 
component lines of a picture can be drawn and refreshed by a random-scan sys- 



Chapter 2 

Overview of Graphics Systems 

Figure 2-8 
Interlacing scan lines on a raster- 
scan display. First, al l  points on the 
wen-numbered (solid) scan lines 
are displayed; then all points along 
the odd-numbered (dashed) lines 
are displayed. 

tem in any specified order (Fig. 2-9). A pen plotter operates in a similar way and 
is an example of a random-scan, hard-copy device. 

Refresh rate on a random-scan system depends on the number of lines to be 
displayed. Picture definition is now stored as a set of linedrawing commands in 
an area of memory r e f e d  to as the refresh display file. Sometimes the refresh 
display file is called the display list, display program, or simply the refresh 
buffer. To display a specified picture, the system cycles through the set of com- 
mands in the display file, drawing each component line in turn. After all line- 
drawing commands have been processed, the system cycles back to the first line 
command in the list. Random-scan displays arr designed to draw all the compo- 
nent lines of a picture 30 to 60 times each second. Highquality vector systems are 
capable of handling approximately 100,000 "short" lines at this refresh rate. 
When a small set of lines is to be displayed, each rrfresh cycle is delayed to avoid 
refresh rates greater than 60 frames per second. Otherwise, faster refreshing oi 
the set of lines could bum out the phosphor. 

Random-scan systems are designed for linedrawing applications and can- 
not display realistic shaded scenes. Since pidure definition is stored as a set of 
linedrawing instructions and not as a set of intensity values for all screen points, 
vector displays generally have higher resolution than raster systems. Also, vector 
displays produce smooth line drawings because the CRT beam directly follows 
the line path. A raster system, in contrast, produces jagged lines that are plotted 
as d h t e  point sets. 

Color CRT Monitors 

A CRT monitor displays color pictures by using a combination of phosphors that 
emit different-colored light. By combining the emitted light from the different 
phosphors, a range of colors can be generated. The two basic techniques for pro- 
ducing color displays with a CRT are the beam-penetration method and the 
shadow-mask method. 

The beam-penetration method for displaying color pictures has been used 
with random-scan monitors. Two layers of phosphor, usually red and green, are 



Figure 2-9 
A random-scan system draws the component lines of an object in any 
order specified. 

coated onto the inside of the CRT screen, and the displayed color depends on 
how far the electron beam penetrates into the phosphor layers. A beam of slow 
electrons excites only the outer red layer. A beam of very fast electrons penetrates 
through the red layer and excites the inner green layer. At intermediate beam 
speeds, combinations of red and green light are emitted to show two additional 
colors, orange and yellow. The speed of the electrons, and hence the screen color 
at any point, is controlled by the beam-acceleration voltage. Beam penetration 
has been an inexpensive way to produce color in random-scan monitors, but only 
four colors are possible, and the quality of pictures is not as good as with other 
methods. 

Shadow-mask methods are commonly used in rasterscan systems (includ- 
ing color TV) because they produce a much wider range of colors than the beam- 
penetration method. A shadow-mask CRT has three phosphor color dots at each 
pixel position. One phosphor dot emits a red light, another emifs a green light, 
and the third emits a blue light. This type of CRT has three electron guns, one for 
each color dot, and a shadow-mask grid just behind the phosphor-coated screen. 
Figure 2-10 illustrates the deltadelta shadow-mask method, commonly used in 
color CRT systems. The three electron beams are deflected and focused as a 
group onto the shadow mask, which contains a series of holes aligned with the 
phosphor-dot patterns. When the three beams pass through a hole in the shadow 
mask, they activate a dot triangle, which appears as a small color spot on the 
screen. The phosphor dots in the triangles are arranged so that each electron 
beam can activate only its corresponding color dot when it passes through the 



Chapter 2 

Overview of Graphics Systems 

Elearon 
Guns 

I Magnified 
I Phos~hor-Do1 ' Trtsngle 

Figure 2-10 
Operation of a delta-delta, shadow-mask CRT. Three electron 
guns, aligned with the triangular colordot patterns on the screen, 
are directed to each dot triangle by a shadow mask. 

shadow mask. Another configuration for the three electron guns is an in-line 
arrangement in which the three electron guns, and the corresponding 
red-green-blue color dots on the screen, are aligned along one scan line instead 
of in a triangular pattern. This in-line arrangement of electron guns is easier to 
keep in alignment and is commonly used in high-resolution color CRTs. 

We obtain color variations in a shadow-mask CRT by varying the intensity 
levels of the three electron beams. By turning off the red and green guns, we get 
only the color coming h m  the blue phosphor. Other combinations of beam in- 
tensities produce a small light spot for each pixel position, since our eyes tend to 
merge the three colors into one composite. The color we see depends on the 
amount of excitation of the red, green, and blue phosphors. A white (or gray) 
area is the result of activating all three dots with equal intensity. Yellow is pro- 
duced with the green and red dots only, magenta is produced with the blue and 
red dots, and cyan shows up when blue and green are activated equally. In some 
low-cost systems, the electron beam can only be set to on or off, limiting displays 
to eight colors. More sophisticated systems can set intermediate intensity levels 
for the electron beams, allowing several million different colors to be generated. 

Color graphics systems can be designed to be used with several types of 
CRT display devices. Some inexpensive home-computer systems and video 
games are designed for use with a color TV set and an RF (radio-muency) mod- 
ulator. The purpose of the RF mCdulator is to simulate the signal from a broad- 
cast TV station. This means that the color and intensity information of the picture 
must be combined and superimposed on the broadcast-muen* carrier signal 
that the TV needs to have as input. Then the cirmitry in the TV takes this signal 
from the RF modulator, extracts the picture information, and paints it on the 
screen. As we might expect, this extra handling of the picture information by the 
RF modulator and TV circuitry decreases the quality of displayed images. 

Composite monitors are adaptations of TV sets that allow bypass of the 
broadcast circuitry. These display devices still require that the picture informa- 



tion be combined, but no carrier signal is needed. Picture information is com- M i o n  2-1 
bined into a composite signal and then separated by the monitor, so the resulting Video Display Devices 
picture quality is still not the best attainable. 

Color CRTs in graphics systems are designed as RGB monitors. These mon- 
itors use shadow-mask methods and take the intensity level for each electron gun 
(red, green, and blue) directly from the computer system without any intennedi- 
ate processing. High-quality raster-graphics systems have 24 bits per pixel in the 
kame buffer, allowing 256 voltage settings for each electron gun and nearly 17 
million color choices for each pixel. An RGB color system with 24 bits of storage 
per pixel is generally referred to as a full-color system or a true-color system. 

Direct-View Storage Tubes 

An alternative method for maintaining a screen image is to store the picture in- 
formation inside the CRT instead of refreshing the screen. A direct-view storage 
tube (DVST) stores the picture information as a charge distribution just behind 
the phosphor-coated screen. Two electron guns are used in a DVST. One, the pri- 
mary gun, is used to store the picture pattern; the second, the flood gun, main- 
tains the picture display. 

A DVST monitor has both disadvantages and advantages compared to the 
refresh CRT. Because no refreshing is needed, very complex pidures can be dis- 
played at very high resolutions without flicker. Disadvantages of DVST systems 
are that they ordinarily d o  not display color and that selected parts of a picture 
cannot he erased. To eliminate a picture section, the entire screen must be erased 
and the modified picture redrawn. The erasing and redrawing process can take 
several seconds for a complex picture. For these reasons, storage displays have 
been largely replaced by raster systems. 

Flat-Panel Displays 

Although most graphics monitors are still constructed with CRTs, other technolo- 
gies are emerging that may soon replace CRT monitc~rs. The term Bat-panel dis- 
play refers to a class of video devices that have reduced volume, weight, and 
power requirements compared to a CRT. A significant feature of flat-panel dis- 
plays is that they are thinner than CRTs, and we can hang them on walls or wear 
them on our wrists. Since we can even write on some flat-panel displays, they 
will soon be available as pocket notepads. Current uses for flat-panel displays in- 
clude small TV monitors, calculators, pocket video games, laptop computers, 
armrest viewing of movies on airlines, as advertisement boards in elevators, and 
as graphics displays in applications requiring rugged, portable monitors. 

We can separate flat-panel displays into two categories: emissive displays 
and nonemissive displays. The emissive displays (or emitters) are devices that 
convert electrical energy into light. Plasma panels, thin-film electroluminescent 
displays, and Light-emitting diodes are examples of emissive displays. Flat CRTs 
have also been devised, in which electron beams arts accelerated parallel to the 
screen, then deflected 90' to the screen. But flat CRTs have not proved to be as 
successful as other emissive devices. Nonemmissive displays (or nonemitters) 
use optical effects to convert sunlight or light from some other source into graph- 
ics patterns. The most important example of a nonemisswe flat-panel display is a 
liquid-crystal device. 

Plasma panels, also called gas-discharge displays, are constructed by fill- 
ing the region between two glass plates with a mixture of gases that usually in- 



Chapter 2 dudes neon. A series of vertical conducting ribbons is placed on one glass panel, 
Overview dGraphics Systems and a set of horizontal ribbons is built into the other glass panel (Fig. 2-11). Firing 

voltages applied to a pair of horizontal and vertical conductors cause the gas at 
the intersection of the two conductors to break down into a glowing plasma of 
elecbons and ions. Picture definition is stored in a refresh buffer, and the firing 
voltages are applied to refresh the pixel positions (at the intersections of the con- 
ductors) 60 times per second. Alternahng-t methods are used to provide 
faster application of the firing voltages, and thus bnghter displays. Separation 
between pixels is provided by the electric field of the conductors. Figure 2-12 
shows a highdefinition plasma panel. One disadvantage of plasma panels has 
been that they were strictly monochromatic devices, but systems have been de- 
veloped that are now capable of displaying color and grayscale. 

Thin-film electroluminescent displays are similar in construction to a 
plasma panel. The diffemnce is that the region between the glass plates is filled 
with a phosphor, such as zinc sulfide doped with manganese, instead of a gas 
(Fig. 2-13). When a suffiaently high voltage is applied to a pair of crossing elec- 
trodes, the phosphor becomes a conductor in the area of the intersection of the 
two electrodes. Electrical energy is then absorbed by the manganese atoms, 
which then release the energy as a spot of light similar to the glowing plasma ef- 
fect in a plasma panel. Electroluminescent displays require more power than 
plasma panels, and good color and gray scale displays are hard to achieve. 

A third type of emissive device is the light-emitting diode (LED). A matrix 
of diodes is arranged to form the pixel positions in the display, and picture defin- 
ition is stored in a refresh buffer. As in xan-line refreshing of a CRT, information 

Figure 2-11 
Basic design of a plasma-panel 
display device. 

Figure 2-12 
A plasma-panel display with a 
resolution of 2048 by 2048 and a 
screen diagonal of 1.5 meters. 
(Courtesy of Photonics Sys tons . )  



M i o n  2-1 

Vldeo Display Devices 

Figure 2-13 
Basic design of a thin-film 
electroluminescent display device. 

is read from the refresh buffer and converted to voltage levels that are applied to 
the diodes to produce the light patterns in the display. - 

~ i ~ u i d & y s t a l  displays (LCDS) are commonly used in small systems, such 
as calculators (Fig. 2-14) and portable, laptop computers (Fig. 2-15). These non- 
emissive devices produce a picture by passing polarized light from the surround- 
ings or from an internal light s o w  through a liquid-aystal material that can be 
aligned to either block or transmit the light. 

The term liquid crystal refers to the fact that these compounds have a crys- 
talline arrangement of molecules, yet they flow like a liquid. Flat-panel displays 
commonly use nematic (threadlike) liquid-crystal compounds that tend to keep 
the long axes of the rod-shaped molecules aligned. A flat-panel display can then 
be constructed with a nematic liquid crystal, as demonstrated in Fig. 2-16. Two 
glass plates, each containing a light polarizer at right angles to the-other plate, 
sandwich the liquid-crystal material. Rows of horizontal transparent conductors 
are built into one glass plate, and columns of vertical conductors are put into the 
other plate. The intersection of two conductors defines a pixel position. Nor- 
mally, the molecules are aligned as shown in the "on state" of Fig. 2-16. Polarized 
light passing through the material is twisted so that it will pass through the op- 
posite polarizer. The light is then mfleded back to the viewer. To turn off the 
pixel, we apply a voltage to the two intersecting conductors to align the mole 
cules so that the light is not .twisted. This type of flat-panel device is referred to as 
a passive-matrix LCD. Picture definitions are stored in a refresh buffer, and the Figure2-14 
screen is refreshed at the rate of 60 frames per second, as in the emissive devices. A hand calculator with an 
Back lighting is also commonly applied using solid-state electronic devices, so (Courtes~of Exus 
that the system is not completely dependent on outside light soufies. Colors can 1N'"ment5.) 

be displayed by using different materials or dyes and by placing a triad of color 
pixelsat each &reen location. Another method for conskctingk13s is to place 
a transistor at each pixel location, using thin-film transistor technology. The tran- 
sistors are used to control the voltage at pixel locations and to prevent charge 
from gradually leaking out of the liquid-crystal cells. These devices are called 
active-matrix displays. 



Figun 2-15 
A backlit, passivematrix, liquid- 
crystal display in a Laptop 
computer, featuring 256 colors, a 
screen resolution of 640 by 400, and 
a saeen diagonal of 9 inches. 
(Caurtesy of Applc Computer, Inc.) 

F i p e  2-16 
The light-twisting, shutter effect used in the design of most liquid- 
crystal display devices. 



Three-Dimensional Viewing Devices Section 2-1 
Video Dtsplay Devices 

Graphics monitors for the display of three-dimensional scenes have been devised 
using a technique that reflects a CRT image from a vibrating, flexible mirror. The 
operation of such a system is demonstrated in Fig. 2-17. As the varifocal mirror 
vibrates, it changes focal length. These vibrations are synchronized with the dis- 
play of an object on  a CRT so  that each point on the object is reflected from the 
mirror into a spatial position corresponding to the distance of that point from a 
specified viewing position. This allows us to walk around an object o r  scene and 
view it from different sides. 

Figure 2-18 shows the Genisco SpaceCraph system, which uses a vibrating 
mirror to project three-dimensional objects into a 25cm by 2 h  by 25- vol- 
ume. This system is also capable of displaying two-dimensional cross-sectional 
"slices" of objects selected at different depths. Such systems have been used in 
medical applications to analyze data fmm ulhasonography and CAT scan de- 
vices, in geological applications to analyze topological and seismic data, in de- 
sign applications involving solid objects, and in three-dimensional simulations of 
systems, such as molecules and terrain. 

-. I-& Vibrating Flsxible Mirror 
-,--. 

I 

Figure 2-1 7 
P 

+ation of a three-dimensional display system using a 
vibrating mirror that changes focal length to match the depth of 
points in a scene. 

D. 

Figure 2-16 
The SpaceCraph interactive 
graphics system displays objects in 
three dimensions using a vibrating, 
flexible mirror. (Courtesy of Genixo 
Compufm Corpornlion.) 

49 



Chapter 2 Stereoscopic and Virtual-Reality Systems 
Overview of Graphics Systems 

Another technique for representing tbd imens iona l  objects is displaying 
stereoscopic views. This method dws  not produce hue three-dimensional im- 
ages, but it does provide a three-dimensional effect by presenting a different 
view to each eye of an observer so that scenes do appear to have depth (Fig. 2-19). 

To obtain a stereoscopic proyxtion, we first need to obtain two views of a 
scene generated from. a yiewing direction corresponding to each eye (left and 
right). We can consma the two views as computer-generated scenes with differ- 
ent viewing positions, or we can use a s t e m  camera pair to photograph some 
object or scene. When we simultaneous look at the left view with the left eye and 
the right view with the right eye, the ~o views merge into a single image and 
we perceive a scene with depth. Figure 2-20 shows two views of a computer- 
generated scene for stemgraphic pmpdiori. To increase viewing comfort, the 
areas at the left and right edges of !lG scene that are visible to only one eye have 
been eliminated. 

-- - .- -. 

Figrrrc 2-19 
Viewing a stereoscopic projection. 
(Courlesy of S1ered;mphics Corpomlion.) 

A stereoscopic viewing pair. (Courtesy ofjtny Farm.) 
5 0  



One way to produce a stereoscopic effect is to display each of the two views M i o n  2-1 
with a raster system on alternate refresh cycles. The sa~en is viewed through Mdeo Display Devices 
glasses, with each lens designed to act as a rapidly alternating shutter that is syn- 
chronized to block out one of the views. Figure 2-21 shows a pair of stereoscopic 
glasses constructed with liquidcrystal shutters and an infrared emitter that syn- 
chronizes the glasses with the views on the screen. 

Stereoscopic viewing is also a component in virtual-reality systems, 
where users can step into a scene and interact with the environment. A headset 
(Fig. 2-22) containing an optical system to generate the stemxcopic views is 
commonly used in conjuction with interactive input devices to locate and manip 
date objects in the scene. A sensing system in the headset keeps track of the 
viewer's position, so that the front and back of objects can be m as the viewer 

Figure 2-21 
Glasses for viewing a 
stereoscopic scene and an 
infrared synchronizing emitter. 

(Courtesy of SfnroCraphics Copra t ion . )  

~ . - -- 

Figure 2-22 
A headset used in virtual-reality systems. (Coudrsy of Virtual 
RPsePrch.) 



Chapter 2 

Overview d Graphics Systems 

Figure 2-23 
Interacting with a virtual-reality environment. (Carrtq of tk 
N a h l  C m t r r ~ b  Svprmmpvting Applbtioru, Unmrrsity of nlinois at 
UrboMCknrpngn.) 

"walks through" and interacts with the display. Figure 2-23 illustrates interaction 
with a virtual scene, using a headset and a data glove worn on the right hand 
(Section 2-5). 

An interactive virtual-reality environment can also be viewed with stereo- 
scopic glasses and a video monitor, instead of a headset. This provides a means 
for obtaining a lowercost virtual-reality system. As an example, Fig. 2-24 shows 
an ultrasound tracking device with six degrees of freedom. The tracking device is 
placed on top of the video display and is used to monitor head movements so 
that the viewing position for a scene can be changed as head position changes. 

- 

Fipm 2-24 
An ultrasound tracking device used 
with Btereoscopic gbsses to track 
head position. ~~ of 
StrrmG* Corpmrrh.) 



2-2 Sedion 2-2 
RASTER-SCAN SYSTEMS Raster-kan Systems 

Interactive raster graphics systems typically employ several processing units. In 
addition to the central pmessing unit, or CPU, a special-purpose processor, 
called the video controller or display controller, is used to control the operation 
of the display device. Organization of a simple raster system is shown in Fig. 2-25. 
Here, the frame buffer can be anywhere in the system memory, and the video 
controller accesses the frame buffer to refresh the screen. In addition to the video 
controller, more sophisticated raster systems employ other processors as co- 
processors and accelerators to impIement various graphics operations. 

Video Controller 

Figure 2-26 shows a commonly used organization for raster systems. A fixed area 
of the system memory is reserved for the frame buffer, and the video controller is 
given direct access to the frame-buffer memory. 

Frarne-buffer locations, and the corresponding screen positions, are refer- 
enced in Cartesian coordinates. For many graphics monitors, the coordinate ori- 

Figure  2-25 
Architedure of a simple raster graphics system. 

Figure 2-26 
Wtectureof a raster system with a fixed portion of the system 
memory reserved for the frame buffer. 



Chapter 2 

Owrview of Graphics Systems 

Figure 2-27 
The origin of the coordinate 
system for identifying screen 
positions is usually specified 
in the lower-left corner. 

gin is'defined at the lower left screen comer (Fig. 2-27). The screen surface is then 
represented as the first quadrant of a two-dimensional system, with positive x 
values increasing to the right and positive y values increasing from bottom to 
top. (On some personal computers, the coordinate origin is referenced at the 
upper left comer of the screen, so the y values are inverted.) Scan lines are then 
labeled from y, at the top of the screen to 0 at the bottom. Along each scan line, 
screen pixel positions are labeled from 0 to x,,. 

In Fig. 2-28, the basic refresh operations of the video controller are dia- 
grammed. Two registers are used to store the coordinates of the screen pixels. Ini- 
tially, the x register is set to 0 and the y register is set to y,. The value stored in 
the frame buffer for this pixel position is then retrieved and used to set the inten- 
sity of the CRT beam. Then the x register is inrremented by 1, and the process re 
peated for the next pixel on the top scan line. This procedure is repeated for each 
pixel along the scan line. After the last pixel on the top scan line has been 
processed, the x register is reset to 0 and the y register is decremented by 1. Pixels 
along this scan line are then processed in turn, and the procedure is repeated for 
each successive scan line. After cycling through all pixels along the bottom scan 
line (y = O), the video controller resets the registers to the first pixel position on 
the top scan line and the refresh process starts over. 

Since the screen must be refreshed at the rate of 60 frames per second, the 
simple procedure illustrated in Fig. 2-28 cannot be accommodated by typical 
RAM chips. The cycle time is too slow. To speed up pixel processing, video con- 
trollers can retrieve multiple pixel values from the refresh b d e r  on each pass. 
The multiple pixel intensities are then stored in a separate register and used to 
control the CRT beam intensity for a group of adjacent pixels. When that group 
of pixels has been processed, the next block of pixel values is retrieved from the 
frame buffer. 

A number of other operations can be performed by the video controller, be- 
sides the basic refreshing operations. For various applications, the video con- 

Figure 2-28 
Basic video-controller refresh operations. 



- - - - . - - - -- 
Figiirc 2-29 
Architecture of a raster-graphics system with a display processor. 

troller can retrieve pixel intensities from different memory areas on different re- 
fresh cycles. In highquality systems, for example, two hame buffers are often 
provided so that one buffer can be used for refreshing while the other is being 
filled with intensity values. Then the two buffers can switch roles. This provides 
a fast mechanism-for generating real-time animations, since different views of 
moving objects can be successively loaded inta the refresh buffers. Also, some 
transformations can be accomplished by the video controller. Areas of the screen 
can be enlarged, reduced, or moved from one location to another during the re- 
fresh cycles. In addition, the video controller often contains a lookup table, so 
that pi;el values in the frame buffer are used to access the lookup tableinstead of 
controlling the CRT beam intensity directly. This provides a fast method for 
changing screen intensity values, and we discuss lookup tables in more detail in 
Chapter 4. Finally, some systems arr designed to allow the video controller to 
mix the frame-buffer image with an input image from a television camera or 
other input device. 

Raster-Scan Display Processor 

Figure 2-29 shows one way to set up the organization of a raster system contain- 
ing a separate display processor, sometimes referred to as a graphics controller 
or a display coprocessor. The purpose of the display processor is to free the CPU 
from the graphics chores. In addition to the system memory, a separate display- 
processor memory area can also be provided. 

A major task of the display pmcessor is digitizing a picture definition given ' - --I-- 
in an application program into a set of pixel-intensity values for storage in the 
frame buffer. This digitization process is caIled scan conversion. Graphics com- k ' ~ l l w  2-.30 
mands specifying straight lines and other geometric objects are scan converted A character defined as a 

into a set of discrete intensity points. Scan converting a straight-line segment, for rcctangu'ar grid of pixel 
positions. example, means that we have to locate the pixel positions closest to the line path 

and store the intensity for each position in the frame buffer. Similar methods are 
used for scan converting curved lines and polygon outlines. Characters can be 
defined with rectangular grids, as in Fig. 2-30, or they can be defined with curved 5 5 



outlines, as in Fig. 2-31. The array size for character grids can vary from about 5 
by 7 to 9 by 12 or more for higher-quality displays. A character grid is displayed 
by superimposing the rectangular grid pattern into the frame buffer at a specified 
coordinate position. With characters that are defined as curve outlines, character 
shapes are scan converted into the frame buffer. 

Display processors are also designed to perform a number of additional op- 
erations. These functions include generating various line styles (dashed, dotted, 
or solid), displaying color areas, and performing certain transformations and ma- 
nipulations on displayed objects. Also, display pmessors are typically designed 
to interface with interactive input devices, such as a mouse. 

F i p r r  2-3 I In an effort to reduce memory requirements in raster systems, methods 
A character defined as a have been devised for organizing the frame buffer as a linked list and encoding 
curve outline. the intensity information. One way to do this is to store each scan line as a set of 

integer pairs. Orre number of each pair indicates an intensity value, and the sec- 
ond number specifies the number of adjacent pixels on the scan line that are to 
have that intensity. This technique, called run-length encoding, ,can result in a 
considerable saving in storage space if  a picture is to be constructed mostly with 
long runs of a single color each. A similar approach can be taken when pixel in- 
tensities change linearly. Another approach is to encode the raster as a set of rec- 
tangular areas (cell encoding). The aisadvantages of encoding runs are that in- 
tensity changes are difficult to make and storage requirements actually increase 
as the length of the runs decreases. In addition, it is difficult for the display con- 
troller to process the raster when many short runs are involved. 

2-3 
RANDOM-SCAN SYSTEMS 

The organization of a simple random-scan (vector) system is shown in Fig. 2-32. 
An application program is input and stored in the system memory along with a 
graphics package. Graphics commands in the application program are translated 
by the graphics package into a display file stored in the system memory. This dis- 
play file is then accessed by the display processor to refresh the screen. The dis- 
play processor cycles through each command in the display file program once 
during every refresh cycle. Sometimes the display processor in a random-scan 
system is referred to as a display processing unit or a graphics controller. 

Figure 2-32 
Architecture of a simple randomscan system. 



Graphics patterns are drawn on a random-scan system by directing the section 2-4 
electron beam along the component lines of the picture. Lines are defined by the Graphics Monilors 
values for their coordinate endpoints, and these input coordinate values are con- and Worksrations 

verted to x and y deflection voltages. A scene is then drawn one line at a time by 
positioning the beam to fill in the line between specified endpoints. 

2-4 
GRAPHICS MONITORS AND WORKSTATIONS 

Most graphics monitors today operate as rasterscan displays, and here we sur- 
vey a few of the many graphics hardware configurations available. Graphics sys- 
tems range h m  small general-purpose computer systems with graphics capabil-, 
ities (Fig. 2+) to sophisticated fullcolor systems that are designed specifically 
for graphics applications (Fig. 2-34). A typical screen resolution for personal com- 

Figure 2-33 
A desktop general-purpose 
computer system that can be used 
for graphics applications. (Courtesy of 
Apple Compula. lnc.) 

-- - -- -- 

Figure 2-34 
Computer graphics workstations with keyhrd and mouse input devices. (a) The Iris 
Indigo. (Courtesyo\ Silicon Graphics Corpa~f ion . )  (b) SPARCstation 10. (Courtesy 01 Sun Microsyslems.) 

5 7 



C h a m  2 puter systems, such as the Apple Quadra shown in Fig. 2-33, is 640 by 480, al- 
Overview of Graphics Systems though screen resolution and other system capabilities vary depending on the 

size and cost of the system. Diagonal screen dimensions for general-purpose per- 
sonal computer systems can range from 12 to 21 inches, and allowable color se- 
lections range from 16 to over 32,000. For workstations specifically designed for 
graphics applications, such as the systems shown in Fig. 2-34, typical screen reso- 
lution is 1280 by 1024, with a screen diagonal of 16 inches or more. Graphics 
workstations can be configured with from 8 to 24 bits per pixel (full-color sys- 
tems), with higher screen resolutions, faster processors, and other options avail- 
able in high-end systems. 

Figure 2-35 shows a high-definition graphics monitor used in applications 
such as air traffic control, simulation, medical imaging, and CAD. This system 
has a diagonal s c m  size of 27 inches, resolutions ranging from 2048 by 1536 to 
2560 by 2048, with refresh rates of 80 Hz or 60 Hz noninterlaced. 

A m u l t i m  system called the MediaWall, shown in Fig. 2-36, provides a 
large "wall-sized display area. This system is designed for applications that re- 
quirr large area displays in brightly lighted environments, such as at trade 
shows, conventions, retail stores, museums, or passenger terminals. MediaWall 
operates by splitting images into a number of Sections and distributing the sec- 
tions over an array of monitors or projectors using a graphics adapter and satel- 
lite control units. An array of up to 5 by 5 monitors, each with a resolution of 640 
by 480, can be used in the MediaWall to provide an overall resolution of 3200 by 
2400 for either static scenes or animations. Scenes can be displayed behind mul- 
lions, as in Fig. 2-36, or the mullions can be eliminated to display a continuous 
picture with no breaks between the various sections. 

Many graphics workstations, such as some of those shown in Fig. 2-37, are 
configured with two monitors. One monitor can be used to show all features of 
an obpct or scene, while the second monitor displays the detail in some part of 
the picture. Another use for dual-monitor systems is to view a picture on one 
monitor and display graphics options (menus) for manipulating the picture com- 
ponents on the other monitor. 

Figure 2-35 
A very high-resolution (2560 by 
2048) color monitor. (Courtesy of 
BARCO Chromatics.) 



 he Mediawall: A multiscreen display system. The image displayed on 
this 3-by-3 array of monitors was created by Deneba Software. (Courtesy 

Figurr 2-37 
Single- and dual-monitor graphics workstations. (Cdurtq of Intngraph 
Corpratiun.) 

Figures 2-38 and 2-39 illustrate examples of interactive graphics worksta- 
tions containing multiple input and other devices. A typical setup for CAD appli- 
cations is shown in Fig. 2-38. Various keyboards, button boxes, tablets, and mice 
are attached to the video monitors for use in the design process. Figure 2-39 
shows features of some types of artist's workstations. 



- - - - - .. - 

Figure 2-38 
Multiple workstations for a CAD group. (Courtesy of Hdctf-Packard 
Complny.) 

Figure 2-39 
An artist's workstation, featuring a color raster monitor, keyboard, 
graphics tablet with hand cursor, and a light table, in addition to 
data storage and telecommunications devices. (Cburtesy of DICOMED 
C0t)mation.) 

2-5 
INPUT DEVICES 

Various devices are available for data input on graphics workstations. Most sys- 
tems have a keyboard and one or more additional devices specially designed for 
interadive input. These include a mouse, trackball, spaceball, joystick, digitizers, 



dials, and button boxes. Some other input dev~ces usea In particular applications W i o n  2-5 - 
are data gloves, touch panels, image scanners, and voice systems. Input Devices 

Keyboards 

An alphanumeric keyboard on a graphics system is used primarily as a device 
for entering text strings. The keyboard is an efficient device for inputting such 
nongraphic data as picture labels associated with a graphics display. Keyboards 
can also be provided with features to facilitate entry of screen coordinates, menu 
selections, or graphics functions. 

Cursor-control keys and function keys are common features on general- 
purpose keyboards. Function keys allow users to enter frequently used opera- 
tions in a single keystroke, and cursor-control keys can be used to select dis- 
played objects or coordinate positions by positioning the screen cursor. Other 
types of cursor-positioning devices, such as a trackball or joystick, are included 
on some keyboards. Additionally, a numeric keypad is,often included on the key- 
board for fast entry of numaic data. Typical examples of general-purpose key- 
boards are given in Figs. 2-1, 2-33, and 2-34. Fig. 2-40 shows an ergonomic 
keyboard design. 

For specialized applications, input to a graphics application may come from 
a set of buttons, dials, or switches that select data values or customized graphics 
operations. Figure 2-41 gives an example of a button box and a set of input dials. 
Buttons and switches are often used to input predefined functions, and dials are 
common devices for entering scalar values. Real numbers within some defined 
range are selected for input with dial rotations. Potenhometers are used to mea- 
sure dial rotations, which are then converted to deflection voltages for cursor 
movement. 

Mouse 

A mouse is small hand-held box used to position the screen cursor. Wheels or 
rollers on the bottom of the mouse can be used to record the amount and direc- 

Figure 2-40 
Ergonomically designed keyboard 
with removable palm rests. The 
slope of each half of the keyboard 
can be adjusted separately. (Courtesy 
of Apple Computer, Inc.) 



Chapter 2 tion of movement. Another method for detecting mouse motion is with an opti- 
Overview of Graphics Svstrms cal sensor. For these systems, the mouse is moved over a special mouse pad that 

has a grid of horizontal and vertical lines. The optical sensor deteds movement 
acrossthe lines in the grid. 

Since a mouse can be picked up and put down at another position without 
change in curs6r movement, it is used for making relative change.% in the position 
of the screen cursor. One, two, or three bunons m usually included on the top of 
the mouse for signaling the execution of some operation, such as recording &- 
sor position or invoking a function. Mast general-purpose graphics systems now 
include a mouse and a keyboard as the major input devices, as in Figs. 2-1,2-33, 
and 2-34. 

Additional devices can be included in the basic mouse design to increase 
the number of allowable input parameters. The Z mouse in Fig. 242 includes 

- - -  

Figuw 2-41 
A button box (a) and a set of input dials (b). (Courtesy of Vcaor Cownl.) 

Figure 2-42 
The 2 mouse features three bunons, 
a mouse ball underneath, a 
thumbwheel on the side, and a 
trackball on top. (Courtesy of 
Multipoinl Technology Corporat~on.) 



three buttons, a thumbwheel on the side, a trackball on the top, and a standard M o n 2 - 5  
mouse ball underneath. This design provides six degrees of freedom to select Input Devices 
spatial positions, rotations, and other parameters. Wtth the Z mouse, we can pick 
up an object, rotate it, and move it in any direction, or we can navigate our view- 
ing position and orientation through a threedimensional scene. Applications of 
the Z mouse include ~irtual reality, CAD, and animation. 

Trackball and Spaceball 

As the name implies, a trackball is a ball that can be rotated with the fingers or 
palm of the hand, as in Fig. 2-43, to produce screen-cursor movement. Poten- 
tiometers, attached to the ball, measure the amount and direction of rotation. 
Trackballs are often mounted on keyboards (Fig. 2-15) or other devices such as 
the Z mouse (Fig. 2-42). 

While a trackball is a two-dimensional positioning device, a spaceball (Fig. 
2-45) provides six degrees of freedom. Unlike the trackball, a spaceball does not 
actually move. Strain gauges measure the amount of pressure applied to the 
spaceball to provide input for spatial positioning and orientation as the ball is 
pushed or pulled in various diredions. Spaceballs are used for three-dimensional 
positioning and selection operations in virtual-reality systems, modeling, anima- 
tion, CAD, and other applications. 

joysticks 

A joystick consists of a small, vertical lever (called the stick) mounted on a base 
that is used to steer the screen cursor around. Most bysticks select screen posi- 
tions with actual stick movement; others respond to inksure on the stick. F I ~  
2-44 shows a movable joystick. Some joysticks are mounted on a keyboard; oth- 
ers lnction as stand-alone units. 

The distance that the stick is moved in any direction from its center position 
corresponds to screen-cursor movement in that direction. Potentiometers 
mounted at the base of the joystick measure the amount of movement, and 
springs return the stick to the center position when it is released. One or more 
buttons can be programmed to act as input switches to signal certain actions once 
a screen position has been selected. 

- . . 
Figure 2-43 
A three-button track ball. (Courlrsyof Mtnsumne~l Sysfems lnc., N o m l k ,  
Connccticul.) 



Chapter 2 

Overview of Graphics Systems 

Figrrr 2-44 
A moveable pystick. (Gurtesy of CaIComp Group; Snndns 
Assm+tes, Inc.) 

In another type of movable joystick, the stick is used to activate switches 
that cause the screen cursor to move at a constant rate in the direction selected. 
Eight switches, arranged in a circle, are sometimes provided, so that the stick can 
select any one of eight directions for cursor movement. Pressuresensitive joy- 
sticks, also called isometric joysticks, have a nonmovable stick. Pressure on the 
stick is measured with strain gauges and converted to movement of the cursor in 
the direction specified. 

Data Glove 

Figure 2-45 shows a data glove that can be used to grasp a "virtual" object. The 
glove is constructed with a series of sensors that detect hand and finger motions. 
Electromagnetic coupling between transmitting antennas and receiving antennas 
is used to provide information about the position and orientation of the hand. 
The transmitting and receiving antennas can each be structured as a set of three 
mutually perpendicular coils, forming a three-dimensional Cartesian coordinate 
system. Input from the glove can be used to position or manipulate objects in a 
virtual scene. A two-dimensional propdion of the scene can be viewed on a 
video monitor, or a three-dimensional projection can be viewed with a headset. 

Digitizers 

A common device for drawing, painting, or interactively selecting coordinate po- 
sitions on an object is a digitizer. These devices can be used to input coordinate 
values in either a two-dimensional or a three-dimensional space. Typically, a dig- 
itizer is used to scan over a drawing or object and to input a set of discrete coor- 
dinate positions, which can be joined with straight-Iine segments to approximate 
the curve or surface shapes. 

One type of digitizer is the graphics tablet (also referred to as a data tablet), 
which is used to input two-dimensional coordinates by activating a hand cursor 
or stylus at selected positions on a flat surface. A hand cursor contains cross hairs 
for sighting positions, while a stylus is a pencil-shaped device that is pointed at 



Section 2-5 

Input Dwices 

. . . - - - - - - . .. 
Figure 2-45 
A virtual-reality xene, displayed 
on a two-dimensional video 
monitor, with input from a data 
glove a d  a spa;eball. (Courfesy o f n e  
Compufrr Graphics Cmfer, Dnrmsfadf, 

positions on the tablet. Figures 2-46 and 2-47 show examples .of desktop and 
floor-model tablets, using hsnd CUTSOTS that are available wiih 2,4, or 16 buttons. 
Examples of stylus input with a tablet am shown in Figs. 2-48 and 2-49. The 
artist's digitizing system in Fig. 2 4 9  uses electromagnetic resonance to detect the 
three-dimensional position of the stylus. This allows an artist to produce different 
brush strokes with different pressures on the tablet surface. Tablet size varies 
from 12 by 12 inches for desktop models to 44 by 60 inches or larger for floor 
models. Graphics tablets provide a highly accurate method for selecting coordi- 
nate positions, with an accuracy that varies from about 0.2 mm on desktop mod- 
els to about 0.05 mm or less on larger models. 

Many graphics tablets are constructed with a rectangular grid of wires em- 
bedded in the tablet surface. Electromagnetic pulses are aenerated in sequence 

Figure 2-46 
The Summasketch 111 desktop tablet with a 16-button 
hand cursor. (Courtesy of Surnmgraphin Corporalion.) 



Ckptw 2 

Overview of Graphics Swerns 

Figure 2-47 
The Microgrid 111 tablet with a 1 6  
button hand cursor, designed for 
digitizing larger drawings. ( C o u r t 9  
@Summngraphics Corporation.) 

along the wires, and an electric signal is induced in a wire coil in an activated sty- 
4 lus or hand cursor to record a tablet position. Depending on the technology, ei- 

ther signal strength, coded pulses, or phase shifts can be used to determine the 
_ - position on the tablet. 

Acoustic (or sonic) tablets use sound waves to detect a stylus position. Ei- - ther strip rnicmphones or point rnicmphones can be used to detect the wund 
emitted by an electrical spark from a stylus tip. The position of the stylus is calcu- 

Figure 2-48 
The NotePad desktop tablet 
with stylus. ( C o u r t q  of 
CaIComp Digitizer Division, 
a p r t  of CaIComp, Inc.) 

Figrrrc 2-49 
An artist's digitizer system, with a 
pressure-sensitive, cordless stylus. 
(Courtesy of Wacom Technology 
Corporalion.) 



lated by timing the arrival of the generated sound at the different microphone 2-5 
positions. An advantage of two-dimensional accoustic tablets is that the micro- Input Devices 
phones can be placed on any surface to form the "tablet" work area. This can be 
convenient for various applications, such as digitizing drawings in a book. 

Three-dimensional digitizers use sonic or electromagnetic transmissions to 
word  positions. One electiomagnetic transmission method is similar to that 
used in the data glove: A coupling between the transmitter and receiver is used 
to compute the location of a stylus as it moves over the surface of an obpct. Fig- 
ure 2-50 shows a three-dimensional digitizer designed for Apple Macintosh com- 
puters. As the points are selected on a nonmetallic object, a wireframe outline of 
the surface is displayed on the computer saeen. Once the surface outline is con- 
structed, it can be shaded with lighting effects to produce a realistic display of 
the object. Resolution of this system is h m  0.8 mm to 0.08 mm, depending on 
the model. 

Image Scanners 

Drawings, graphs, color and black-and-whte photos, or text can be stored for 
computer processing with an image scanner by passing an optical scanning 
mechanism over the information to be stored. The gradations of gray scale or 
color are then recorded and stored in an array. Once we have the internal repre- 
sentation of a picture, we can apply transformations to rotate, scale, or crop the 
picture to a particular screen area. We can also apply various image-processing 
methods to modify the array representation of the picture. For scanned text 
~nput, various editing operations can be performed on the stored documents. 
Some scanners are able to scan either graphical representations or text, and they 
come in a variety of sizes and capabilities. A small hand-model scanner is shown 
in Fig. 2-51, while Figs 2-52 and 2-53 show larger models. 

- 

Fi,yurr 2-56 
A three-dimensional digitizing 
system for use with Apple 
Macintosh computers. (Courtesy of 

' M m  lmnphg.) 



Overview of Graphics Systems 

- -- 
Figure 2-51 
A hand-held scanner that can be used to input 
either text or graphics images. (Courtesy of 
T h u h r e ,  lnc.) 

Figure 2-52 
Desktop full-color scanners: (a) Flatbed scanner with a resolution of 600 dots per inch. 
(Courtesy of Sharp Elcclmnics Carpomtion.) (b) Drum scanner with a selectable resolution from 50 
to 4000 dots per inch. (Courtrsy cjHautek, Inc.) 

Touch Panels 

As the name implies, touch panels allow displayed objects or screen positions to 
be selected with the touch of a finger. A typical application of touch panels is for 
the selection of processing options that are repmented with graphical icons. 
Some systems, such as the plasma panels shown in Fig. 2-54, are designed with 
touch screens. Other systems can be adapted for touch input by fitting a transpar- 
ent device with a touchsensing mechanism over the video monitor screen. Touch 
input can be recorded using optical, electrical, or acoustical methods. 

Optical touch panels employ a line of infrared light-emitting diodes (LEDs) 
along one vertical edge and along one horizontal edge of the frame. The opposite 
vertical and horizontal edges contain light detectors. These detectors are used to 
record which beams are intenupted when the panel is touched. The two crossing 



)ccUon 2-5 

Input Devices 

- -- p~ 

Figum 2-53 
A liuge floor-model scanner used to 
scan architeaural and e@aerhg 
drawings up to 40 inches wide and 
100 feet long. (Courtesy of 
Summagraphin Corpomfion.) 

beams that are interrupted idenhfy the horizontal and vertical coordinates of the 
screen position selected. Positions tin be selected with an accuracy of about 1/4 
inch With closely spaced LEDs, it is possible to b d  two horizontal or two ver- 
tical beams simultaneously. In this case, an average position between the two in- 
terrupted beams is recorded. The LEDs operate at infrared frequenaes, so that 
the light is not visible to a user. Figure 2-55 illustrates the arrangement of LEDs in 
an optical touch panel that is designed to match the color and contours of the 
system to which it is to be fitted. 

An electrical touch panel is constructed with two transparent plates sepa- 
rated by a small distance. One of the plates is coated with a mnducting material, 
and the other plate is coated with a resistive material. When the outer plate is- 
touched, it is f o d  into contact with the inner plate. This contact creaks a volt- 
age drop aaoss the msistive plate that is converted to the coordinate values of 
the selected screen position. 

In acoustical touch panels, high-frequency sound waves are generated in 
the horizontal and vertical directions aaoss a glass plate. Touclung the saeen 
causes part of each wave to be reflected from the finger to the emitters. The saeen 
position at the point of contact is calculated from a measurement of the time in- 
terval between the transmission of each wave and its reflection to the emitter. 

Figum 2-54 
Plasma panels with touch screens. (Courtesy of Phofonies Systm.) 



'=w 2 
Ovecview of Graphics Syhms 

Fiprr 2-55 
An optical touch panel, showing 
the aRangement of infrared LED 
uni6 and detectors mund the 
edgea of the frame. (Courfesy of Ckmdl 
T d ,  Inc.) 

Light Pens 

Figure 2-56 shows th; design of one type of light pen. Such pencil-shaped de- 
vices are used to selezt screen positions by detechng the light coming from points 
on the CRT saeen. They are sensitive to the short burst of light emitted from the 
phosphor coating at the instant the electron beam strikes a particular point. Other 
Light sources, such as the background light in the room, are usually not detected 
by a light pen. An activated light pen, pointed at a spot on the screen as the elec- 
tron beam hghts up that spot, generates an electrical pulse that causes the coordi- 
nate position of the electron beam to be recorded. As with cursor-positioning de- 
vices, recorded Light-pen coordinates can be used to position an object or to select 
a processing option. 

Although Light pens are still with us, they are not as popular as they once 
were since they have several disadvantages compamd to other input devices that 
have been developed. For one, when a light pen is pointed at the screen, part of 
the m n  image is obscumd by the hand and pen. And prolonged use of the 
hght pen can cause arm fatigue. Also, light pens require special implementations 
for some applications because they cannot detect positions within bla* areas. To 
be able b select positions in any screen area with a light pen, we must have some 
nonzero intensity assigned to each screen pixel. In addition, light pens. sometimes 
give false readings due to background lkghting in a room. 

Voice Systems 

Speech recognizers are used in some graphics workstations as input devices to 
accept voice commands The voice-system input can be used to initiate graphics 



Stdh 2-5 
Input Dev~ca 

Figurn 2-56 
A light pen activated with a button switch. (Courtesy oflntmwtiue Gmputn 
Products.) 

operations or to enter data. These systems operate by matching an input a g h t  
a predefined dictionary of words and phrase$. 

A dictionary is set up for a particular operator by having, the operator speak 
the command words to be used into the system. Each word is spoke? several 
times, and the system analyzes the word and establishes a frequency pattern for 
that word in the dictionary along with the corresponding function to be per- 
formed. Later, when a voice command is given, the system searches the dictio- 
nary for a frequency-pattern match. Voice input is typically spoken into a micro- 
phone mounted on a headset, as in Fig. 2-57. The mtcrophone is designed to 
minimize input of other background sounds. If a different operator is to use the 
system, the dictionary must be reestablished with that operator's voice patterns. 
Voice systems have some advantage over other input devices, since the attention 
of the operator does not have to be switched from one device to another to enter 
a command. 

- ~ -. 
Figure 2-57 
A speech-recognition system. (Coutiesy of ThmhoU Tahnology, Inc.) 



Chapter 2 
Overview of Graphics 5- 

2-6 
HARD-COPY DEVICES 

We can obtain hard-copy output for o w  images in several formats. For presenta- 
tions or archiving, we can send image files to devices or service bureaus that will 
produce 35-mm slides or overhead transparencies. To put images on film, we can 
simply photograph a scene displayed on a video monitor. And we can put our 
pictures on paper by directing graphics output to a printer or plotter. 

The quality of the piaures obtained from a device depends on dot size and 
the number of dots per inch, or Lines per inch, that can be displayed. To produce 
smooth characters in printed text shings, higher-quality printers shift dot posi- 
tions so that adjacent dots overlap. 

Printers produce output by either impact or nonimpact methods. Impact 
printers press formed character faces against an inked ribbon onto the paper. A 
line printer is an example of an impact device, with the typefaces mounted on 
bands, chains, drums, or wheels. Nonimpact printers and plotters use laser tech- 
niques, ink-jet sprays, xerographic pmesses (as used in photocopying ma- 
chines), eledrostatic methods, and electrothermal methods to get images onto 
Paper. 

Character impact printers often have a dot-matrix print head containing a 
rectangular array of protruding wire pins, with the number of pins depending on 
the quality of the printer. Individual characters or graphics patterns are obtained 
by wtracting certain pins so that the remaining pins form the pattern to be 
printed. Figure 2-58 shows a picture printed on a dot-matrix printer. 

In a laser device, a laser beam mates a charge distribution on a rotating 
drum coated with a photoelectric material, such as selenium. Toner is applied to 
the d m  and then transferred to paper. Figure 2-59 shows examples of desktop 
laser printers with a resolution of 360 dots per inch. 

Ink-jet methods produce output by squirting ink in horizontal rows across a 
roll of paper wrapped on a drum. The electrically charged ink stream is deflected 
by an electric field to produce dot-matrix patterns. A desktop ink-jet plotter with 

Figure 2-58 
A pictwe generated on a dot-mahix printer showing how the 
density of the dot patterns can be varied to produce light and 
dark areas. (Courtesy of Apple Computer, Inc.) 



Stclii 2-6 
Hard-Copy Devices 

Figure 2-59 
Small-footprint laser printers. 
(Courtesy of Texas 111~lmmmts.) 

a resolution of 360 dok per inch is shown in Fig. 2-60, and examples of larger 
high-resolution ink-jet printer/plotters are shown in Fig. 2-61. 

An electrostatic device places a negative charge on the paper, one complete 
row at a time along the length of the paper. Then the paper is exposed to a toner. 
The toner is positively charged and so is attracted to the negatively charged 
areas, where it adheres to produce the specified output. A color electrostatic 
printer/plotter is shown in Fig. 2-62. Electrothennal methods use heat in a dot- 
matrix print head to output patterns on heatsensitive paper. 

We can get limited color output on an impact printer by using different- 
colored ribbons. Nonimpact devices use various techniques to combine three 
color pigments (cyan, magenta, and yellow) to produce a range of color patterns. 
Laser and xerographic devices deposit the three pigments on separate passes; 
ink-jet methods shoot the three colors simultaneously on a single pass along each 
print tine on the paper. 

Figure 2-60 
A mot-per-inch desktop ink-jet 

, plotter. (Courtcsyof Summgmphirs 
Corpmlion.) 



-- - -. 
Figurn 2-61 
Floor-model, ink-jet color printers that use variable dot size to achieve 
an equivalent resolution of 1500 to 1800 dots per inch. (Courtesy of IRIS 
Cmphio Inc., B c d w ,  Ma%nchuscih.) 

F i g u n  2-62 
An e ~ ~ t a t i c  printer that can 
display 100 dots per inch. (Courtesy of 
CaIComp Digitim Dioisia, a prf of 
CPICmnp, Inc.) 

Drafting layouts and other drawings are typically generated with ink-jet or 
pen plotters. A pen plotter has one or more pens mounted on a camage, or cross- 
bar, that spans a sheet of paper. Pens with varying colors and widths are used to 
produce a variety of shadings and line styles. Wet-ink, ball-point, and felt-tip 
pens are all posible choices for use with a pen plotter. Plotter paper can lie flat or 
be rolled onto a drum or belt. Crossbars can be either moveable or stationary, 
while the pen moves back and forth along the bar. Either clamps, a vacuum, or 
an eledmstatic charge hold the paper in position. An example of a table-top 
flatbed pen plotter is given in F i p  2-63, and a larger, rollfeed pen plotter is 
shown in Fig. 2-64. 



Section 2-7 

Graphics Sdnvare 

Figure 2 6 3  
A desktop pen plotter with a 
resolution of 0.025 mm. (Courlcsy o f  
Summagraphifs Cmponriim~.) 

Figure 2-64 
A large, rollfeed pen plotter with 
automatic mdticolor &pen changer 
and a resolution of 0.0127 mm. 
(Courtesy of Summgraphin Carpomtion.) 

2-7 
GRAPHICS SOFTWARE 

There are two general classifications for graphics software: general programming 
packages and special-purpose applications packages. A general graphics pro- 
gramming package provides an extensive set of graphics functions that can be 



Charm 2 used in a high-level programming language, such as C or FORTRAN. An exam- 
Overview of Graphics Systems ple of a general graphics programming package is the GL (Graphics Library) sys- 

tem on Silicon Graphics equipment. Basic functions in a general package include 
those for generating picture components (straight lines, polygons, circles, and 
other figures), setting color and intensity values, selecting views, and applying 
ensformations. By conhast, application graphics packages are designed for 
nonprogrammers, so that users can generate displays without worrying about 
how graphics operations work. The interface to the graphics routines in such 
packages allows users to communicate with the programs in their own terms. Ex- 
amples of such applications packages are the artist's painting programs and vari- 
ous business, medical, and CAD systems. 

Coordinate Representations 

With few exceptions, general graphics packages are designed to be used with 
Cartesian coordinate specifications. If coordinate values for a picture are speci- 
fied in some other reference frame (spherical, hyberbolic, etc.), they must be con- 
verted to Cartesian coordinates before they can be input to the graphics package. 
Special-purpose packages may allow use of other coordinate frames that are ap- 
propriate to the application. In general; several different Cartesian reference 
frames are used to construct and display a scene. We can construct the shape of 
individual objects, such as trees or furniture, in a scene within separate coordi- 
nate reference frames called modeling coordinates, or sometimes local coordi- 
nates or master coordinates. Once individual object shapes have been specified, 
we can place the o b F s  into appropriate positions within the scene using a refer- 
ence frame called world coordinates. Finally, the world-coordinate description of 
the scene is t r a n s f e d  to one or more output-device reference frames for dis- 
play. These display coordinate systems are referred to as device coordinates. or 
screen coordinates in the case of a video monitor. Modeling and world- 
coordinate definitions allow us to set any convenient floating-point or integer di- 
mensions without being hampered by the constraints of a particular output de- 
vice. For some scenes, we might want to spedy object dimensions in fractions of 
a foot, while for other applications we might want to use millimeters, kilometers, 
or light-years. 

Generally, a graphics system first converts world-coordinate positions to 
normalized device coordinates, in the range from 0 to 1, before final conversion 
to specific device coordinates. This makes the system independent of the various 
devices that might be used at a particular workstation. Figure 2-65 illustrates the 
sequence of coordinate transformations from modeling coordinates to device co- 
ordinates for a two-dimensional application. An initial modeling-coordinate p 
sition (x , ,  y,) in this illustration is transferred to a device coordinate position 
( x ~ ,  ydc) with the sequence: 

The modeling and world-coordinate pit ions ' in this transformation can be any 
floating-point values; normalized coordinates satisfy the inequalities: 0 5 x,,, 1 ,  
0 5 y, 5 1; and the device coordinates xdc and ydc are integers within the range 
(0,O) to (I-, y,) for a particular output device. To accommodate differences in 
scales and aspect ratios, normalized coordinates are mapped into a square area of 
the output device so that proper proportions are maintained. 



Graphics Functions ~ection 2-7 
Graphics Software 

A general-purpose graphics package provides users with a variety of functions 
for creating and manipulating pictures. These routines can be categorized accord- 
ing to whether they deal with output, input, attributes, transformations, viewing, 
or general control. 

The basic building blocks for pidures am referred to as output primitives. 
They include character strings and geometric entities, such as points, straight 
lines, curved Lines, filled areas (polygons, circles, etc.), and shapes defined with 
arrays of color points. Routines for generating output primitives provide the 
basic tools for conshucting pictures. 

Attributes are the properties of the output primitives; that is, an attribute 
describes how a particular primitive is to be displayed. They include intensity 
and color specifications, line styles, text styles, and area-filling patterns. Func- 
tions within this category can be used to set attributes for an individual primitive 
class or for groups of output primitives. 

We can change the size, position, or orientation of an object within a scene 
using geometric transformations. Similar modeling transformations are used to 
construct a scene using object descriptions given in modeling coordinates. 

Given the primitive and attribute definition of a picture in world coordi- 
nates, a graphics package projects a selected view of the picture on an output de- 
vice. Viewing transformations are used to specify the view that is to be pre- 
sented and the portion of the output display area that is to be used. 

Pictures can be subdivided into component parts, called structures or seg- 
ments or objects, depending on the software package in use. Each structure de- 
fines one logical unit of the picture. A scene with several objects could reference 
each individual object in a-separate named structure. ~out ines  fol processing 

- - - 

Figure 2-65 
The transformation sequence from modeling coordinates to device coordinates for a two- 
dimensional scene. Object shapes a= defined in local modeling-coordinate systems, then 
positioned within the overall world-cmrdinate scene. World-coordinate specifications are 
then transformed into normalized coordinates. At the final step, individual device drivers 
transfer the normalizedcoordinate representation of the scene to the output devices for 
display 



Chapr'r 2 structures carry out cqx.r,lt~onh 5ui-11 as the creation. modification, and transfor- 
Overv~ew oiGraphirs Systems mation ot structures. 

Interactive graphics ,ipplications use various kinds of input devices, such as  
a mouse, a tablet, or a pystick. Input functions are used tu control and process 
the data flow from thew interactive devices. 

Finally, a graphic5 package contains a number of housekeeping tasks, such 
as clearing a display 5,-reen and initializing parameters, We can lump the hnc-  
tions for carrying out t h t w  chores under the heading control operations. 

Sottware Standcird5 

The primary goal of st'indardized graphics software is portability. When pack- 
ages are designed with 4andard graphics hnctions, software can he moved eas- 
ily from one hardware system to another and used in different implementations 
and applications. Withtut standards, programs designcti for one hardware sys- 
tem often cannot be transferred to another system without extensive rewriting of 
the programs. 

International and national standards planning organizations in many coun- 
tries have cooperated i l l  an cffort to develop a generally accepted standard for 
con~puter graphlcs. Aftu considerable effort, this work on standards led to the 
development of the Graphical Kernel System (GKS). This system was adopted 
as the first graphics soitware standard by the Internatio~lal Standards Organiza- 
tion (150) and by variou,; national standards organizations, including the kmeri- 
can National Standards Institute (ANSI). Although GKS was originally designed 
as a two-dimensional gr,\phics package, a three-dimensional GKS extension was 
subsequently developed. The second software standard to be developed and a p  
proved by the standards orgainzations was PHIGS (Programmer's Hierarchical 
Interactive Graphics standard), which is an extension ~ G K S .  Increased capabil- 
ities for object rnodel~ng. color specifications, surface rendering, and picture ma- 
nipulations are provided In I'HIGS. Subsequently, an extension of PHIGS, called 
PHIGS+, was developed to provide three-dimensional surface-shading capahili- 
ties not available in PHI( ,S. 

Standard graphics lunctions are defined as a set of ipecifications that is In- 
dependent of anv progr::mming language. A language binding is then defined 
for a particular high-le\zcl programming language. This brnding gives the syntax 
tor scccssing the various shndarJ graphics functions from this language. For ex- 
ample, the general forni of the PHIGS (and GKS) function for specifying a se- 
quence of rr - 1 connected two-dimensional straight Iine segments is 

In FORTRAN, this procrzure is implemented as a subroutine with the name GPL. 
A graphics programmer, using FOKTRAN, would rnvoke this procedure with 
the subroutine call statcwwnt CRLL GPL ( N ,  X ,  Y), where X and Y are one- 
dimensional arrays 01 ;o*mlinate values for the line endpoints. In C, the proce- 
dure would be invoked with p p c l y l i n e ( n ,  p t s ) ,  where pts is the list of co- 
ordinate endpoint positicns. Each language hinding is defined to make bcst use 
of the corresponding language capabilities and to handle various syntax issues, 
such as data types, parameter passing, and errors. 

In the following chapters, we use the standard functions defined in PHIGS 
as a framework for discussing basic graphics concepts and the design and appli- 
cation of graphics packages. Example programs are presented in Pascal to illus- 



hate the algorithms for implementation of the graphics functions and to illustrate 
also some applications of the functions. Descriptive names for functions, based summan 
on the PHlGS definitions, are used whenever a graphics function is referenced in 
a program. 

Although PHIGS presents a specification for basic graphics functions, it 
does not provide a standard methodology for a graphics interface to output d e  
vices. Nor does it specify methods for storing and transmitting pictures. Separate 
standards have been developed for these areas. Standardization for device inter- 
face methods is given in the Computer Graphics Interface (CGI) system. And 
the Computer Graphics Metafile (CGM) system specifies standards for archiv- 
ing and transporting pictures. 

PHlGS Workstations 

Generally, the t e rn  workstation refers to a computer system with a combination of 
input and output devices that is designed for a single user. In PHIGS an'd GKS, 
however, the term workstation is used to identify various combinations of 
graphics hardware and software. A PHIGS workstation can be a single output 
device, a single input device, a combination of input and output devices, a file, or 
even a window displayed on a video monitor. 

To define and use various "workstations" within an applications program, 
we need to specify a workstation identifier and the workstation type. The following 
statements give the general structure of a PHlGS program: 

openphigs (errorFile, memorysize) 
openworkstation (ws, connection. :ype) 

{ create and display picture) 
closeworkstation (ws) 
closephigs 

where parameter errorFile is to contain any error messages that are gener- 
ated, and parameter memorysize specifies the size of an internal storage area. 
The workstation identifier (an integer) is given in parameter ws, and parameter 
connection states the access mechanism for the workstation. Parameter type 
specifies the particular category for the workstation, such as an input device, an 
output device, a combination outin device, or an input or output metafile. 

Any number of workstations can be open in n particular application, with 
input coming from the various open input devices and output directed to all the 
open output devices. We discuss input and output methods in applications pro- 
grams in Chapter 6, after we have explored the basic procedures for creating and 
manipulating pictures. 

SUMMARY 

In this chapter, we have surveyed the major hardware and software features of 
computer graphics systems. Hardware components include video monitors, 
hard-copy devices, keyboards, and other devices for graphics input or output. 
Graphics software includes special applications packages and general program- 
ming packages. 

The predominant graphics display device is the raster refresh monitor, 
based on televis~on technology. A raster system uses a frame buffer to store inten- 
s ~ t y  information for each screen position (pixel). Pictures are then painted on the 



C h a w r  2 screen by retrieving this information from the frame buffer as the electron beam 
Overview of Craph~cs Systems in the CRT sweeps across each scan line, from top to bottom. Older vector dis- 

plays construct pictures by drawing lines between specified line endpoints. Pic- 
ture information is then stored as a set of line-drawing instructions. 

Many other video display devices are available. In particular, flat-panel dis- 
plav technology is developing at a rapid rate, and these devices may largely re- 
place raster displays in the near future. At present, flat-panel displays are com- 
monly used in small systems and in special-purpose systems. Flat-panel displays 
include plasma panels and liquid-crystal devices. Although vector monitors can 
be used to display high-quality line drawings, improvements in raster display 
technology have caused vector monitors to be largely replaced with raster sys- 
tems. 

Other display technologies include three-dimensional and stereoscopic 
viewing systems. Virtual-reality systems can include either a stereoscopic head- 
set or a'standard video monitor. 

For graphical input, we have a range of devices to choose from. Keyboards, 
button boxes, and dials are used to input text, data values, or programming o p  
tions. The most popular "pointing" device is the mouse, but trackballs, space- 
balls, joysticks, cursor-control keys, and thumbwheels are also used to position 
the screen cursor. In virtual-reality environments, data gloves are commonly 
used. Other input dev~ces include image scanners, digitizers, touch panels, light 
pens, and voice systems. 

Hard-copy devices for graphics workstations include standard printers and 
plotters, in addition to devices for producing slides, transparencies, and film out- 
put. Printing methods include dot matrix. laser, ink jet, electrostatic, and elec- 
trothermal. Plotter methods include pen plotting and combination printer-plotter 
devices. 

Graphics software can be roughly classified as applications packages or 
programming packages Applications graphics software include CAD packages, 
drawing and painting programs, graphing packages, and visualization pro- 
grams. Common graphics programming packages include PHIGS, PHIGS+, GKS, 
3D GKS, and GL. Software standards, such as PHIGS, GKS, CGI, and CGM, are 
evolving and are becoming widely available on a variety of machines. 

~ o r m a l l ~ ,  graphics backages require coordinate specifications to be given 
with respect to Cartesian reference frames. Each object for a scene can be defined 
in a separate modeling Cartesian coordinate system, which is then mapped to 
world coordinates to construct the scene. From world coordinates, objects are 
transferred to normalized device coordinates, then to the final display device co- 
ordinates. The transformations from modeling coordinates to normalized device 
coordinates are independent of particular devices that might be used in an appli- 
cation. Device drivers arc8 then used to convert normalized coordinates to integer 
device coordmates. 

Functions in graphics programming packages can be divided into the fol- 
lowing categories: output primitives, attributes, geometric and modeling trans- 
formations, viewing transformations, structure operations, input functions, and 
control operations. 

Some graphics systems, such as  PHIGS and GKS, use the concept of a 
"workstation" to specify devices or software that are to be used for input or out- 
put in a particular application. A workstation identifier in these systems can refer 
to a file; a single device, such as a raster monitor; or a combination of devices, 
such as a monitor, keyboard, and a mouse. Multiple workstations can be open to 
provide input or to receive output in a graphics application. 



REFERENCES 
Exercises 

A general treatment of electronic displays, mcluding flat-panel devices, is available in Sherr 
(1993). Flat-panel devices are discussed in Depp and Howard (1993). Tannas (1985) pro- 
vides d reference for both flat-panel displays and CRTs. Additional information on raster- 
graphics architecture can be found in Foley, et al. (1990). Three-dimensional terminals are 
discussed in  Fuchs et al. (1982), johnson (1982), and lkedo (1984). Head-mounted dis- 
plays and virtual-reality environments arediscussed in Chung el al. (1989). 

For information on PHlGS and PHIGSt, see Hopgood and Duce (19911, Howard et al. 
(1991), Gaskins (1992), and Blake (1993). Information on the two-dimensional GKS stan- 
dard and on the evolution of graphics standards is available in Hopgood et dl. (1983). An 
additional reference for GKS i s  Enderle, Kansy, and Pfaff ( 1  984). 

EXERCISES 

2-1. List the operating characteristics for [he following (lisplay technologies: raster refresh 
systems, vector refresh systems, plasma panels, and .CDs. 

2-2. List some applications appropriate for each of thedi,play technologies in Exercke 2-1. 

2-3 .  Determine the resolution (pixels per centimeter) in the x and y directions for the video 
monitor in use on your system. Determine the aspect ratio, and explain how relative 
proportions of objects can be maintained on your jvstem. 

2-4. Consider three different raster systems with resolutiuns of 640 by 400, 1280 by 1024, 
and 2560 by 2048. What size frame buffer (in bvtejl is needed for each of these sys- 
tems to store 12 bits per pixel? Hov, much storap: is required for each system if 24 
bits per pixel are to be stored? 

2-5. Suppose an RGB raster system i s  to be designed using an 8-incl? by 10-inch screen 
with a resolution of 100 pixels per inch in each d~rection. If we want to store h bit5 
per pixel in the frame buffer, how much storage ( ~ r i  bytes) do we need for the franie 
buffer? 

2 6. How long would it take to load a 640 by 4U0 frame buffer w ~ t h  1 2  bits pel pixel, i i  
lo5 bits can be transferred per second! How long *odd it take to load a 24-bit per 
pixel frame buffer with a resolution of 1280 by 102-1 using this hame transfer rate? 

2-7. Suppose we have a computer with 32 bits per word and a transfer rate of 1 mip ( o w  
million instructions per second). How long would I take to iill the frame buffer o i  a 
300-dpi (dot per inch) laser printer with a page sire oi 8 112 Inches by 11 inches? 

2-8.  Consider two raster systems with resolutions of 640 by 480 and 1280 by 1024. How 
many pixels could be accessed per second in each of these systems by a display ton. 
troller that ref:eshes the screen at a rate o i  60 fr2nies per second? What is the acces 
time per pixel in nach system? 

2-9. Suppose we have a video monitor with a display area that measures 12 inches across 
and 9.6 inches high. If the resolution is 1280 by 1024 and the aspect ratio is I ,  what is 
the diameter of each screen point? 

2-10. How much time is spent scanning across each row of pixels durmp, screen refresh on a 
raster system with a resolution of 1280 by 1024 ~ r ~ d  a refresh rate of 60 trames per 
second? 

2-1 1 .  Consider a noninterlaced raster monitor with a resolution of n by nt ( m  scan l~nes and 
n p~xels per scan line), a refresh rate of r frames p:r second, a horizontal rerrace time 
of tk,,,, and a vertical retrace time oft,,,. What is  the fraction of the total refresh tinw 
per frame spent in retrace of the electron beam? 

2-1 2 .  What is the fraction of the total refresh trme per Ir~rne spent in retrace of the electron 
beam for ;I noninterlaced raster system with a cesolution of 1280 by 1024, a refresh 
rate of 60 Hz, a horizontal retrace time of 5 microwconds, and a vertical retrace time 
of 500 microseconds? 



Chapter 2 2-13. Assuming that a cer1.1in full-color (24-bit per pixel) RGB raster system has a 512-by- 
Overview of Graphics Systems 51 2 frame buffer, how many d~stinrt color choices (~ntensity levels) would we have 

available? HOW many differen~ colors could we displav at any one time? 

2-14. Compare the advantages and disadvantages of a three-dimensional monitor using a 
varifocal mirror with a stereoscopic system. 

2-15. List the different Input and output components that are ;ypically used with virtual- 
reality systems. Also explain how users interact with a virtual scene displayed with diC 
ferent output devices, such as two-dimensional and stereoscopic monitors. 

2-1 6. Explain how viflual-reality systems can be used in des~gn applications. What are some 
other applications for virtual-reality systems? 

2-1 7. List some applications for large-screen displays. 

2-1 8. Explain the differences between a general graphics system designed for a programmer 
and one designed for ,I speciflc application, such as architectural design? 





A picture can be described in several ways. Assuming we have a raster dis- play, a picture is completely specified by the set of intensities for the pixel 
positions in the display. At the other extreme, we can describe a picture as a set of 
complex objects, such as trees and terrain or furniture and walls, positioned at 
specified coordinate locations within the scene. Shapes and colors of the objects 
can be described internally with pixel arrays or with sets of basic geometric struc- 
tures, such as straight line segments and polygon color areas. The scene is then 
displayed either by loading the pixel arrays into the frame buffer or by scan con- 
verting the basic geometric-structure specifications into pixel patterns. Typically, 
graphics programming packages provide functions to describe a scene in terms 
of these basic geometric structures, referred to as output primitives, and to 
group sets of output primitives into more complex structures. Each output primi- 
tive is specified with input coordinate data and other information about the way 
thal object is to be displayed. Points and straight line segments are the simplest 
geometric components of pictures. Additional output primitives that can be used 
to construct a picture include circles and other conic sections, quadric surfaces, 
spline curves and surfaces, polygon color areas, and character strings. We begin 
our discussion of picture-generation procedures by examining device-level algo- 
rithms for d~splaying two-dimensional output primitives, with particular empha- 
sis on scan-conversion methods for raster graphics systems. In this chapter, we 
also consider how oulput functions can be provided in graphics packages, and 
we take a look at the output functions available in the PHlGS language. 

3-1 
POINTS AND LINES 

Point plotting is accomplished by converting a single coordinate position fur- 
nished by an application program into appropriate operations for [he output de- 
vice in use. With a CRT monitor, for example, the electron beam is turned on to il- 
luminate the screen phosphor at the selected location. How the electron beam is 
positioned depends on the display technology. A random-scan (vector) system 
stores point-plotting instructions in the display list, and coordinate values in 
these instructions are converted to deflection voltages that position the electron 
beam at the screen locations to be plotted during each refresh cycle. For a black- 
and-white raster system, on the other hand, a point is plotted by setting the bit 
value corresponding to A specified screen position within the frame buffer to 1. 
Then, as the electron beam sweeps across each horizontal scan line, it emits a 



burst of electrons (plots a point) whenever a value of I is encounted in the sMian3-1 
frame buffer. With an RGB system, the frame buffer is loaded with the color Pointsand hnes 
codes for the intensities that are to be displayed at the s m n  pixel positions. 

Line drawing is accomplished by calculating intermediate positions along 
the line path between two specified endpoint positions. An output device is then 
directed to fill in these positions between the endpoints. For analog devices, such 
as a vector pen plotter or a random-scan display, a straight line can be drawn 
smoothly from one endpoint to the other. Linearly varying horizontal and verti- 
cal deflection voltages are generated that are proportional to the required 
changes in the x and y directions to produce the smooth line. 

Digital devices display a straight line segment by plotting discrete points 
between the two endpoints. Discrete coordinate positions along the line path are 
calculated from the equation of the line. For a raster video display, the line color 
(intensity) is then loaded into the frame buffer at the corresponding pixel coordi- 
nates. Reading from the frame buffer, the video controller then "plots" the screen 
pixels. Screen locations are xeferenced with integer values, so plotted positions 
may only approximate actual Line positions between two specified endpoints. A 
computed line position of (10.48,20.51), for example, would be converted to pixel 
position (10,211. Tlus rounding of coordinate values to integers causes lines to be 
displayed with a stairstep appearance ("the jaggies"), as represented in Fig 3-1. 
The characteristic stairstep shape of raster lines is particularly noticeable on sys- 
tems with low resolution, and we can improve their appearance somewhat by 
displaying them on high-resolution systems. More effective techniques for 
smoothing raster lines are based on adjusting pixel intensities along the line 
paths. 

For the raster-graphics device-level algorithms discussed in this chapter, ob- 
p-t positions are specified directly in integer device coordinates. For the time 
being, we will assume that pixel positions are referenced according to scan-line 
number and column number (pixel position across a scan line). This addressing 
scheme is illustrated in Fig. 3-2. Scan lines are numbered consecutively from 0, 
starting at the bottom of the screen; and pixel columns are numbered from 0, left 
to right across each scan line. In Section 3-10, we consider alternative pixel ad- 
dressing schemes. 

To load a specified color into the frame buffer at a position corresponding 
to column x along scan line y, we will assume we have available a low-level pro- 
cedure of the form 

Figure 3-1 
Staintep effect (jaggies) produced 
when a line is generated as a series 
of pixel positions. 



Line 
Number 

- 
Plxd Column 
Number 

Figure 3-2 
Pie1 positions referenced by scan- 
line number and column number. 

We sometimes will also want to be able to retrieve the current framebuffer 
intensity setting for a specified location. We accomplish this with the low-level 
fundion 

getpixel (x,  y )  

3-2 
LINE-DRAWING ALGORITHMS 

The Cartesian slope-intercept equation for a straight line is 

with rn representing the slope of the line and b as they intercept. Given that the 
two endpoints of a h e  segment are speafied at positions (x,, y,) and (x,  yJ, as 
shown in Fig. 3-3, we can determine values for the slope rn and y intercept b with 
the following calculations: 

b = y , - m . x l  (3-3) 

Algorithms for displaying straight h e s  are based on the line equation 3-1 and 
the calculations given in Eqs. 3-2 and 3-3. 

For any given x interval Ax along a line, we can compute the corresponding 
y interval from ~ 4 . 3 - 2  as 

- 

Figure 3-3 
Lie path between endpoint Ay=rnAx (3-4) 
positions (x,, y,) and (x,, y2). 

Similarly, wecan obtain the x interval Ax corresponding to a specified Ay as 

These equations form the basis for determining deflection voltages in analog de- 



vices. For lines with slope magnitudes I m I < 1,  Ax can be set proportional to a *""" 3-2 
small horizontal deflection voltage and the corresponding vertical deflection is L1ne-DrawingA'gorithms 

then set proportional to Ay as calculated from Eq. 3-4. For lines whose slopes 
have magnitudes 1 m I > 1,  Ay can be set proportional to a smaU vertical deflec- 
tion voltage with the corresponding horizontal deflection voltage set propor- 
tional to Ax, calculated from Eq. 3-5. For lines with m = 1, Ax = Ay and the hori- 
zontal and vertical deflections voltages are equal. In each case, a smooth line with 
slope m is generated between the specified endpoints. 

On raster systems, lines are plotted with pixels, and step sizes in the hori- 
zontal and vertical directions are constrained by pixel separations. That is, we 
must "sample" a line at discrete positions and determine the nearest pixel to the / line at each sampled position. T h s  scanconversion process for straight lines is il- v ,  
lustrated in Fig. 3-4, for a near horizontal line with discrete sample positions 
along the x axis. 

XI x2 

DDA Algorithm 

The digital drflerential analyzer (DDA) is a scan-conversion line algorithm based on f'igure 3-4 
calculating either Ay or Ax, using Eq. 3-4 or Eq. 3-5. We sample the line at unit in- Straight linesegment with 
tervals in one coordinate and determine corresponding integer values nearest the five sampling positions along 
line path for the other coordinate. the x ax% between x ,  and x2. 

Consider first a line with positive slope, as shown in Fig. 3-3. I f  the slope is 
less than or equal to 1,  we sample at unit x intervals ( A x  = 1) and compute each 
successive y value as 

Subscript k takes integer values starting from 1, for the first point, and increases 
by 1 until the final endpoint is reached. Since n1 can be any real number between 
0 and 1, the calculated y values must be rounded to the n e m t  integer. 

For lines with a positive slope greater than 1, we reverse the roles of x and 
y. That is, we sample at unit y intervals ( A y  = 1) and calculate each succeeding x 
value as 

Equations 3-6 and 3-7 are based on the assumption that lines are to be 
processed from the left endpoint to the right endpoint (Fig. 3-3). If this processing 
is reversed, so  that the starting endpoint is at the right, then either we have 
Ax = - 1  and 

or (when the slope is greater than I )  we have Ay = -1 with 

Equations 3-6 through 3-9 can also be used to calculate pixel positions a l o n ~  
a line with negative slope. If the absolute value of the slope is less than I and the 
start endpoint is at the left, we set Ax = 1 and calculate y values with Eq. 3-6. 



Chapfer When the start endpoint is at the right (for the same slope), we set Ax = -1 and 
Output Primitives obtain y positions from Eq. 3-8. Similarly, when the absolute value of a negative 

slope is wa te r  than 1, we use Ay = -1 and Eq. 3-9 or we use Ay = 1 and Eq. 3-7. 
This algorithm is summarized in the following procedure, which accepts as 

input the two endpolnt pixel positions. Horizontal and vertical differences be- 
tween the endpoint positions are assigned to parameters dx and dy. The differ- 
ence with the greater magnitude determines the value of parameter steps. Start- 
ing with pixel position (x,, yo), we determine the offset needed at each step to 
generate the next pixel position along the line path. We loop through this process 
steps times. If the magnitude of dx is greater than the magnitude of dy and xa 
is less than xb, the values of the increments in the x and y directions are 1 and m, 
respectively. If the greater change is in the x direction, but xa is greater than xb, 
then the decrements - 1 and -m are used to generate each new point on the line. 
Otherwise, we use a unit increment (or decrement) in they direction and an x in- 
crement (or decrement) of l / m .  

- - - -  -- -- 

#include 'device. h" 

void lineDDA (int xa, int ya, int xb, int yb) 
( 
int dx = xb - xa, dy = yb - ya, steps, k; 
float xrncrement, yIncrement, x = xa, y = ya; 

i t  (abs (dx) > abri (dyl) steps = abs ( d x )  ; 
else steps = abs dy); 
xIncrement = dx i (float) sceps; 
yIncrement = dy 1 (float) steps 

setpixel (ROUNDlxl, ROUND(y) ) :  
for (k=O; k<steps; k++) ( 

x + =  xIncrment; 
y += yIncrement; 
setpixel (ROUNDlx), ROVNDly) 

1 
1 

The DDA algorithm is a faster method for calculating pixel positions than 
the direct use of Eq. 3-1. It eliminates the multiplication in Eq. 3-1 by making use 
of raster characteristics, so that appropriate increments are applied in the x or y 
direction to step to pixel positions along the line path. The accumulation of 
roundoff error in successive additions of the floating-point increment, however, 
can cause the calculated pixel positions to drift away from the true line path for 
long line segments. Furthermore, the rounding operations and floating-point 
arithmetic in procedure lineDDA are still time-consuming. We can improve the 
performance of the DDA algorithm by separating the increments m and l / m  into 
integer and fractional parts so that all calculatio& are reduced to integer opera- 
tions. A method for calculating l / m  intrernents in integer steps is discussed in 
Section 3-11. In the following sections, we consider more general scan-line proce- 
dures that can be applied to both lines and curves. 

Bresenham's Line Algorithm 

An accurate and efficient raster line-generating algorithm, developed by Bresen- 



ham, scan converts lines using only incrementa1 integer calculations that can be 
adapted to display circles and other curves. Figures 3-5 and 3-6 illustrate sections 
of a display screen where straight line segments are to be drawn. The vertical 
axes show-scan-line positions, and the horizontal axes identify pixel columns. 
Sampling at unit x intervals in these examples, we need to decide which of two 
possible pixel positions is closer to the line path at each sample step. Starting 
from the left endpoint shown in Fig. 3-5, we need to determine at the next sample 
position whether to plot the pixel at position (11, 11) or the one at (11, 12). Simi- 
larly, Fig. 3-6 shows-a negative slope-line path starting from the left endpoint at 
pixel position (50, 50). In this one, do we select the next pixel position as (51,501 
or as (51,49)? These questions are answered with Bresenham's line algorithm by 
testing the sign of an integer parameter, whose value is proportional to the differ- 
ence between the separations of the two pixel positions from the actual line path. 

To illustrate ~Gsenharn's approach, we- first consider the scan-conversion 
process for lines with positive slope less than 1. Pixel positions along a line path 
are then determined by sampling at unit x intervals. Starting from the left end- 
point (x, yo) of a given line, we step to each successive column ( x  position) and 
plot the pixel whose scan-line y value is closest to the line path. Figure 3-7 
demonstrates the Mh step in this process. Assuming we have determined that the 
pixel at (xk, yk) is to be displayed, we next need to decide which pixel to plot in 
column x k + , .  Our choices are the pixels at positions &+l, ykl and (xk+l, yk+l). 

At sampling position xk+l, we label vertical pixel separations from the 
mathematical line path as d ,  and d2 (Fig. 3-8). They coordinate on the mathemati- 
cal line at pixel column position r k + l  is calculated as 

Then 

and 

The difference between these two separations is 

A decision parameter pk for the kth step in the line algorithm can be ob- 
tained by rearranging Eq. 3-11 so that it involves only integer calculations. We ac- 
complish this by substituting m = AyIAx, where Ay and Ax are the vertical and 
horizontal separations of the endpoint positions, and defining: 

The sign of p, is the same as the sign of dl - d,, since dr > 0 for our example. Pa- 
ri.meter c i s  constant and has the value 2Ay + Ax(2b - l), which is independent 

Figlrw 3-5 
Section of a display screen 
where a straight line segment 
1s to be plotted, starting from 
the pixel at column 10 on scan 
Line 11 

~ 

r i j y c  3-h 
Section of a display screen 
where a negative slope line 
segment 1s to be plotted, 
starting from the pixel a t  
column 50 on scan line 50. 



Figure 3-7 
Section of the screen grid 
showing a pixel in column xk 
on scan line yk that is to be 
plotted along the path of a 
line segment with slope 
O < m < l .  

Figure 3-8 
Distances between pixel 
positions and the line y 
coordinate at sampling 
position xk+ I. 

of pixel position and will be eliminated in the recursive calculations for pk. If the 
pixel at yk is closer to the line path than the pixel at yk+l (that is, dl < d,), then de- 
cision parameter pk is negative. In that case, we plot the lower pixel; otherwise, 
we plot the upper pixel. 

Coordinate changes along the line occur in unit steps in either the x or y di- 
rections. Therefore, we can obtain the values of successive decision parameters 
using incremental integer calculations. At step k + 1, the decision parameter is 
evaluated from Eq. 3-12 as 

Subtracting Eq. 3-12 from the p d i n g  equation, we have 

But xk+, = xk + 1, so that 

where the term yk+, - yk is either 0 or 1, depending on the sign of parameter pk. 
This m r s i v e  calculation of decision parameters is performed at each inte- 

ger x position, starting at the left coordinate endpoint of the line. The first para- 
meter, p,, is evaluated from Eq. 3-12 at the starting pixel position (xo, yo) and with 
m evaluated as Ay/Ax: 

We can summarize Bresenham line drawing for a line with a positive slope 
less than 1 in the following listed steps. The constants 2Ay and 2Ay - 2Ax are cal- 
culated once for each line to be scan convcrtcd, so the arithmetic involves only 
integer addition and subtraction of these two constants. 

resenham's Line-Drav,ina Algorithm for I n~ 1 < 1 
1. Input the twoline endpoints and store the left endpoint in (xo, yo) 
2. Load (xo, yd into the frame buffer; that is, plot the first point. 
3. Calculate constants Ax, hy, 2Ay, and 2Ay - ZAr, and obtain the start- 

ing value for the decision parameter as 

po = 2Ay - AX 

4. At each xk along the line, starting at k = 0, perform the following test: 
If Pr < 0, the next point to plot is (g + I ,  yd and 

P ~ + I  = P k  + ~ A Y  

Otherwise, the next point to plot is (xi + I ,  yr + 1) and 

pk+,  = pk + 2Ay - 2Ax 

5. Kepeat step 4 Ax times. 



Section 3-2 
Example 3-1 Bresenham Line Drawing ~ine-Drawing Algorithms 

To illustrate the algorithm, we digitize the line with endpoints (20, 10) and (30, 
18). This line has a slope of 0.8, with 

The initial decision parameter has the value 

and the increments for calculating successive decision parameters are 

We plot the initial point (xo, yo) = (20, lo) ,  and determine successive pixel posi- 
tions along the line path from the decision parameter as 

A plot of the pixels generated along this line path is shown in Fig. 3-9. 

An implementation of Bresenham line drawing for slopes in the range 0 < 
rn < 1 is given in the following procedure. Endpoint pixel positions for the line 
are passed to this procedure, and pixels are plotted from the left endpoint to the 
right endpoint. The call to setpixel loads a preset color value into the frame 
buffer at the specified ( x ,  y) pixel position. 

void lineares (int x a ,  i:it y a ,  int x b ,  int yb) 
( 
int dx = a b s  ( x a  - x b l ,  d y  = abs (ya - yb): 
int p = 2 * dy - d x ;  
int twoDy = 2 ' dy, twoDyDx = 2 ' l dy  - A x ) ;  
int x ,  y, xEnd: 

/ '  Determine which point to use a s  start, which as end * /  
if : x a  > x b )  ( 

x  = x b ;  
Y = yb; 
xEnd = x a ;  

) 
! else I 



x = xa; 
Y = ya; 
xEnd = xb; 

1 
setpixel (x ,  y); 

while (x < xEnd) ( 
x++; 
if lp  < 0 )  

$3 += twoDy; 
else [ 

y + + ;  
g += twoDyDx; 

) 
setpixel ( x ,  y); 

1 
1 

Bresenham's algorithm is generalized to lines with arbitrary slope by con- 
sidering the symmetry between the various octants and quadrants of the xy 
plane. For a line with positive slope greater than 1, we intelrhange the roles of 
the x and y directions. That is, we step along they direction in unit steps and cal- 
culate successive x values nearest the line path. Also, we could revise the pro- 
gram to plot pixels starting from either endpoint. If the initial position for a line 
with positive slope is the right endpoint, both x and y decrease as we step from 
right to left. To ensure that the same pixels are plotted regardless of the starting 
endpoint, we always choose the upper (or the lower) of the two candidate pixels 
whenever the two vertical separations from the line path are equal (d ,  = dJ. For 
negative slopes, the procedures are similar, except that now one coordinate de- 
creases as the other increases. Finally, specla1 cases can be handled separately: 
Horizontal lines (Ay = 01, vertical lines (Ar = O), and diagonal lines with I Ar 1 = 
I Ay 1 each can be loaded directly into the frame buffer without processing them 
through the line-plotting algorithm. 

Parallel Line Algorithms 

The line-generating algorithms we have discussed so far determine pixel posi- 
tions sequentially. With a parallel computer, we can calculate pixel positions 

Figure 3-9 
Pixel positions along the line path 
between endpoints (20.10) and 
(30,18), plotted with Bresenham's 
hne algorithm. 



along a line path simultaneously by partitioning the computations among the Wion3-2  
various processors available. One approach to the partitioning problem is to Line-Drawing ~lgorithms 
adapt an existing sequential algorithm to take advantage of multiple processors. 
Alternatively, we can look for other ways to set up the processing so that pixel 
positions can be calculated efficiently in parallel. An important consideration in 
devising a parallel algorithm is to balance the processing load among the avail- 
able processors. 

Given n, processors, we can set up a parallel Bresenham line algorithm by 
subdividing :he line path into n, partitions and simultaneously generating line 
segments in each of the subintervals. For a line with slope 0 < rn < I and left 
endpoint coordinate position (x, yo), we partition the line along the positive x di- 
rection. The distance between beginning x positions of adjacent partitions can be 
calculated as 

where Ax is the width of the line, and the value for partition width Ax. is com- 
puted using integer division. Numbering the partitiois, and the as 0, 
1,2, up to n, - 1, we calculate the starting x coordinate for the kth partition as 

As an example, suppose Ax = 15 and we have np = 4 processors. Then the width 
of the partitions is 4 and the starting x values for the partitions are xo, xo + 4, x, + 
8, and x, + 12. With this partitioning scheme, the width of the last (rightmost) 
subintewal will be smaller than the others in some cases. In addition, if the line 
endpoints are not ~ntegers, truncation errors can result in variable width parti- 
tions along the length of the line. 

To apply Bresenham's algorithm over the partitions, we need the initial 
value for the y coordinate and the initial value for the decision parameter in each 
partition. The change Ay, in they direction over each partition is calculated from 
the line slope rn and partition width Ax+ 

Ay, = mAxP (3-1 7i 

At the kth partition, the starting y coordinate is then 

The initial decision parameter for Bresenl:prn's algorithm at the start of the kth 
subinterval is obtained from Eq. 3-12: 

Each processor then calculates pixel positions over its assigned subinterval using 
the starting decision parameter value for that subinterval and the starting coordi- 
nates (xb yJ. We can also reduce the floating-point calculations to integer arith- 
metic in the computations for starting values yk and pk by substituting m = 
Ay/Ax and rearranging terms. The extension of the parallel Bresenham algorithm 
to a line with slope greater than 1 is achieved by partitioning the line in the y di- 



rection and calculating beginning x values for the partitions. For negative slopes, 
----- I ;i we increment coordinate values in one direction and decrement in the other. Another way to set up parallel algorithms on raster systems is to assign each pmessor to a particular group of screen pixels. With a sufficient number of processors (such as a Connection Machine CM-2 with over 65,000 processors), we 

can assign each processor to one pixel within some screen region. Thii approach 
I I 

v1 - - - - J  
can be adapted to line display by assigning one processor to each of the pixels 

-AX- Whin the limits of the line coordinate extents (bounding rectangle) and calculating 
pixel distances from the line path. The number of pixels within the bounding box 
of a line is Ax. Ay (Fig. 3-10). Perpendicular distance d from the line in Fig. 3-10 to 

Xl a pixel with coordinates (x,  y) is obtained with the calculation 

Figure 3-10 d = A x + B y + C  (3-20) 
Bounding box for a Line with 
coordinate extents b a n d  Ay. where 

A =  - A ~ ,  
linelength 

Ax 
B = 

linelength 

with 

linelength = 

Once the constants A, B, and C have been evaluated for the line, each processor 
needs to perform two multiplications and two additions to compute the pixel 
distanced. A pixel is plotted if d is less than a specified line-thickness parameter. 

lnstead of partitioning the screen into single pixels; we can assign to each 
processor either a scan line or a column of pixels depending on the line slope. 
Each processor then calculates the intersection of the line with the horizontal row 
or vertical column of pixels assigned that processor. For a line with slope 1 m I < 
1, each processor simply solves the line equation for y, given an x column value. 
For a line with slope magnitude greater than 1, the line equation is solved for x 
by each processor, given a scan-line y  value. Such direct methods, although slow 
on sequential machines, can be performed very efficiently using multiple proces- 
SOTS. 

3-3 
LOADING THE FRAME BUFFER 

When straight line segments and other objects are scan converted for display 
with a raster system, frame-buffer positions must be calculated. We have as- 
sumed that this is accomplished with the setpixel  procedure, which stores in- 
tensity values for the pixels at corresponding addresses within the frame-buffer 
array. Scan-conversion algorithms generate pixel positions at successive unit in- 



- - .- - . . - - -. 
F i p r r  3-1 I 
Pixel screen pos~t~ons stored linearly in row-major order withm the frame buffer. 

tervals. This allows us to use incremental methods to calculate frame-buffer ad- 
dresses. 

As a specific example, suppose the frame-bulfer array is addressed in row- 
major order and that pixel positions vary from (0. 0) at the lower left screen cor- 
ner to (x,, y,,,) at the top right corner (Fig. 3-11). For a bilevel system (1 bit per 
pixel), the frame-buffer bit address for pixel position (x, y) is calculated as 

Moving across a scan line, we can calculate the frame-buffer address for the pixel 
at (X + 1, y) as  the following offset from the address for position (x, y): 

Stepping diagonally up to the next scan line from (x, y), we get to the frame- 
buffer address of (x + 1, y + 1) with the calculation 

addr(x + 1, y + 1) = addr(x, yl  + x,,, -1- 2 (3-23) 
where the constant x,,, + 2 is precomputed once for all line segments. Similar in- 
cremental calculations can be obtained fmm Eq. 3-21 for unit steps in the nega- 
tive x and y screen directions. Each of these address calculations involves only a 
single integer addition. 

Methods for implementing the setpixel procedure to store pixel intensity 
values depend on the capabilities of a particular system and the design require- 
ments of the software package. With systems that can display a range of intensity 
values for each pixel, frame-buffer address calculations would include pixel 
width (number of bits), as well as the pixel screen location. 

3-4 
LINE FUNCTION 

A procedure for specifying straight-line segments can be set u p  in a number of 
different forms. In PHIGS, GKS, and some other packages, the two-dimensional 
line function is 



Chapter 3 polyline (n, wcpoints) 

Output Primit~ves 
where parameter n is assigned an integer value equal to the number of coordi- 
nate positions to be input, and wcpoints is the array of input worldcoordinate 
values for line segment endpoints. This function is used to define a set of n - 1 
connected straight line segments. Because series of connected line segments 
occur more often than isolated line segments in graphics applications, polyline 
provides a more general line function. To display a single shaight-line segment, 
we set n -= 2 and list the x and y values of the two endpoint coordinates in 

As an example of the use of polyline, the following statements generate 
two connected line segments, with endpoints at (50, 103, (150, 2501, and (250, 
100): 

wcPoints[ll .x = SO; 
wcPoints[ll .y = 100; 
wcPoints[21 .x = 150; 
wc~oints[2l.y = 250; 
wc~oints[3l.x = 250; 
wcPoints[31 . y  = 100; 
polyline ( 3 ,  wcpoints); 

Coordinate references in the polyline function are stated as absolute coordi- 
nate values. This means that the values specified are the actual point positions in 
the coordinate system in use. 

Some systems employ line (and point) functions with relative co- 
ordinate specifications. In this case, coordinate values are stated as offsets from 
the last position referenced (called the current position). For example, if location 
(3,2) is the last position that has been referenced in an application program, a rel- 
ative coordinate specification of (2, -1) corresponds to an absolute position of (5, 
1). An additional function is also available for setting the current position before 
the line routine is summoned. With these packages, a user lists only the single 
pair of offsets in the line command. This signals the system to display a line start- 
ing from the current position to a final position determined by the offsets. The 
current posihon is then updated to this final line position. A series of connected 
lines is produced with such packages by a sequence of line commands, one for 
each line section to be drawn. Some graphics packages provide options allowing 
the user to specify Line endpoints using either relative or absolute coordinates. 

Implementation of the polyline procedure is accomplished by first per- 
forming a series of coordinate transformations, then malung a sequence of calls 
to a device-level line-drawing routine. In PHIGS, the input line endpoints are ac- 
tually specdied in modeling coordinates, which are then converted to world c e  
ordinates. Next, world coordinates are converted to normalized coordinates, then 
to device coordinates. We discuss the details for carrying out these twodimen- 
sional coordinate transformations in Chapter 6. Once in device coordinates, we 
display the plyline by invoking a line routine, such as Bresenham's algorithm, 
n - 1 times to connect the n coordinate points. Each successive call passes the c c ~  
ordinate pair needed to plot the next line section, where the first endpoint of each 
coordinate pair is the last endpoint of the previous section. To avoid setting the 
intensity of some endpoints twice, we could modify the line algorithm so that the 
last endpoint of each segment is not plotted. We discuss methods for avoiding 
overlap of displayed objects in more detail in Section 3-10. 



3-5 
CIRCLE-GENERATING ALGORITHMS 

Since the circle is a frequently used component in pictures and graphs, a proce- 
dure for generating either full circles or circular arcs is included in most graphics 
packages. More generally, a single procedure can be provided to display either 
circular or elliptical curves. 

Properties of Circles 

A c k l e  is defined as the set of points that are all at a given distance r from a cen- 
ter position (x,, y,) (Fig. 3-12). This distance relationship is expressed by the 
Pythagorean theorem in Cartesian coordinates as 

We could use this equation to calculate the position of points on a ciicle circum- 
ference by stepping along the x axis in unit steps from x, - r to x, + r and calcu- 
lating the corresponding y values at each position as 

But this is not the best method for generating a circle. One problem with this a p  
proach is that it involves considerable computation at each step. Moreover, the 
spacing between plotted pixel positions is not uniform, as demonstrated in Fig. 
3-13. We could adjust the spacing by interchanging x and y (stepping through y 
values and calculating x values) whenever the absolute value of the slope of the 
circle is greater than 1. But this simply increases the computation and processing 
required by the algorithm. 

Another way to eliminate the unequal spacing shown in Fig. 3-13 is to cal- 
culate points along the circular boundary using polar coordinates r and 8 (Fig. 
3-12). Expressing the circle equation in parametric polar form yields the pair of 
equations 

When a display is generated with these equations using a fixed angular step size, 
a circle is plotted with equally spaced points along the circumference. The step 
size chosen for 8 depends on the application and the display device. Larger an- 
gular separations along the circumference can be connected with straight line 
segments to approximate the circular path. For a more continuous boundary on a 
raster display, we can set the step size at l / r .  This plots pixel positions that are 
approximately one unit apart. 

Computation can be reduced by considering the symmetry of circles. The 
shape of the circle is similar in each quadrant. We can generate the circle section 
in (he second quadrant of the xy plaie by noting that the two circle sections are 
symmetric with respect to they axis. And circle sections in the third and fourth 

Figure 3-12 
Circle with center coordinates 
(x,, y,) and radius r. 

Figure 3-13 
Positive half of a circle 
plotted with Eq. 3-25 and 
with (x,, y,) = (0.0). 

quadrants can be obtained from sections in the first and second quadrants by 



Y I considering symmetry about the x axis. We can take this one step further and - - 
note that there is alsd symmetry between octants. Circle sections in adjacent oc- 
tants within one quadrant are symmetric with respect to the 45' line dividing the 
two octants. These symmehy conditions are illustrated in Fig.3-14, where a point 
at position ( x ,  y) on a one-eighth circle sector is mapped into the seven circle 
points in the other octants of the xy plane. Taking advantage of the circle symme- 
try in this way we can generate all pixel positions around a circle by calculating 
only the points within the sector from x = 0 to x = y. 

Determining pixel positions along a circle circumference using either Eq. 
3-24 or Eq. 3-26 still requires a good deal of computation time. The Cartesian 

I 
equation 3-24 involves multiplications and squar&oot calculations, while the 

-- parametric equations contain multiplications and trigonometric calculations. 
Figure 3-14 More efficient circle algorithms are based on incremental calculation of decision 
Symmetry of a circle. -parameters, as in the Bresenham line algorithm, which mvolves only simple inte- 
Calculation of a circle point ger operations, 
(I, y) in one &ant yields the Bresenham's line algorithm for raster displays is adapted to circle genera- 
circle points shown for the tion by setting up  decision parameters for finding the closest pixel to the circum- other seven octants. 

ference at each sampling step. The circle equation 3-24, however, is nonlinear, so 
that squaremot evaluations would be required to compute pixel distances from a 
circular path. Bresenham's circle algorithm avoids these square-mot calculations 
by comparing the squares of the pixel separation distances. 

A method for direct distance comparison is to test the halfway position b e  
tween two pixels to determine if this midpoint is inside or outside the circle 
boundary. This method is more easily applied to other conics; and for a n  integer 
circle radius, the midpoint approach generates the same pixel positions as the 
Bresenham circle algorithm. Also, the error involved in locating pixel positions 
along any conic section using the midpoint test is limited to one-half the pixel 
separation. 

Midpoint Circle Algorithm 

As in the raster line algorithm, we sample at unit intervals and determine the 
closest pixel position to the specified circle path at each step. For a given radius r 
and screen center position ( x ,  y,), we can first set u p  our algorithm to calculate 
pixel positions around a circle path centered at the coordinate origin (0,O). Then 
each calculated position (x, y) is moved to its proper screen position by adding x, 
to x and y, toy. Along the circle section from x = 0 to x = y in the first quadrant, 
the slope of the curve varies from 0 to -1. Therefore, we can take unit steps in 
the positive x direction over this octant and use a decision parameter to deter- 
mine which of the two possible y positions is closer to the circle path at  each step. 
Positions ih the other seven octants are then obtained by symmetry. 

To apply the midpoint method, we define a circle function: 

Any point ( x ,  y) on the boundary of the circle with radius r satisfies the equation 
/cin,,(x, y) = 0. If the point is in the interior of the circle, the circle function is nega- 
tive. And if the point is outside the circle, the circle function is positive. To sum- 
marize, the relative position of any point ( x .  v )  can be determined by checking the 
sign of the circle function: 



f < 0 ,  if ( x ,  V) is inside the drde  boundary I l l 1  - - "  
if (x, y) is on the circle boundary xz + yt  - rz - 0  

> 0, if (x, y) is outside the circle boundary 

The circle-function tests in 3-28 are performed for the midpositions between pix- 
els near the circle path at each sampling step. Thus, the circle function is the deci- xk x, + 1 x, + 2 
sion parameter in the midpoint algorithm, and we can set up incremental calcu- 
lations for this function as we did in the line algorithm. 

' 

Figure 3-15 shows the midpoint between the two candidate pixels at Sam- Figrrre3-15 

pling position xk + 1. Assuming we have just plotted the pixel at (xk, yk), we next Midpoint between candidate 
pixels at sampling position n d  to determine whether the pixel at position (xk + 1, yk) or the one at position 
xk+l cirrular path. 

(xk  + 1,  yk -- 1) is closer to the circle. Our decision parameter is the circle function 
3-27 evaluated at the midpoint between these two pixels: 

If pk < 0, this midpoirat is inside the circle and the pixel on scan line yb is closer to 
the circle boundary. Otherwise, the midposition is outside or on the circle bound- 
ary, and we select the pixel on scanline yk - 1. 

Successive decision parameters are obtained using incremental calculations. 
We obtain a recursive expression for the next decision parameter by evaluating 
the circle function at sampling p~sit ionx~,,  + 1 = x, + 2: 

where yk ,, is either yi or yk-,, depending on the sign of pk. 
increments for obtaining pk+, are either 2r,+, + 1 (if pk is negative) or 2r,+, 

+ 1 - 2yk+l. Evaluation of the terms Zk+, and 2yk+,  can also be done inaemen- 
tally as 

At the start position (0,  T) ,  these two terms have the values 0 and 2r, respectively. 
Each successive value is obtained by adding 2 to the previous value of 2x and 
subtracting 2 from the previous value of 5. 

The initial decision parameter is obtained by evaluating the circle function 
at the start position (x0, yo) = (0, T ) :  



C h a w  3 
Output Primitives 

5 
p O = c r  

(3-31) 

If the radius r is specified as an integer, we can simply round po to 

po = 1 - r (for r  an integer) 

since al l  inmments are integers. 
As in Bresenham's line algorithm, the midpoint method calculates pixel po- 

sitions along the circumference of a cirde using integer additions and subtrac- 
tions, assuming that the circle parameters are specified in integer screen coordi- 
nates. We can summarize the steps in the midpoint circle algorithm as follows. 

Midpoint Circle Algorithm 

1. hput radius r and circle center (x, y,), and obtain the first point on 
the circumference of a circle centered on the origin as 

I 2. cdculate the initial value of the decision parameter as 

3. At each xk position, starting at k = 0, perform the following test: If 
pk C 0, the next point along the circle centered on (0,O) is (xk,,, yk) and 

I Otherwise, the next point along the circle is (xk + 1, yk - 1) and 

where 2xk+, = kt + 2 and 2yt+, = 2yt - 2. 
4. ~eterrnine symmetry points in the other seven octants. 
5. Move each calculated pixel position (x, y) onto the cirmlar path cen- 

tered on (x, yc) and plot the coordinate values: 

x = x + x c ,  y = y + y c  

6. Repeat steps 3 through 5 until x r y. 



Section 3-5 

C ircle-Generating Algorithms 

-- 
Figure 3-16 
Selected pixel positions (solid 
circles) along a circle path with 
radius r = 10 centered on the origin, 
using the midpoint circle algorithm. 
Open circles show the symmetry 
positions in the first quadrant. 

Example 3-2 Midpoint Circle-Drawing 

Given a circle radius r = 10, we demonstrate the midpolnt circle algorithm by 
determining positions along the circle octant in the first quadrant hum x = 0 to 
x = y. The initial value of the decision parameter is 

For the circle centered on the coordinate origin, the initial point is (x,, yo) - 
(0, lo), and initial increment terms for calculating the dxision parameters are 

Successive decision parameter values and positions along the circle path are cal- 
culated using the midpoint method as 

A plot c )f the generated pixel positions in the first quadrant is shown in Fig. 3-10. 

The following procedure displays a raster t i d e  on a bilevel monitor using 
the midpoint algorithm. Input to the procedure are the coordinates for the circle 
center and the radius. Intensities for pixel positions along the circle circumfer- 
ence are loaded into the frame-buffer array with calls to the set pixel routine. 



Chapter 3 

Ouipur Pr~mitives 

Figure 3-1 7 
Ellipse generated about foci 
F, and F,. 

#include 'device . h  

void circleMidpoint (int Kenter, int yCenter, int radius) 
I 
int x = 0; 
int y = radius; 
int p = 1 - radius; 
void circlePlotPoints (int, int, int, int); 

/ '  Plot first set of points ' /  
circlePlotPoints (xcenter. *enter. x, yl; 

while (x < y) ( 
x++ ; 
if (P < O! + 

p * =  2 
else I 
Y--; 
p + z  2 ' (x - Y) + 1; 

void circlePlotPolnts (int xCenter, int yCenter, int x, int yl 
( 
setpixel (xCenter + x, $enter + y ) ;  
setpixel (xCenter - x. $enter + yl; 
setpixel (xCenter + x, $enter - y); 
setpixel (xCenter - x ,  $enter - y ) ;  
setpixel (xCenter + y, $enter + x); 
setpixel (xCenter - y ,  $enter + x); 
setpixel (xCenter t y ,  $enter - x); 
setpixel (xCenter - y ,  $enter - x); 

1 

3-6 
ELLIPSE-GENERATING ALGORITHMS 

Loosely stated, an ellipse is an elongated circle. Therefore, elliptical curves can be 
generated by modifying circle-drawing procedures to take into account the dif- 
ferent dimensions of an ellipse along the mapr and minor axes. 

Properties of Ellipses 

An ellipse is defined as the set of points such that the sum of the distances from 
two fi.ted positions (foci) is the same for all points (Fig. b17). Lf the distances to 
the two foci from any point P = (x ,  y) on the ellipse are labeled dl and d2, then the 
general equation of an ellipse can be stated as 

d ,  + d, = constant (3-321 

Expressing distances d, and d, in terms of the focal coordinates F, = (x,, y,) and 
F2 = (x, y2), we have 



By squaring this equation, isolating the remaining radical, and then squaring 
again, we can rewrite the general ellipseequation in the form 

Ax2 + By2 + Cxy + Dx + Ey + F = 0 (3-34)  
where the coefficients A, B, C, D, E ,  and Fare evaluatcul in terms of the focal coor- 
dinates and the dimensions of the major and minor axes of the ellipse. The major 
axis is the straight line segment extending from one side of the ellipse to the 
other through the foci. The minor axis spans the shorter dimension of the ellipse, 
bisecting the major axis at the halfway position (ellipse center) between the two 
foci. 

An interactive method for specifying an ellipse in an arbitrary orientation is 
to input the two foci and a point on the ellipse boundary. With these three coordi- 
nate positions, we can evaluate the constant in Eq. 3.33. Then, the coefficients in 
Eq. 3-34 can be evaluated and used to generate pixels along the elliptical path. 

Ellipse equations are greatly simplified if the major and minor axes are ori- 
ented to align with the coordinate axes. In Fig. 3-18, we show an ellipse in "stan- 
ddrd position" with major and minor axes oriented parallel to the x and y axes. 
Parameter r, for this example labels the semimajor axis, and parameter r,, labels 
the semiminor axls. The equation of the ellipse shown in Fig. 3-18 can be written 
in terms of the ellipse center coordinatesand parameters r ,  and r, as 

Using polar coordinates r and 0. we can also describe the ellipse in standard posi- 
tion with the parametric equations: 

T = x,. t r ,  cosO 

y = y,. + r ,  sin9 

Symmetry considerations can be used to further reduce con~putations. An ellipse 
in stdndard position is symmetric between quadrants, but unlike a circle, it is not 
synimrtric between the two octants of a quadrant. Thus, we must calculate pixel 
positions along the elliptical arc throughout one quadrant, then we obtain posi- 
tions in the remaming three quadrants by symmetry (Fig 3-19). 

Our approach hrrr is similar to that used in displaying d raster circle. Given pa- 
rameters r,, r!, a ~ l d  (x,, y,.), we determine points ( x ,  y) for an ellipse in standard 
position centered on the origin, and then we shift the points so the ellipse is cen- 
tered at ( x ,  y,). 1t we wish also tu display the ellipse in nonstandard position, we 
could then rotate the ellipse about its center coordinates to reorient the major and 
minor  axes. For the present, we consider only the display of ellipses in standard 
position We discuss general methods for transforming object orientations and 
positions in Chapter 5. 

The midpotnt ellipse niethtd is applied throughout thc first quadrant in 
t\co parts. Fipurv 3-20 shows the division of the first quadrant according to the 
slept, of an  ellipse with r ,  < r,. We process this quadrant by taking unit steps in 
the .j directwn where the slope of the curve has a magnitude less than 1, and tak- 
ing unit steps in thcy direction where the s l o p  has a magnitude greater than 1. 

Regions I and 2 (Fig. 3-20), can he processed in various ways. We can start 
at position (0. r,) c*nd step clockwise along the elliptical path in the first quadrant, 

. .- --- 
Figure 3-18 
Ellipse centered at (x,, y,) with 
wmimajor axis r, and 
st:miminor axis r,. 



Clldprer 3 shlfting from unit steps in x to unit steps in y when the slope becomes less than 
~utpul Pr~rnitives -1. Alternatively, we could start at (r,, 0) and select points in a countexlockwise 

order, shifting from unit steps in y to unit steps in x when the slope becomes 
greater than -1. With parallel processors, we could calculate pixel positions in 
the two regions simultaneously. As an example of a sequential implementation of 

( -x .  v )  (, y,  the midpoint algorithm, we take the start position at (0, ry) and step along the el- & lipse path in clockwise order throughout the first quadrant. We define an ellipse function from Eq. 3-35 with (x,, y,) = (0,O) as - 
I- x. - yl ( X  - y )  which has the following properties: 

1 0, if (x ,  y) is inside the ellipse boundary 

Symmetry oi an cll~pse > 0 if (x, y) is outside the ellipse boundary 
Calculation I J ~  a p i n t  (x, y) 
In one quadrant yields the Thus, the ellipse function f&,(x, y) serves as the decision parameter in the mid- 
ell'pse points shown for the point algorithm. At each sampling position, we select the next pixel along the el- 
other three quad rants. lipse path according to the sign of the ellipse function evaluated at the midpoint 

between the two candidate pixels. 
v t Starting at (0, r,), we take unit steps in the x direction until we reach the 

boundary between region 1 and region 2 - ( ~ i ~ .  3-20). Then we switch to unit steps 
in the y direction over the remainder of the curve in the first quadrant. At each 
step, we need to test the value of the slope of the curve. The ellipse slope is calcu- 
lated from Eq. 3-37 as 

.- 

1 At the boundary between region 1 and region 2, dy/dx = - 1 and 
- - - - . - - .. . . - . - - - 
F ~ , y ~ r n '  3-20 
Ellipse processing regions. 
Over regior I ,  the magnitude 
of the ellipse slope is less 
than 1; over region 2, the 
magnitude of the slope is 
greater than I .  

Therefore, we move out of region 1 whenever 

Figure 3-21 shows the midpoint between the two candidate pixels at sam- 
piing position xk + 1 in the first regon. Assuming position (xk, yk) has been se- 
lected at the previous step, we determine the next position along the ellipse path 
by evaluating the decision parameter (that is, the ellipse function 3-37) at this 
midpoint: 

If pl ,  < 0, the midpoint is inside the ellipse and the pixel on scan line y, is closer 
to the ellipse boundary. Otherwise, the midposition is outside or on the ellipse 
boundary, and we select the pixel on scan line yt - 1. 



At the next sampling position (xk+, + 1 = x, + 2), the decision parameter 
for region 1 is evaluated as 

Y t  

p1i+l  = feUip(xk+l + yk+, - i) 
v ,  - 1 

( - f)'-r:rt = r;[(xk + 1) + 112 + T: Y k + ,  
x, X, + 1 

l 2  M';dpoint between candidate 
plk+ ,  = ~ 1 ~ + 2 r ; ( x k + l ) + r i  + r ; [ ( y k + r  k ) 2 7 ( y k -  (M2)  pixels at sampling position 

xl + 1 along an elliptical path. 
where yk+, is either yl, or yk - 1, depending on the sign of pl,. 

Decision parameters are incremented by the following amounts: 

2r,?~k+~ + r:, if plk < 0 
increment = 

2 + r - 2 if plk 2 0 

As in the circle algorithm, increments for ihe decision parameters can be calcu- 
lated using only addition and subtraction, since values for the terms 2r;x and 
2r:y can also be obtained incrementally. At the initial position (0, r,), the two 
terms evaluate to 

As x and y are incremented, updated values are obtained by adding 2ri to 3-43 
and subtracting 21: from 3-44. The updated values are compared at each step, 
and we move from region 1 to region 2 when condition 3-40 is satisfied. 

Jn region 1, the initial value of the decision parameter is obtained by evalu- 
ating the ellipse function at the start position ( x ,  yo) =: (0, r,): 

1 
pl, = r,? - r;ry + - r,2 

4 
(3 -45)  

Over region 2, we sample at unit steps in the negative y direction, and the 
midpoint is now taken between horizontal pixels a t  each step (Fig. 3-22). For this 
region, the decision parameter is evaJuated as 



Chaoter 3 

Oufput Primitives 

Figlrrc 3-22 
Midpoint between candidate pixels 
at sampling position y, - 1 along an 

x, x, + 1 x ,  + 2 elliptical path. 

If p2, > 0, the midposition is outside the ellipse boundary, and we select the pixel 
at xk. If pa 5 0, the midpoint is inside or on the ellipse boundary, and we select 
pixel position x, ,  ,. 

To determine the relationship between successive decision parameters in 
region 2, we evaluate the ellipse function at the next sampling step yi+, - 1 - 
y~ - 2: 

with xk + , set either to x, or to xk + I, depending on the sign of ~ 2 ~ .  
When we enter region 2, ;he initial position (xo, yJ is taken as the last posi- 

tion selected in region 1 and the initial derision parameter in region 2 is then 

To simplify the calculation of p&, we could select pixel positions in counterclock- 
wise order starting at (r,, 0). Unit steps would then be taken in the positive y di- 
rection up  to the last position selected in rrgion 1. 

The midpoint algorithm can be adapted to generate an ellipse in nonstan- 
dard position using the ellipse function Eq. 3-34 and calculating pixel positions 
over the entire elliptical path. Alternatively, we could reorient the ellipse axes to 
standard position, using transformation methods discussed in Chapter 5, apply 
the midpoint algorithm to determine curve positions, then convert calculated 
pixel positions to path positions along the original ellipse orientation. 

Assuming r,, r,, and the ellipse center are given in integer screen coordi- 
nates, we only need incremental integer calculations to determine values for the 
decision parameters in the midpoint ellipse algorithm. The increments r l ,  r:, 2r:, 
and 2ri  are evaluated once at the beginning of the procedure. A summary of the 
midpoint ellipse algorithm is listed in the following steps: 



Midpoint Ellipse Algorithm 

1. Input r,, r,, and ellipse center (x,, y,), and obtain the first point on an 
ellipse centered on the origin as 

2. Calculate the initial value of thedecision parameter in region 1 as 

3. At each x, position in region 1, starting at k = 3, perform the follow- 
ing test: If p l ,  < 0, the next point along the ellipse centered on (0, 0) 
is (x, . I, yI) and 

Otherwise, the next point along the circle is ( x k  + 1, yr, - 1) and 

with 

and continue until 2 r i x  2 2rty. 
4. Calculate the initial value of the decision parameter in region 2 using 

the last point (xo,  yo) calculated in region 1 as 

5. At each yk position in region 2, starting at k = 0, perform the follow- 
ing test: If pZk> 0, the next point along the ellipse centered on (0, 0) is 
(xk ,  yk .- 1) and 

Otherwise, the next point along the circle i s  (.rk + 1, yt - 1) and 

using the same incremental calculations for .I and y as in region 1. 
6 .  Determine symmetry points in the other three quadrants. 
7. Move each calculated pixel position (x, y) onto the elliptical path cen- 

tered on (x,, y,) and plot the coordinate values: 

8. Repeat the steps for region 1 until 2 6 x  2 2rf.y 

Ellipse-Generating Algorilhrns 



Chapter 3 

Oucpur Example 3-3 Midpoint Ellipse Drawing 

Given input ellipse parameters r, = 8 and ry = 6, we illustrate the steps in the 
midpoint ellipse algorithm by determining raster positions along the ellipse path 
in the first quadrant. lnitial values and increments for the decision parameter cal- 
culations are 

2r:x = 0 (with increment 2r; = 7 2 )  

Zr f y=2r f r y  (withincrement-2r:=-128) 

For region 1: The initial point for the ellipse centered on the origin is (x,, yo) = 
(0,6), and the initial decision parameter value is 

1 
pl, = r; - rfr, t - r: = -332 

4 

Successive decision parameter values and positions along the ellipse path are cal- 
culated using the midpoint method as  

We now move out of region 1, since 2r;x > 2r:y. 

For region 2, the initial point is ( x ,  yo) = V,3) and the initial decision parameter 
is 

The remaining positions along the ellipse path in the first quadrant are then cal- 
culated as 

A plot of the selected positions around the ellipse boundary within the first 
quadrant is shown in Fig. 3-23. 

In the following procedure, the midpoint algorithm is used to display an el- 
lipsc: with input parameters RX, RY, xcenter, and ycenter. Positions along the 



Section 3 6  

Flltpse-Generating Algorithms 

Figure 3-23 
Positions along an elliptical path 
centered on the origin with r, = 8 
and r, = 6 using the midpoint 
algorithm to calculate pixel 
addresses in the first quadrant. 

curve in the first quadrant are generated and then shifted to their proper screen 
positions. Intensities for these positions and the symmetry positions in the other 
t h  quadrants are loaded into the frame buffer using the set pixel mutine. 

void ell ipseMidpoint ( i n t  xCenter, i n t  yCenter, i n t  Rx, i n t  Ry) 
( 

i n t  Rx2 = Rx4Rx; 
i n t  RyZ = RygRy; 
i n t  twoRx2 = 2.Rx2; 
i n t  twoRy2 = 2*RyZ; 
i n t  p; 
i n t  x = 0; 
i n t  y = Ry; 
i n t  px = 0; 
i n t  py = twoRx2 y;  
void e l l i p seP lo tPo in t s  ( i n t ,  i n t ,  i n t ,  i n t ) ;  

1 .  Plot the first s e t  of points ' I  
e l l ip seP lo tPo in t s  (xcenter,  yCenter, X ,  Y ) ;  

/ *  Region 1 * I  
P = R O W  (Ry2 - (Rx2 Ry) + (0.25 . -2 ) ) ;  
while (px < PY) { 

x++;  
px += twoxy2; 
i f  (p  c 0) 

p += Ry2 + px; 
e l s e  ( 

y - - ;  
py -= twoRx2; 
p += Ry2 + px - py; 

1 

/ *  Region 2 * /  
p = ROUND (RyZ*(x+0.5) '(%+0.5) + Rx2*(y- l ) ' ( y - l )  - Rx2.Ry2); 
while ( y  > 0 )  ( 

Y--; 
py -=  twoRx2; 
i f  (p  > 0) 

p += Rx2 - py; 
e l s e  ( 

x++;  
px += twoRy2: 
p + =  Rx2 - PY + Px; 



Chanter 3 1 

Output Primitives 1 e 1 l : p o e P l o t F o ~ n : s  ( x C e l l L r ~ ,  y c e n t e r ,  x ,  yl; 

v o i d  e l l i p s e P l o t P o - n t s  ( i n t  xCenter,  i n t  yCenter,  i n t  x ,  i n t  yl 
( 

s e t p i x e l  (xCentel.  + x ,  yCenter + yl : 
s e t p i x e l  (xCente1- - x,  yCencer + y ) ;  
s e t p i x e l  (xCente1- t x ,  yCenter - y); 
s e t p i x e l  (xCenter - x ,  $enter  - y): 

OTHER CURVES 

Various curve functions are useful in object modeling, animation path specifica- 
tions, data and function graphing, and other graphics applications. Commonly 
encountered curves include conics, trigonometric and exponential functions, 
probability distributions, general polynomials, and spline functions. Displays of 
these curves can be generated with methods similar to those discussed for the 
circle and ellipse functions. We can obtain positions along curve paths directly 
from explicit representations y = f(x) or from parametric forms Alternatively, we 
could apply the incremental midpoint method to plot curves described with im- 
plicit functions f i x ,  y) = 1). 

A straightforward method for displaying a specified curve function is to ap- 
proximate it with straight line segments. Parametric representations are useful in 
this case for obtaining equally spaced line endpoint positions along the curve 
path. We can also generate equally spaced positions from an explicit representa- 
tion by choosing the independent variable according to the slope of the curve. 
Where the slope of y = ,f(x) has a magnitude less than 1, we choose x as the inde- 
pendent variable and calculate y values at equal x increments. To obtain equal 
spacing where the slope has a magnimde greater than 1, we use the inverse func- 
tion, x = f -'(y), and calculate values of x at equal y steps. 

Straight-line or cun7e approximations are used to graph a data set of dis- 
crete coordinate points. We could join the discrete points with straight line seg- 
ments, or we could use linear regression (least squares) to approximate !he data 
set with a single straight line. A nonlinear least-squares approach is used to dis- 
play the data set with some approximatingfunction, usually a polynomial. 

As with circles and ellipses, many functions possess symmetries that can be 
exploited to reduce the computation of coordinate positions along curve paths. 
For example, the normal probability distribution function is symmetric about a 
center position (the mean), and all points along one cycle of a sine curve can be 
generated from the points in a 90" interval. 

Conic Sectior~s 

In general, we can describe a conic section (or conic) with the second-degree 
equation: 

.4x2 + By2 + Cxy + Dx + Ey + F = 0 (3 -.50) 



where values for parameters A, B, C, D, E, and F determine the kind of curve we section 3-7 
are to display. Give11 this set of coefficients, we can dtatermine the particular conic Other Curves 
that will be generated by evaluating the discriminant R2 - 4AC: 

[< 0, generates an ellipse (or circle) 

B2 - 41C { = 0, generates a parabola ( . 3 - 5  1 ) 
I> 0, generates a hyperbola 

For example, we  get the circle equation 3-24 when .4 = B = 1, C = 0, D = -2x,, 
E = -2y( ,  and F = x: + yf - r2. Equation 3-50 also describes the "degenerate" 
conics: points and straight lines. 

Ellipses, hyperbolas, and parabolas are particulilrly useful in certain aninia- 
tion applications. These curves describe orbital and other motions for objects 
subjected to gravitational, electromagnetic, or nuclear forces. Planetary orbits in 
the solar system, for example, are ellipses; and an object projected into-a uniform 
gravitational field travels along a parabolic trajectory. Figure 3-24 shows a para- 
bolic path in standard position for a gravitational field acting in the negative y di- 
rect~on. The explicit equation for the parabolic trajectory of the object shown can 
be written as 

y = yo + a(x - x,J2 + b(x - :to) 

with constants a and b determined by the initial velocity g cf the object and the 
acceleration 8 due to the uniform gravitational force. We can also describe such 
parabolic motions with parametric equations using a time parameter t ,  measured 
in seconds from the initial projection point: xo 

X = Xo S G r o t  (3-33 ,  F ~ , ~ I ~ w  .3-24 
1 

I/ yo + v,,t - 2 g f 2  P,lrabolic path of an  object tossed into a downward 
Here, v,, and v,yo are the initial velocity components, and the value of g near the gravitational field at the 

ir.~tial position (x,,, ,yo). 
surface of the earth is approximately 980cm/sec2. Object positions along the par- 
abolic path are then calculated at selected time steps. 

Hyperbolic motions (Fig. 3-25) occur in connection with the collision of 
charged particles and in certain gravitational problems. For example, comets or 
meteorites moving around the sun may travel along hyperbolic paths and escape 
to outer space, never to return. The particular branch (left or right, in Fig. 3-25) 
describing the motion of an object depends on the forces involved in the prob- 
lem. We can write the standard equation for the hyperbola c e n t e d  on the origin 
in Fig. 3-25 as 

-r 

(3-51) 

with x 5 -r, for the left branch and x z r, for the right branch. Since this equa- - 
tion differs from the standard ellipse equation 3-35 only in the sign between the 

FIKllrr 3-25 
x2 and y2 terms, we can generate points along a hyperbolic path with a slightly ~~f~ and branches of a 
modified ellipse algorithm. We will return to the discussion of animation applica- hyperbola in standard 
tions and methods in more detail in Chapter 16. And in Chapter 10, we discuss position with symmetry axis 
applications of computer graphics in scientific visuali~ation. along the x axis. 

111 



Chapter 3 Parabolas and hyperbolas possess a symmetry axis. For example, the 
O u ~ p u ~  Prirnit~ves parabola described by Eq. 3-53 is symmetric about the axis: 

The methods used in the midpoint ellipse algorithm can be directly applied to 
obtain points along one side of the symmetry axis of hyperbolic and parabolic 
paths in the two regions: (1) where the magnitude of the curve slope is less than 
1, and (2) where the magnitude of the slope is greater than 1. To do this, we first 
select the appropriate form of Eq. 3-50 and then use the selected function to set 
up expressions for the decision parameters in the two regions. 

Polynomials dnd Spline Curves 

A polynomial function of nth degree in x is defined as 

where n is a nonnegative integer and the a, are constants, with a. Z 0. We get a 
quadratic when n = 2; a cubic polynomial when n = 3; a quartic when n = 4; and 
so forth. And we have a straight line when n = 1. Polynomials are useful in a 
number of graphics applications, including the design of object shapes, the speci- 
fication of animation paths, and the graphing of data trends in a discrete set of 
data points. 

Designing object shapes or motion paths is typically done by specifying a 
few points to define the general curve contour, then fitting.the selected points 
with a polynomial. One way to accomplish the curve fitting is to construct a 
cubic polynomial curve section between each pair of specified points. Each curve 
section is then described in parametric form as 

/ y = a,,, + a,,u + a,,u2 + a,,u3 (3-57) 

f--' where parameter u varies over the interval 0 to 1. Values for the coefficients of u in the parametric equations are determined from boundary conditions on the 
curve &ions. One boundary condition is that two adjacent curve sections have 

Figure 3-26 the same coordinate position at the boundary, and a second condition is to match 
A spline curve formed with the two curve slopes at the boundary so that we obtain one continuous, smooth 
individual cubic curve (Fig. 3-26). Continuous curves that are formed with polynomial pieces are 
sections between specified called spline curves, or simply splines. There are other ways to set up spline 
coordinate points. curves, and the various spline-generating methods are explored in Chapter 10. 

3-8 - 
PARALLEL CURVE ALGORITHMS 

Methods for exploiting parallelism in curve generation are similar to those used 
in displaying straight line segments. We can either adapt a sequential algorithm 
by allocating processors according to cune  partitions, or we could devise other 



methods and assign processors to screen partitions. 
A parallel midpoint method for displaying circles is to divide the circular 

arc from 90" to 45c into equal subarcs and assign a separate processor to each 
subarc. As in the parallel Bresenham line algorithm, we then need to set up com- 
putations to determine the beginning y value and decisicn parameter pk value for 
each processor. Pixel positions are then calculated throughout each subarc, and 
positions in the other circle octants are then obtained by symmetry. Similarly, a 
parallel ellipse midpoint method divides the elliptical arc over the first quadrant 
into equal subarcs and parcels these out to separate processors. Pixel positions in 
the other quadrants are determined by symmetry. A screen-partitioning scheme 
for circles and ellipses is to assign each scan line crossing the curve to a separate 
processor. In this case, each processor uses the circle or ellipse equation to calcu- 
late curve-intersection coordinates. 

For the display of elliptical a m  or other curves, we can simply use the scan- 
line partitioning method. Each processor uses the curve equation to locate the in- 
tersection positions along its assigned scan line. With processors assigned to indi- 
vidual pixels, each processor would calculate the distance (or distance squared) 
from the curve to its assigned pixel. If the calculated distance is less than a prede- 
fined value, the pixel is plotted. 

3-9 
CURVE FUNCTIONS 

Routines for circles, splines, and other commonly used curves are included in 
many graphics packages. The PHIGS standard does not provide explicit func- 
tions for these curves, but it does include the following general curve function: 

generalizedDrawingPrimitive In, wc~oints, id, datalist) 

where wcpoints is a list of n coordinate positions, da ta1  ist contains noncoor- 
dinate data values, and parameter i d  selects the desired function. At a particular 
installation, a circle might be referenced with id = 1, an ellipse with i d  = 2, and 
SO on. 

As an example of the definition of curves through this PHIGS function, a 
circle (id = 1, say) could be specified by assigning the two center coordinate val- 
ues to wcpoints and assigning the radius value to d a t a l i s t .  The generalized 
drawing primitive would then reference the appropriate algorithm, such as the 
midpoint method, to generate the circle. With interactive input, a circle could be 
defined with two coordinate points: the center position and a point on the cir- 
cumference. Similarly, interactive specification of an ellipse can be done with 
three points: the two foci and a point on the ellipse boundary, all s t o d  in wc- 
points .  For an ellipse in standard position, wcpoints could be assigned only the 
center coordinates, with daZalist assigned the values for r, and r,. Splines defined 
with control points would be generated by assigning the control point coordi- 
nates to wcpoints. 

Functions to generate circles and ellipses often include the capability of 
drawing curve sections by speclfylng parameters for the line endpoints. Expand- 
ing the parameter list allows specification of the beginning and ending angular 
values for an arc, as illustrated in Fig. 3-27. Another method for designating a cir- 

Section 3-9 

Curve Functions 

Figure 3-27 
Circular arc specified by 
beginning and ending angles. 
Circle center is at the 
coordinate origin. 



Chapter 3 cular or elliptical arc is to input the beginning and ending coordinate positions of 
Output Prim~t~ves the arc. 

Figure 3-28 
Lower-left section of the 
screen grid referencing 
Integer coord~nate positions. 

Figure 0-29 
Line path for a series oi 
connected line segments 
between screen grid 
coordinate positions. 

Figure 3-30 
lllum~nated pixel a1 raster 
position (4,5). 
114 

PIXEL ADDRESSING AND OBJECT GEOMETRY 

So far we have assumed that all input positions were given in terms of scan-line 
number and pixel-posihon number across the scan line. As we saw in Chapter 2, 
there are, in general, several coordinate references associated with the specifica- 
tion and generation of a picture. Object descriptions are given in a world- 
reference frame, chosen to suit a particular application, and input world coordi- 
nates are ultimately converted to screen display positions. World descriptions of 
objects are given in terms of precise coordinate positions, which are infinitesi- 
mally small mathematical points. Pixel coordinates, however, reference finite 
screen areas. If we want to preserve the specified geometry of world objects, we 
need to compensate for the mapping of mathematical input points to finite pixel 
areas. One way to d o  this is simply to adjust the dimensions of displayed objects 
to account for the amount of overlap of pixel areas with the object boundaries. 
Another approach is to map world coordinates onto screen positions between 
pixels, so that we align object boundaries with pixel boundaries instead of pixel 
centers. 

Screen Grid Coordinates 

An alternative to addressing display posit~ons in terms of pixel centers is to refer- 
ence screen coordinates with respect to the grid of horizontal and vertical pixel 
boundary lines spaced one unit apart (Fig. 3-28). A screen soordinale position is 
then the pair of integer values identifying a grid interswtion position between 
two pixels. For example, the mathematical line path for a polyline with screen 
endpoints (0, O), (5,2), and (1,4) is shown in Fig. 3-29. 

With the coordinate origin at the lower left of the screen, each pixel area can 
be referenced by the mteger grid coordinates of its lower left corner. Figure 3-30 
illustrates this convention for an 8 by 8 section of a raster, w ~ t h  a single illumi- 
nated pixel at screen coordinate position (4, 5). In general, we identify the area 
occupied by a pixel with screen coordinates (x, y) as the unit square with diago- 
nally opposite corners at (x, y) and (x + 1, y + 1). This pixel-addressing scheme 
has several advantages: I t  avoids half-integer pixel boundaries, it facilitates pre- 
a s e  object representations, and it simplifies the processing involved in many 
scan-conversion algorithms and in other raster procedures. 

The algorithms for line drawing and curve generation discussed in the pre- 
ceding sections are still valid when applied to input positions expressed as screen 
grid coordinates. Decision parameters in these algorithms are now simply a mea- 
sure of screen grid separation differences, rather than separation differences from 
pixel centers. 

Maintaining Geometric: Properties of Displayed Objects 

When we convert geometric descriptions of objects into pixel representations, we 
transform mathematical points and lines into finite screen arras. If we are to 
maintain the original geomehic measurements specified by the input coordinates 



Figure 3-31 
Line path and corresponding pixel 
display for input screen grid 
endpoint coordinates (20,lO) and 
(30,18). 

for an object, we need to account for the finite size of pixels when we transform 
the object definition to a screen display. 

Figure 3-31 shows the line plotted in the Bmenham line-algorithm example 
of Section 3-2. Interpreting the line endpoints (20, 10) and (30,18) as precise grid 
crossing positions, we see that the line should not extend past screen grid posi- 
tion (30, 18). If we were to plot the pixel with screen coordinates (30,181, as in the 
example given in Section 3-2, we would display a line that spans 11 horizontal 
units and 9 vertical units. For the mathematical line, however, Ax = 10 and Ay = 
8. If we are addressing pixels by their center positions, we can adjust the length 
of the displayed line by omitting one of the endpoint pixels. If we think of scmn 
coordinates as addressing pixel boundaries, as shown in Fig. 3-31, we plot a line 
using only those pixels that are "interior" to the line path; that is, only those pix- 
els that are between the line endpoints. For our example, we would plot the leh- 
most pixel at (20, 10) and the rightmost pixel at (29,17). This displays a line that 

F i p r e  3-32 
Conversion of rectangle (a) with verti-es at sawn 
coordinates (0, O), (4, O), (4,3), and (0,3) into display 
(b) that includes the right and top boundaries and into 
display (c) that maintains geometric magnitudes. 

Section 3-10 

Pixel Addressing and Object 
Geometry 



Chapter 3 has the same geometric magnitudes as the mathematical line from (20, 10) to  
Ou~put Primitives (30, 18). 

For an enclosed area, input geometric properties are maintained by display- 
ing the area only with those pixels that are interior to the object boundaries. The 
rectangle defined with the screen coordinate vertices shown in Fig. 3-32(a), for 
example, is larger when we display it filled with pixels u p  to and including the 
border pixel lines joining the specified vertices. As defined, the area of the 
rectangle is 12 units, but as displayed in Fig. 3-32(b), it has an area of 20 units. In 
Fig. 3-32(c), the original rectangle measurements are maintained by displaying 

Figure 3-33 
Circle path and midpoint circle algorithm plot of a circle with radius 5 
in screen coordinates. 

Figure 3-34 
Modification of the circle plot in Fig. 333 to maintain the specified circle 
diameter of 10. 



only the jnternal pixels. The right boundary of the ~nput  rectangle is at r = 4. To Sec'i0n3-11 

maintain this boundary in the display, we set the rightmost pixel grid cwrdinate Prirnitivcs 

at .r = 3. The pixels in this vertical column then span the interval from x = 3 to 1' 
= 4. Similarly, the mathematical top boundary of the rectangle is at y = 3, so we 
set the top pixel row for the displayed rectangle at y = 2. 

These compensations for finite pixel width along object boundaries can be 
applied to other polygons and to curved figures so that the raster display main- 
tains the input object specifications. A circle of radius 5 and center position (10, 
lo), for instance, would be displayed as in Fig. 3 3 3  by the midpoint circle algo- 
rithm using screen grid coordinate positions. But the plotted circle has a diameter 
of 11. To plot the cmle with the defined diameter of 10, we can modify the circle 
algorithm to shorten each pixel scan line and each pixel column, as in Fig. 3-34. 
One way to d o  this is to generate points clockwise along the circular arc in the 
third quadrant, starting at screen coordinates (10, 5). For each generated point, 
the other seven circle symmetry points are generated by decreasing the 1' coordi- 
nate values by 1 along scan lines and decreasing the y coordinate values by 1 
along pixel culumns. Similar methods are applied in ellipse algorithms to main- 
tain the specified proportions in the display of an ellipse. 

3-1 1 
FILLED-AREA PRIMITIVES 

I\ standard output primitive in general graphics packages is a solid-color or pat- 
terned polygon area. Other kinds of area primitives are sometimes available, but 
polygons are easier to process since they have linear boundaries 

There are two basic approaches to area filling on raster systems. One way to 
fill an area is to determine the overlap mtervals for scan lines that cross the area. 
Another method for area filling is to start from a given interior position and paint 
outward from this point until we encounter the specified boundary conditions. 
The scan-line approach is typically used in general graphics packages to fill poly- 
gons, circles, ellipses, and other simple curves. All methods starting from an inte- 
rior point are useful with more complex boundaries and in interactive painting 
systems. In the following sections, we consider n~etliods for solid fill of specified 
areas. Other fill options are discussed in Chapter 4. 

St an-Lint. Polygon F i l l  Algorithm 

Figure 3-35 illustrates the scan-line procedure for soha tilling of polygon areas. 
For each scan line crossing a polygon, the area-fill algorithm locates the intersec- 
tion points of the scan line with the polygon edges. These intersection points are 
then sorted from left to right, and the corresponding frame-buffer positions be- 
tween each intersection pair are set to the specified fill color. In the example of 
Fig. 3-35, the four pixel intersection positions with the polygon boundaries define 
two stretches of interior pixels from x = 10 to x = 14 and from x = 18 to x = 24. 

Some scan-line intersections at polygon vertices require special handling. A 
scan line passing through a vertex intersects two edges at that position, 
adding two points to the list of intersections for the scan line. Figure 3-36 shows 
two scan lines at positions y and y'  that intersect edge endpoints. Scan line y in- 
tersects five polygon edges. Scan line y', however, intersects an even number of 
edges although it also passes through a vertex. Intersection points along scan line 



Output Primitives 

Figure 3-35 
Interior pixels along a scan line 
passing through a polygon area 

y' correctly identify the interior pixel spans. But with scan line y, we need to d o  
some additional processing to determine the correct interior points. 

The topological difference between scan line y and scan line y '  in Fig. 3-36 is 
identified by noting the position of the intersecting edges relative to the scan line. 
For scan line y, the two intersecting edges sharing a vertex are on opposite sides 
of the scan line. But for scan line y', the two intersecting edges are both above the 
scan line. Thus, the vertices that require additional processing are those that have 
connecting edges on opposite sides of the scan line. We can identify these vertices 
by tracing around the polygon boundary either in clockwise or counterclockwise 
order and observing the relative changes in vertex y coordinates as we move 
from one edge to the next. If the endpoint y values of two consecutive edges mo- 
notonically increase or decrease, we need to count the middle vertex as a single 
intersection point for any scan line passing through that vertex. Otherwise, the 
shared vertex represents a local extremum (minimum or maximum) on the polv- 
gon boundary, and the two edge intersections with the scan line passing through 
that vertex can be added to the intersection list. 

Filprrrr 3-36 
Intersection points along scan lines that intersect polygon vertices. Scan 
line y generates an odd number of intersections, but scan line y' 
generals an even number of intersections that can be paired to identify 
correctly the interior pixel spans. 



One way to resolve the question as to whether we should count a vertex as section 3-11 
one intersection or two is to shorten some polygon edges to split those vertices Filled-Area Primitives 
that should be counted as  one intersection. We can process nonhorizontal edges 
around the polygon boundary in the order specified, either clockwise or counter- 
clockwise. As we process each edge, we can check to determine whether that 
edge and the next nonhorizontal edge have either monotonically increasing or 
decreasing endpoint y values. If so, the lower edge can be shortened to ensure 
that only one mtersection point is generated for the scan line going through the 
common vertex joining the two edges. Figure 3-37 illustrates shortening of an 
edge. When the endpoint y coordinates of the two edges are increasing, the y 
value of the upper endpoint for the current edge 1s decreased by 1, as in Fig. 
3-37(a). When the endpoint y values are monotonically decreasing, as in Fig. 
3-37(b), we decrease they coordinate of the upper endpoint of the edge following 
the current edge. 

Calculations performed in scan-conversion and other graphics algorithms 
typically take advantage of various coherence properties of a scene that is to be 
displayed. What we mean by coherence is simply that the properties of one part 
of a scene are related in some way to other parts of the scene so that the relation- 
ship can be used to reduce processing. Coherence methods often involve incre- 
mental calculations applied along a single scan line or between successive scan 
lines. In determining edge intersections, we can set u p  incremental coordinate 
calculations along any edge by exploiting the fact that the slope of the edge is 
constant from one scan line to the next. Figure 3-38 shows two successive scan 
lines crossing a left edge of a polygon. The slope of this polygon boundary line 
can be expressed in terms of the scan-line intersection coordinates: 

Since the change in y coordinates between the two scan lines is simply 

P P 
/ / 

/ / 4% Scan Line y + 1 -+r Scan Line y / I r Scan Lme y - 1 / / d I{ 
(a1 (b) 

Figure 3-37 
Adjusting endpomt I/ values for a polygon, as we process edges in  order 
around the polygon perimeter. The edge currently being processed is 
indicated as a solid line. In (a), they coordinate of the upper endpoint of 
the current edge is decreased by 1. In tb), they coordinate of the upper 
endpoint of the next edge is decreased by 1. 



Chaoler 3 

Output Pr~rnilives 
( x ,  . ,, Y k .  11 A Scan Line y, + 1 

d- \ 
Scan Line y,  

Figrtrc 3-38 
Two successive scan lines 
tntersecting a polygon boundary. 

the x-intersection value xi,, on the upper scan line can be determined from the 
x-intersection value x k  on the preceding scan line as 

Each successive x intercept can thus be calculated by adding the inverse of the 
slope and rounding to the nearest integer. 

An obvious parallel implementation of the fill algorithm is to assign each 
scan line crossing the polygon area to a separate processor. Edge-intersection cal- 
culations are then performed independently. Along an edge with slope m, the in- 
tersection xk value for scan line k above the initial scan line can be calculated as 

In a sequential fill algorithm, the increment of x values by the amount l / n i  
along an edge can be accomplished with integer operations by recalling that thc 
slope m is the ratio of two integers: 

where Ax and Ay are the differences between the edge endpoint x and y coordi- 
nate values. Thus, incremental calculations of x intercepts along an edge for suc 
cessive scan lines can be expressed as 

Using this equation, we can perform integer evaluation of the x intercepts by ini- 
tializing a counter to 0, then incrementing the counter by the value of Ax each 
time we move up  to a new scan line. Whenever the counter value becomes equal 
to or greater than Ay, we increment the current x intersection value by 1 and de- 
crease the counter by the value Ay. This procedure is equivalent to maintaining 
integer and fractional parts for x intercepts and incrementing the fractional part 
until we reach the next integer value. 

As an example of integer incrementing, suppose we have an edge with 
slope rn = 7/3. At the initial scan line, we set the counter to 0 and the counter in- 



Scan. 

Number 

Scen Line yo 

Scan Line y, 

- ----. . - .- - - - -. 
Figu rc 3-39 
A polygon and its sorted edge table, with e d g e m  shorlened by one unit in they 
direction. 

crement to 3. As we move up to the next three scan lines along this edge, the 
counter is successively assigned the values 3, 6,  and 9. On the third scan line 
above the initial scan line, the counter now has a value greater than 7. So we in- 
crement the x-intersection coordinate by 1, and reset the counter to the value 
9 - 7 = 2. We continue determining the scan-line intersections in this way until 
we reach the upper endpoint of the edge. Similar calcutations are carried out to 
obtain intersections for edges with negative slopes. 

We can round to the nearest pixel x-intersection value, instead of truncating 
to obtain integer positions, by modifying the edge-intersection algorithm so that 
the increment is compared to Ay/2. This can be done with integer arithmetic by 
incrementing the counter with the value 2Ax at each step and comparing the in- 
crement to Ay. When the increment is greater than or equal to Ay, we increase the 
x value by 1 and decrement the counter by the value of 2Ay. In our previous ex- 
ample wGh rrr = 7/3,  the counter valucs for the first few scan lines above the ini- 
tial scan line on this edge would now be 6, 12 (reduced to -2), 4, 10 (reduced to 
-4), 2, 8 (reduced to -6), 0, 6, and 12 (reduced to - 2). Now x would be incre- 
mented on scan lines 2, 4, 6, 9, etc., above the initial scan line for this edge. The 
extra calculations required for each edge are 2Ax = dl + Ax and 2Ay = A ~ - +  Ay. 

To efficiently perform a polygon fill, wt can first store the polygon bound- 
ary in a sorted edge table that contains a11 the information necessary to process the 
scan lines efficiently. Proceeding around the edges in either a clockwise or a 
counterclockwise order, we can use a bucket sort to store the edges, sorted on the 
smallest y value of cach edge, in the correct scan-he positions. Only nonhorizon- 
tal edges are entered into the sorted edge table. As the edges are processed, we 
can also shorten certain edges to resolve the vertex-~ntersection question. Each 
entry in the table for a particular scan line contains tht! maximum yvalue for that 
edge, the x-intercept value (at the lower vertex) for the edge, and the inverse 
slope of the edge. For each scan line, the edges are in sorted order from left to 
nght. F~gure 3-39 shows a polygon and the associated sorted edge table. 



Chdpter 3 Next, we process the scan lines from the bottom of the polygon to its top, 
OUIPUI Prlm~tiv~s producing an nrtivr r d ~ c .  lisf for each scan line crossing thc polygon boundaries. 

The active edge list for a scan line contains all edges crossed by that scan line, 
with iterative coherence calculations used to obtain the edge inte;sections. 

Implementation of edge-intersection calculations tan also be facilitated by 
storing Ax and ly values in the sorted edge table. Also, to ensure that we cor- 
rectly fill the interior of specified polygons, we can apply the considerations dis- 
cussed in Section 3-10. For each scan line, we fill in the pixel spans for each pair 
of x-intercepts starting from the leftmost x-intercept value and ending at one po- 
sition before the rightnlost r intercept. And each polygon edge can be shortened 
by one unit in they direction at the top endpoint. These measures also guarantee 
that pixels in adjacent polygons will not overlap each other. 

The following procedure performs a solid-fill scan conversion For an input 
set of polygon vertices. For each scan line within the vertical extents of the poly- 
gon, an active edge list is set up and edge intersections are calculated. Across 
each scan line, the interior fill is then applied between successive pairs of edge 
intersections, processed from left to right.- 

- 

Pinclude "device.h" 

typedef struct tEdge { 
int yupper; 
float xlntersect. dxPerScan; 
struct tEdge * next; 

1 Edge: 

. '  Inserts edge into list in order of increas.ng x1n;essect field. * I  
void insertEdge (Edge ' list, Edge edge) 
{ 

Edge ' p .  ' q = list; 

p = q->next; 
while ( p  ! =  NULL) i 

i f  (edge->xIntersect < p->xIntersectl 
p : NULL; 

else { 
9 = P: 
p = p->next; 

1 
1 
edge->next = q->next; 
q->next = edge; 

1 

: '  For an index, return y-coordinate of next nonhorizontal li?e ' /  
lnt yNext (int k, int cnt, dcPt * p t s )  
i 

int j :  



j++; 
return (pts[j 1 . y) ; 

I 

/ '  Srore lower-y coordiaate and inverse slope for each edge. Adjust 
and store upper-y coordinate for edges that are the lower member 
of a monotonic all\^ ixreasing or decreasing pair of edges '/ 

void makeEdgeRec 
(dcPt lower, dcPt upper, int yComp, Edge ' edge, Edge edges[]) 

( 
edge-~dxPerScan = 

(float)   upper.^ - 1ower.x) / (upper.y - 1ower.y); 
edge-~xIntersect = 1orer.x; 
if (upper.y < yComp) 
edge->yUpper = upper.y - 1: 

else 
edge->yUpper = upper.y; 

insertEdge (edges [lower. yl , edge) ; 
1 

void buildEdgeList (int cnt, dcPt pts, Edge ' edges[]) 
( 
Edge ' edge: 
dcPr vl, v2; 
int i, yPrev = pts[cnt - 21 .Y; 

v1.x = pts[cnt-l1.x; v1.y = ptstcnt-l1.y; 
for (i=O; i<cnt: i++) { 
v2 = ptslil; 
if Iv1.y ! =  v2.y) ( /' nonhorizontal line '/ 
edge = (Zdge * )  malloc (sizeof (Edge)); 
if (v1.y < v2.y) / +  up-going edge * /  
makeEdgeRec (vl, v2, yNext (i, cnt, pts), edge, edges) ; 

else /' down-going edge * /  
mdkeEdgeRec (v2, vl , yPrev, edge, edges) : 

I 

void buildActiveList (int scan, Edge ' active, Edge ' edges[]) 
( 

p = edges[scanl->next; 
while (p)  { 

q = p->next; 
insertEdge (active, p); 
P = g; 

I 
1 

void fillscan lint scan, Edge ' active) 

Edge * pl, p2 ; 
int i; 

pl = active->next; 
while (pl) ( 
p2 = pl->next; 



for i1=pl-,xI1tersect; 1cg2-zx1nterr:ect; - + + )  
setpixel i iintl i, scan); 

pl = p2->next; 

void deleteAfter (Edge ' ql 
( 

q->next = p->next; 
free ( p )  : 

I '  Delete completed edges. Update 'xIntersect' field Eor others ' /  1 ;oid updateActiveList iint scan. Edge activrl 
I while lp) 

if (scan >= p->yUpper) I 
p p->next; 
deleLrAfter iq): 

else ( 
p->x~ntersect = p->xintersect + p->dxPer.;can: 

p = p->next; 

void rescrtActiveList (Edge active) 

Edge q. ' p = active->next : 

active->next : NULL; 
while (p)  ( 

q = p->next; 
insertEdge (active, p); 
P = 9; 

i 
) 

void scanFill (int cnt, dCPt ' pts) 1 ( 
Edge * edges [WINDOW-HEIGHT1 , + actlve; 
inc i. scan; 

for (i=O; icWINCOW-HEIGHT; i + + )  ( 
edgesli] = (Edge ' I  malloc ( s i z e o f  (Edge)); 
edgesiii->next NULL; 

buildEdgeList (cnt, pts, edges); 
active = (Edge ' 1  malloc (sizeof (Edge)); 
active->next = NULL; 

for (scan=O; scan<WINWW-HEIGHT; scan++) ( 
buildActiveList (scan, active, edges); 
if (actlve->next) ( 
fillscan (scan, active); 
updateActlveList (scan, active) ; 
ressrtActiveLisc (active); 

I 



1 
/+  Free edge records that have been malloc'ed . . .  ' I  

Inside-Outside Tests 

Area-filling algorithms and other graphics processes often need to identify inte- 
rior regions of objects. So far, we have discussed area filling only in terms of stan- 
dard polygon shapes. In elementary geometry, a polygon is usually defined as 
having no self-intersections. Examples of standard polygons include triangles, 
rectangles, octagons, and decagons. The component edges of these objects are 
joined only at the vertices, and otherwise the edges have no common points in 
the plane. Identifying the interior regions of standard polygons is generally a 
straightforward process. But in most graphics applications, we can specify any 
sequence for the vertices of a fill area, including sequences that produce intersect- 
ing edges, as in Fig. 3-40. For such shapes, it is not always clear which regions of 
the xy plane we should call "interior" and which regions we should designate as 
"exterio!" to the object. Graphics packages normally use either the odd-even rule 
or the nonzero winding number rule to identify interior regions of an object. 

We apply the odd-even rule, also called the odd parity rule or the even- 
odd rule, by conceptually drawing a line from any position P to a distant point 
outside the coordinate extents of the object and counting the number of edge 
crossings along the line. If the number of polygon edges crossed by this line is 
odd, then P is an interior point. Otherwise, P is an exterior point. To obtain an ac- 
curate edge count, we must be sure that the line path we choose does not inter- 
sect any polygon vertices. Figure 340(a) shows the interior and exterior regions 
obtained from the odd-even rule for a self-intersecting set of edges. The scan-line 
polygon fill algorithm discussed in the previous section is an example of area fill- 
ing using the odd-even rule. 

Another method for defining interior regions is the nonzero winding num- 
ber rule, which.counts the number of times the polygon edges wind around a 
particular point in the counterclockwise direction. This count is called the wind- 
ing number, and the interior points of a two-dimensional object are defined to be 

Nonnm W~nding Number Rule 

Ibl 

Figure 3-40 
Identifying interior and exterior regions for a self-intersecting polygon. 



Chapter 3 those that have a nonzeru value for the winding number. We apply the nonzero 
Output Prtmitives winding number rule to polygons by initializing the winding number tu C and 

again imagining a line drawn from any position P to a distant point bcjoi ... the 
coordinate extents of the object. The line we choose must not pass through any 
vertices. As we move along the line from position P to the distant point, we count 
the number of edges that cross the line in each direction. We add 1 to the winding 
number every time we intersect a polygon edge that crosses the line from right to 
left, and we subtract 1 every time we intersect an edge that crosses from left to 
right. The final value of the winding number, after all edge crossings have been 
counted, determines the relative position of P. If the winding number is nonzero, 
P is defined to be an interior point. Otherwise, P is taken to be an exterior point. 
Figure 3-40(b) shows the interior and exterior regions defined by the nonzero 
winding number rule for a self-intersecting set of edges. For standard polygons 
and other simple shapes, the nonzero winding number rule and the odd-even 
rule give the same results. But for more complicated shapes, the two methods 
may yield different interior and exterior regions, as in the example of Fig. 3-40. 

One way to determine directional edge crossings is to take the vector cross 
product of a vector u along the line from P to a distant point with the edge vector 
E for each edge that crosses the line. If the z'component of the cross product 
u X E for a particular edge is positive, that edge crosses from right to left and we  
add 1 to the winding number. Otherwise, the edge crosses from left to right and 
we subtract 1 from the winding number. An edge vector is calculated by sub- 
tracting the starting vertex position for that edge from the ending vertex position. 
For example, the edge vector for the first edge in the example of Fig. 3-40 is 

where V, and V, represent the point vectors for vertices A and B. A somewhat 
simpler way to compute directional edge cmssings is to use vector dot products 
instead of cross products. To d o  this, we set up  a vector that is perpendicular to u 
and that points from right to left as we look along the line from P in the direction 
of u. If the components of u are (u, ,  u,), then this perpendicular to u has compo- 
nents (-u,, u,) (Appendn A). Now, if the dot product of the perpendicular and 
an edge vector is positive, that edge crosses the line from right to left and we add  
1 to the winding number. Otherwise, the edge crosses the h e  from left to right, 
and we subtract 1 from tho winding number. 

Some graphrcs packages use the nonzero wind~ng number rule to ~mple-  
ment area filling, since it is more versatile than the odd-even rule. In general, ob- 
jects can be defined with multiple, unconnected sets of vertices or disjoint sets of 
closed curves, and the direction specified for each set can be used to define the 
interior regions of objects Exanlples include characters, such as letters of the nl- 
phabet and puwtuation symbols, nested polygons, and concentric circles or el- 
lipses. For curved lines, the odd-even rule is applied by determining intersec- 
tions with the curve path, instead of finding edge intersections. Sin~ilarly, w ~ t h  
the nonzero winding number rule, we need to calculate tangent vectors to the 
curves at the crossover intersection points with the line from position P. 

Scan-Line Fil l  of Cirrvtd Bnunr1~1.y Arens 

In general, scan-line f i l l  n! regions with curved boundarie requires more work 
than polygon filling, siwe intersection calculationi now involve nonlinear 
boundaries. For simple curves such as circles or ellipses, perform~ng a scan-line 
fill is a straightforward process. We only need to calculate the two scan-line Inter- 



sections ~n opposite sides ot the curve. This is the same as generating pixel posi- 
tions along the curve boundary, and we can do that with the midpoint method- 
Then we simply fill in the horizontal pixel spans between the boundary points on 
opposik sii'es of the curve. Symmetries between quadrants (and between octants 
for circles) are used to reduce the boundary calculations. 

Similar methods can be used to generate a fill area for a curve section. An 
elliptical arc, for example, can be filled as in Fig. 341. The interior region is 
boundd by the ellipse section. and a straight-line segment that closes the curve 
by joining the begmnmg and-ending positions of the arc. Symmetries and incre- 
mental calculations are exploited whenever possible to reduce computations. 

Boundary-Fill Algorithm 

Another approach to area filling is to start at a point inside a region and paint the 
interior outward toward the boundary. If the boundary is specified in a single 
color, the fill algorithm proceeds outward pixel by pixel until the boundary color 
is encountered. This method, called the boundary-till algorithm, is particularly 
useful in interactive painting packages, where interior points are easiiy selected. 
Using a graphics tablet or other interactive device, an artist or designer can 
sketch a figure outline, select a fill color or pattern from a color menu, and pick 
an interior point. The system then paints the figure interior. To display a solid 
color region (with no border), the designer can choose the fill color to be the same 
as the boundary color. 

A boundary-fill procedure accepts as input the coordinates of an interior 
point ( x ,  y), a fill color, and a boundary color. Starting from (x, y), the procedure 
tests neighboring positions to determine whether they are of the boundary color. 
If not, they are painted with the fill color, and their neighbors are tested. This 
process continues until all pixels up to the boundary color for the area have been 
tested. Both inner and outer boundaries can be set up to specify an area, and 
some examples of defining regions for boundary fill are shown in Fig. 3-42. 

Figure 3-43 shows two methods for proceeding to neighboring pixels from 
the current test position. In Fig. 343(a), four neighboring points are tested. These 
are the pixel positions that are right, left, above, and below the current pixel. 
Areas filled by this method are called konnected. The second method, shown in 
Fig. 3-43(b), is used to fill more complex figures. Here the set of neighboring posi- 
tions to be tested includes the four diagonal pixels. Fill methods using this ap- 
proach are called &connected. An 8conneded boundary-fill algorithm would 
correctly fill the interior of the area defined in Fig. 3-44, but a 4-connected bound- 
ary-fill algorithm produces the partial fill shown. 

Figrirc 3-42 
Example color boundaries for a boundary-fill procedum. 

Fipre  3-41 
Interior fill of an elliptical arc 

Figure 3-43 
Fill methods applied to a 
4-connected area (a) and to an 
8-connected area (b). Open 
circles represent pixels to be 
tested from the current test 
position, shown as a solid 
color 

1 2 7  



Chapter 3 The following procedure illustrates a recursive method ror filling a 4- 
Output Primitives connected area with an intensity specified in parameter f ill u p  to a boundary 

color specified with parameter boundary. We can extend this procedure to fill an 
Sconnected -ion by including four additional statements to test diagonal 
positions, such i s  ( x  + 1, y + 1). 

void boundaryFill4 (int x, int y, int fill, int boundary) 
( 
int current: 

current = getpixel (x, y); 
if ((current ! =  boundary) && (current ! =  fill)) { 
setcolor (fill) ; 
setpixel (x, y): 
boundary~ill4 (x+l, y, fill, boundary); 
boundaryFill4 (x-1, y, fill, boundary) : 
boundaryFill4 (x, y+l, fill, boundary); 
boundaryFill4 ( x ,  y-1, fill, boundary) ; 

) 
1 

Recursive boundary-fill algorithms may not fill regions correctly if some in- 
terior pixels are already displayed in the fill color. This occurs because the algo- 
rithm checks next pixels both for boundary color and for fill color. Encountering 
a pixel with the fill color can cause a recursive branch to terminate, leaving other 
interior pixels unfilled. To avoid this, we can first change the color of any interior 
pixels that are initially set to the fill color before applying the boundary-fill pro- 
cedure. 

Also, since this procedure requires considerable stacking of neighboring 
points, more efficient methods are generally employed. These methods fill hori- 
zontal pixel spans across scan lines, instead of proceeding to 4-connected or 
8-connected neighboring points. Then we need only stack a beginning position 
for each horizontal pixel span, instead of stacking all unprocessed neighboring 
positions around the current position. Starting from the initial interior point with 
this method, we first fill in the contiguous span of pixels on this starting scan 
line. Then we locate and stack starting positions for spans on the adjacent scan 
lines, whew spans are defined as the contiguous horizontal string of positions 

Star! Position 

(a l  

- - .  - 
Figure 3-44 
The area defined within the color boundan (a) is only 
partially filled in (b) using a 4-connected boundary-fill 
algorithm. 



Filled Pixel Spans Stacked Positions 

Figme 3-45 
Boundary fill across pixel 
spans for a 4-connected area. 
(a) The filled initial pixel 
span, showing the position of 
the initial point (open circle) 
and the stacked positions for 
pixel spans on adjacent scan 
lines. (b) Filled pixel span on 
the first scan line above the 
initial scan Line and the 
current contents of the stack. 
(c) Filled pixel spans on the 
first two scan lines above the 
initial xan line and the 
current contents of the stack. 
(d) Completed pixel spans for 
the upper-right portion of the 
defined region and the 
remaining stacked positions 
to be processed. 



Chapter 3 bounded by pixels displayed in the area border color. At each subsequent step, 
Output Primitives we unstack the next start position and repeat the process. 

An example of how pixel spans could be filled using this approach is illus- 
trated for the 4-connected fill region in Fig. 3-45. In this example, we first process 
scan lines successively from the start line to the top boundary. After all upper 
scan lines are processed, we fill in the pixel spans on the remaining scan lines in 
order down to the bottom boundary. The leftmost pixel position for each hori- 
zontal span is located and stacked, in left to right order across successive scan 
lines, as shown in Fig. 3-45. In (a) of this figure, the initial span has been filled, 
and starting positions 1 and 2 for spans on the next scan lines (below and above) 
are stacked. In Fig. 345(b), position 2 has been unstacked and processed to pro- 
duce the filled span shown, and the starting pixel (position 3) for the single span 

. . .  on the next scan line has been stacked. After position 3 is processed, the filled 
spans and stacked positions are as shown in Fig. 345(c). And Fig. 3-45(d) shows 
the filled pixels after processing all spans in the upper right of the specified area. 

Figure 3-46 Position 5 is next processed, and spans are filled in the upper left of the region; 
An area defined within then position 4 is pcked u p  to continue the processing for the lower scan lines. 
multiple color boundaries. 

Flood-Fill Algorithm 

Sometimes we want to fill in (or recolor) an area that is not defined within a sin- 
gle color boundary. Figure 3-46 shows an area bordered by several different color 
regions. We can paint such areas by replacing a specified interior color instead of 
searching for a boundary color value. This approach is called a flood-fill algo- 
rithm. We start from a specified interior point ( x ,  y) and reassign all pixel values 
that are currently set to a given interior color with the desired fill color. If the area 
we want to paint has more than one interior color, we can first reassign pixel val- 
ues so that all interior points have the same color. Using either a Cconnected or 
8-connected approach, we then step through pixel positions until all interior 
points have been repainted. The following procedure flood fills a konnected re- 
gion recursively, starting from the input position. 

vo iQf loodFi l l4  ( i n t  x ,  i n t  y ,  i n t  f i l l c o l o r ,  i n t  o l d c o l o r )  
f 

i f  ( g e t p i x e l  (x .  y )  = =  o l d c o l o r )  ( 
s e t c o l o r  ( f i l l c o l o r ) ;  
s e t p i x e l  ( x ,  y ) :  
floodFill4 ( x + l ,  y ,  f i l lColor ,  o ldColor ) :  
f loodf i l l4  (x-1 ,  y ,  f i l l c o l o r ,  o l d c o l o r ) ;  
floodPill4 ( x ,  y + l ,  f i l l c o l o r ,  o l d c o l o r ) ;  
floodFill4 ( x ,  y-1 ,  f i l lColor ,  o l d c o l o r ) ;  

1 

We can modify procedure f loodFill4 to reduce the storage requirements 
of the stack by filling horizontal pixel spans, as discussed for the boundary-fill al- 
gorithm. In this approach, we stack only the beginning positions for those pixel 
spans having the value oldcolor . The steps in this modified flood-fill algo- 
rithm are similar to those illustrated in Fig. 3 4 5  for a boundary fill. Starting at 
the first position of each span, the pixel values are replaced until a value other 
than oldcolor is encountered. 



3-1 2 Section 3-1 2 -- 
FILL-AREA FUNCTIONS FIl-Arca Funct~ons 

We display a filled polygon in PHlGS and GKS wirh the function 

fillArea ( n ,  wcvertices) 

The displayed polygon area is bounded by a series of n straight line segments 
connecting the set of vertex positions specified in w c v e r t i c e s .  These packages 
d o  not provide fill functions for objects with curved boundaries. 

lmplementntion of the f i l l A r e a  function depends on the selected type of 
interior fill. We can display the polygon boundary surrounding a hollow interior, 
or we can choose a solid color or pattern fill with no border for the display of the 
polygon. For solid fill, the f i l l A r e a  function is implemented with the scan-line 
fill algorithm to display a single color area. The various attribute options for dis- 
playing polygon fill areas in I'HlGS are discussed In the next chapter. 

Another polygon primitive available in PHlGS is f i l l A r e a S e t .  This func- 
t ~ o n  allows a series of polygons to be displayed by specifying the list of,vertices 
for each polygon. Also, in other graphics packages, functions are often provided 
for displaying a variety of commonlv used fill areas besides general polygons. 
Some examples are f i l l R e c t a n g l e ,  f i l l C i r c l e ,  f i l l C i r c l e A r c ,  f i l l -  
E l l i p s e , a n d  f i l L E l l i p s e A r c .  

- ~- 

C E L L  ARRAY 

The cell array is a pnmitive that allows users to display an arbitmq shape de- 
fined as a two-dimensional grid pattern. A predefined matrix of color values is 
mapped by this function onto a specified rectangular coordinate region. The 
PHIGS wrsion of this function is 

ivhere co;ornrrcly is the n by m matrix of integer color values and w c p o i n t s  
lists the limits of the rectangular coordinate region: (xmin, ymn) and ix,,,, y,,,). 
Figi~re 3-47 shows the distribution of the elements of the color matrix over the CQ- 
ordinate rectangle. 

Each coordinate cell in  Flg. 3-47 has width (x,,, - x , , ) / n  and height 
(ymax - yn,,,J/m. Pixel color values are assigned according to the relative positions 
of the pixel center coordinates I f  the center of a pixel lies within one of the n by m 
coordinate cells, that pixel is assigned the color of the corresponding element in 
the matrix c o l o r A r r a y .  

3-1 4 
(:HAKA('TEK GENERATION 

Letters, numbers, and other characters can be displilyed in a variety of sizes and 
stvles. The overall design style for a set (or family) of characters is called a type- 



Figure 3-47 
Mapping an n by m -11 array into a rectangular coordinate region. 

face. Today, there are hundreds of typefaces available for computer applications. 
Examples of a few common typefaces are Courier, Helvetica, New York, Palatino, 
and Zapf Chancery. Originally, the term font referred to a set of cast metal char- 
acter forms in a particular size and format, such as 10-point Courier Italic or 12- 
point Palatino Bold. Now, the terms font and typeface are often used inter- 
changeably, since printing is no longer done with cast metal forms. 

Typefaces (or fonts) can be divided into two broad groups: m'f and sans 
serif. Serif type has small lines or accents at the ends of the main character 
strokes, while sans-serif type does not have accents. For example, the text in this 
book is set in a serif font (Palatino). But this sentence is printed in a sans-serif font 
(Optima). Serif type is generally more readable; that is, it is easier to read in longer 
blocks of text. On the other hand, the individual characters in sans-serif type are 
easier to rpcognize. For this reason, sans-serif type is said to be more legible. Since 
sans-serif characters can be quickly recognized, this typeface is good for labeling 
and short headings. 

Two different representations are used for storing computer fonts. A simple 
method for representing the character shapes in a particular typeface is to use 
rectangular grid patterns. The set of characters are then referred to as a bitmap 
font (or bitmapped font). Another, more flexible, scheme IS to describe character 
shapes using straight-line and curve sections, as in PostScript, for example. In 
this case, the set of characters is called an outline font. Figure 3-48 illustrates the 
two methods for character representation. When the pattern in Fig. 3-48(a) is 
copied to an area of the frame buffer, the 1 bits designate which pixel positions 
are to be displayed on the monitor. To display the character shape in Fig. 3-48(b), 
the interior of the character outline must be filled using the scan-lime fill proce- 
dure (Sedion 3-11). 

Bitmap fonts are the simplest to define and display: The character grid only 
needs to be mapped to a frame-buffer position. In general, however, bitmap fonts 



require more space, because each variation (size and format) must be stored in a 
font cache. It is possible to generate different sizes and other variations, such as 
bold and italic, from one set, but this usually does not produce good results. 

In contrast to bitrnap fonts, outline fonts require less storage since each vari- 
ation does not require a distinct font cache. We can produce boldfae, italic, or 
different sizes by n~anipulating the curve definitions for the character outlines. 
But it does take more time to process the outline fonts, because they must be scan 
converted into the frame buffer. 

A character string is displayed in PHIGS with the following function: 

text (wcpoint, string) 

Parameter string is assigned a character sequence, which is then displayed at 
coordinate position wcpoint = (x, y). For example, the statement 

text (wcpoint, "Popula~ion Distribution") 

along with the coordinate specification for wcpoint., could be used as a label on 
a distribuhon graph. 

Just how the string is positioned relative to coordinates (x, y) is a user o p  
tion. The default is that (x, y) sets the coordinate location for the lower left comer 
of the first character of the horizontal string to be displayed. Other string orienta- 
tions, such as vertical, horizontal, or slanting, are set as attribute options and will 
be discussed in the next chapter. 

Another convenient character function in PHIGS is one that pIaces a desig- 
nated character, called a marker symbol, at one or more selected positions. This 
function is defined with the same parameter list as in the line function: 

polymarker (n, wcpoints) 

A predefined character is then centered at each of the n coordinate positions in 
the list wcpoints. The default symbol displayed by polymarker depends on the 

Section 3-14 

Character Generation 

Figure 3-48 
The letter B represented in (a) with an 8 by 8 bilevel bihnap pattern and 
in (b) with an outliie shape defined with straight-line and curve 
segments. 



41- 94 
59 43 
85 74 
110 59 

50 121 89 
149 122 

Figure 3-49 
---+ x Sequence of data values plotted 

with thepol marker  function. 

particular imp!ementatio~~, but we assume for now that an asterisk is to be used. 
Figure 3-49 illustrates plotting of a data set with the statement 

polymarker (6 ,  w r p o i n t s )  

SUMMARY 

The output primitives discussed in this chapter provide the basic tools for con- 
structing pictures with straight lines, curves, filled areas, cell-amay patterns, and 
text. Examples of pictures generated with these primitives are given in Figs. 3-50 
and 3-51. 

Three methods that can be used to plot pixel positions along a straight-line 
path are the DDA algorithm, Bresenham's algorithm, and the midpoint method. 
For straight lines, Bresenham's algorithm and the midpoint method are identical 
and are the most efficient Frame-buffer access in these methods can also be per- 
formed efficiently by incrementally calculating memory addresses. Any of the 
line-generating algorithnij can be adapted to a parallel implementation by parti- 
tioning line segments. 

Circles and ellipse can be efficiently and accurately scan converted using 
midpoint methods and taking curve symmetry into account. Other conic sec- 
tions, parabolas and hyperbolas, can be plotted with s~milar methods. Spline 
curves, which are piecewise continuous polynomials, are widely used in design 
applications. Parallel implementation of curve generation can be accomplished 
by partitioning the curve paths. 

To account for the fact that displayed l ~ n e s  and curves have finite widths, 
we must adjust the pixel dimensions of objects to coincide to the specified geo- 
metric dimensions. This can be done with an addressing scheme that references 
pixel positions at their lower left corner, or by adjusting line lengths. 

Filled area primitives in many graphics packages refer to filled polygons. A 
common method for providing polygon fill on raster systems is the scan-line fill 
algorithm, which determines interior pixel spans across scan lines that intersect 
the polygon. The scan-line algorithm can also be used to fill the interior of objects 
with curved boundaries. Two other methods for filling the interior regions of ob- 
jects are the boundary-fill algorithm and the flood-fill algorithm. These two fill 
procedures paint the interior, one pixel at a time, outward from a specified inte- 
rior point. 

The scan-line fill algorithm is an example of fillirg object interiors using the 
odd-even rule to locate the interior regions. o ther  methods for defining object in- 
teriors are also w f u l ,  particularly with unusual, self-intersecting objects. A com- 
mon example is the nonzero winding number rule. This rule is more flexible than 
the odd-even rule for handling objects defined with multiple boundaries. 



Figure 3-50 
A data plot generated with straight 
line segments, a curve, ci\rcles (or 
markers), and text. ? h f t e s y  of 
Wolfmrn h r c h ,  Inc., The M a h  of 
M a l h t i c a . J  

Additional primitives available in graphics packages include cell arrays, 
character strings, and marker symbols. Cell arrays are used to define and store 
color patterns. Character strings are used to provide picture and graph labeling. 
And marker symbols are useful for plotting the position of data points. 

Table 3-1 lists implementations for some of the output primitives discussed 
in this chapter. 

TABLE 3-1 
OUTPUT PRIMITIVE IMPLEMENTATIONS 

typedef struct ( float x ,  y; ) wcPt2; 
Defines a location in 2-dimensional world-coordinates. 

ppolyline tint n, wcPt2 pts) 
Draw a connected sequence of n-1 line segments, specified in pts . 

pCircle (wcPt2 center, float r) 
Draw a circle of radius r at center. 

ppillarea (int n, wcPt2 pts) 
Draw a filled polygon with n vertices, specified in pts . 

pCellArray (wcPt2 pts, int n, int m,'int colors) 
Map an n by m array of colors onto a rectangular area defined by pts . 

pText (wcPt2 position, char ' txt) 
Draw the character string txt at position 

Figure 3-51 
An electrical diagram drawn 
with straight line sections, 
circle., filled rectangles, and 
text. (Courtesy oJ Wolfram 
Rcsmrch, Inc., The h4aker of 
rnthonrrtia7.J 

ppolymarker (int n, wcPt2 pts) 

Draw a collection of n marker svmbols at pts. 



Output Primitives 
Here, we p m t  a few example programs illustrating applications of output 
primitives. Functions listed in Table 3-1 are defined in the header file graph- 
ics. h, along with the routines openGraphics, closeGraphics, setcolor, 
andsetBackground. 

The first program produces a line graph for monthly data over a period of 
one year. Output of this procedure is drawn in Fig. 3-52. This data set is also used 
by the second program to produce the bar graph in Fig. 3-53 . 

(include <stdio.h> 
(include 'graphica.h' 

Cdehne WINM)W-WIDTH 6 0 0  
#define WINDOWNDOWHEIGHT 500 
/ *  Ainount of space to leave on each side of the chart * /  
#define MARGIN-WIDTH 0 , 0 5  ' WINDOW-WIDTH 
#define N-DATA 12 

typedef enum 
( Jan. Feb. Mar. Apr, May. Jun. Jul, Aug, Sep, Oct, Nov, Dec ) Months: 

char monthNames[N-DATA] = ( 'Jan'. 'Feb', 'Mar', 'Apr', 'May', 'JUn', 
'Jul'. 'Aug', 'sep'. 'Oct', 'NOV', 'Dec" ) ;  

int readData (char infile, float + data) 
I 
int fileError = FALSE; 
FILE ' fp;  
Months month: 

if ( ( f p  = fopen (inFiie. 'r')) == NULL) 
fileError = TRUE; 

else ( 
for (month = Jan: month <= Dec: month++) 

fscanf ( f p ,  '%fa, Ldatalmonthl); 
â€¢â€™close (fp) : 

return 4fileError) ; 
i 

void linechart (floclt ' data) 
[ 
wcPt2 dataPos[N-DATA], labelpos; 
Months in: 
float mWidth = (WINDOW-WIDTH - 2 MARGIN-WIDTH) / N-DATA; 
int chartBottom = 0.1 WINDOW-HEIGHT; 
int offset = 0 . 0 5  WINLXX-HEIGHT; / ' Space between data and labels ' i  
int labellength = 24: / ' Assuminq bed-width 8-pixel characters '/ 

1abelPos.y = chartBottom: 
for (m = Jan; m c =  Dec; m++) 1 

/ *  Calculate x and y positions for data markers * /  
dataPosIm1.x = MARGIN-WIDTH + m mWidth + 0.5 ' mwidth: 
dataPoslm1.y = chartBottom + offset + data(m1; 
/ *  Sh;ft the label to the left by one-half its length * /  
1abelPos.x = dataPos1ml.x - 0 . 5  ' labellength; 
pText (labelPos, monthNames[ml): 

1 
ppolyline (N-DATA, dataPosl ; 
ppolymarker (N-DATA, datapos); 



Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec 

Figrrre 3-52 
A line plot of data points output by 
the linechart procedure. 

I void main lint argc. char * *  argvi 
float data[N-DATA]; 
int dataError = FALSE; 
long windowID; 

if (argc < 2 )  ( 
fprintf (stderr, 'Usage: 0s dataFileName\n". argv[Ol); 
exit 0 ;  

) 
dataError : readhta largvlll, data); 
if (dataError) ( 
fprintf (stderr. "0s error. Can't read file %s\n'. argv[ll); 
exit 0; 

windowID = openGraphics ('argv, WINDOW-WIDTH. WINDOWJEIGHT): 
setEackground (WHITE1 ; 
setcolor (BLACK): 
linechart (data) ; 
sleep (10); 
closeGraphics (windowID); 

1 

void barchart (float ' data) I {  wcPt2 dataPos[41, labelpas; 
Months m; 
float x, mWidth = (WINDOW-WIDTH - 2 MARGIN-WIDTH) / N-DATA; 
int chdrtBotLom = 0.1 WINDOW-HEIGHT; 
int offset = 0.05 WINDOw-HEIGHT; / *  Space between data and labels * /  
int labelLength = 24; /' Assuming futed-width 8-pixel characters * I  

1abelPos.y = chartBottom; 
for (m = Jan; m <= Dec; m++) { 

/ '  Find the center of this month's bar ' /  
x = MARGIN-WIDTH + rn mWidth + 0.5 mWidth; 

/ '  Shift the label to the left by one-half its assumed length * I  
1abelPos.x = x - 0.5 labellcngth; 



- - 

Jan Feb Mar Apr M a y  JG ~i Aug Sep Oct Nov & 

F i p r c  1-53 
A bar-chart plot output by the 
barchart procedure. 

i' Get the coordinates for this month's bar ' I  
dataFosi0l.x = dataPos:3l.x = x - 0.5 ' lsbellength; 
dataFos[ll.x = daraPosl2l.x = x + 0.5 la3eiLengch; 
dataFos[Ol.y = dataPosll1.y = chartBottom offset; 
dataFosI2l.y = dataPosl3].y = charcBottom t offset + datalm]; 
pFillArea 1 4 ,  dataPos) ; 

1 

Pie charts are used to show the percentage contribution of individual parts 
to the whole. The next procedure constructs a pie chart, M ith the number and rel- 
ative size of the slices determined by input. A sample output from this procedure 
appears in Fig. 3-54. 

void pieCharc (float * data) 
( 

wcPt 2 pts [ 2 I , center; 
float rajius = WINDOW-HEIGHT / 4.0; 
float ne.&lice, total : 0.0, lastslice = 0 0. 
Months month; 

cente1.x = WIND3d-WIDTH / 2; 
 center.^ = WINCOX-HEIGHT / 2: 
pCircle Icentrr, radius) ; 
for (month = .Jan; month <= Dec; manch+t) 
total +: data[nonth]; 

ptsl0l.x =  center.^; ptsl0l.y =  center.^; 
for (month = Jan; month <= Dec; montht+) ( 
newSi~ce = TWO-PI ' dataImonth1 / tocal 4 lastslice; 
ptsil1.x =  center.^ + radius ' cosf (newS1ic.-): 
ptsIl1.y = center.y + radius * sinf (newS11ce); 
ppolyline ( 2 ,  pts): 
lastslice = ne;uSlice; 

) 
I 



Some variations on the circle equations are output by this next procedure. 
The shapes shown in Fig. 3-55 are generated by varying the radius r of a circle. 
Depending on how we vary r, we can produce a spiral, cardioid, limaqon, or 
other similar figure 

#defule TWO-PI 6.28 

/ -  Limacon equation is r = a * ccsitheta) b. Cardioid is the same, 
with a == b, so r : a * (1 t cos{theta)! 

, 
~ypedef e n m  ( spiral, cardioid, threeleat. fourleaf, limacon Fig; 

void drawCurlyFig (Fig figure, wcPt2 pos, int ' p) 
( 
float r, theta = 0.0. dtheta = 1.0 / (float) p101; 
int nPoints -- (int) ceilf (TWO-PI ' p1011 + 1; 
wcPt2 ' p:; 

i t  ((pt = ,wcPt2 ' )  malloc (nPolnts ' size05 (wcPt2))) =: NULL) ( 
fprlnt f (stderr, "Couldn't allocace pc:;ts\nW) : 
re:urn, 

I 

/ *  Set R r s r  point for figure ' /  
pt[Ol . y  = p0s.y; 
switch (figure) ( 
rasespiral: pt[O].x=pos.x; break; 
case limacon: pt[Ol.x = p0s.x p[01 +  ill; break; 
case cardioid: pt[Ol .x = p0s.x t p[O] ' 2 ;  break; 
case threeleaf: pt[Ol .x = pos.x + p(O1: break; 
cascfourLeaf: pr[Ol.x=pos.x+.pIOl; break; 
) 

npoixts = i; 
while (theta < TWO-PI) { 

switch (figure) { 
case spiral: r = p[Ol ' tketa; break; 
casc lirnacon: r = p[Ol ' cosf (theta) + p[ll; break; 
case cardioid: r = p101 (1 + cosf (tteta)); break; 
case threeleaf: r = p101 ' cosf ( 3  . theta): break; 
case fourleaf: r = p[01 ' cosf I 2  * theta); break; 
1 
pt[nPointsl.x = p0s.x t r ' cosf (thrtd,: 
ptlnPoints1.y = p0s.y + r * sinâ€¢â€™ (theta), 
nPainrs++; 
theta += dtheta: 

1 

ppolyllne (nroints. pt) : 
free (pt); 

1 

void main (int argc, char *. arp) 
( 



Figure 3-55 
Curved figures produced with the drawshape procedure. 

Figure 3-54 
Output generated from the 
piechart procedure. 

long windowID = openGraphics ('argv, 400, 1001; 
Fig f; 
/ *  Center positions for each fqure '/ 
wcPt2 centerll = ( 50. 50. 100, 50, 175. 50, 250, 50, 300, 50 1 ;  

/ +  Parameters ta define each figure. First four need one parameter. 
Fifth figure (limacon) needs two. * /  

int p[5] [2] = ( 5 ,  -1, 20. -1, 30, -1. 30, - 1 ,  40. 10 ) :  

setBackground (WHITE) ; 
setcolor (BLACK); 
for (â€¢â€™=spiral; f<=limacon: f++) 
drawCurlyFig ( f .  centerrf], p[f]); 

sleep 110); 
c:oseGraphics (windowID) ; 

1 

REFERENCES 

Information on Bresenham's algorithms can be found in Brerenham (1965, 1977). For mid- 
point methods, see Kappel (1985). Parallel methods for generating lines and circles are 
discussed in Pang (1 990) and in Wright (1 990). 

Additional programming examples and information on PHIGS primitives can be iound in 
Howard, et al .  (1991), Hopgood and Duce (1991), Caskins (19921, and Blake (1993). For 
information on GKS ourput primitive iunctionr, see Hopgood et al. (1983) and Enderle, 
Kansy, and Pfaff (1 984). 

EXERCISES 

3-1. Implement the polyl ine function using the DDA algorithm, given any number (n) of 
input pants. A single point is to be plotted when n = 1. 

3-2. Extend Bresenham's line algorithm to generate lines with any slope, taking symmetry 
between quadrants into account, Implement the polyline function using this algorithm 
as a routine that displays the set of straight lines connecting the n input points. For 
n = 1, the rowtine displays a single point. 



3-3. Dev~se a consistent scheme for implement~ng the polyline funct~on, for any set of 
input line endpoints, using a modified Bresenhani line algorithm so that geometric trenrises 
magnitudes are maintained (Section 3-1 0). 

3-4. Use the midpoint method to derive decision parameters for generating points along a 
straight-line path with slope in the range 0 < rn < 1 .  Show that the midpoint decision 
parameters are the same as those in the Bresenham line algorithm. 

3-5. Use the midpoint method to derive decision parameters that can be used to generate 
straight line segments with any slope. 

3-6. Set up a parallel version of Bresenham's line algorithm for slopes in the range 0 < m 
< 1. 

3-7. Set up a parallel version of Bresenham's algorithm for straight lines of any slope. 

3-8. Suppose you have a system with an 8-inch by l0.inch video monitor that can display 
100 pixels per inch. If memory is orgamzed in one-byte words, the starting frame- 
buffer address is 0, and each pixel is assigned one byte of storage, what is the frame- 
buffer address of the pixel with screen coordinates (>, v)? 

3-9. Suppose you have a system with an &inch by 10-~nch video monitor that can display 
100 pixels per inch. If memory is organized in one-byte words, the starting frame- 
buffer address is 0, and each pixel is assigned 6 bits of storage, what IS the frame- 
buffer address (or addresses) of the pixel with screen coordinates (x, y)? 

3-10. Implement the s e t p i x e l  routine in Bresenham's l ~ne  algorithm using iterative tech- 
niques for calculating frame-buffer addresses (Section 3-3). 

3-1 1 .  Rev~se the midpoint circle algorithm to display v) that geometric magnitudes are 
maintained (Section 3-10). 

3-1 2 .  Set up a procedure for a parallel implementation of the midpoint circle algorithm. 
3-1 3. Derive decision parameters for the midpoint ell~pse algorithm assuming the start posi- 

tion is (r, ,  0) and points are to be generated along the curve path in counterclockwise 
order. 

3-1 4. Set up a procedure for a parallel implementation of the midpoint ellipse algorithm 
3-1 5. Devise an efficient algorithm that takes advantage of symmetry propertie to display a 

sine function. 

3-16. Dcvisc an efficient algorithm, taking function symmetry into account, to display d plo~ 
of damped harmonic motion: 

y - Ae-" sin (ox'+ 0) 
where w is the angular frequency and 0 is the phase of the sine function. Plot y as a 
function of x for several cycles of Ihe sine function or until the maximum amplitude is 
reduced to A/10. 

3-1 7. Using the midpoint method,-md taking symmetry into account, develop an efficient 
algorithm for scan conversion of the follow~ng curve over the Interval -10 5 x 5 10: 

3-1 8. Use the midmint method and symmetry considerations to scan convert the parabola 

over the interval - 10 I x 5 10. 
1-19. Use the midpoint method and symmetry considerations to scan convert the parabola 

forthe interval -10 5 y 5 10. 



Chapter J 3-20. Set up a midpoint algorithm, taking symmetry considerat~ons into account to scan 
Output Prim~lives convert any parabola of th? form 

with input values for parameters a, b, and the range of u 
3-21.  Write a program to $can convert the interior of a specified ell~pse Into a solid color. 

3-22. Devise an algorithm for determining interior regions for any input set of vertices using 
the nonzero winding number rule and cross-product calculations to identify the direc- 
tion of edge crossings 

3-23. Devise an algor~thm ic~r determ~ning interior regions for any input set of vertices using 
the nonzero winding number rule and dot-product calculations to identify the direc- 
tion of edge crossings. 

3-24. Write a prcedure (01 filling the interior of any specif~cd set of "polygon" vertices 
using the nonzero winding number rule to identify interior regions. 

3-25.  Modily the boundaly-(ill algorithm for a 4-connected region to avoid excessi~e stack- 
ing by incorporating scan-line methods. 

3-26. Write a boundary-fill procedure to fill an 8-connected region. 

3-27. Explain how an ellipse displayed with the midpoint method could be properly filled 
with a boundary-fill algorithm. 

3-28. Develop and mplenent a flood-fill algorithm to fill the interior of any specified area. 

3-29. Write a routine to implement the t e x t  function. 

3-30. Write a routine to implement the polymarker function 

3-31. Write a program to display a bar graph using the p o l y l i n e  function. lnput to the 
program is to include :he data points and thc labeling reqi~ired for the x and y axes. 
The data points are to be scaled by the program so that the graph i s  displayed across 
the full screen area. 

3-32. Write a Drogram to d~splay a bar graph in any selected sclren area. Use the p o l y -  
l i n e  function to draw the bars. 

3-33 Write a procedure to display a line graph lor any input sel ol data points in any se- 
lected area of the scrrtn, with the input dam set scaled to f ~ t  the selected screen area. 
Data points are to be displayed as asterisks joined with straight line segments, and the 
x and y axes are to be labeled according to input speciiica~~ons. (Instead of asterisks. 
small circles or some orher symbols could be used to plot the data points.) 

3-34. Using d c i r c l e  function, write a routine todisplay a ple chart with appropriate label- 
ing. lnput to the routine is to include a data set giving the distribution of the data over 
some set of intervals, the name of the pie chart, and the names of the intervals. Each 
section label IS to be displayed outside the boundary of the pie chart near the corre- 
sponding pie section. 





I n general, any parameter that affects the way a primitive is to be displayed is referred to as an attribute parameter Some attribute parameters, such as 
color and size, determine the fundamental characteristics of a primitive. Others 
specify how the primitive is to be displayed under special conditions. Examples 
of attributes in this class include depth information for three-dimensional view- 
ing and visibility or detectability options for interactive object-selection pro- 
grams. These special-condition attributes will be considered in later chapters. 
Here, we consider only those attributes that control the basic display properties 
of primitives, without regard for special situations. For example, lines can be dot- 
ted or dashed, fat or thin, and blue or orange. Areas might be filled with one 
color or with a multicolor pattern. Text can appear reading from left to right, 
slanted diagonally across the screen, or in vertical columns. Individual characters 
can be displayed in different fonts, colors, and sizes. And we can apply intensity 
variations at the edges of objects to smooth out the raster stairstep effect. 

One way to incorporate attribute options into a graphics package is to ex- 
tend the parameter list associated with each output primitive function to include 
the appropriate attributes. A linedrawing function, for example, could contain 
parameters to set color, width, and other properties, in addition to endpoint coor- 
dinates. Another approach is to maintain a system list of current attribute values. 
Separate functions are then included in the graphics package for setting the cur- 
rent values in the attribute list. To generate an output primitive, the system 
checks the relevant attributes and invokes the display routine for that primitive 
using the current attribute settings. Some packages provide users with a combi- 
nation of attribute functions and attribute parameters in the output primitive 
commands. With the GKS and PHIGS standards, attribute settings are accom- 
plished with separate functions that update a system attribute list. 

4-1 
LINE ATTRIBUTES 

Basic attributes of a straight line segment are its type, its width, and its color. In 
some graphics packages, lines can also be displayed using selected pen or brush 
options. In the following sections, we consider how linedrawing routines can be 
modified to accommodate various attribute specifications. 

Line Type 

Possible selections for the line-type attribute include solid lines, dashed lines, 
and dotted lines. We modify a linedrawing algorithm to generate such lines by 
setting the length and spacing of displayed solid sections along the line path. A 
dashed line could be displayed by generating an interdash spacing that is equal 
to the length of the solid sections. Both the length of the dashes and the interdash 
spacing are often specified as user options. A dotted line can be displayed by 



generating very short dashes with the spacing equal to or greater than the dash 4-1 
size. Similar methods are used to produce other line-type variations. Lme Attributes 

To set line type attributes in a PHICS application program, a user invokes 
the function 

setLinetype ( I t )  
where parameter 1 t is assigned a positive integer value of 1,2,3, or 4 to generate 
lines that are, respectively, solid, dashed, dotted, or dash-dotted. Other values for 
the line-type parameter It could be used to display variations in the dotdash 
patterns. Once the line-type parameter has been set in a PHKS application pro- 
gram, all subsequent line-drawing commands p d u c e  lines with this Line type. 
The following program segment illustrates use of the linetype command to 
display the data plots in Fig. 4-1. 

Winclude <stdio.h> 
#include "graphics.h' 

#define MARGIN-WIDTH 0.05 ' WINDOW-WIDTH 

int readData (char ' inFile, float data) 
( 
int fileError = FALSE; 
FILE fp; 
int month; 

if ((fp = Eopen (inFile, 'r")) == NULL) 
fileError = TRUE; 

else t 
for (month=O; month<l2; month++) 
Escanf (fp, "%f" . &data[monthl) ; 

â€¢â€™close (fp); 
) 
return (fileError1; 

1 

void chartData (float data, pLineType lineme) 
( 
wcpt2 pts [l21: 
float monthwidth = (WIh?X)W-WIDTH - 2 ' MARGIN-WIDTH) / 12; 
int i: 

Eor (i=O: i<12; i + + l  [ 
pts[i].x = MARGIN-WIDTH + i monthwidth + 0.5 ' monthwidth; 
ptslil .y = datali]; 

1 

int main (int argc, char * '  argv) 
( 
long windowIO = openGraphics ( 'arw,  WINDOWNDOWWIDTX. WINDOW-HEIGHT); 
float datatl21; 

setBackground (WHITE); 
setcolor (BLUE);  
readllata ("../data/datal960', data); 
chartData (data, SOLID) ; 
readData ('../data/datal970", data); 
chartData (data, DASHED) : 
readData ("../data/datal980", data); 
chartData (data, DOTTED) ; 
sleep (10) ; 
closeGraphics (windowlD) ; 

1 



Chapter 4 
Anributes oi Ourput Prlrnilives 

plbthng three data sets with three 
differenr line types, as output by the 
che r t  ca ta procedure. 

Raster line algor~thnis display line-type attr~butes by plotting pixel spans. 
For the various dashcxl, dotted, and dot-dashed pattern..,, the line-drawing proce- 
dure outputs sections of contiguous pixels along the line path, skipping over a 
number of intervening pixels between the solid spans. Pixel counts for the span 
length and interspan spacing can be specified in a pixel mask, which is a string 
containing the digits I and 0 to indicate which positions to plot along the line 
path. The mask 1111000, ior instance, could be used to display a dashed line with 
a dash length of four ptxels and an interdash spacing uf three pixels. On a bilevel 
system, the mask gives Ihe bit values that should be loaded into the frame buffer 
along the line path to display the selected line type. 

Plotting dashes I&-ith a fixed number of pixels t.aw~lts in unequal-length 
dashes for different lints orientations, as illustrated in Fig. 4-2. Both dashes shown 
are plotted with four pixels, but the diagonal dash is longer by a factor of fi. For 
precision drawings, dash lengths should remain approximately constant for any 
line orientation. To accomplish this, we can adjust the pxel counts for the solid 
spans and interspan spacing according to the line slope. In Fig. 4-2, we can dis- 
play approximately eyrld-length dashes by reducing the diagonal dash to three 

a pixels. Another method for maintaining dash length is tc) treat dashes as indi\.id- ual line segments. Endpoint coordinates for each dash are located and passed to 
l a )  the line routine, which then calcvlates pixel positions aloilg the dash path. 

.me. Line Width 
(b)  Implementation of line- width options depends on the mpabilities of the output 

-- - - - - - - device. A heavy line on ..I \kieo monitor could bc displayed as adjacent parallel 
r i p r e  4-2 lines, while a pen plotter mght  require pen changes. As with other PHIGS a t t i b  
Unequal-length dashes utes, a line-width coninlmd is used to set the current line-width value in the at- 
dis~layed with the sanic tribute list. This value 15 then used by line-drawing algorithms to ~ o n t r o l  the 
number of pixels. th~ckness of lines generated with subsequent output primitive commands 

We set the line-wdth attribute with the command: 

Line-width parameter lr. is assigned a positive number to indicate the relative 
width of the line to be d ~ y i a y e d .  A value of 1 specifies a .;tandard-width line. On 
n pen plotter, for instance, a user could set lw to a \.slue of 0.5 to plot a line 
whose width is half that of the standard line. Values greater than 1 produce lines 
thicker than the standard. 



For raster implementation, a standard-width line is generated with single %ion 4-1 
pixels at each sample position, as in the Bresenham algorithm. Other-width link l ine  Amibutes 
are displayed as positive integer multiples of the standard line by plotting addi- 
tional pixels along adjacent parallel line paths. For lines with slope magnitude 
less than 1, we can modify a line-drawing routine to display thick lines by plot- 
ting a vertical span of pixels at each x position along the line. The number of pix- 
els in each span is set equal to the integer magnitude of parameter lw.  In Fig. 4-3, 
we plot a double-width line by generating a parallel line above the original line 
path. At each x gmpling position, we calculate the corresponding y coordinate 
and plot pixels with screen coordinates ( x , ' y )  and (x ,  y+l). -We display lines with 
1w 2 3 by alternately plotting pixels above and below the single-width line path. 

For lines with slope magnitude greater than 1, we can plot thick lines with 
horizontal spans, alternately picking u p  pixels to the right and left of the line 
path. This scheme is demonstrated in Fig. 4-4, where a line width of 4 is plotted 
with horizontal pixel spans. 

Although thick lines are generated quickly by plotting horizontal or vertical 
pixel spans, the displayed width of a line (measured perpendicular to the line 
path) is dependent on its slope. A 45" line will be displayed thinner by a factor of 
1 / ~  compared to a horizontal or vertical line plotted with the same-length 
pixel spans. 

Another problem with implementing width options using horizontal or 
vertical pixel spans is that the method produces lines whose ends are horizontal 
or vertical regardless of the slope of the line. This effect is more noticeable with 
very thick lines. We can adjust the shape of the line ends to give them a better a p  
pearance by adding line caps (Fig. 4-5). One lund of line cap is the butt cap ob- 
tained by adjusting the end positions of the component parallel lines so that the 
thick line is displayed with square ends that are perpendicular to the line path. If 
the specified line has slope m, the square end of the thick line has slope - l / m .  
Another line cap is the round cap obtained by adding a filled semicircle to each 
butt cap. The circular arcs are centered on the line endpoints and have a diameter 
equal to the line thickness. A third type of line cap is the projecting square cap. 
Here, we simply extend the line and add butt caps that are positioned one-half of 
the line width beyond the specified endpoints. 

Other methods for producing thick Lines include displaying the line as a 
filled rectangle or generating the line with a selected pen or brush pattern, as dis- 
cussed in the next section. To obtain a rectangle representation for the line 

. . -- - 
Figure 4-3 
Double-wide raster line with slope I ml < 1 generated with 
vertical pixel spans. 



Figure 4-4 
Raster line with slope lm l > 1 
and line-width parameter lw = 4 
plotted wrth horizontal pixel spans. 

boundary, we calculate the posltion of the rectangle vertices along perpendicu- 
lars to the line path so that vertex coordinates are displaced from the line end- 
points by one-half the line width. The rectangular linc then appears as in Fig. 
4-5(a). We could then add round caps to the filled rectangle or extend its length to  
display projecting square caps. 

Generating thick polylines requires some additional considerations. In gen- 
eral, the methods we have considered for displaying a single line segment will 
not produce a smoothly connected series of line segments. Displaying thick lines 
using horizontal and vertical pixel spans, for example, leaves pixel gaps at the 
boundaries between lines of different slopes where there is a shift from horizon- 
tal spans to vertical spans. We can generate thick polylines that are smoothly 
joined at the cost of additional processing at the segment endpoints. Figure 4-6 
shows three possible methods for smoothly joining two line segments. A miter 
jo~n is accomplished by extending the outer boundaries of each of the two lines 
until they meet. A round join is produced by capping the connection between the 
two segments with a circular boundary whose diameter is equal to the line 

I'igure 4-5 
Thick lines drawn with (a! butt caps, (b) mund caps, and (c) projecting 
square caps 



Figure 4-6 
Thick line segments connected with (a) miter join, [b) round join, and (c) 
beveI join. 

width. And a bezlel join is generated by displaying the line segments with butt 
caps and filling in the triangular gap where the segments meet. If the angle be- 
tween two connected line segments is very small, a miter join can generate a long 
spike that distorts the appearance of the polyline. A graphics package can avoid 
this effect by switching from a miter join to a bevel join, say, when any two con- 
secutive segments meet at a small enough angle. 

Pen and Brush Options 

With some packages, lines can be displayed with pen or brush selections. Op- 
tions in this category include shape, size, and pattern. Some possible pen or 
brush shapes are given in Fig. 4-7. These shapes can be stored in a pixel mask 
that identifies the array of pixel positions that are to be set along the line path. 
For example, a rectangular pen can be implemented with the mask shown in Fig. 
4-8 by moving the center (or one corner) of the mask along the line path, as in 
Fig. 4-9. To avoid setting pixels more than once in the frame buffer, we can sim- 
ply accumulate the horizontal spans generated at each position of the mask and 
keep track of the beginning and ending x positions for the spans across each scan 
line. 

Lines generated with pen (or brush) shapes can be displayed in various 
widths by changing the size of the mask. For example, the rectangular pen line in 
Fig. 4-9 could be narrowed with a 2 X2 rectangular mask or widened with a 4 X 4  
mask. Also, lines can be displayed with selected patterns by superimposing the 
pattern values onto the pen or brush mask. Some examples of line patterns are 
shown in Fig. 4-10. An additional pattern option that can be provided in a paint 
package is the display of simulated brush strokes. Figure 4-11 illustrates some 
patterns that can be displayed by modeling different types of brush strokes. 

Cine Color 

When a system provides color (or intensity) options, a parameter giving the cur- 
rent color index is included in the list of system-attribute values. A polyline rou- 
tine displays a line in the current color by setting this color value in the frame 
buffer at pixel locations along the line path using the setpixel procedure. The 
number of color choices depends on the number of bits available per pixel in the 
frame buffer. 

We set the line color value in PHlCS with the function 



Custom Document Brushes 

[RBL'(?l.f] 
e 
[Cancel) 

Figr~w 4-7 
Penand brush shapes for linc display. 

Nonnegative integer values, corresponding to allowed color choices, are assigned 
to the line color parameter lc. A line drawn in the background color is invisible, 
and a user can erase a previously displayed line by respecifying it in the back- 
ground color (assuming the line does not overlap more than one background 
color area). 

An example of the uie of the various line attribute commands in an applica- 
'ions program is given by the following sequence of statements: 

setlinetns 12 1 ; 
setLinewiCthScaleFactor ( 2 : ;  
set~olylir.eColourIndex ( 5 )  ; 
polyline ( n l  , wcpolntsl) : 

set~olyline~:clourIndex ( 6 1  ; 
polyline in7 wcpoints?) : 

This program segment would d~splay two figures, dr'lwn with double-wide 
dashed lines. The first is displayed in a color corresponding to code 5, and the 
second in color 6. 



Figure 4-8 
(a) A pixel mask for a rectangular 
pen, and (b) the associated array of 
pixels displayed by centering the 
mask over a specified pixel 
position. 

Figure 4-9 
Generating a line with the pen 

I I I I I I  shape of Fig. 4-8. 

- 
Fipw 4-10 
C w e d  Lines drawn with a paint program using various shapes and 
patterns. From left to right, the brush shapes are square, round, 
diagonal line, dot pattern, and faded airbrush. 



Chaoter 4 

Attributes of Output Primitives 

Figure 4-11 
A daruma doll, a symbol of good . 
fortune in Japan, drawn by 
computer artist Koichi Kozaki using 
a paintbrush system. Daruma dolls 
actually come without eyes. One 
eye is painted in when a wish is 
made, and the other is painted in 
when the wish comes hue. 
(Courtesy of Wacorn Technology, Inc.) 

4-2 
CURVE ATTRIBUTES 

Parameters for curve attributes are the same as those for line segments. We can 
display curves with varying colors, widths, dotdash patterns, and available pen 
or brush options. Methods for adapting curve-drawing algorithms to accommo- 
date attribute selections are similar to those for line drawing. 

The pixel masks di-ssed for implementing line-type options are also used 
in raster curve algorithms to generate dashed and dotted patterns. For example, 
the mask 11100 produces the dashed circle shown in Fig. 4-12. We can generate 
the dashes in the various odants using circle symmetry, but we must shift the 
pixel positions to maintain the correct sequence of dashes and spaces as we move 
from one octant to the next. Also, as in line algorithms, pixel masks display 
dashes and interdash spaces that vary in length according to the slope of the 
curve. If we want to display constant-length dashes, we need to adjust the num- 
ber of pixels plotted in each dash as we move around the circle circumference. ln- 
stead of applying a pixel mask with constant spans, we plot pixels along equal 
angular arcs to produce equal length dashes. 

Raster curves of various widths can be displayed using the method of hori- 
zontal or vertical pixel spans. Where the magnitude of the curve slope is less than 
1, we plot vertical spans; where the slope magnitude is greater than 1, we plot 
horizontal spans. Figurr 4-13 demonstrates this method for displaying a circular 
arc of width 4 in the first quadrant. Using circle symmetry, we generate the circle 
path with vertical spans in the octant from x = 0 to x = y, and then reflect pixel 
positions about thdine y = x to obtain the remainder of the curve shown. Circle 
sections in the other quadrants are obtained by reflecting pixel positions in the 



first quadrant about the coordinate axes. The thickness of curves displayed with k h n  4-2 
this method is again a function of curve slope. Circles, ellipses, and other curves Curve Attributes 
will appear thinnest where the slope has a magnitude of 1. 

Another method for displaying thick curves is to fill in the area between 
two parallel curve paths, whose separation distance is equal to the desired width. 
We could d o  this using the specdied curve path as  one boundary and setting up 
the second boundary either inside or outside the original curve path. This a p  
proach, however, shifts the original curve path either inward or outward, de- 
pending on which direction we choose for the second boundary. We can maintain 
the original curve position by setting the two boundary curves at a distance of 
one-half the width on either side of the speclfied curve path. An example of this 
approach is shown in Fig. 4-14 for a circle segment with radius 16 and a specified 
width of 4. The boundary arcs are then set at a separation distance of 2 on either 
side of the radius of 16. To maintain the proper dimensions of the cirmlar arc, as  
discussed in Section 3-10, we can set the radii for the concentric boundary arcs at 
r = 14 and r = 17. Although this method is accurate for generating thick circles, 
in general, it provides only an approximation to the true area of other thick 

- 
Figure 4-12 
A dashed circular arc displayed 
with a dash span of 3 pixels and an 
interdash spacing of 2 pixels. 

Figurc 4-13 
Circular arc of width 4 plotted with 
pixel spans. 



Chapter 4 

Attributes of Output Primitives 

Figure 4-14 
A circular arc of width 4 and radius 
16 displayed by filling the region 
between two concentric arcs. 

Figure 4- 13 
Circular arc displayed with 
rectangular pen. 

curves. For example, the inner and outer boundaries of a fat ellipse generated 
with this method d o  not have the same foci. 

Pen (or brush) displays of curves are generated using the same techniques 
discussed for straight line segments. We replicate a pen shape along the line path, 
as llustrated in Fig. 4-15 for a circrular arc in the first quadrant. Here, the center of 
the rectangular pen is moved to successive curve positions to produce the curve 
shape shown. Curves displayed with a rectangular pen in this manner will be 
thicker where the magnitude of the curve slope is 1. A uniform curve thickness 
can be displayed by rotating the rectangular pen to align it with the slope direc- 
tion as we move around the curve or by using a circular pen shape. Curves 
drawn with pen and bmsh shapes can be displayed in different sizes and with 
superimposed patterns or simulated brush strokes. 

- - 
COLOR A N D  GRAYSCALE LEVELS 

Various color and intensity-level options can be made available to a user, de- 
pending on the capabilities and design objectives of a particular system. General- 
purpose raster-scan systems, for example, usually provide a wide range of colors, 
while random-scan monitors typically offer only a few color choices, if any. Color 



options are numerically'coded with values ranging from 0 through the positive 
integers. For CRT monitors, these color codes are then converted to intensity- 
level settings for the electron beams. With color plotters, the codes could control 
ink-jet deposits or pen selections. 

In a color raster system, the number of color choices available depends on 
the amount of storage provided per pixel in the frame buffer Also, color-informa- 
tion can be stored in the frame buffer in two ways: We can store color codes di- 
rectly in the frame buffer, or we can put the color codes in a separate table and 
use pixel values as an index into this table. With the direct storage scheme, when- 
ever a particular color code is specified in an application program, the corre- 
sponding binary value is placed in the frame buffer for each-component pixel in 
the output primitives to be displayed in that color. A minimum number of colors 
can be provided in t h ~ ~  scheme with 3 bits of storage per pixel, as shown in Table 
41.  Each of the three bit positions is used to control the intensity level (either on 
or off) of the corresponding electron gun in an RGB monitor. The leftmost bit 
controls the red gun,- the middle bit controls the green gun, and the rightmost bit 
controls the blue gun. Adding more bits per pixel to the frame buffer increases 
the number of color choices. With 6 bits per pixel, 2 bits can be used for each gun. 
This allows four diffewnt intensity settings for each of the three color guns, and a 
total of 64 color values are available foreach screen pixel. With a G l u t i o n  of 
1024 by 1024, a full-color (24bit per pixel) RGB system needs 3 megabytes of 
storage for the frame buffer. Color tables are an alternate means for providing ex- 
tended color capabilities to a user without requiring large frame buffers. Lower- . - -  
cost personal computer systems, in particular, often use color tables to reduce 
frame-buffer storage requirements. 

Color Tables 

Figure 4-16 illustrates a possible scheme for storing color values in a color 
lookup table (or video lookup table), where frame-buffer values art- now used 
as indices into the color table. In this example, each pixel can reference any one of 
the 256 table positions, and each entry in the table uses 24 bits to spec* an RGB 
color. For the color code 2081, a combination green-blue color is displayed for 
pixel location ( x ,  y). Systems employing this particular lookup table would allow 

TABLE 4-1 
THE EIGHT COLOK CODES FOR A THKEE-BIT 
PER PIXEL FRAME BUFFER 

Stored Color Values Displayed 
Color in Frame Buffer Color 

Code RED GREEN BLUE 

0 Black 
1 Blue 
0 Green 
1 Cyan 
0 Red 
1 Magenta 
0 Yellow 
1 White 

Seawn 4-3 
Color and Cravscale Levels 



Charnerd a user to select any 256 colors for simultaneous display fmm a palette of nearly 
Attributes of Output Primitives 17 million colors. Comuared to a fullalor svstem. this scheme reduces the num- 

ber of simultaneous cdlors that can be dispiayed,. but it also reduces the frame- 
buffer storage requirements to 1 megabyte. Some graphics systems provide 9 bits 
per pixel in the frame buffer, permitting a user to select 512 colors that could be 
used in each display. 

A user can set color-table entries in a PHIGS applications program with the 
function 

setColourRepresentation (ws,  c i ,  colorptrl 

Parameter ws identifies the workstation output device; parameter c i  speclhes 
the color index, which is the color-table position number (0 to 255 for the ewm- 
ple in Fig. 4-16); and parameter colorptr points to a hio of RGB color values (r, 
g, b)  each specified in the range from 0 to 1. An example of possible table entries 
for color monitors is given in Fig. 4-17. 

There are several advantages in storing color codes in a lookup table. Use of 
a color table can pmvide a "reasonable" number of simultaneous colors without 
requiring Iarge frame buffers. For most applications, 256 or 512 different colors 
are sufficient for a single picture. Also, table entries can be changed at any time, 
allowing a user to be able to experiment easily with different color combinations 
in a design, scene, or graph without changing the attribute settings for the graph- 
ics data structure. Similarly, visualization applications can store values for some 
physical quantity, such as energy, in the frame buffer and use a lookup table to 
try out various color encodings without changing the pixel values. And in visual- 
ization and image-processing applications, color tables are a convenient means 
for setting color thresholds so that all pixel values above or below a specified 
threshold can be set to the same coldr. For these reasons, some systems provide 
both capabilities for color-code storage, so that a user can elect either to use color 
tables or to store color codes directly in the frame buffer. 

Color 
Lookup 

I I - To Eiur Gun 

-- 

Figure 4-16 
A color lookup table with 24 bits per entry accessed fmm a frame buffer with 8 bits per 
pixel. A value of 196 stored at pixel position (x ,  y) references the location in this table 
containing the value 2081. Each 8-bit segment of this entry controk the intensity level of 
one of the three electron guns in an RGB monitor. 



WS =- 2 

C i  Color 

Section 4-3 

Color and Graywale Levels 

- - 

Figure 4-17 
Workstation color tables. 

Crayscale 

With monitors that have no color capability, color hmctions can be used in an ap- 
plication program to set the shades of gray, or grayscale, for displayed primi- 
tives. Numeric values over the range from 0 to 1 can be used to specify grayscale 
levels, which are then converted to appropriate binary codes for storage in the 
raster. This allows the intensity settings to be easily adapted to systems with dif- 
fering grayscale capabilities. 

Table 4-2 lists the specifications for intens~ty codes for a four-level gray- 
scale system. In this example, any intensity input value near 0.33 would be stored 
as the binary value 01 in the frame buffer, and pixels with this value would be 
displayed as dark gray. If additional bits per pixel are available in the frame 
buffer, the value of 0.33 would be mapped to the nearest level. With 3 bits per 
pixel, we can accommodate 8 gray levels; while 8 bits per pixel wbuld give us 256 
shades of gray. An alternative scheme for storing the intensity information is to 
convert each intensity code directly to the voltage value that produces this gray- 
scale level on the output device in use. 

When multiple output devices are available at an installation, the same 
color-table interface may be used for all monitors. In this case, a color table for a 
monochrome monitor can be set up  using a range o f  RGB values as in Fig. 4-17, 
with the display intensity corresponding to a given color index c i  calculated as 

intensity = 0.5[min(r, g, b) + max(r, g, b)]  

TABLE 4-2 
INTENSITY CODES FOR A FOUR-LEVEL 
GRAYSCALE SYSTEM 

Intensity Stored Intensity Displayed 
Codes Values In The Cra ysca k 

Frame Buffer (Binary Cod4 

0.0 0 (00) Black 
0.33 1 (01) Dark gray 
0.67 2 (1  0) Light gray 
1 .O 3 (11) White 



Hollow 
(a1 

Figirrc- 4-18 
Polygon fill styles. 

4-4 
AREA-FILL ATTRIBUTES 

Options for filling a defined region include a choice between a solid color or a 
patterned fill and choices for the particular colors and patterns. l3ese fill options 
can be applied to polygon regions or to areas defined with curved boundaries, 
depending on the capabilities of the available package. In addition, areas can be 
painted using various brush styles, colors, and transparency parameters. 

Fi l l  Styles 

Areas are displayed with three basic fill styles: hollow with a color border, filled 
with a solid color, or Wed with a specified pattern or design. A basic fill style is 
selected in a PHIGS program with the function 

Values for the fill-style parameter f s include hollow, solid, and pattern (Fig. 4-18). 
Another value for fill style is hatch, which is used to fill an area with selected 
hatching patterns-parallel lines or crossed lines--as in Fig. 4-19. As with line at- 
tributes, a selected fillstyle value is recorded in the list of system attributes and 
applied to fill the interiors of subsequently specified areas. Fill selections for pa- 
rameter f s  are normally applied to polygon areas, but they can also be imple- 
mented to fill regions with curved boundaries. 

Hollow areas are displayed using only the boundary outline, with the inte- 
rior color the same as the background color. A solid fill is displayed in a single 
color up to and including the borders of the region. The color for a solid interior 
or for a hollow area outline is chosen with 

where fillcolor parameter f c  is set to the desired color code. A polygon hollow 
fill is generated with a linedrawing routine as a closed polyline. Solid fill of a re- 
gion can be accomplished with the scan-line procedures discussed in Section 
3-11. 

Other fill options include specifications for the edge type, edge width, and 
edge color of a region. These attributes are set independently of the fill style or 
fill color, and they provide for the same options as the line-attribute parameters 
(line type, line width, and line color). That is, we can display area edges dotted or 
dashed, fat or thin, and in any available color regardless of how we have filled 
the interior. 

Diagonal 
Hatch Fill 

Diagonal 
Cross.Hatch Fill 

Figure 4-19 
Polygon fill using hatch patterns. 



Pattern Fi l l  

We select fill patterns with 

TABLE 4-3 
A WORKSTATION 

where pattern index parameter p i  specifies a table position. For example, the fol- PATTERN TABLE WITH 

lowing set of statements would fill the area defined in the f i l l n r e a  command US'NC 

with the second pattern type stored in the pattern table: T H E  COLOR CODES OF 
TABLE 4-1  

S e t I n t e r i o r S t y l e  ( p a t t e r n ) ;  
s e t ~ n t e r i o r S t y l e I n d e x  ( 2 ) ;  
f i l l A r e a  (n. p o i n t s ) ;  

Index Pattern 
(pi l ( cp )  

[: :] Separate tables are set up  for hatch patterns. I f  we had selected hatch fill for the 1 
interior style in this program segment, then the value assigned to parameter p i  is 
an index to the stored patterns in the hatch table. 

For fill style pattcm, table entries can be created on individual output de- 
vices with 2 

SetPatternRepresentatlon l w s ,  p . ,  nx, ny, cp)  

Parameter p i  sets the pattern index number for workstation code ws, and cp is a 
two-dimensional array of color codes with nx colunms and ny rows. The follow- 
ing program segment illustrates how this function could be used to set the first 
entry in the pattern table for workstation 1. 

setPatcernRepresentatian ( 3 ,  1. ;. 2 ,  cp); 
Table 4-3 shows the first two entries for this color table. Color array cp in this ex- 
ample specifies a pattern that produces alternate red and black diagonal pixel 
lines on an eight-color system. 

When a color array cp is to be applied to hll a region, we need to specify 
the size of the area that is to be covered by each element of the array. We d o  this 
by setting the rectangular coordinate extents of the pattern: 

s e t p a t t e r n s i z e  (dx ,  dy) 

where parameters dx and dy give the coordinate width and height of the array 
mapping. An example of the coordinate size associated with a pattern array is 
given in Fig. 4-20. If the values for dx and dy in t h ~  figure are given in screen co- 
ordinates, then each element of the color array would be applied to a 2 by 2 
screen grid containing four pixels. 

A reference position for starting a puttern fill 1s assigned with thestatement I-- ~ X = B  -I 
- - - - - . -. . 

setPatcernReferencePoint (positicn) I i ,q111~ 4 -20  
A pattern array with 4 

Parameter posi t  i o n  is a pointer to coordinates ( x p ,  yp) that fix the lower left columns and 3 rows mappd 
comer of the rectangular pattern. From this starting position, the pattern is then to an 8 by 6 coordinate 
replicated in the x and y directions until the defined area is covered by nonover- rectangle 159  



Chapter 4 lapping copies of tlie pattern array. The process of filling an area with a rectangu- 
Attributes of Output Primitives lar pattern is called tiling and rectangular fill patterns are sometimes referred to 

as tiling patterns. Figure 4-21 demonstrates tiling of a triangular fill area starting 
from a pattern reference point. 

To illustrate the use of the pattern commands, the following program exam- 
ple displays a black-and-white pattern in the interior of a parallelogram fill area 
(Fig. 422). The pattern size in this program is set to map each array element to a 
single pixel. 

void patternFil1 0 

wcpta p t s ~ 4 1 ;  
intbwPattern[3][31 = ( 1 ,  0. 0, 0, 1, 1, 1, 0, 0 1 ;  

~SetPatternRepresentation (WS, 8, 3, 3, bwPattern); 

~SetFillAreaInteriorStyle (PATTERN); 
pSetFillAreaPatternIndex (8); 
pSetPatternReferencePoint (14, 11); 

Pattern fill can be implemented by modifying the scan-line procedures dis- 
cussed in Chapter 3 so that a selected pattern is superimposed onto the scan 
lines. Beginning from a specified start position for a pattern fill, the rectangular 
patterns would be mapped vertically to scan lines between the top and bottom of 

Start the fill area and horizontally to interior pixel positions across these scan lines. 
Position Horizontally, the pattern array is repeated at intervals specified by the value of 

size parameter dx. Similarly, vertical repeats of the pattern are separated by inter- 
vals set with parameter dy. This scan-line pattern procedure applies both to poly- 
gons and to areas bounded by curves. 

Ftgure 4-21 
Xlmg an area from a 
designated start position . . ,+ 
Nonoverlapping adjacent 
patterns are laid out to cover 

Plxel 
all scan lines passing through GL.. ,. j :--& $%:+, 
the defined area Posmon * L 4 1 ,  

er 
6 

I 0 1  , !7 1 

Figure 4-22 
A pattern array (a) superimposed on a paraIlelogram fill area to 
produce the display (b). 



IHatcL lill is applied to regions by displaying sets of parallel lines. The fill section4-4 
procedures are implemented to draw either sinfile hatching or cross hatching. S\wa-F~ll A t t d ~ ~ l t . 3  
Spacing and slope for the hatch lines can be set as parameters in the hatch table. 
o n  raster systems, a hatch fill can be specified as a pattern array that sets color 
values for groups of diagonal pixels. 

In many systems, the pattern reference point :'xp, !//I) IS assigned by the sys- 
tem. For instance, the reference point could be set automatically at a polygon ver- 
tex. In general, for any fill region, the reference p n t  can be chosen as the lower 
left corner of the bounding rerlar~gle (or bounding box) determined by the coordi- 
nate extents of the region (Fig. 4-23). To simplify selection of the reference coordi- 
nates, some packages always use the screen coordinate origin as the pattern start 
position, and window systems often set the reference point at the coordinate ori- 
gin of the window. Always setting ( x p ,  yp) at the coordinate origin also simplifies 
the tiling operations when each color-array element of a pattern is to be mapped 
to a single pixel. For example, if the row positions in the pattern array are refer- 
enced in reverse (that is, from bottom to top starting at I), a pattern value is then 
assigned to pixel position (1, y) in screen or window coordinates as 

s e t p i x e l  ( x, y ,  cp(y mod ny + 1 ,  x mod nx + 1) i 

where ny and nx specify the number of rows and number of columns in the pat. 
tern array. Setting the pattern start position at the coordinate origin, however, ef- 
fectively attaches the pattern fi l l  to the screen or window backgmund, rather 
than to the fill regions. Adjacent or overlapping areas filled with the same pattern 
would show no apparent boundary between the areas. Also, repositioning and 
refilling an object with the same pattern can result In a shift in the assigned pixel - .  
values over the object interior. movlng object would appear to be transparent 
against a stationary pattern background, instead oi moving with a fixed interior 
pattern. 

It is also possible tc combine a fill pattern ivith background colors (includ- 
ing grayscale) in various ways. With a b~tmap pattern containing only the digits 1 
and 0, the 0 values could be used as transparency indicators to let the back- 
ground show through. Alternatively, the 1 and 0 digits can be used to fill an inte- 
rior with two-color patterns. In general, color-fill patterns can be combined in 
several other ways with background colors. The pattern and background colors 
can be combined using Boolean operations, or the pattern colors can simply re- 
place the background colors. Figure 4-24 demonstrates how the Boolean and re 
place operations for a 2 by 2 fill pattern would set pixel values on a binary (black- 
and-white) system against a particular background pattern. 

rJJ -. Bounding frxurc 4-2.3 rectangle for a region - - -  - - 1  with coordinate extents x,, x,,, 
x y ,,,,, and y,,, in the x and y 

x rn ,h Xma. directions. 



Chapter 4 

Attributes of Output Primitives 

Pattern Background 

Pixel Values 
. - 

F i p ~  4-24 
Combining a fiJi pattern with a backgrouna pattern using 
Boolean operations, and, or, and ror lexclusiw or), and using 
simple replacement. 

Soft Fill 

Modified boundary-fill and flood-~III procedures that are applied to repaint areas 
so that the fill color is combined with the background colors are referred to as 
soft-till or t i n t 4  algorithms. One use for these fill methods is to soften the fill 
colors at object borders that have been blurred to antialias the edges. Another is 
to allow repainting of a color area that was originally filled with a semitranspar- 
ent brush, where the current color is then a mixture of the brush color and the 
background colors "behind" thearea. In either case, we want the new fill color to 
have the same variations over the area as the current fill color. 

As an example of this type of fill, the linear soft-fill algorithm repaints an 
area that was originally painted by merging a foreground color F with a single 
background color 8, where F + B. Assuming we know the values for F and 8, we 
can determine how these colors were originally combined by checking the cur- 
rent color contents of the frame buffer. The current RGB color P of each pixel 
within the area to be refilled is some linear combination of F and B: 

where the "transparency" factor t has a value between 0 and 1 for each pixel. For 
values of t less than 05, the background color contributes more to the interior 
color of the region than does the fill color. Vector Equation 4-1 holds for'each 



RGB component of the colors, with Section 4-5 
Character Attributes 

P = ( P R ,  PC, PR), F = (FR, Fc, FR), B = (BR, Bc, BB) (4-2)  

We can thus calculate the value of parameter f using one of the RGB color com- 
ponents as 

where k = R,  G, or B; and Fk * Bk. Theoretically, parameter t has the same value 
for each RCB component, but roundoff to integer codes can result in different 
values of f for different components. We can minimize this roundoff error by se- 
lecting the component with the largest difference between F and B. This value of 
I is then used to mix the new fill color NF with the background color, using either 
a modified flood-fill or boundary-fill procedure. 

Similar soft-fill procedures can be applied to an area whose foreground 
color is to be merged with multiple background color areas, such as a checker- 
board pattern. When two background colors B, and B, are mixed with fore- 
ground color F, the resulting pixel color P is 

P = 1â€¢â€¹F t tlB, t (1 - f, ,  -- tJB2 (4-4; 

where the sum of the coefficients to, t,, and (1 - 1, - t,) on the color terms must 
equal 1. We can set up  two simultaneous equations using two of the three RGB 
color components to solve for the two proportionality parameters, to  and f ,  
These parameters are then used to mix the new fill color with the two back- 
ground colors to obtain the new pixel color. With three background colors and 
one foreground color, or with two background and two foreground colors, we 
nccd all thrcc RCB cquations to obtain the relative amounts of the four colors. 
For some foreground and background color combinations, however, the system 
of two or three RCB equations cannot be solved. This occurs when the color val- 
ucs are all very similar or when they are all proportional to each other. 

CHARACTER ATTRIBUTES 

The appearance of displayed characters is controlled by attributes such as font, 
size, color, and orientation. Attributes can be set Ooth for entire character strings 
(text) and for individual characters defined as  marker symbols. 

There are a great many text options that can be made available to graphics pro- 
grammers. First of all, there is the choice of font (or typeface), which is a set of 
characters with a particular design style such as New York, Courier, Helvetica, 
London, 'Times Roman, and various special symbol groups. The characters in a 
selected font can also be displayed with assorted underlining styles (sox, d,ot- 
ted, double), in boldface, in italics. and in outline or shadow styles. A particular . , . . . . . - 



Chapwr 4 font and associated stvle is selected in a PHlCS program by setting an integer 
.411rihutrsoi Output Pr~nimves code for the text font parameter t f in the function 

Font options can be made a\~ailable a s  predefined sets of grid patterns or as char- 
acter sets designed with polylines and spline curves. 

Color settings for ,displayed text are stored m the system attribute list and 
u s e d  by the procedures that load character definitions into the frame buffer. 
When a character string is to be displayed, the current (color i; used to set pixel 
values in the frame hufier corresponding to the character shapes and positions. 
Control of text color (or intensity) is managed from an application program with 

where text color piramcter t c  specifies an  allowable color code. 
We can a d j ~ ~ s t  text size by scaling theoverall dimensions (height and  width) 

of characters or by scaling only the character width. Character size is specified by 
printers and con7positors in poirrls, where 1 point is 0.013837 inch (or approxi- 
mateJy 1/72 inch). For example, the text you are now reading is a 10-point font 
Point measurements specify the size of the body of a character (Fig. 4-25), hut dif- 
ferent fonts with the same p i n 1  specifications can have different character sirc.5, 
depending on the design of the typeface. The distance between the bottorrrlirie and 
the lopline of the character body is the same for all characters in a particular size 
and typeface, but thr body width may vary. Proportior~olly spaced for~ts assign J 
smaller body width to narrow characters such as i, j, 1, and f compared to hroad 
characters such as W or M. Character heigk: is defined as the distance between thc 
baseline and the cuplint- of characters. Kerned characters, such as f and j in Fig. 
4-25, typically extend beyond the character-body limits, and letters with descend- 
ers (g, j, p, q, y) extend below the baseline. Each character is positioned within 
the character body by ;I font designer to allow suitable spacing along and h~ 
tween print lines when text is displayed with character hodies touching. 

Text size can be adjusted without changing the width-to-height ratio of 
characters with 

character 

kern 

kern 

- , character 
I' body 



Height 1 wctior 4-5 

Height 2 
Character Anr~butes 

H i h 3 l ' i p r c  4-26 
The effect of different character- 
height settings an displayed text 

Parameter ch is assigned a real value greater than 0 to set the coordinate height 
of capital letters: the distance between baseline and capline in user coordinates. 
This setting also affects character-body size, so that the width and spacing of 
characters is adjusted to maintam the same text proportions. For instance, dou- 
bling the height also doubles the character width and the spacing between char- 
acters. Figure 4-26 shows a character string disulaved with three different charac- 
ter heights. 

The width only of text can be set wlth the function 

where the character-width parameter cw IS set ton positive real value that scales 
the body width of characters. Text height is unaffected by this attribute setting. 
Examples of text displayed with different character expansions is given in Fig. 
4-27. 

Spacing between characters is controlled separately with 

where the character-spacing parameter cs can he asslgned any real value. The 
value assigned to cs determines the spacing between character bodes along 
print lines. Negative values for cs  overlap character bodies; positive values in- 
sert space to spread out the displayed characters. Assigning the value 0 to c s  
causes text to be displayed with no space between character bodies. The amount 
of spacing to be applied is determined by mult~plying the value of cs by the 
character height (distance between baseline and capline). In Fig. 4-28, a character 
string is displayed with three different settings for the character-spacing para- 
meter. 

The orientation for a displayed character string is set according to the direc- 
tion of the character up vector: 

setcharacterupvector (upvect ) 

Parameter upvec t in this function is asslgned two vdlues that specify the x and y 
vector components Text is then displayed so that the orientation of characters 
from baseline to capline is in the direction of the up vector. For example, with 
upvect = (I, I), the direction of the up  vector is 45" and text would bedisplayed 
as shown in Fig. 4-29. A procedure for orienting text rotates characters so that the 
sides of character bodies, from baseline to capline, are aligned with the up vector. 
The rotated character shapes are then scan converted into the frame buffer. 

width 1.0 

width 2.0 

Figwe 4-27 
The effect of different 
character-width settingson 
displayed text. 

Spacing 0.0 

S p a c i n g  0 . 5  

S p a c i n g  1 . 0  

Figure 4-28 
The effect of different 
character spacings on 
displayed text. 



Chapter 4 

Alfributrh oi Oulpul Prim~tives 

F l g ~ r p  11-30 
Text path alt~ibuks LIII he set 
to produce horuontai or 
vertical arrangement\ of 
character strings. 

- - --- - - .- -. -. - 
F i g ~ t r c  1-31 
Text displayed with tile four 
trxt-path options. 

controls the orientation of 
displayed text (b). 

It is useful in many ~pplications to be able to arrange character strings verti- 
cally or horizontally (Fig. 4-30). An attribute parameter for this option is set with 
the statement 

where the text-path par;lmeter tp can be assigned the value: right, left, u p ,  or 
down Examples of text d~splayed with these four options are shown in Fig. 4-31. 
A procedure for implementing this option must transform the character patterns 
into the specified orientation before transferring them to the frame buffer. 

Character strings can also be oriented using a combination of up-vector and 
text-path specifications to produce slanted text. Figure 4-32 shows the directions 
of character strings generated by the various text-path settings for a 45" up vec- 
tor. Examples of text generated for text-path values dmw and right with this u p  
vector are illustrated in Fig. 4-33. 

Another handy attribute for character strings is alignment. This attribute 
specifies how text is to bt. positioned with respect to the $tart coordinates. Aligtl- 
ment attributes arc set ~ i t h  

serTextAl Lg~unent ( h ,  v )  

where parameters h and -I control horizontal and vertical alignment, respectively. 
Horizontal alignment is set by assigning h a value of left,  centrc, or righhl. Vertical 
alignment is set by assigning v a value of top, cap, hnff, hase, or bottom. The inter- 
pretation of these alignnient values depends on the current setting for the text 
path. Figure 4-34 shows the position of-the alignment settings when text is to be 
displayed hori~ontally to the right or vertically down. Similar interpretations 
apply to text path value5 of left and u p .  The "most naturid" alignment for a par- 
ticular text path is chosen by assigning the value norm01 to the h and v parame- 
ters. Figure 4-35 illustrntcs cAmmon alignment positions for horizontal and verti- 
cal text labels. 

A precision specifici~tion for text display is given with 

where text precis~on parameter t p r  1s ass~gned one of the values: string, char, or 
stroke. The highest-quality text is displayed when the precision parameter is set to 
the value strokc. For this precision setting, greater detail would be used in defin- 
ing the character shapes, and the processing of attribute selections and other 



string-manipulation procedures would be carried out to the highest possible ac- section 4-5 
curacy. The lowest-quality precision setting, strinp, is used for faster display of Character Anributes 
character strings. At this precision, many attribute selections such as text path are 
ignored, and string-manipulation procedures are simplified to reduce processing 

J 

time. / 
Marker Attribute: 

, 
Direction of 

Character up Vector 
A marker symbol is a single character that can he displayed in different colors (a) 
and in different sizes. Marker attributes are implemented bv procedures that load , . 
the chosen character into the raster at the defined positions with the specified 
color and size. +,-A 

We select a particular character to be the marker symbol with 

where marker type parameter m t  is set to an integer code. Typical codes for O N  '4 
marker type are the integers 1 through 5, specifying, respectively, a dot (.I, a ver- Oh+ % '% 
tical cross (+), an asterisk (*), a circle (o), and a diagonal cross (X). DispIayed Text Path Direction 
marker types are centered on the marker coordinates. I b) 

We set the marker size with 

setMarkerSizeScaleFactor (ms) 
Fgrrre 4-32 
An up-vector specification (a) 
controts the direction of the 

with parameter marker size m s  assigned a positive number. This scaling parame- text (b), 
ter is applied to the nominal size for the particular marker symbol chosen. Values 
greater than 1 produce character enlargement; values less than 1 reduce the 
marker size. 

o0 ,+ 
1 sTI<ING 1::z; half 
I--- bottom 

lefl center righr 

--top 
-- cap 

. - - - - . . -. half 

N 

left j right 

center 

I rgurr 4-33 
The 45" up vector in Fig. 4-32 
produces the display (a) for a 
down psth and the display (b) 
for a right path. 

Figure 4-34 
Alignment attribute values for 
horizontal and vertical strings. 



Marker color is spv~~l ' ied  with 

setPo Lymdrk~~rColour Index (mc  i 

A srlected color c ~ d t l  101 parameter m c  is stored in the current attribute list and 
used to display srtbsrqu~~ntly specified marker primitives. 

ri N I I F '  , . 1 , :  4-6 
. - 

BUNDLED ATTRIBUTES 

I yrrc 4 -  15 
C.harartrr-strint! al,g,lmc.l,ts With the procedures \\.t have comidered so far, each function references a single 

attribute that specifies e,.actly how a primitive is to be displayed with that at- 
tribute settir.g. Thew qxclfications are called individual (or unbundled\ attrib- 
utes, and they are meant to be used with an output device that is capable of dis- 
playing primitives in thtt way specified. If an  application program, employing 
individual attributes, is interfaced to several output deviws, some of the devices 
may not have thc capability to display the intended attributes. A program using 
individual color 'lttributi,~, for example, may have to be modified to produce ac- 
ceptable output on a monochromatic monitor. 

Individual attributt commands provide a simple and direct niethod for 
specifying attributes whiw ,i single output device is used When several kinds of 
output devices are avaihhlz at a graphics installation, it is convenient for a user 
to be able to say how attributes are to be interpreted on trach of the different de- 
vices. This is accomplis1ic.d by setting up  tables for each output device that lists 
sets of attribute values that are to be used on that devict, to display each primi- 
t i w  tvpe. A part~cular s t t  of attribute values tor a primitive on each output de- 
\.irt. is then chosen bv sp:iiiying the appropriate table index. Attributes speciiied 
in this manner called bundled attributes. The table for each primitive that de- 
fines groups of attribute \.slues lo be used when d i sp la~ ing  that primitive on a 
particular output devtcc 15 called a bundle table. 

Attributes that niav b r  bundled into the workstation table entries are those 
that d o  rwt involve coordinate specifications, such as cl>lor and line type. The 
choice between a bundltd or an unbundled specification is madc by se t t~ng  a 
switch called the aspect source flag for each of these attributes: 

set1r:dlvid-ilASF (attributeptr, flagptr) 

where parameter a t t r ; .Ddtept r  points to a list of attributes, and parameter 
f lagptr  points to the corresponding list of aspect source flags. Each aspect 
source flag can be assigned a value of individual or  bundled. Attributes that may 
be bundled are listed in the following sections. 

Bundled l i r e  Attrihurc+ 

Entries in the bundle Inble for line attributes on a speciiied workstation are set 
with the function 

set~olylin+iepresentation (ws, 11, I t ,  lw, l c l  



Parameter ws is the workstation identifier, and line index parameter li defines section 4-6 
the bundle table position. Parameters It, lw, and lc are then bundled and as- Bundled Attributes 
signed values to set the line type, line width, and line color specifications, respec- 
tively, for the designated table index. For example, the following statements de- 
fine groups of line attributes that are to be referenced as index number 3 on two 
different workstations: 

A polyline that is assigned a table index value of 3 would then be displayed 
using dashed lines at half thickness in a blue color on workstation 1; while on 
workstation 4, this same index generatessolid, standard-sized white lines. 

Once the bundle tables have been set up, a group of bundled line attributes 
is chosen for each workstation by specifying the table index value: 

Subsequent polyline commands would then generate lines on each worksta- 
tion according to the set of bundled attribute values defined at the table position 
specified by the value of the line index parameter 1 i. 

Bundled Arca-Fill Attributes 

Table entries for bundled area-fill attributes are set with 

set1nteriorRepresentation ( w s ,  fi, fs, pi, ic) 

which defines the attribute list corresponding to fill index f i on workstation ws. 
Parameters f s, pi ,  and fc  are assigned values for the fill style, pattern index, 
and fill color, respectively, on the designated workstation. Similar bundle tables 
can also be set up  for edge attributes of polygon fill areas. 

A particular attribute bundle is then selected from the table with the func- 
tion 

Subsequently defined fill areas are then displayed on each active workstation ac- 
cording to the table entry specified by the fill index parameter f i. Other fill-area 
attributes, such as pattern reference point and pattern size, are independent of 
the workstation designation and are set with the functions previously described. 

Bundled Text Attributes 

The function 

setText~epresentation ( V S ,  ti, t i ,  tp, te, ts, tc) 

bundles values for text font, precision, expansion factor, size, and color in a table 
position for workstation ws that is specified by the value assigned to text index 



Chapter 4 

Atlributes of Oulpul Primitives 
parameter ti. Other text attributes, including character up vector, text path, 
character height, and text alignment are set individually. 

A particular text index value is then chosen with the function 

setText Index (ti) 

Each text function that is then invoked is displayed on each workstation with the 
set of attributes wferenced by this table position. 

Bundled Marker Attr~butes 

Table entries for bundled marker attributes are set up with 

setPolymarkerRepresentation (ws, mi, mt, ms, mc) 

This defines the marker type, marker scale factor, and marker color for index m i  
on workstation w s .  Bundle table selections are then made with the function 

4-7 
INQUIRY FUNCTIONS 

Current settings for attributes and other parameters, such as workstation types 
and status, in the systcm lists can be retrieved with mquiry functions. These 
functions alIow current values to be copied into specified parameters, which can 
then be saved for later reuse or used to check the current state of the system if an 
error occurs. 

We check current attribute values by stating the name of the attribute in the 
inquiry function. For example, the functions 

and 

copy the current values tor line index and fill color into parameters l a s t l i  and 
lastfc. The following program segment ~llustrates reusing the current 11ne type 
value after a set of lines are drawn with a newq line type. 



4-8 Section 4-8 
ANTIALIASING Antialiasing 

Displayed primitives generated by the raster algorithms discussed in Chapter 3 
have a jagged, or stairstep, appearance because the sampling process digitizes co- 
ordinate p i n t s  on an object to discrete integer pixel positions. This distortion of 
information due to low-frequency sampling (undersampling) is called aliasing. 
We can improve the appearance of displayed raster lines by applying antialias- 
ing methods that compensate for the undersampling process. 

An example of the effects of undersampling is shown in Fig. 4-36. To avoid 
losing information from such periodic objects, we need to set the sampling fre- 
quency to at least twice that of the highest frequency occurring in the object, re- 
ferred to as the Nyquist sampling frequency (or Nyquist sampling rate) fs: 

Another way to state this is that the sampling interval should be no larger than 
one-half the cycle interval (called the Nyquist sampling interval). For x-interval 
sampling, the Nyquist sampling interval Ax, is 

where Axqck = l/fm,,. In Fig. 4-36, our sampling interval is one and one-half 
times the cycle interval, so the sampling interval is at least three times too big. If 
we want to recover all the object information for this example, we need to cut the 
sampling interval down to one-third the size shown in the figure. 

One way to increase sampling rate with raster systems is simply to display 
objects at higher resolution. But even at the highest resolution possible with cur- 
rent technology, the jaggies will be apparent to some extent. There is a limit to 
how big we can make the frame buffer and still maintain the refresh rate at 30 to 
60 frames per second. And to represent objects accurately with continuous para- 
meters, we need arbitrarily small sampling intervals. Therefore, unless hardware 
technology is developed to handle arbitrarily large frame buffers, increased 
screen resolution is not a complete solution to the aliasing problem. 

+ - Sampling 
Positions 

(a) 

Figure 4-36 
Sampling the periodic shape in (a) at 
the marked positions produces the 
aliased lower-frequency 
representation in (b). 



With raster systems that are capable of displaying more than two intensity 
levels (color or gray sc,lle), we can apply antialiasing methods to modify pixel in- 
tensities. By appropriatelv varving the intensities oi pixels along the boundaries 
of primitives, we can s~nooth the edges to lessen the jagged appearance. 

A straightforward antialiasing method is to increase sampling rate by treat- 
ing the screen as if it were covered with a finer grid than is actually available. We 
can then use multiple sample points across this finer grid to determine an appro- 
priate intensity level for each screen pixel. This technique of sampling object 
characteristics at a high resolution and displaying the results at a lower resolu- 
tion is called supersampling (or postfiltering, since the general method involves 
computing intensities ,it subpixel grid positions, then combining the results to 
obtain the pixel intensities). Displayed pixel positions are spots of light covering 
a finite area of the screen, and not infinitesimal mathematical points. Yet in the 
line and fill-area algorlrhms we have discussed, the intcnsity of each pixel is de- 
termined by the location of a single point on the object boundary. By supersani- 
pling, we obtain intensity information from multiple points that contribute to the 
o\-erall intensitv of a pixel. 

An alternative to qxrsampl ing  is to determine pixel intensity by calculat- 
ing the areas of overlak% of each pixel with the objects to be displayed. Antialias- 
in): by computing overlap areas is referred to as area sampling (or prefiltering, 
since the intensity of th* pixel as a whole is determined without calculating sub- 
pixel intensities). Pixcl o\.erlap areas are obtained 5y dctermining where object 
boundaries intersect ind i\*idual pixel boundaries. 

Raster objects can also be antialiased by shiftinl; the display location of 
pixel are'ls. This techni~quc, called pixel phasing, is applied by "microposition- 
ing" the electron beam In relation to object geometry. 

Supersampling Straight Line Segments 

Supersampling straight lines can be performed in several wavs. Fnr the gray- 
scale display of a straight-line segment, we can divide cach pixel into a number 
of subpixels and count the number of subpixels that are along the line path. The 
intensity level for each pixel is then set to a value that ib proportional to this sub- 
p~xel count. An example of this method is given in Fig 4-37. Each square pixel 
area is divided into nine cqual-sized square subpixels, and the shaded regions 
show the subpixels t h ~ t  would be selccted by Brescnhani's algorithm. This 
scheme provides for three intensity settings abovc zero, since the maximum 
number of subpixels that can be selected within any pixcl is three. For t h ~ s  exam- 
ple, the pixel at position (10. 20) is set to the maximum :ntmsity (level 3); pixels 
at (11, 21) and (12,Zl) are each set to the next highest intensity (level 2); and pix- 
els at (11, 20) and (12, 221 are each set to the lowest inten4ty above zero (level 1). 
Thus the line intensity IS  spread out over a greater nuniber of pixels, and the 
stairstep effect i.; smoothed by displaying A somewhat blurred line path in the 
vicinity of the stair step. (between horizontal runs). li w want to use more inten- 
sity levels: to antialiase tbt. line with this method, we increase the number of sam- 
pling positions across e x h  pixel. Sixteen subpixels gives 11s four intensity le\,els 
above zero; twenty-five subpixels gives us five levels; a d  so on. 

In the supersampling example of Fig. 4-37, we considered pixel areas of fi-  
nite size, but we treated the line as a mathematical entit) with zero width. Actu- 
ally, displayed lines habe a width approximately equal to that of a pixel. I f  we 
take the.finile width of the line into account, we can perform supersampling by 
setting each pixel intenbity proportional to the nuniber of subpixels inside the 



- 

Figure 4-37 
Supersampling subpixel positions 
along a straight line segment whose 
left endpoint is at screen 
coordinates (10,20), 

polygon representing the line area. A subpixel can be considered to be inside the 
line if its lower left comer is inside the polygon boundaries. An advantage of this 
supersampling p r o d u w  is that the number of possible intensity levels for each 
pixel is equal to the total number of subpixels within the pixel area. For the ex- 
ample in Fig. 4-37, we can represent this line with finite width by positioning the 
polygon boundaries parallel to the line path as in Fig. 4-38. And each pixel can 
now be set to one of nine possible brightness levels above zero. 

Another advantage of supersampling with a finite-width line is that the 
total line intensity is distributed over more pixels. In Fig. 4-38, we now have the 
pixel at grid position (10,21) tumed on (at intensity level 21, and we also pick up 
contributions from pixels immediately below and immediately to the left of posi- 
tion (10,21). Also, if we have a color display, we can extend the method to take 
background colors into account. A particular line might cross several different 
color areas, and we can average subpixel intensities to obtain pixel color settings. 
For instance, if five subpixels within a particular pixel area are determined to be 
inside the boundaries for a red line and the remaining four pixels fall within a 
blue background area, we can calculate the color for this pixel as 

pixek, = (5 . red + 4 . blue)/9 
The trade-off for these gains from supersampling a finite-width line is that 

idenhfying interior subpixelsrequires more calcul&o& than simply determining 
which subpixels are along the line path. These calculations are also complicated 
by the positioning of theline bounharies in relation to the line path. 'Il;is posi- 

Figure 4-38 
Supersampling subpixel positions 
in relation to the interior of a line of 
finite width. 

sea& 4 8  
Antialiasing 



tioning depends on the slope of the line. For a 45" line, the line path is centered 
on the polygon area; but lor either a horizontal or a vertic.11 line, we want the line 
path to be one of the pol~~gon boundaries. For instance, a horizontal line passing 
through grid coordinates (10,201 would be represented as the polygon bounded 
by horizontal grid lines y = 20 and y = 21. Similarly, the polygon representing a 
vertical line through (10, LO) would have vertical boundaries along grid lines x = 
10 and x = 1 1 .  For lines with slope I " 1 1  < 1 ,  the mathematical line path is posi- 
tioned propcrtionately closer to the lower polygon boundary; and for lines with 
slope I m I > 1 ,  this line path is placed closer to the upper polygon boundary. 

Pixel-Weighting Masks 

Supersampling algorithms are often implemented by giving more weight to sub- 
pixels near the center of a pixel area, since we would expect these subpixels to be 
more important in determining the overall intensity of a pixel. For the 3 bv 3 
pixel subdivisions we have considered so far, a weighting scheme as in Fig. 4-39 
could be used. The center subpixel here is weighted four times that of the corner 
subpixels and twice that of the remaining subpixels. Intensit~es calculated for 
each grid of nine subpixels would then be averaged so that the center subpixel is 
weighted by a factor or 1/4; the top, bottom, and side subpixels are each 
weighted by a factor of 1 !8; and the corner subpixels are each weighted by a fac- 
tor of 1 /16. An atray of values specifying the relative im-sortancc of subpixels is 
sometimes referred to as a "mask" of subpixel weights. Similar masks can be set 
up for larger subpixel grids. Also, these masks are often extended to include con- 
tributions from subpixels belonging to neighboring pixels, so that intensities can 
be averaged over adjacent pixels. 

Area Sampling Straight Line Segments 

We perform area sampling for a straight line by setting each pixel intensity pro- 
portional to the area of overlap of the pixel with the finite-width line. The line 
can be treated as a rectangle, and the section of the line area between two adja- 
cent vertical (or two adjacent horizontal) screen grid lines is then a trapezoid. 
Overlap areas for pixels are calculated by determining hob' much of the trape- 
zoid o"erlaps e a ~ h - ~ i x e l  In that vertical column (or ho&ontal row). In Fig. 4-38, 
the pixel with screen grid coordinates (10,20) is about 90 percent covered by the 
line area, so its intensitv would be set to 90 percent of the maximum intensity. 
Similarly, the pixel at (10 21) would be set to an intensity of about 15-percent of  
maximum. A method tor estimating pixel overlap areas is illustmted"by the su- 
persampling example in Fig. 4-38. The total number of si~bpixek within the line 
boundaries is approxirnatelv equal to the overlap area, and this estimation is im- 
proved by using iiner subpixel grids. With color displavs, the areas of pixel over- 
lap with different color regions is calculated and the final pixcl color is taken as 
the average color of the various overlap areas. 

1 2 1  
1 .- 

Filtering Techniques 

figure 4-39 A more accurate method for antialiasing lines is to use filtering techniques. The 
Relative weights for a grid of method is similar to applying a weighted pixel mask, but now we imagine a con- 
3 by 3 subpixels tinuous weighting surfrtcc, (or filter function) covering the pixel. Figure 4-40 shows 

examples of rectangular. conical, and Gaussian filter functions. Methods for ap- 
plying the filter function are similar to applying a weighting mask, but now we 

174 



integrate over the pixel surface to obtain the weighted average intensity. l o  re- section 4-8 
duce computation, table lookups are commonly used to evaluate the integrals. Anl~aliasing 

Pixel Phasing 

On raster systems that can address subpixel positions within the screen grid, 
pixel phasing can be used to antialias obpcts. Stairsteps along a line path or ob- 
ject boundary are smodthed out by moving (micropositioning) the electron beam 
to more nearly approximate positions specific by the object geometry. Systems 
incorporating this technique are designed so that individual pixel positions can 
be shifted by a fraction of a pixel diameter. The electron beam is typically shifted 
by 1/4, 1/2, or 3/4 of a pixel diameter to plot points closer to the true path of a 
line or object edge. Some systems also allow the size of individual pixels to be ad- 
justed as an additional means for distributing intensities. Figure 4-41 illustrates 
the antialiasing effects of pixel phasing on a variety of line paths. 

Compensating for Line Intensity Differences 

Antialiasing a line to soften the stairstep effect also compensates for another 
raster effect, illustrated in Fig. 4-42. Both lines are plotted with the same number 
of pixels, yet the diagonal line is longer than the horizontal line by a factor of fi. 
The visual effect of this is that the diagonal line appears less bright than the hori- 
zontal line, because the diagonal line is displayed with a lower intensity per unit 
length. A linedrawing algorithm could be adapted to compensate for this effect 
by adjusting the intensity of each line according to its slope. Horizontal and verti- 
cal lines would be displayed with the lowest intensity, while 45O lines would be 
given the highest intensity. But if antialiasing techniques are applied to a display, 

A----- 
Box F I ! IP~  

8 . ,  

Gaussian Filter 

(c) 

- 

Fiptrr 4-40 
Common filter functions used to antialias h e  paths. The volume of 
each filter is normalized to I ,  and the height gives the relative weight at 
any subpixel position. 



Chaper 4 intensities alp automatically compensated. When the f i ~ t e  width of lines is taken 
AItribules of Outpu~ Primitivs into account, pixel intensities are adjusted so that Lines display a total intensity 

proportional to their length. 

Antialiasing Area Boundaries 

The antialiasing concepts we have discussed for lines can also be applied to the 
boundaries of areas to remove their jagged appearance. We can incorporate these 
procedures into a scan-line algorithm to smooth the area outline as the area is 
generated. 

If system capabilities permit the repositioning of pixels, area boundaries can 
be smoothed by adjusting boundary pixel positions so that they are along the line 
defining an area boundary. Other methods adjust each pixel intensity at a bound- 
ary position according to the percent of pixel area that is inside the boundary. In 
Fig. 4-43, the pixel at position (x,  y) has about half its area inside the polygon 
boundary. Therefore, the intensity at that position would be adjusted to one-half 
its assigned value. At the next position ( x  + 1, y + 1) along the boundary, the in- 
tensity is adjusted to about one-third the assigned value for that point. Similar 
adjustments, based on the percent of pixel area coverage, are applied to the other 
intensity values amllnA the boundary. 

Figure 4-41 
Jagged l ies (a), plotted on the Merlin in system, are smoothed 
(b) with an antialiasing technique called pixel phasing. This techmque 
increases the number of addressable points on thesystem from 768 x 
576 to 3072 X 2304. (Courtesy of Megatek Corp.) 



Figure 4-42 
Unequal-length lines displayed 
with the same number of pixels in 
each line. 

Supersampling methods can be applied by subdividing the total area and 
determining the number of subpixels inside the area boundary. A pixel partition- 
ing into four subareas is shown in Fig. 4-44. The original 4 by 4 grid of pixels is 
turned into an 8 by 8 grid, and we now process eight scan lines across this grid 
instead of four. Figure 4-45 shows one of the pixel areas in this grid that overlaps 
an object boundary. Along the two scan lines we determine that three of the sub- 
pixel areas are inside the boundary. So we set the pixel intensity at 75 pexent of 
its maximum value. 

Another method for determining the percent of pixel area within a bound- 
ary, developed by Pitteway and Watkinson, is based on the midpoint line algo- 
rithm. This a1 orithm selects the next pixel along a line by detennining which of 
two pixels is c 7 oser to the line by testing the location of the midposition between 
the two pixels. As in the Bresenham algorithm, we set up a decision parameter p 
whose sign tells us which of the next two candidate pixels is closer to the line. By 
slightly modifying the form of p, we obtain a quantity that also gives the percent 
of the current pixel area that is covered by an object. 

We first consider the method for a line with slope rn in the range from 0 to 1. 
In Fig. 4-46, a straight line path is shown on a pixel grid. Assuming that the pixel 
at position (xi, y3 has been plotted, the next pixel nearest the line at x = xk+ 1 is 
either the pixel at yk or the one at y, + 1. We can determine which pixel is nearer 
with the calculation 

This gives the vertical distance from the actual y coordinate on the line to the 
halfway point between pixels at position yt and yt + 1. If this difference calcula- 
tion is negative, the pixel at yk is closer to the line. lf the difference is positive, the 

Section 4-8 

Antialiasing 

I 2 , . l I  I area boundary. 



pixel at yk + 1 is closer. We can adjust this calculation so that it produces a posi- 
Anributes of O u W  Primitives tive number in the range from 0 to 1 by adding the quantity 1 - m: 

Figure 4-44 
A 4 by 4 pixel section of a 
raster display subdivided into 
an 8 by 8 grid. 

Figure 4-45 
A subdivided pixel area with 
three subdivisions inside an 
object boundary l ie.  

Now the pixel at yk is nearer if p < 1 - m, and the pixel at yk + 1 is nearer if 
p > 1  - m .  

Parameter p also measures the amount of the current pixel that is over- 
lapped by the area. For the pixel at (x,, yk) in Fig. 4-47, the interior part of the 
pixel has an area that can be calculated as 

area = mxk + b - y, + 0.5 (4-9) 

This expression for the overlap area of the pixel at (x, y,) is the same as that for 
parameter p in Eq. 4-8. Therefore, by evaluating p to determine the next pixel po- 
sition along the polygon boundary, we also determine the percent of area cover- 
age for the current pixel. 

We can generalize this algorithm to accommodate lines with negative 
slopes and lines with slopes greater than 1. This calculation for parameter p could 
then be incorporated into a midpoint line algorithm to locate pixel positions and 
an obpd edge and to concurrently adjust pixel intensities along the boundary 
lines. Also, we can adjust the calculations to reference pixel coordinates at their 
lower left coordinates and maintain area proportions as discussed in Section 3-10. 

At polygon vertices and for very skinny polygons, as shown in Fig. 4-48, we 
have more than one boundary edge passing through a pixel area. For these cases, 
we need to modify the Pitteway-Watkinson algorithm by processing all edges 
passing through a pixel and determining the correct interior area. 

Filtering techniques discussed for line antialiasing can also be applied to 
area edges. Also, the various antialiasing methods can be applied to polygon 
areas or to regions with curved boundaries. Boundary equations are used to esti- 
mate area ov&lap of pixel regions with the area to b;! displayed. And coherence 
techniques are used along and between scan lines to simplify the calculations. 

SUMMARY 

F i p r e  4-46 
Boundary edge of an area 
passing through a pixel grid 
section. 

In this chapter, we have explored the various attributes that control the appear- 
ance of displayed primitives. Procedures for displaying primitives use attribute 
settings to adjust the output of algorithms for line-generation, area-filling, and 
text-string displays. 

The basic line attributes are line type, line color, and line width. Specifica- 
tions for line type include solid, dashed, and dotted lines. Line-color speclfica- 
tions can be given in terms of RGB components, which control the intensity of 
the three electron guns in an RGB monitor. Specifications for line width are given 
in terms of multiples of a standard, one-pixel-wide line. These attributes can be 
applied to both straight lines and curves. 

To reduce the size of the frame buffer, some raster systems use a separate 
color lookup table. This limits the number of colors that .can be displayed to the 
size of thelookup table. Full<olor systems are those that provide 24 bits per pixel 
and no separate color lookup table. 



- ~ -  -- - 
Figure 4-47 
Overlap area of a pixel rectangle, centered at position (xb yk), with the 
interior of a polygon area. 

Fill-area attributes include the fill style and the nU color or the fill pattern. 
When the fill style is to be solid, the fill color specifies the color for the solid fill of 
the polygon interior. A hollow-fill style produces an interior in the background 
color and a border in the fill color. The third type of fill is patterned. In this case, a 
selected array pattern is used to fill the polygon interior. 

An additional fill option provided in some packages is soft fill. This fill has 
applications in antialiasing and in painting packages. Soft-fill procedures provide 
a new fill color for a region that has the same variations as the previous fill color. 
One example of this approach is the linear soft-fill algorithm that assumes that 
the previous fill was a linear combination of foreground and background colors. 
This same linear relationship is then determined from the frame-buffer settings 
and used to repaint the area in a new color. 

Characters, defined as pixel grid patterns or as outline fonts, can be dis- 
played in different colors, sizes, and orientation~. To set the orientation of a char- 
acter string, we select a direction for the character up vector and a direction for 
the text path. In addition, we can set the alignment of a text string in relation to 
the start coordinate position. Marker symbols can be displayed using selected 
characters of various sizes and colors. 

Graphics packages can be devised to handle both unbundled and bundled 
attribute specifications. Unbundled attributes are those that are defined for only 
one type of output device. Bundled attribute specifications allow different sets of 
attributes to be used on different devices, but accessed with the same index num- 
ber in a bundle table. Bundle tables may be installation-defined, user-defined, or 
both. Functions to set the bundle table values specify workstation type and the 
attribute list for a gwen attribute index. 

To determine current settings for attributes and other parameters, we can 
invoke inquiry functions. In addition to retrieving color and other attribute infor- 
mation, we can obtain workstation codes and status values with inquiry func- 
tions. 

Because scan conversion is a digitizing process on raster systems, displayed 
primitives have a jagged appearance. This is due to the undersampling of infor- 
mation which rounds coordinate values to pixel positions. We can improve the 
appearance of raster primitives by applying antialiasing procedures that adjust 
pixel intensities. One method for doing this is to supersample. That is, we con- 

Figure 4-48 
Polygons with more than one 
boundary line passing 
through individual pixel 
regions. 

sider each pixel to be composed of subpixels and we calculate the intensity of the 



Chapter 4 

Attributes of Output Primitives 
subpixels and average the values of al l  subpixels. Alternatively, we can perform 
area sampling and determine the percentage o f  area coverage for a screen pixel, 
then set the pixel intensity proportional to this percentage. We can also weight 
the subpixel contributions according fo position, giving higher weights to the 
central subpiiels. Another method for antialiasing is to build special hardware 
configurations that can shift pixel positions. 

Table 4-4 lists the attributes discussed in this chapter for the output primi- 
tive classifications: line, fill area, text, and marker. The attribute functions that 
can be used in graphics packages are listed for each category. 

TABLE 4-4 
SUMMARY OF ATTRIBUTES 

Output Bundled- 
Primitive Associated A~ibute-Setting Attribute 

Type Attributes Functions Functions 

Line TYP 
Width 
Color 

F i l l  Area F i l l  Style 
Fi l l  Color 
Pattern 

Text Font 
Color 
Size 

Orientation 

Marker Type 
Size 
Color 

setLinetype 
setLineWidthScaleFactor 
set Pol ylineColourIndex 
setInteriorStyle 
setInteriorColorIndex 
setInteriorStyleIndex 
setPatternRepresentation 
setpatternsize 
setPatternReferencePoint 
setTextFonr 
setTextCo1ourIndex 
setcharacter~eight 
setCharacterExpansionFactor 
setCharaccerUpVector 
setTextPath 
setTextAlignment 
setMarkeirype 
setMarkerSizeScalePactor 
setPolyrnarkerColourIndex 

setPolymarkerIndex 
set PolymarkerRepresentation 

REFERENCES 

Color and grayscale considerations are discussed in Crow (1978) and in Heckben (1982). 
Soft-fill techniques are given in Fishkin and Barsky (1984). 
Antialiasing techniques. are discussed in Pittehay and Watkinson (1980). Crow (1981). 

Turkowski (1982), Korein and Badler (1983), and Kirk and Avro, Schilling, and Wu (1991). 
Attribute functions in PHlGS are discussed in Howard et al. (1991), Hopgood and Duce 

(1991), Gaskins (1 992). and Blake (1 993). For information on GKS workstations and anrib- 
utes, see Hopgood et al. (1 983) and Enderle, Kansy, and Pfaff (1 984). 

EXERCISES 

4-1. Implement the line-type function by modifying Bresenham's linedrawing algorithm to 
display either solid, dashed, or doned lines. 



4 2 .  Implernrni tlw line-type function with ,I mitlpoinl I ~ n c  algtirilhm to d~splay either 
solid, dashed, o~ dotted lines. Exercrses 

4-3 .  Ilevibr a pardllel method for implenirnting the Imc. Ivpe tunct~on 

4-4 I3c>v1sr. d paralld method for ~rnplementing the line-w~dth function 

4-5 .  A line spec~lied by two endpoints and a width (.an be converted to a rectangular poly- 
gon with four vertices and then displayed usmg a sc.an-lme method. Develop an effi- 
cient algorithm for computing thr four vertires nethded to define such a rectangle 
using the l ~ n r  endpoints and line width 

4-6. lmpler~ient tile i~ne-width function in a line-drawing xogram so that any one of three 
line iwdths can be displayed. 

4 . 7 .  Writr a prograrn to output a line graph o i  threr data wts defined ovel the same x coor- 
d~nate range. lnput to the program is to include the three sets of data values, labeling 
for the axes, m d  the coordinates for the display ate'l on the screen. The data sets arr 
to be scaled to iit the specified area, each ploned line is to be displayed In a differert 
line type (solid, dashed, dotted), and the axrs are to be labeled. (Instead of changing 
the line type. the three data sets can be piotted in d~iierent colors. ) 

4 -8 .  Set up an algorithm for displaying thick lines with e~ther bun caps, round caps, or prc- 
jccting square caps. These optlons can be provided 11.1 an option menu. 

4-9. Devise an algwithm for drsplay~ng thlck polyl~nes wrth either a miter join, a round 
jo~n, or a bevel join. These options can be provided 1'1 an option menu. 

4-10, Implement pen and brush menu options for a line-drawing procedure, including at 
leas: two options: round and square shapes. 

4- I I. Modity a Iiric-drawing algorithm so that the ~ntensit). of the output line is set according 
to its slope. That is, by adjusting pixel ~ntensities according to the value of the slope, 
all lnes are displayed with the same intensity per unlt length. 

4 . 1 2  Define and ~mplement a function for controlling the line type  solid, dashed, dotted) of 
d~splaved ellipses. 

4.13. Define and implement a function for setting the w~dtti of displayed ellipses 

4 14. Write a routlne to display a bar graph in anv specfled screen area. Input is to include 
Ihc data set, labeling for the coordmate ,ixes, and th,' coordinates for the screen area. 
The data set is to be scaled to fit the designdted wrwn area, and the bars are to be dis- 
played In drsignated colors or patleriis. 

4 - 1  5 .  Write d proc-edure to display two data sets defined wer  the same xcoordmate range, 
with rhe data values scaled lo  f ~ t  a specified regioi of the d~splay screen. The bars for 
one of the data sets are to be displaced horcrontally to produce an overlapping bar 
pattern for easy cornparison of the two scts of d a t ~  i!sc d different color or J different 
fill pattern for the two sets of bars. 

4-1 h.  Devise an algorithm for implementingd color lookup table and the setColourRep- 
resenta t ion operation. 

4-1 7. Suppwe you have d system with an %inch bv 10 irich video screen that can display 
100 pixels per inch. If a color lookup table with 64 positions is  used with th~s system, 
what is the smallest possible size (in bytes) ior the frame buffer? 

4-18 .  Consider an RGB raster system that has a 5 1 2 - b y 5 1 2  irame buffer with a 20 bits per 
pixel and a color lookup table with 24 bits per pixe . (a) How many dibtinct gray lev- 
els can be displayed with this system? (b) How many distinct colors (including gray 
levels) can be displayed? (c) How many c.olors c a i  be displayed at any one time? 
(J) What is the total memory size? (e) Explain two methods for reducing memory size 
while maintaining the same color capabilities. 

4-19. Modify the scan-line algorithm to apply any speciiied rectangular fill pattern to a poly- 
gon interior, starting from a designated pattern pos~tion. 

4-20. Write a procedure to fill the interior oi a given ellipse with a specified pattern. 

4-21. Write a procedure to implement the serPa:ternR.epresentation function. 



C1uplt.r 4 4-22. Ddine and implerne~lt ,i procedure for rhanging the sizr r ~ t  an exlstlng rectangular id1 

.411r1bu!cs 01 Outpirt Prtmitives pattern. 
4-23. Write a procedure to iniplement a soft-fill algorithm. Caretully def~ne what the soh-iill 

algorithm is to accon~plish and how colors are to be conib~ned. 

4.24. Dev~se an algorithm -or adjusting the height and width of r haracters defined as rectan- 
gular grid patterns 

4-25. Implement routines tor setting the character up vector and the text path for con troll in^ 
the display of characler strings. 

4.26. Write a program to align text as specified by input valuec for the alignment parame- 
ters. 

4.27. Dkvelop procedures ior implementing the marker attribute functions. 

4.28. Compare attribute-implementation procedures needed by systems that employ bun- 
dled anrihutes to tho5e needed by systems using unbundled anrlbutes. 

4-29. Develop procedures for storing and accessing attributes in unbundled system attribute 
tables. The procedures are to be designed (o store desbgnated attribute values in the 
system tables, to pass attributes to the appropriate output routines, and to pass attrlh- 
Utes to memory locations specified in inq~iry commands. 

4-30. Set up the same procedures described In the previou, exercise for bundled system at- 
tribute tables. 

4-31. Implement an antial~~rsing procedure by extending Bresenham's lhne algorithm to ad- 
just pixel intensities in the vicin~ty of a line path. 

4-32. Implement an antialiasing procedure for the midpoint line >lgorithrn. 

4-3 3 .  Develop an algorithn- for antialiasing elliptical boundarie!. 
4-35 )Modify the scan-line algorithm for area fill to incorporate antialiasing lJse coherence 

techniques to reduce calculations on successive scan lines 

4-35. Write d program to implement the Pitteway-Watkinwn arbaliasing algorithm 
as a scan-line procedure to fill a polygon interior. Use the routlne 
se tp ixe l  ( x ,  y, intensity) to load the inlensit), ~ q l u e  inlo the frame buifer at 
location ( x ,  y) .  





w ith the procedures for displaying output primitives and their attributes, we can create variety of pictures and graphs. In many applications, 
there is also a need for altering or manipulating displays. Design applications 
and facility layouts are created by arranging the orientations and sizes of the 
component parts of the scene. And animations are produced by moving the 
"camera" or the objects in a scene along animation paths. Changes in orientation, 
size, and shape are accomplished with geometric transformations that alter the 
coordinate descriptions of objects. The basic geometric transformations are trans- 
lation, rotation, and scahng. Other transformations that are often applied to ob- 
jects include reflection and shear. We first discuss methods for performing geo- 
metric transformations and then consider how transformation functions can be 
incorporated into graphics packages. 

Here, we first discuss general procedures for applying translation, rotation, and 
scaling parameters to reposition and resize two-dimensional objects. Then, in 
Section 5-2, we consider how transformation equations can be expressed in a 
more convenient matrix formulation that allows efficient combination of object 
transformations. 

A translation is applied to a n  object by repositioning it along a straight-line path 
from one coordinate location to another. We translate a two-dimensional point by 
addlng translation distances, f, and t,, to the original coordinate position (x, y) to 
move the point to a new position ( x ' ,  y') (Fig. 5-1). 

x'  = x + t , ,  y '  = y + t, (.i 7 )  
The translat~on distance pair (t,, t,) is called a translation vector or shift vector. 

We can express the translation equations 5-1 as a single matrix equation by 
usng  column vectors to represent coordinate positions and the translation vec- 
tor: 



This allows us to write the two-dimensional translation equations in the matrix 
form: 

Sometimes matrix-transformation equations are expressed in terms of coordinate 
row vectors instead of column vectors. In this case, we would write the matrix 
representations as  P = [ x  y] and T = [ k ,  $1. Since the column-vector representa- 
tion for a point is standard mathematical notation, and since many graphics 
packages, for example, GKS and PHIGS, also use the column-vector representa- 
tion, we will follow this convention. 

Translation is a rigid-body transformution that moves objects without defor- 
mation. That is, every point on the object is translated by the same amount. A 
straight Line segment is translated by applying the transformation equation 5-3 to 
each of the line endpoints and redrawing the line between the new endpoint po- 
sitions. Polygons are translated by adding the translation vector to the coordinate 
position of each vertex and regenerating the polygon using the new set of vertex 
coordinates and the current attribute settings. Figure 5-2 illustrates the applica- 
tion of a specified translation vector to move an object from one position to an- 
other. 

Similar methods are used to translate curved objects. To change the position 
of a circle or ellipse, we translate the center coordinates and redraw the figure in 
the new location. We translate other curves (for example, splines) by displacing 
the coordinate positions defining the objects, then we reconstruct the curve paths 
using the translated coordinate points. 

- 
tigr~rc 5 - 1  
Translating a point from 
position P to'position P' with 
translation vector T. 

-. . . - ., .- . -. - - - . . - -- - - 
r iprrv  5 - 2  

0 
Moving a polygon from position (a) 

5 10 IS 20 to position (b) with the translation 
(b) vector (-5.W, 3.75). 



Figure 5-3 
Rotation of an o b j t  through 
angle 0 about the pivot point 
(x,, y,). 

Figure 5-4 
Rotalion of a point from 
position (x,  y) to position 
( x ' ,  y ') through an angle 8 
relative to thecoordinate 
origin. The original angular 
displacement of the point 
from the x axis is 6. 

Rotation 

A two-dimensional rotation is applied to an object by repositioning it along a cir- 
cular path in the xy plane. To generate a rotation, we specify a rotation angle 0 
and the position (x, ,  y,l of the rotation point (or pivot point) about which the ob- 
ject is to be rotated (Fig. 5-3). Positive values for the rotation angle define coun- 
terclockwise rotations about the pivot point, as  in Fig. 5-3, and negative values 
rotate objects in the clockwise direction. This transformation can also be de- 
scribed as a rotation about a rotation axis that is perpendicular to the xy plane 
and passes through the pivot point. 

We first determine the transformation equations for rotation of a point posi- 
tion P when the pivot point is at the coordinate origin. The angular and coordi- 
nate relationships of the original and transformed point positions are shown in 
Fig. 5-4. In this fijyre, r is the constant distance of the poinl from the origin, angle 
4 is the original angular position of the point from the horizontal, and t3 is the ro- 
tation angle. Using standard trigonometric identities, we can express the trans- 
formed coordinates in terms of angles 0 and 6 as 

The original coordinates of the point in polar coordinates are 

x = r cos 4, y = r sin & (5 .5 )  

Substituting expressions 5-5 into 5-4, we obtain the kansiormation equations for 
rotating a point at position (x ,  y) through an angle 9 about the origin: 

x ' = x c o s O - y s i n 0  

y'= xsin 0 + y cos 0 

With the column-vector representations 5-2 for coordinate positions, we can write 
the rotation equations in the matrix form: 

where the rotation matrix is 

cos 0 -sin 0 
R = [  

sin t3 cos 8 1 
When coordinate positions are represented as row vectors instead of col- 

umn vectors, the matrix product in rotation equation 5-7 is transposed so that the 
transformed row coordinate vector Ix' y'l iscalculated as 

where PT = (x y], and the transpose RT of matrix R is obtained by interchanging 
rows and columns. For a rotation matrix, the transpose i s  obtained by simply 
changing the sign of the sine terms. 



Rotatton of a point about an arbitrary pivot position is iltustrated in Fig. 5-5. 
Using lhc trigonometric relationships in this figure, we can generalize Eqs. 5-6 to 
obtain the transformation equations for rotation of a point about any specified m- 
tation position (x,,!~,): 

X ' = X  , + (  a - x,) cos V - (y - y,) sin 0 

y = , + ( 1  - v , )  sin H + (y - y,) cos B (5-9) 

These general rotation equations differ from Eqs. 5-6 by the inclusion of additive 
terms, as well as the multiplicative factors on the coordinate values. Thus, the 
matrix expression 5-7 could be modified to includt: pivot coordinates by matrix 
addition of a column vector whose elements contain the additive (translational) 
terms In Eqs. 5-9. There are better ways, however, to formulate such matrix equa- 
tions, and we discuss in Section 5-2 a more consistent scheme for representing the 
transformation equations. 

A s  with translations, rotations are rigid-body transformations that move 
objects without deformation. Every point on an object is rotated through the 
same anglc. A straight line segment is rotated by applying the rotation equations 
5-9 to each ot tht' line endpoints and redrawing the line between the new end- 
point positions. Polygons are rotated by displacing each vertex through the speci- 
fied rotation angle and regenerating the polygon using the new vertices. Curved 
lines arc rotatcd by repositioning the defining p ~ r ~ t s  and redrawing the curves. 
A circle k>r ,117 ellipse, for instance, can be rotated about a noncentral axis by mov- 
ins the center position through the arc that subtcncs thc sprcified rotation angle. 
An ellipse can be rotated about its center coordinates by rotating the major and 
minor axes. 

Scaling 

A scaling transformation alters the size of an object. This operation can be car- 
ried out for polygons by multiplying the coordinate values ( x ,  y) of each vertex 
by scaling factors s, and s, to produce the transformed coordinates (x', y'): 

Scaling factor s, scales objects in the x direction, while sy scales in they direction. 
The transformation equations 5-10 can also be written in the matrix form: 

Section 5-1 

liasuc Trans(orma1ions 

-- 
.'r,qu rr 5-5 
Kotating a poinl from 
position ( x ,  y) to position 
11: y ') through an angle 8 
about rotation point ( x ,  . y,). 

whew S is the 2 by 2 scaling matrix in Eq. 5-11. 
,Any positive numeric values can be assigned to the scaling factors s, and sy. 

Values less than 1 reduce the size of objects; values greater than 1 produce an en- 
largement. Specifying a value of 1 for both s, and s, leaves the size of objects un- 
changed. When s, and s, are assigned the same value, a uniform scaling is pro- 



Chapter 5 

Two.D~mens~onal Geometric 
Trdnsformalions 

F ~ p r e  5-6 
Turnmg a square (a) Into a 
rectangle (b) wlth scaling 
factors s, = 2 and by - 1. 

- - - - - . . . - - - . - . . . -. . - -, - - 
F i g ~ ~ r c  5 - 7  
A line scaled with Eq 5-12 
using s, - 3 .  = 0.5 is reduced 
in size and moved closer to 
the coordinate ongin. 

Figurr 5-8 
Scaling relatwe to a chosen 
fixed point (I,, y,) Distanm 
from each polygon vertex to 
the fixed point are scaled by 
transformation equations 
5-13. 

duced that maintains relative object proportions. Unequai values for s, and s, re- 
sult in a differential scaling that is often used in design applications, whew pic- 
tures are constructed from a few basic shapes that can be adjusted by scaling and 
positioning transformations (Fig. 5-6). 

Objects transformed with Eq. 5-11 are both scaled and repositioned. Scaling 
factors with values less than 1 move objects closer to the coordinate origin, while 
values greater than 1 move coordinate positions farther irom the origin. Figure 
5-7 illustrates scaling a line by assigning the value 0.5 to both s, a t ~ d  sr in Eq. 
5-11. Both the line length and the distance from the origin are reduced by a 
factor of 1 /2. 

We can control the location of a scaled object by choosing a position, called 
the fixed point, that is to remain unchanged after the scaling transformation. Co- 
ordinates for the fixed point (xl, y,) can be chosen as one of the vertices, the object 
centroid, or any other position (Fig. 5-8). A polygon is then scaled relative to the 
fixed point by scaling the distance from each vertex to the fixed point. For a ver- 
tex with coordinates (x: y.i, the scaled coord~nates ( x ' ,  y ') are calculated as 

We can rewrite these scaling transformations to separate. the mdtiplicative and 
additive terms: 

where the additive terms r,(l - s,) and y,(l - s,) are constant for all points in the 
object. 

Including coordinat~?~ for a hxed point in the scalin~: equations is similar to 
including coordinates for a pivot point in the rotation equations. We can set up  a 
column vector whose elements are the constant terms in Eqs. 5-14, then we add 
this column vector to the product S P in Eq. 5-12. In the next section, we discuss 
a matrix formulation for the transformation equations that involves only matrix 
mu1 tiplication. 

Polygons are scaled by applying transformations ,514 to each vertex and 
then regenerating the polygon using the transformed vertices. Other objects are 
scaled by applylng the scaling transformation equations to the parameters defin- 
ing the objects. An ellipse in standard position is resized by scaling the semima- 
jor and semiminor axes and redrawing the ellipse about the designated center co- 
ordinates. Uniform scaling of a circle is done by simply adjusting the radius. 
Then we redisplav the circle about the center coordinates using the transformed 
radius. 

5-2 
MATRlX  RFPRESENtATlONS A N D  HOMOGENEOUS 
C O O R D I N A T E S  

Many graphics applications involve sequences of geometric transformations. An 
animation, for example, might require a n  obpct to be translated and rotated at 
each increment of the motion. In design and picture construction applications, 



we perform translations, rotations, and scalings to tit the picture components into 5-2 
their proper posihons. Here we consider how the matrix representations dis- Matrix Rewesentations and 
cussed in the previous sections can be reformulatej so that such transformation iOmo~eneous Coordinates 

sequences can be efficiently processed. 
We have seen in Section 5-1 that each of the basic transformations can be ex- 

pressed in the general matrix form 

with coordinate positions P and P' represented as c..dumn vectors. Matrix MI is a 
2 by 2 array containing multiplicative factors, and M, is a two-element column 
nratrix containing translational terms. For translation, MI is the identity matrix. 
For rotation or scaling, M2 contains the translational terms associated with the 
pivot pornt or scaling fixed point. To produce a sequence of hansformations with 
these equations, such as scaling followed by rotation then translation, we must 
calculate the transformed coordinates one step at i1 time. First, coordinate posi- 
tions are scaled, then these scaled coordinates are rotated, and finally the rotated 
coordinates are translated. A more efficient approxh would be to combine the 
transformations so that the final coordinate pnsitions are obtained directly from 
the initial coordinates, thereby eliminating the calculation of intermediate coordi- 
nate values. To he able to d o  this, we need to reformulate Eq. 5-15 to eliminate the 
matrix addition associated with the translation terms in M2. 

We can combine the multiplicative and translational terms for two-dimen- 
sronal geometric transformations into a single nratrix representation by expand- 
ing the 2 by 2 matrix representations to 3 by 3 matrices. This allows us to express 
all transformation equations as matrix multiplications, providing that we also ex- 
pand the matrix representations for coordinate positions. To express any two-di- 
mensional transformation as a matrix multiplication, we represent each Cartesian 
coordinate pos~tion ( 1 .  y )  with the homogeneous coordinate triple (x , ,  y,,, h), 
where 

 IS, a general lroniogeneous coordinate representation can also be written as ( h .  
r-, h .y. ! I ) .  For two-dimensional geomctric transformations, we can choose the ho- 
mogencous parameter h to be any nonzero value. Thus, there is an infinite num- 
ber of tsquivalent homogeneous representations i o ~  each coordinate point (x ,  y). 
A co~ivtwicnt choice is simply to set h = 1. Each txcl-d~nrensional position is then 
represented with homogeneous coordinates ( x ,  y, 1 ) .  Other values for parameter h 
are needed, for example, in matrix formulations of threedimensional viewing 
transformations 

The term I~or~;c~g~i~eorts  co,~rdrrmh~s is used in mathematics to refer to the ef- 
fect of this representation on Cartesian equations. \%hen a Cartesian point ( x ,  y) is 
convrrted to a homogeneous representahon (x , ,  y,,, h), equations containing x 
and I/, such as  I i x ,  y) = 0, become homogeneous tytations in the three parame- 
ters x,, y,, and 11. 'This just means that if each of thtl three parameters is replaced 
b y  any value n times that parameter, the value 7; c,ln he factored out of the equa- 
tions. 

Exp1-esSing positions in homogeneous rmrdin.ltes allows us to represent all 
geometric transformation equations as matrix multiplications. Coordinates are 



Chapter 5 represented with three-element column vectors, and transformation operations 
Two-Dimensional Geometric are written as 3 bv 3 matrices. For Wanslation. we have 

Transformations 

which we can write in the abbreviated form 

with T(t,, 1,) as the 3 by 3 translation matrix in Eq. 5-17. The inverse of thc trans- 
lation matrix is obtained by replacing the translation parameters 1,  and 1, with 
their negatives: - t ,  and -1,. 

Similarly, rotation transformation equations about the coordinate origin are 
now written as 

The rotation transformation operator R(8) 1s the 3 by 3 matrix in Eq. 5-19 with 
rotation parameter 8. We get the inverse rotation matr~x when 8 is replaced 
with -8. 

Finally, a scaling transformation relative to the coordinate or~gin is now ex- 
pressed as the matrix multiplication 

where Sk,, s,) is the 3 by 3 matrix in Eq. 5-21 with piirameters s, and sy. Replac- 
ing these parameters w ~ t h  their multiplicative inverses ( l i s ,  and I/sJ yields the 
inverse scaling matrix. 

Matrix representations are standard methods for implementing transforma- 
tions in graphics systems. In many systems, rotation and scaling functions pro- 
duce transformations with respect to the coordinate origin, as in Eqs. 5-19 and 
5-21. Rotations and scalings relative to other reference positions are then handled 
as a succession of transformation operations. An alternate approach in a graphics 
package is to provide parameters in the transformation functions for the scaling 
fixed-point coordinates and the pivot-point coordinates General rotation and 
scaling matrices that include the pivot or fixed point are then set u p  directly 
without the need to invoke a succession of transformation functions. 



5-3 Section 5-3 

COMPOSITE TRANSFORMATIONS Compo~ire Transformal1on5 

With the matrix representations of the previous sei:tion, we can set up a matrix 
for any sequence of transformations as a composite transformation matrix by 
calculating the matrix product of the individual transformations. Fonning prod- 
ucts of transformation matrices is often referred to as a concatenation, or compo- 
sition, of matrices. For column-matrix representation of coordinate positions, we 
form composite transformations by multiplying matrices in order from right to 
left. That is, each successive transformation matrix premultiplies the product of 
the preceding transformation matrices. 

Translatons 

I f  two successive translation vectors (t,,, tyl)  and (I,,, ty2) are applied to a coordi- 
nate position P, the final transformed location P' is calculated as 

where P and P are represented as homogeneous-coordinate column vectors. We 
can verify this result by calculating the matrix product for the two associative 
groupings Also, the composite transformat~on matrix for thls sequence of trans- 
lations is 

which demonstrates that two successive translatiolr:; are additive. 

Kota t~ons  

Two successive rotations applied to pomt p product. the transformed position 

P' = R(B2) . IR(0 , )  . P' 
= {R(&) . R(0, )I  1' (.i Z r  1 

By multiplying the two rotation matrices, we can vl?rify that two successive rota- 
tions are additive: 

so that the final rotated coordinates can be calculated with the composite rotation 
matrix as 



Chapter 5 

Two-Dimensional Geometric 
Transformations 

Scaling 

Concatenating transformation matrices for two successive scaling operations pro- 
duces the following composite scaling matrix: 

The resulting matrix in this case indicates that successive scaling operations are 
multiplicative. That is, if we were to triple the size of an object twice in succes- 
sion, the final size would be nine times that of the original. 

General Pivot-Point Rotation 

With a graphics package that only provides a rotate function for revolving objects 
about the coordinate origin, we can generate rotations about any selected pivot 
point (x, y,) by performing the following sequence of translate-rotatetranslate 
operations: 

1. Translate the object so that the pivot-point position is moved to the coordi- 
nate origin. 

2. Rotate the object about the coordinate origin. 
3. Translate the object so that the pivot point is returned to its original posi- 

tion. 

This transformation sequence is illustrated in Fig. 5-9. The composite transforma- 

TranNmnon of 
ObiSaramM 
tha Pivor Point 

IS RsturMld 
to Position 

I x,. v.) 

Figurc 5-9 
A transformation sequence for rotating an objed about a specified pivot mint using the 
rotation matrix R(B) of transformation 5-19. 



tlon ni,ltr~x for thls sequence is obtained with the cc.mcatenation 

cos H -sin tJ x , ( l  - ros 9) t y, sin 9 
9 y.(l - cos @) - x,sin 8 

I I 
which can be expressed in the form 

ivhere 'r( -x , ,  - y,) = T '(x,, y,). In general, a rotate function can be set up to ac- 
cept parameters for pivot-point coordinates, as well as the rotation angle, and to 
generate automatically the rotation matrix of Eq. 5-31 

Gentval Fixed-Po~nt Scaling 

Figure 5-10 illustrates a transformation sequence tcs produce scaling with respect 
tu a selected fixed position (x!, y,) using a scaling hmction that can only scale rela- 
lve to the coordinate origin. 

1. Translate object so that the fixed point coincichrs with the coordinate origin. 
2. Scale the object with respect to the coordinate origin. 
3. Use the inverse translation of step 1 to return the object to its original posi- 

tion. 

Concatenating the matrices for these three operations produces the required scal- 
ing matrix 

T h ~ s  transiormation is automatically generated on systems that provide a scale 
function that accepts coordinates for the fixed point 

Genw '11 Scal~ng Directions 

Section 5-3 

Cc~rnpusite Transformallons 

Parameters s, and s, scale objects along the x and y directions. We can scale an ob- 
ject in other directions by rotating the object to align the desired scaling direc- 
tions with the coordinate axes before applying the scaling transformation. 

Suppose we want to apply scaling factors with values specified by parame- 
ters sl and S2 in the directions shown in Fig. 5-11. TCI accomplish the scaling with- 



Tranmlab O b j d  
m that Ihe Fired Pdnl 

Is R e t u r d  to 
Pcnitim (x,. v,) 

Figure 5-10 
A transformation sequence for &g an object with =pea to a specified fixed position 
using the scaling matrix S(s,, s,) of transformation 5-21. 

out changing the orientation of the object, we first perform a rotation so that the 
directions for s, and s2 coincide with the x and y axes, respectively. Then the scal- 
ing transformation is applied, followed by an opposite rotation to return points 
to their original orientations. The composite matrix resulting from the product of 
these three transformations is 

s, cos2 8+ s2 sin2 8 (s2 - s,) cos 8 sin 8 0 
@sin 8 sl sin2 8+ s2 cos28 (5-35) 

0 01 1 
Figure 5-11 As an example of this scahg transformation, we turn a unit square into a 

paam- sl and parallelogram (Fig. 512)  by shttching it along the diagonal from (0, 0) to (1, 1). 
s, am to be applied in We rotate the diagonal onto they axis and double its length with the transforma- 
orthogonal directions 
defined by the angular tion parameters 8 = 4 5 O ,  s, = 1, and s2 = 2. 

displacement 6. In Eq. 535, we assumed that scaling was to be performed relative to the ori- 
gin. We could take this scaling operation one step further and concatenate the 
matrix with translation operators, so that the composite matrix would include 
parameters for the specification of a scaling fixed position. 

Concatenation Properties 

Matrix multiplication is associative. For any three matrices, A, B, and C, the ma- 
trix product A - B . C can be performed by first multiplying A and B or by first 
multiplying B and C: 

Therefore, we can evaluate matrix products using either a left-to-right or a right- 
teleft associative grouping. 

On the other hand, transformation products may not be commutative: The 
matrix product A. B is not equal to B - A, in general. This means that if we want 



Figure 5-12 
A square (a) is converted to a parallelogram (b) using the composite 
transformation matrix 5-35, with s, = 1, s2 = 2, and 0 = 45". 

to translate and rotate an object, we must be careful aboc! the order in which the 
composite matrix is evaluated (Fig. 5-13). For some special cases, such as a se- 
quence of transformations ali of the same kind, the multiplication of transforma- 
tion matrices is commutative. As an example, two successive rotations could be 
performed in either order and the final position would be the same. This commu- 
iative property holds also for two succ&sive translations or two successive scal- 
ings. Another commutative pair of operations is rotation and uniform scaling 

General Composite Transformations and Computational Efficiency 

A general two-dimensional transformation, representing a combination of trans- 
lations, rotations, and scalings, can be expressed as 

The four elements rs,, are the multiplicative rotation-scaling terms in the transfor- 
mation that involve only rotation angles and scaling factors. Elements trs, and 
trs, are the translational terms containing combinations of translation distances, 
pivot-point and fixed-point coordinates, and rotation angles and scaling parame- 
ters. For example, if an object is to be scaled and rotated about its centroid coordi- 
nates (x, y,) and then translated, the values for the elements of the composite 
transformation matrix are 

TUX, t,) . Nx,, y,, 9) . S(x,, y,, s,, s,) 
s, cos 0 -s, sin 0 x,(l - s,cos 0) + y,s, sin 0+ t, 

0 y,(l - sy cos 9) - x,s, sin t, I (5-38) 1 
Although matrix equation 5-37 requires nine multiplications and six addi- 

tions, the explicit calculations for the transformed coordinates are 

Composite Transformations 



Chapter 5 
Twdlimensional Geometric 

Transformations 
Final 

- - - - - - - . . . . . . . - . . . . - - - - - - - - - - - - - -. - 
Figure 5-13 
Reversing the order in which a sequence of transformation> IS 
performed may affect the transformed position of an object. In (r ) ,  an 
object is first translated, then rotated In (b), the o b j t  is mtated first, 
then translated. 

Thus, we actually only need to perform fbur multiplications and four additions 
to transform coordinate positions, This is the maximum number of computation., 
required for any translormation sequence, once the individual n~atricps h a w  
been concatenated and the elements of the composite matrix cvaluatcd. Withour 
concatenation, the md~c:dual transformations would bt applied one at a time 
and the number of calnrlations could be significantly rncrrascd. Ail cff~c~ent in: 
plementation for the trar~sformatiun operations, therefor*, is to formulate trans- 
formation matrices, concatenate any transformation sequence, and calculnt~ 
transformed coordinates using Eq. 5-39. On parallei systems, direct matrix multi 
plications wlth the composite transformation matrix of Eq. 5-37 can be equally cf- 
ficient. 

A general rigid-body transformation matrix, in\dving onlv t r a n s l a t i ~ ~ ~ i ~  
and rotations, can be expressed in the form 

where the four elements r,, are the multiplicative rotation terms, and elements tr ,  
and t ry  are the translatior~al terms. A rigid-body change in coordinate position is 
also sometimes referred to as a rigid-motion transformation. All angles and dis- 
tances between coordinate positions are unchanged by the transformation. In ad- 
dition, matrix 5-40 has the property that its upper-left 2-bv-2 submatrix is an or- 
thogonal matrix. This means that i f  we consider each rot< of the submatrix as a 
vector, then the two vectors (r,,, r,,) and (r,,, r,) form an orthogonal set of unit 
vectors: Each vector has unit length 

and the vectors are perpendicular (their dot product is 0): 



Therefore, if these unit vectors are transformed by the rotatign submatrix, (r,,, r,) sech15-3 
is converted to a unit vector along the x axis and (ryl, rW) is transformed into a Composite Transformations 
unit vector along they axis of the coordinate system: 

As an example, the following rigid-body transformation first rotates an object 
through an angle %about a pivot point (I,, y,) and then translates: 

T(t,, t,). R(x,, y,, 0) 

cos 0 -sin 0 x,(l - cos 0) + y, sin 6 + t ,  
8 y,(l - cos 0) - x, sin 6 + t, 

1 I 
Here, orthogonal unit vectors in the upper-left 2-by-2 submatrix are (cos 0, 
-sin %) and (sin 0, cos 6), and 

Similarly, unit vector (sin 0, cos 0) is converted by the transformation matrix in 
Eq. 5-46 to the unit vector (0,l) in they direction. 

The orthogonal property of rotation matrices is useful for constructing a ro- 
tation matrix when we know the final orientation of an obpct rather than the 
amount of angular rotation necessary to put the object into that position. Direc- 
tions for the desired orientation of an obpct could be determined by the align- 
ment of certain ob* in a scene or by selected positions in the scene. Figure 5-14 
shows an object that is to be aligned with the unit direction vectors u' and v'. As- 
suming that the original object orientation, as shown in Fig. 5-14(a), is aligned 
with the coordinate axes, we construd the desired transformation by assigning 
the elements of u' to the first row of the rotation matrix and the elements of v' to 
the second row. This can be a convenient method for obtaining the transfonna- 
tion matrix for rotation within a local (or "object") coordinate system when we 
know the final orientation vectors. A similar transformation is the conversion of 
object descriptions from one coordinate system to another, and in Section 5-5, we 
consider how to set up transformations to accomplish this coordinate conversion. 

Since rotation calculations q u i r e  trignometric evaluations and several 
multiplications for each transformed point, computational efficiency can become 
an important consideration in rotation hansfonktions. In animations and other 
applications that involve many repeated transformations and small rotation an- 
gles, we can use approximations and iterative calculations to reduce computa- 



Chapter 5 

Two-Dimensional Geometric 

- - 

Figure 5-14 
The rotahon matrn for revolving an object from position (a) to position 
(b) can be constmcted with the values c.f thp unlt orientation vectors u' 
and v' relative tc the original orientation 

tions in the composite transformation equations. Whcn the rotation angle is 
small, the trigonometric functions can be replaced with approximation values 
based on the first few ttrrms of their power-series expansions. For small enough 
angles (less than lo0), cos 0 is approximately 1 and sln 0 has a value very close to 
the value of 8 in radians. If we are rotating in small angular steps about the ori- 
gin, for instance, we can set cos 8 to 1 and reduce transformation calculations at 
each step to two multiplications and two additions for each set of coordinates to 
be rotated: 

where sin 6 is evaluated once lor all steps, assuming the rotation angle does not 
change. The error introduced by this approximation at each step decreases as the 
rotation angle decreases. But even with small rotat~on angles, the accumulated 
error over many steps can become quite large. We can control the accumulated 
error by estimating the error in x' and y '  at each step and resetting object posi- 
tions when the error accumulation becomes too great. 

Composite transformations often involve inverse matrix calculations. Trans- 
formation sequences for general scaling directions and for reflections and shears 
(Section 5-9, for example, can be described with inverse rotation components. As 
we have noted, the inverse matrix representations for the basic geometric Erans- 
formations can be generated with simple procedvres. An inverse translation ma- 
trix is obtained by changing the signs of the translation distances, and an i n v e w  
rotation matrix is obtained by performing a matrix transpose (or changing the 
sign of the sine terms). These operations are much simpler than direct inverse 
matrix calculations. 

An implementation of composite transformations is given in the following 
procedure. Matrix M is initialized to the identity matrix. As each individual 
transformation is specified, it is concatenated with the total transformation ma- 
trix M. When all transformations have been specified, this composite transforma- 
tion is applied to a given object. For this example, a polygon is scaled and rotated 
about a given reference point. Then the object is translated. Figure 5-15 shows the 
original and final positions of the polygon transformed by this sequence. 



Section 5.3 

Figure 5-15 
A polygon (a) is transformed ~nto 
(b) by the composite operations in 
the following procedure. 

Winclude <math.h> 
Yinclude 'graphics.hm 

typedef float Matrix3x3 131 131 : 
Matrix3x3 thenatrix: 

void matrix3~3SetIdentity (Matrix3x3 rn) 
( 
int ;, j; 

Composite Transformalions 

for li=O; ic3; i++) for lj=O: j<3; j + r )  n[il[j] = (i == j); 
) 

/ *  Multiplies matrix a times b, putting result in b ' /  
void matrix3~3PreMultiply (Matrix3x3 a. Matrix3x3 b) 
i 
int r,c: 
Matrix3x3 tmp: 

for [r = 0; r < 3: r++) 
for (C = 0; c < 3; c++) 
tm~Irllcl = 
alrlIOl'bI0l[cI t a[rlIlltbllllcl + alrlI21'bl211cl: 

for (r = 0: r < 3: r++) 
for Ic = 0; c < 3: c + + )  
blrl Icl - tmplrl lcl: 

1 

void translate2 (int tx, int ty) 
( 
Matrix3x3 m: 

rnatrix3~3SetIdentity (n) : 
m[01[21 = tx; 
m111121 = ty: 
matrix3~3PreMultiply (m, theMatrix): 



vold scale2 (float sx.  rloat sy, wcPt2 refpL: 
( 
Macrix3xl m. 

matrix3~3SetIdentity (ml: 
m101 [OI = sx; 
m[0][2] = ( 1  - sx) ' refpt.x; 
mll] Ill = sy; 

void rotate2 (float a, wcPt2 refPc) 
i 
Matrix3x3 m; 

matrix3~3SetIdentity (m): 
a = pToRadians La); 
m[Ol L O !  z cosf (a); 
m[01 111 = sin: (a) ; 
m[0] [21 = rcfPt.x - (1 - cosf (a)) + refPt.y sinf (a); 
m[1] ( 0 1  = sinf (a); 
m[ll Ill = cosf (a]; 
m[l] [Z] = refPt.y (1 - cosf (a) - refPt.x ' sinf ( a ) ;  
matrix3~3PreMultiply (m, theMatrix); 

) 

void transformPoints2 (int npts, wcPt2 'ptsl 
( 
int k: 
float tmp ; 

for (k = 0; k npts: kt+) i 
trnp = ehcMatrix101 I01 ' ptsrk] . x  * theMatrix[O) lll ' 
pts1kl.y t theMatrix[0][21; 

pts(k1.y =. theMatrix[ll [O] * ptsikl .X + theMatrixl1) I11 
pts[kl .y r theMatrix[l] 121; 

pts(k1 .x tmp;  

1 

void main (int argc, char " argv) 
( 
wcPt2 ptsi31 : { 50.0, 50.0, 150.0, 50.0, 100.0, 150.0); 
wcPt2 refPt :. (100.0. 100.0); 
long windowID -; openGraphics (*a~gv, 200, 350); 

set8ac:iground ('NHITE) ; 
setcolor (BLUE); 
pFillArea 13, prs): 
matrix3~3SetIdentity LtheMatrix); 
scale2 (0.5, 0.5, refPt): 
rotate2 (90.0, refPt); 
translate2 (0, 1 5 0 ) ;  
transformpoints2 ( 3 ,  pts) 
pFillArca (3.pts) ; 
sleep !lo); 
closeGraphics (window1D) ; 

I 



5-4 Section 5-4 

OTHER TRANSFORMATIONS Uther Transformations 

Basic transformations such as translation, rotation, and scaling are included in 
most graphics packages. Some packages provide a few additional transforma- 
tions that are useful in certain applications. Two such transformations are reflec- 
tion and shear. 

Reflection 

Y 

A reflection is a transformation that produces a mimr image of an obpct. The 
mirror image for a two-dimensional reflection is generated relative to an axis of 
reflection by rotating the object 180" about the reflection axis. We can choose an 
axis of reflection in the xy plane or perpendicular to the xy plane. When the re- 
flection axis is a line in the xy plane, the rotation path about this axis is in a plane 
perpendicular to the xy plane. For reflection axes that are perpendicular to the xy 
plane, the rotation path is in the xy plane. Following are examples of some com- 
mon reflections. 

Reflection about the line y = 0, the x axis, is accomplished with the transfor- 
mation matrix 

14\ 

/I \\\ Original 
/ \\ Position 
: '  2 L ------- 

I \ 3  

This transformation keeps x values the same, but "flips" the y values of coordi- 
nate positions. The resulting orientation of an object after it has been reflected 
about the x axis is shown in Fig. 5-16. To envision the rotation transformation 
path for this reflection, we can think of the flat object moving out of the xy plane 
and rotating 180" through three-dimensional space about the x axis and back into 
the xy plane on the other side of the x axis. 

A reflection about the y axis flips x coordinates while keeping y coordinates 
the same. The matrix for this transformation is 

Figure 5-17 illustrates the change in position of an object that has been reflected 
about the line x = ,O. The equivalent rofation in this case is 180" through threedi- 
mensional space about they axis. 

We flip both the x and y coordinates of a point by reflecting relative to an 
axis that is perpendicular to the xy plane and that passes through the coordinate 
origin. This transformation, referred to as a reflection relative to the coordinate 
origin, has the matrix representation: 

Reflected 
Position '7'' I ' 

Figure ,516 
Reflection of an object about 
the x axis. 

Original I Refleded Position Position 

- 
Figuw 5-1 7 
Reflection of an object about 
they  axis. 



Original 
Position 

Reflected 
Position 

3' 

Figure 5-18 
Reflection of an object relative 
to an axis perpendicular to 
the ry plane and passing 
through the coordinate origin. 

,,;3 Original /' 
I \ Position / 

,/ \ 
/ '. 1 

2 // \'A1 // Reflected 

/ 3 ' 
/ 

/ 
/ 

-- - - - -- 
Figure 5-19 
Refledion of an object relative to an axis perpendicular to 
the xy plane and passing through point P,,,. 

An example of reflection about the origin is shown in Fig. 5-18. The reflection ma- 
tnx 5-50 is the rotation matrix R(9) with 6 = 180'. We are simply rotating the ob- 
ject in thc ry plane half a revolution about the origin. 

Reflection 5-50 can be generalized to any reflecticm point in the ry plane 
(Fig. 5-19). This reflection is the same as a 180" rotation in the xy plane using the 
reflection point as the pivot point. 

If we chose the reflection axis as  the diagonal line y = x (Fig. 5-20), the re- 
flection matrix is 

We can derive this matrix by concatenating a sequence of rotation and coordi- 
nate-axis reflection matrices. One possible sequence is shown In Fig. 5-21. Here, 
we first perform a clockwise rotation through a 45" angle, which rotates the line y 
= x onto the x axis. Next, we perform a rcflcction with respect to the x axis. The 
final step is to rotate the line y = x back to its original position with a counter- 
clockwise rotation through 45". Ar. equivalent sequence of transformations is first 
to reflect the object about the x axis, and then to rotate counterclockwise 90". 

To obtain a transfonnation matrix for reflection about the diagonal y = -x, 
we could concatenate matrices for the transformation sequence: (1) clockwise ro- 
tation by 45', (2) reflection about the y axis, and (3) counterc~ockwise rotation by 
45". The resulting transformation matrix is 

- - - - . - .. . - - - - . - . - - 
/.iprc3 .i-20 
Reflection of an obpct with 
mpect to the line y = x. 



Figure 5-22 shows the original and final positions for an object transformed with section 5-4 
thls reflection matrix. Other Transformalions 

Reflections about any line y = rnx t h in the ry plane can be accomplished 
with a combination of translatcrotate-reflect transfor~nations. In general, we first 
translate the Line so that it passes through the origin. Then we can rotate the line 
onto one of the coordinate axes and reflect about that axis. Finally, we restore the 
line to its original position with the inverse rotation and translation transforma- 
lions. 

We can implement reflections with respect to the coordinate axes or coordi- -- 
nate origin as scaling transformations with negative scaling factors. Also, ele- , 
ments of the reflection matrix can be set to values other than tl. Values whose 
magnitudes are greater than 1 shift the mirror image farther from the reflection (a) 
axis, and values with magnitudes less than 1 bring the mirror image closer to the 
reflection axis. 

Shear 
L - - J  

A transformation that distorts the shape of an object such that the transformed 
shape appears as if the object were composed of internal layers that had been -*-- 
caused to slide over each other is called a shear. Two common shearing transfor- 
mations are those that shift coordinate w values and those that shift y values. 

An x-direction shear relative to the x axis is produced with the transforma- 
tion matrix 

w 
(b) 

which transforms coordinate positions as 

Any real number can be assigned to the shear parameter sh,. A coordinate posi- 
tion (.u, y) is then shifted horizontally by an amount proportional to its distance (y 
value) from the x axis (y = 0). Setting sh, to 2, for example, changes the square in 
Fig. 5-23 into a parallelogram. Negative values for sh, shift coordinate positions 
to the left. 

We can generate x-direction shears relative to other reference lines with 

with coordinate positions transformed as 

F~gurc 5-21 
S~quence of transformations 
to produce reflection about 
the line y = x: (a) clockwise 
rotation of 4S0, (b) reflection 
about the x axis;and (c) 
counterclockwise rotation 
by 45". 

An example of this shearing transformation is given In Fig. 5-24 for a shear para- 
meter value of 1 /2  relative to the line yd = -1. 



Figure 5-23 
A unit square (a) is converted to a parallelogram (b) using the x- 
direction shear matrix 5-53 with sh, = 2. 

A y-direction shear relative to the line x = x,,+ is generated with the trans- 
Frgwe 5-22 formation matrix 
Reflection with respect to the 
line y = -x. 

which generates transformed coordinate positions 

This transformation sh~f t s  a coordinate position vertically by an amount propor- 
tional to its distance from the reference line x = x,,. Figure 5-25 illustrates the 
conversion of a square into a parallelogram with shy = 1 i'2 and x, = -1. 

Shearing operations can be expressed as sequences of basic transfomatio-. 
The x-direction shear matrix 5-53, for example, can be written as a composite 
transformation involv~ng a serles of rotation and scaling matrices that would 
scale the unit square of Fig. 5-23 along its diagonal, while maintaining the origi- 
nal lengths and orientations of edges parallel to thex axis. Shifts in the positions 
of objects relative to shearing reference lines are equivalent to translations. 

-- - - .  - 
Figure 5-24 
A unit square (a) is transformed to a shifted parallelogram (b) 
with sh, = 1!2 and y,, = - 1 in the shear matrix 5.55. 



-ion 5-5 

Transformations between 
Coordinate Systems 

Fipre 5-25 
A unit square (a) is turned into a shifted parallelogram (b) with 
parameter values shy = 1 /2  and x,, = - 1 in the y d i i o n  using 
shearing transformation 5-57. 

5-5 
TRANSFORMATIONS BETWEEN COORDINATE SYSTEMS 

Graphics applications often require the transformation of object descriptions 
from one coordinate svstem to another. Sometimes obieas are described in non- 
Cartesian reference frames that take advantage of o b p a  symmetries. Coordinate 
descriptions in these systems must then be converted to Cartesian device coordi- 
nates for display. Some examples of twedimensional nonCartesian systems are 
polar coordinates, elliptical coordinates, and parabolic coordinates. In other 
cases, we need to transform between two Cartesian systems. For modeling and 
design applications, individual obpds  may be d e h e d  in their own local Carte- 
sian references, and the local coordinates must then be transformed to position 
the objects within the overall scene coordinate system. A facility management 
program tor office layouts, for instance, has individual coordinate reference de- 
scriptions for chairs and tables and other furniture that can be placed into a floor 
plan, with multiple copies of the chairs and other items in different positions. In 
other applications, we may simply want to reorient the coordinate reference for 
displaying a scene. Relationships between Cartesian reference systems and some 
c%mrnon non-Cartesian systems are given in Appendix A. Here, we consider 
transformations between two Cartesian frames of reference. 

Figure 5-26 shows two Cartesian systems, with the coordinate origins at (0, 
0) and (xO, yo) and with an orientation angle 8 between the x and x '  axes. To trans- 
form object descriptions from xy coordinates to x'y' coordinates, we need to set 
up a transformation that superimposes the x'y' axes onto the xy axes. This is 
done in two steps: 

1. Translate so that the origin (x, yo) of the x'y' system is moved to the origin 
of the xy system. 

2. Rotate the x '  axis onto the x axis. 

Translation of the coordinate origin is expressed with the matrix operation 



Chapter 5 

Two-D~rnensional Cmrnelric 
Transformations 

y axis 1 

A Cartesian x'y' system positioned 
at (rb y,,) with orientation 0 in an x.v 4' XD cirtesian system. 

and the orientation of the two systems after the translation operation would a p  
pear as  in Fig. 5-27. To get the axes of the two systems into coincidence, we then 
perform the clockwise rotation 

Concatinating these two transformations matrices gives us the complete compos- 
ite matrix for transforming object descriptions from the ry system to the x'y' sys- 
tem: 

An alternate method for giving the orientation of the second coordinate sys- 
tem is to specify a vector V that indicates the direction for the positive y' axis, a s  
shown in Fig. 5-28. Vector V is specified as a point in the xy reference Frame rela- 
tive to the origin of the xy system. A unit vector in the y '  direction can then be 
obtained as 

And we obtain the unit vector u along the x '  axis by rotating v 90" clockwise: 

-- 

4 Figure 5-27 Position of the reference frames 
shown in Fig. 5-26 after translating 
the origin of the x'y' system to the 

X XaXiS coordinate origin of the xy system. 



y axis,  Section 5-5 

Transformarions beIween 
Coordinate Systems 

v , Yo \ Figure 5-28 Cartesian system x'y'  with origin at 
. . .. - - . - - . -- : Po = (x, yo) and y' axis parallel to 

O ]  Xo xaxis vector V. 

In Section 5-3, we noted that the elements of any rotation matrix could be ex- 
pressed as elements of a set of orthogonal unit vectors. Therefore, the matrix to 
rotate the r'y' system into coincidence with the xy system can be written as 

As an example, suppose we  choose the orientation for they' axis as V = (- 1,0), 
then the x' axis is in the positive y direction and the rotation transformation ma- 
trix is 

Equivalently, we  can obtain this rotation matrix from 5-60 by setting the orienta- 
tion angle as  8 = 90". 

In an interactive application, it may be more convenient to choose the direc- 
tion for V relative to position Po than it is to specify it relative to the xy-coordi- 
nate origin. Unit vectors u and v would then be oriented as shown in Fig. 5-29. 
The components of v are now calculated as 

and u is obtained as the perpendicular to v that forms a right-handed Cartesian 
system. 

v axis 

K F i p r r  5-29 Yo 7 A Cartesian x'y' system defined 
Po with two coordinate positions, Po 

i 
. .., -- .. --... +- ---- and P,, within an xy reference 

0 I 
Xo x axis frame. 



ChaptwS 5 4  
T w * D ' m e " s i ~ I ~ s ~ a m ~ ~ ~  AFFINE TRANSFORMATIONS 

A coordinate transfomation of the form 

is called a two-dimensional affine transformation. Each of the transformed coor- 
dinates x' and y ' is a linear fundion of the original coordinates x and y, and para- 
meters a,, and bk are constants determined by the transformation type. Affine 
transformations have the general properties that parallel lines are transformed 
into parallel lines and finite points map to finite points. 

Translation, rotation, scaling, reflection, and shear are exampks of two-di- 
mensional affine transformations. Any general two-dimensional affine transfor- 
mation can always be expressed as  a composition of these five transformations. 
Another affine transformation is the conversion of coordinate descriptions fmm 
one reference system to another, which can be described as a combination of 
translation and rotation An affine transformation involving only rotation, trans- 
lation, and reflection preserves angles and lengths, as  well as parallel lines. For 
these three transformations, the lengths and angle between two lines remains the 
same after the transformation. 

5-7 
TRANSFORMATION FUNCTIONS 

Graphics packages can be structured so that separate commands are provided to 
a user for each of the basic transformation operations, as in p r o c e d u r e  trans- 
fo rmobjec t .  A composite transformation is then set up  by referencing individ- 
ual functions in the order required for the transfomtion sequence. An alternate 
formulation is to provide users with a single transformation hnction that in- 
cludes parameters for each of the basic transformations. The output of this func- 
tion is the composite transformation matrix for the specified parameter values. 
Both options are useful. Separate functions are convenient for simple transfoma- 
tion operations, and a composite function can provide an expedient method for 
specifying complex transfomation sequences. 

The PHIGS library provides users with both options. Individual commands 
for generating the basic transformation matrices are 

translate (trans-atevector, matrixTranslate) 
rotate (theta, matrixRotate) 
scale (scalevector, matrixscale) 

Each of these functions produces a 3 by 3 transformation matrix that can then be 
used to transform coordinate positions expressed as homogeneous column vec- 
tors. Parameter translatevector is a pointer to the pair of translation dis- 
tances 1, and ty. Similarly, parameter scalevector specifies the pair of scaling 
values s, and s,. Rotate and scale matrices ( m a t r i x T r a n s l a t e  and m a t r i x -  
Scale) transform with respect to the coordinate origin. 



We concatenate transformation matrices that have been previously set up kt ion5-7  
with the function rransformation Functions 

composeMatrix (matrix2, matrixl, matr~xout)  

where elements of the composite output matrix are calculated by postmultiply- 
ing matrix2 by matr ix l .  A composite transfornation matrix to perform a com- 
bination scaling, rotation, and translation is produced with the function 

buildTransformationMatrix (referencepoint, translatevector, 
theta, scalevector, matrix) 

Rotation and scaling are canied out with mpect to the coordinate position speci- 
fied by parameter r e f  erencepoint .  The order for the transformation sequence 
is assumed to be (1) scale, (2) rotate, and (3) translate, with the elements for the 
composite transformation stored in parameter matrix.  We can use this function 
to generate a single transformation matrix or a composite matrix for two or three 
transformations (in the order stated). We could generate a translation matrix by - 
setting sca levec to r  = (1, I), t h e t a  = 0, and assigning x and y shift values to 
parameter t r ans la t evec to r .  Any coordinate values could be assigned to pa- 
rameter r e f  erencepoint ,  since the transformation calculations are unaffected 
by this parameter when no scaling or rotation takes place. But if we only want to 
set up a translation matrix, we can use function t r a n s l a t e  and simply specify 
the translation vector. A rotation or scaling transfonnation matrix is specified by 
setting t r a n s l a t e v e c t o r  = (0,O) and assigning appropriate values to parame- 
ters referencepoint ,  the ta ,  and scalevector.  To obtain a rotation matrix, 
we set s ca levec to r  = (1,l); and for scaling only, we set t h e t a  = 0. If we want 
to rotate or scale with respect to the coordinate origin, it is simpler to set up the 
matrix using either the r o t a t e  or s c a l e  function. 

Since the function buildTransformationMatrix always generates the 
transfonnation sequence in the order (1) scale, (2) rotate, and (3) translate, the fol- 
lowing function is provided to allow specification of other sequences: 

composeTransformationMatrix (matrixIn, referencepoint, 
translatevector, theta, scalevector, matrixout) 

We can use this function in combination with the bu i  ldTransf ormationMa- 
t r i x  function or with any of the other matrix-constmction functions to compose 
any transformation sequence. For example, we could set up a scale matrix about 
a fixed point with the buildTransf o r m a t i o m a t r i x  function, then we could 
use the composeTransformationMatrix function to concatenate this scale 
matrix with a rotation about a specified pivot point. The composite rotate-scale 
sequence is then stored in matrixout.  

After we have set up a transformation matrix, we can apply the matrix to 
individual coordinate positions of an object with the function 

transfonnPoint (inpoint, matrix, outpoint) 

where parameter inpo in t  gives the initial xy-coordinate position of an object 
point, and parameter ou tpo in t  contains the corresponding transformed coordi- 
nates. Additional functions, discussed in Chapter 7, are available for performing 
two-dimensional modeling transformations. 



Chapter .S 5-8 
Two-Dimensional Geometric 

Trandrmnalion~ RASTER METHODS F O R  TRANSFORMATIONS 

F-I~SIIIV .5-.%0 
Translating an object from 
screen positlon (a) to pos~tion 
(b) by nroving a rectangular 
block oi pixel values. 
Coordinate positions P,,,,, 
and P,,,, specify the limits 
of the rectangular block to 
be moved, and P, is the 
destination reference 
poslllon. 

The particular capabilities of raster systems suggest an alternate method for 
transforming objects. Raster systems store picture information as pixel patterns 
in the frame buffer. Therefore, some simple transformations can be carried out 
rapidly by simply moving rectangular arrays of stored pixel values from one lo- 
cat~on to another within the frame buffer. Few arithmetic operations are needed, 
so the pixel transformations are particularly efficient. 

Raster functions that manipulate rectangular pixel arrays are generally re- 
ferred to as raster ops. Moving a block of pixels from one location to another is 
also called a block transfer of pixel values. On a bilevel svstem, this operation is 
called a bitBlt (bit-block transfer), particularly when the function is hardware 
implemented. The term pixBlt is sometimes used for block transfers on multi- 
level systems (multiple bits per pixel). 

Figure 5-30 illustrates translation performed as a block transfer of a raster 
area. All bit settings in the rectangular area shown are copied as a block into an- 
other part of the raster. We accomplish this translation by first reading pixel in- 
tensities fmm a specified rectangular area of a raster into an array, then we copv 
the array back into the raster at the new location. The original object could be 
erased by filling its rectangular area with the background ~ntensity (assuming the 
object does not overlapother objects in the scene). 

Typical raster functions often provided in graphics packages are: 

COW - move a pixel block from one raster area to anothcr. 
rend - save a pixel block in a designated array. 
write - transfer a pixel array to a position in the frame buffer. 

Some implementations provide options for combining pixel values. In re~~ki r .~~  
mode, pixel values are simply transfered to the destination positions. Other OF 
tions for combining ptxd values include Boolean operations (mid, or, and t w l ~ t -  
sivc or) and b i n a ~  arithmetic operations. With the e x c l ~ l s i w  or mode, two succes- 
sive copies of a block to the same raster area restores the values that were 
originally present in that area. This technique can be u3ed to move an object 
across a scene without destroying the background. Another option for adjusting 
pixel values is to combine the source pixels with a specified mask. This alloris 
only selected positions within a block to be transferred or shaded by the patterns 

-- 

Figrtrc 5-31 
Rotating an array of pixel values. Thc original array 
orientation I S  shown in (a), the array orientation after a 
90" counterclockwise rotation IS shown in (b), and the 
array orient,~tion after a 180' rotation is shown i n  (c). 



Section 5-8 
Raster Methods lor 
Transforrnalions 

Rotated 
P~xel 

Arrav 

Destination 
Pixel Areas 

Dostination 
Pixel A ~ a v  

Figure 5-32 
A raster rotation for a rectangular 
block of pixels is accomplished by 
mapping the destination pixel areas 
onto the rotated block. 

Rotations in 90-degree increments are easily accomplished with block trans- 
fers. We can rotate an object 90" counterclockwise by first reversing the pixel val- 
ues in each row of the array, then we interchange rows and columns. A 180" rota- 
tion is obtained by reversing. the order of the elements in each row of the array, 
then reversing the order of the rows. Figure 5-31 demonstrates the array manipu- 
lations necessary to rotate a pixel block by 90" and by 180". 

For array rotations that are not multiples of 90â€¢â€¹, we must perform more 
computations. The general p m e d u m  is illustrated in Fig. 5-32. Each destination 
pixel area is mapped onto the rotated array and the amount of overlap with the 
rotated pixel areas is calculated. An intensity for the destination pixel is then 
computed by averaging the intensities of the overlapped source pixels, weighted 
by their percentage of area overlap. 

Raster scaling of a block of pixels is analogous to the cell-array mapping 
discussed in Section 3-13. We scale the pixel areas in the original block using 
specified values for s, and s, and map the scaled rectangle onto a set of destina- 
tion pixels. The intensity of each destination pixel is then assigned according to 
its area of overlap with the scaled pixel areas (Fig. 5-33). 

. . . .  
L--L--l--{-~-;--+--~--+-~ I I I  Destination 
I I I I I 1  I I -PixelArray 
L - - L - J - - d - - A - - A - - L - - L - A  
I l l  1 1 1 1  
I I I ~ 1 1 1 1 !  

Figure 5-33 
Mapping destination pixel areas onto a waled array of 
pixel values. Scaling factors s, = s, = 0.5 am applied 
relative to fixed point (x,, y,). 



Chapter 5 

Two-Dimensional Ceomelric 
SUMMARY 

Trmsiurrnations 
The basic geometric transformations are translation, rotation, and scaling. Trans- 
lation moves a n  object in a straight-line path from one position to another. Rota- 
tion moves an object from one position to another in a circular path around a 
specified yivot point (rotation point). Scaling changes the dimensions of an object 
relative to a specified fixed point. 

We can express two-dimensional geometric transforn~ations as 3 by 3 ma- 
trix operators, so that sequences of transformations can be concatenated into a 
single con~posite matrix. This is an efficient formulation, since it allows us to re- 
duce computations by applying the composite matrix to the initial coordinate po- 
sitions of an object to obtain the final transformed pos~tions. To d o  this, we also 
need to express two-dimensional coordinate positions as three-element column 
or row matrices. We choose a column-matrix representation for coordinate points 
because this is the standard mathematical convention and because many graph- 
ics packages also follow this convention. For two-dimensional transformations, 
coordinate positions arc: then represented with three-element homogeneous coor- 
dinates with the third (homogeneous) coordinate assigned the value I. 

Composite transformations are formed as multiplications of any combina- 
tion of translation, rotation, and scaling matrices. We can use combinations of 
translation and rotation for animation applications, and we can use combinations 
of rotation and scaling to scale objects in any specified direction. In general, ma- 
trix multiplications are not commutative We obtain different results, for exam- 
ple, if we change the order of a translate-rotate sequence. A transformation se- 
quence involving only translations and rotations is a rigid-body transformation, 
since angles and distances are unchanged. Also, the upper-left submatrix of a 
rigid-body transformation is an orthogonal matrix. Thus, rotation matrices can be 
formed by setting the upper-left 2-by-2 submatrix equal to the elements of two 
orthogonal unit vectors. Computations in rotationgl transformations can be re- 
duced by using approx~mations for the sine and cosine functions when the rota- 
tion angle is small. Over many rotational steps, however, the approximation error 
can accumulate to a significant value. 

Other transformations include reflections and shears. Reflections are trans- 
formations that rotate an object 180" about a reflection axis. This produces a mir- 
ror image of the object with respect to that axis. When the reflection axis is per- 
pendicular to the xy plane, the reflection is obtained as a rotat~on in the xy plane. 
When the reflection axls is in the xy plane, the reflection is obtained as a rotation 
in a plane that is perpendicular to the xy plane. Shear transformations distort the 
shape of an object by shifting x or y coordinate values by an amount 
to the coordinate distance from a shear reference line. 

Transformations between Cartesian coordinate sys tem are accomplished 
with a sequence of translaterotate transformations. One way to specify a new co- 
ordinate reference frame is to give the position of the new coordinate origin and 
the direction of the new y axis. The direction of the new x axis is then obtained by 
rotating they direction vector 90' clockwise. Coordinate descriptions of objects in 
the old reference frame <Ire transferred to the new reference w ~ t h  the transforma- 
tion matrix that superimposes the new coordinate axes onto the old coordinate 
axes. This transformatmn matrix can be calculated as the concatentation of a 
translation that moves the new origin to the old coordinate origin and a rotation 
to align the two sets of axes. The rotation matrix is obtained from unit vectors in 
the x and y directions tor the new system. 



Two-dimensional geometric transformations are a t h e  transformations. 
That is, they can be expressed as a linear function of coordinates x and y. Affine Fxercises 
transformations transform parallel lines to parallel lines and transform finite 
points to finite points. Geometric transformations that d o  not involve scaling or 
shear also preserve angles and lengths. 

Transformation functions in graphics packages are usually provided only 
for translation, rotation, and scaling. These functions include individual proce- 
dures for creating a translate, rotate, or scale matrix. and functions for generating 
a composite matrix given the parameters for a transformation sequence. 

Fast raster transformations can be performed by moving blocks of pixels. 
This avoids calculating transformed coordinates for an object and applying scan- 
conversion routines to display the object at the new position. Three common 
raster operations (bitBlts or pixBlts) are copy, read, and write. When a block of 
pixels is moved to a new position in the frame buffer, we can simply replace the 
old pixel values or we can combine the pixel values using Boolean or arithmetic 
operations. Raster translations are carried out by copying a pixel block to a new 
location in the frame buffer. Raster rotations in multiples of 90' are obtained by 
manipulating row and column positions of the pixel values in a block. Other 
rotations are performed by first mapping rotated pixel areas onto destination po- 
sitions in the frame buffer, then calculating overlap areas. Scaling in raster trans- 
formations is also accomplished by mapping transformed pixel areas to the 
frame-buffer destination positions. 

REFERENCES 

For additional information on homogeneous coordinates in computer graphics, see Blinn 
(I977 and 1978). 

Transformation functions in PHlGS are dixusscd in  Hopgood and Duce (1991), I loward et 
al. (1991), Caskins (1992), and Blake (1993). For information on GKS transformation funr- 
lions, see Hopgood et al. (1983) and Enderle, Kansy, and Pfaff (19841. 

EXERCISES 

5-1 Write a program to continuously rotate an object about a pivot point. Small angles are 
to be used for each successive rotation, and approximations to the sine and cosine 
functions are to be used to speed up the calculations. The rotation angle for each step 
is to be chosen so that the object makes one complete revolution in Ien than 30 sec- 
onds. To avoid accumulation of coordinate errors, reset the original coordinate values 
for the object at the start of each new revolution. 

5-2 Show tha~ the composition of two rotations is additive by concatiridting the matrix 
representations for R(0,) and R(Oz) to obtain 

5 -3  Write a sel of procedures to implement the buildT~ansformationMatrix and the 
composeTransformat~onMatrix functions to produce a composite transforma- 
tion matrix for any set of input transformation parameters. 

5-4 Write a program that applies any specified sequence of transformat~ons to a displayed 
object. The program is to be designed so that a user selects the transforniation se- 
quence and associated parameters from displayed menus, and the composite transfor- 



Chapter 5 matlon is then calculated and used to transform the object. Display the original object 

Two-Dimens~onal Ce~rne:~-c and thetransformed object in different colors or d~fferent iill patterns. 
TransfOrna gns 5-5  Modify the transformation matrix (5-35), ior scaling In an arbitrary dlrection, to In- 

clude coordinates for m y  specified scaling fixed point h, y o .  
5-6 Prove that the multiplication d transformation matrices (or each o i  the following se- 

quence of operations is commutative: 
(a) Two successive rotations. 
(b) Two successive translations. 
(c) Two successjve scalings. 

5-7 Prove that a uniform scaling (5, = 5,) and a rotation form a commutative pair of opera- 
tions but that, in general, scaling and rotation are not commutativeoperations. 

5-8 Multiply the individual scale, rotate, and translate matrices in Eq. 5-38 to verify the el- 
ements in the composite transformation matrix. 

5-9 Show that transformation matrix (5-511, for a reflection about the line y = x, is equtva- 
lent to a reflection relative to the x axis followed by 2 counterclockwise rotation of 
90'. 

5-10 Show that transformat~on matrix (5-52), for a reflection about the line y = - x ,  is 
equivalent to a reflection relatibe to they axis followed by a counterclockwise rotation 
of 90" 

5-1 1 Show that twtrzucces~ive reflections about either of,the coordinate axes is equivalent 
to a single rotation about the coordinate origin. 

5-1 2 Determine the form oi the transfonnation matrix for a reflection about an arbitrary line 
with equation y = m + b. 

5-1 .( Show that two successive reflections about any line passi-tg through the coordinate 
orig~n isequivalent to a single rotation about the origin 

5-14 Delermine a sequence of basic transformatrons that are equivalent to the x-direction 
shearing matrix (5-53). 

5-15 Determine a sequence of basic transformations that are equivalent to the ydirection 
shearing matrix (5-571 

5-10  Set up a shearing procedure to display italic characters, given a vector font definitior. 
That is, all character shapes in this font are defined with straight-line segments, and 
italic characters are formed with shearing transformations. Determine an appropriat* 
value for the shear parameter by comparing italics and plain text in some mailable 
font. Define a simple vector font for input to your routine. 

5-17 Derive the following equations for transforming a coordinate point P = (x, y : ~  in one 
Cartwan system to the coordinate values (x',  y ' )  in another C~rteslan system that is  ro- 
tated by an angle 0, as In Fig. 5-27. Project point P onto each of the four axe< and 
analyse the resulting right triangles. 

5-18 Writc a procedure to compute the elements of the matrix for transforming object de- 
scriptions from one C.~rtesian coordinate system to another. The second coordindtr 
system i s  to be deficed with an origin point Po and a vector V that gives the directton 
for the positive y'axis ot  this system. 

5-19 Set up procedures for mplementing a block transfer ol a rectangular area of a iramr 
buffer, using one i u ~ c t ~ o n  to read the area into an array and another function to cop\ 
the array into the designated transfer area. 

5-20 Determine the results cf perforn~~ng two successive block trmsfers Into the same area 
o i  a frame buffer usin): !he various Boolean operations. 

5-21 What are the results o i  performing two successive block transfers into the same area oi 
a frame buffer using the binary arithmetic operations! 



5-22 lrnplemcnt A routine to perform block 1r.7nsicvs in '1 tr,lne buiter using any sprcified 
Boolcan operation or a replacement (copy) operation E.<ercisrz 

5 - 2 3  Write a routine lo ~rnplemenl rotations In ~ntrrnients of 90" in frame-buffer block 
transfers. 

5 2 4  Write a routine to implement rotations by any specified angle in a frame-buffer block 
transier. 

5-25 Write a routine lo implement scaling as a raster lransforrnation of a pixel block. 



C H A P T E R  

6 Two-Dimensional Viewing 
diewing Coordinate 
Ni ndow , 

Normalized Space 

Viewport 

ws 1 
Window 

Monitor 1 

ws2 Viewport i 

i 
Monitor 2 



w e now consider the formal mechanism for displaying views of a picture on an output device. Typically, a graphics package allows a user to 
specify which part of a defined pi&ke is to be display& and where that part is 
to be placed on the display device. Any convenient Cartesian coordinate system, 
referred to as the world-coordinate reference frame, can be used to define the pic- 
ture. For a two-dimensional pidure, a view is selected by specifying a subarea of 
the total picture area. A user can select a single area for display, or several areas 
could be selected for simultaneous display or for an animated panning sequence 
across a scene. The pidure parts within the selected areas are then mapped onto 
specified areas of the device coordinates. When multiple view areas are selected, 
these areas can be placed in separate display locations, or some areas could be in 
serted into other, larger display areas. Transformations from world to device co 
ordinates involve translation, rotation, and scaling operations, as well as proce- 
dures for deleting those parts of the picture that are outside the limits of a 
selected display area. 

6- 1 
THE VIEWING PIPELINE 

A world-coordinate area selected for display is called a window. An area on a 
display device to which a window is mapped is called a viewport. The window 
defines whnt is to be viewed; the viewport defines where it is to be displayed. 
Often, windows and viewports are rectangles in standard position, with the rec- 
tangle edges parallel to the coordinate axes. Other window or viewport geome- 
tries, such as general polygon shapes and circles, are used in some applications, 
but these shapes take longer to process. In general, the mapping of a part of a 
world-coordinate scene to device coordinates is referred to as a viewing transfor- 
mation. Sometimes the two-dimensional viewing transformation is simply re- 
ferred to as the window-to-viewport transformation or the windowing transformation. 
But, in general, viewing involves more than just the transformation from the win- 
dow to the viewport. Figure 6-1 illustrates the mapping of a pidure section that 
falls within a rectangular window onto a designated &angular viewport. 

In computer graphics terminology, the term wrndow originally referred to an 
area of a picture that is selected for viewing, as defined at the beginning of this 
section. Unfortunately, the same tern is now used in window-manager systems 
to refer to any rectangular screen area that can be moved about, resized, and 
made active or inactive. In this chapter, we will only use the term window to 



World Coordinates I Device Coordinales 

- -- - - . . . - . - - - - . -. . -, . - . . . . . . . . . - -. . - - - . - . - . . . . . - 
Ficyrrrc 6-1 
A viewing transformation using standard rectang~es lor the window and viewport. 

refer to an area of a world-coordinate scene that has btvn selected for display 
When we consider graphical user interfaces in Chapter F, we will discuss screen 
wmdows and window-manager systems. 

Some graphics packages that provide window and viewport operations 
allow only standard rectimgles, but a more general approach is to allow the rec- 
tangular window to haw any orientation. In this case, we carry out the viewing 
transformation in several steps, as indicated in Fig. 6-2. First, we construct the 
scene in world coordinates using the output primitives and attributes discussed 
in Chapters 3 and 4. Next. to obtain a particular orientation for the window, w e  
can set up a two-dimensional viewing-coordinate system in the world-coordi- 
nate plane, and define a tvindow In the viewing-coordinate system. The viewing- 
coordinate reference frame is used to provide a method for setting up arbitrary 
orientations for rectangular windows. Once the viewing reference frame is estah- 
lishcd, we can transforw descr~ptions in world coordinntes to viewing coordi- 
nates. We then define a \.iewport in normalized coordinates (in the range from O 
to I )  and map the viewing-coordinate description of the scene to normalized co- 
ordinates. At the final step, .11I parts of the picture that he outside the viewport 
are clipped, and the contents of the viewport are transierred to device coordi- 
nates. Figure 6-3 i1lustratt.s a rotated viewing-coordinate reference frame and the 
mapping to normalized coordinates. 

By changing the position of the viewport, we can view objects at different 
positions on the display area of an output device. Also, by varying the size of 
viewports, we can change the size and proportions of displayed objects. We 
achieve zooming effects by successively mapping different-sized windows on a 

construct ' 
World-Coordinate 

MC Scene Using - Modeling-Cootdinme 
Transformations I Conven World- WC Coordinates - to Viewing Coordinates Map Viewing Coordinates to I 'JC Normalized I- Viewing Coordinates ' using Window-Viewpolt, Specifications '. 

2 ,' * 
Map Normalizgd 

Viewport to 
NVC - Coordinates ' 

- - - 
F i p r i *  6-2 
The two-dimensional viewing-transformation pipeline. 
218 



I ' 
X o  x world 
World Coordinates 

Section 6-2 

Wewing Coordinate Reference 
Frame 

Normalized 
Device Coordinates 

Figure 6-3 
Setting up  a rotated world window in viewing coordmates and the 
corresponding normalized-coordinate viewport. 

fixed-size viewport. As the windows are made smaller, we zoom in on some part 
of a scene to view details that are not shown with larger windows. Similarly, 
more overview is obtained by zooming out from a section of a scene with succes- 
sively larger windows. Panning effects are produced by moving a fixed-size win- 
dow across the various objects in a scene. 

Viewports are typically defined within the unit square (normalized coordi- 
nates). This provides a means for separating the viewing and other transforma- 
tions from specific output-device requirements, so that the graphics package is 
largely device-independent. Once the scene has been transferred to normalized 
coordinates, the unit square is simply mapped to the display area for the particu- 
lar output device in use at that time. Different output devices can be used by pro- 
viding the appropriate device drivers. 

when iil &ordinate transformations are completed, viewport clipping can 
be performed in normalized coordinates or in device coordinates. This allows us 
to reduce computations by concatenating the various transformation matrices. 
Clipping procedures are of fundamental importance in computer graphics. They 
are used not only in viewing transformations, but also in window-manager sys- - .  
tems, in and drawkg packages to eliminate parts of a picture inside or 
outside of a designated screen area, and in many other applications. 

6-2 
VIEWING COORDINATE REFERENCE FRAME 

This coordinate system provides the reference frame for speafying the world- 
coordinate window We set up the viewing coordinate system using the procc 
dures discussed in Section 5-5. First, a viewing-coordinate origin is selected at 
some world position: Po = (x,, yo). Then we need to establish the orientation, or 
rotation, of this reference frame. One way to do this is to spec~fy a world vector V 
that defines the viewing y, direction. Vector V is called the view up vector. 

Given V, we can calculate the components of unit vectors v = (v,, VJ and 
u = (u,, UJ for the viewing y, and x, axes, respectively. These unit vectors are 
used to form the first and second rows of the rotation matrix R that aligns the 
viewing r,,y,. axes with the world x,y,, axes. 



Chapter 6 
Y Two-Dimensional V~ewing world1 ,/" Y world I riew 

x view 

. 

Figure 6 4  
A viewing-coordinate frame is moved into coincidence with the world 
frame in two steps: (a) translate the viewing origin to the world origin, 
then (b) rotate to align the axes of the two systems. 

We obtain the matrix for converting worldcoordinate positions to viewing 
coordinates as a two-step composite transformation: First, we translate the view- 
ing origin to the world origin, then we rotate to align the two coordinate refer- 
ence frames. The composite twc-dimensional transformation to convert world 
coordinates to viewing coordinate is 

where T is the translation matrix that takes the viewing origin point Po to the 
world origin, and R is the rotation matrix that aligns the axes of the two reference 
frames. Figure 6-4 illustrates the steps in this coordinate transformation. 

6-3 
WINDOW-TO-VIEWPORT COORDINATE TRANSFORMATION 

Once object descriptions have been transferred to the viewing reference frame, 
we choose the window extents in viewing coordinates and select the viewport 
limits in normalized conrdinates (Fig. 6-31. Object descriptions are then trans- 
ferred to normalized device coordinates. We do this using a transformation that 
maintains the same relative placement of objects in normalized space as they had 
in viewing coordinates. If a coordinate position is at the center of the viewing 
window, for instance, it will be displayed at the center of the viewport. 

Figure 6-5 illustrates the window-to-viewport mapping. A point at position 
(m, yw) in the window 1s mapped into position (xv, yv) in the associated view- 
port. To maintain the same relative placement in the viewport as in the window, 
we q u i r e  that 



Section 6-3 

Window-to-Vewwrt Cwrdlnale 

-- 

Figure 6-5 
A point at position (xw, yro) in a designated window is mapped to 
viewport coordinates (xu, yv) so that relative positions in the two areas 
are the same. 

Solving these expressions for the viewport position (XU,  yv), we have 

XU = XU,,, + (xw - XW,,,)SX 

where the scaling factors are 

Equations 6-3 can also be derived with a set of trnnsformtions that converts the 
window area into the viewport area. This conversion is performed with the fol- 
lowing sequence of transformations: 

1. Perform a scaling transformation using a fixed-point position of (xw,,, 
yw,,,) that scales the window area to the size of the viewpdrt. 

2. Translate the scaled window area to the position of the viewport. 

Relative proportions of objects are maintained if the scaling factors are the 
same ( s x  = s y ) .  Otherwise, world objects will be stretched or contracted in either 
the x or y direction when displayed on the output device. 

Character strings can be handled in two ways when they are mapped to a 
viewport. The simplest mapping maintains a constant character size, even 
though the viewport area may be enlarged or reduced relative to the window. 
Tt.is method would be employed when text is formed with standard character 
fonts that cannot be changed. In systems that allow for changes in character size, 
string definitions can be windowed the same as other primitives. For characters 
formed with line segments, the mapping to the viewport can be carried out as a 
sequence of line transformations. 

From normalized coordinates, object descriptions are mapped to the vari- 
ous display devices. Any number of output devices can be open in a part'cular 
application, and another window-to-viewport transformation can be performed 
for each open output device. This mapping, called the workstation transforma- 

Tranriormation 



Chapter 6 

Two-Dimenstonal V~ewlng 
Viewing coorainate 
Window . 

Figrtrc 6-6 
Mapping selected parts of a scene in normalized coord~nates to 
different video monitors with workstation transformations. 

tion, IS accomplished by selecting a window area in normalized space and a 
viewport area in the coordinates of the display device. With the workstation 
transformation, we gain some additional control over the positioning of parts of 
a scene on individual output devices. As illustrated in Fig. 1,-6, we can use work- 
station transformations to partition a view so that different parts of normalized 
space can bc displaycd on diffcrcnt output dcvices. 

6-4 
TWO-DIMENSIONAL \/IEWING FUNCTIONS 

We define a viewing reference system in a PHIGS application program with the 
following function: 

evaluateViewOrient~tionMatrix (xO, y o ,  xV,  yi ' .  
error, viewMatrixl 

where parameters xO and yo are the coordinates of the viewing origm, and para- 
meters xV and yV are the world-coordinate positions for the view up  vector. An 
integer error code is generated if the input parameters are in error; otherwise, the 
viematrix for the world-to-viewing transformation is calculated. Any number 
of viewing transformation matrices can be defined in anapplication. 

To set up  the elements of a window-to-viewport mapping matrix, we in- 
voke the fknction 

-.valuate~iewMap~inyMatrix (xwmin, M a x ,  v i n ,  Y?maX. 
-in, =ax. .pmin, yvmax, error,  view~zpping~atrix) 

Here, the window limits in viewing coordinates are chosen with parameters 
-in, -ax, ywmin, and ywmax; and the viewport limit> are set with the nor- 



niahzed coordinate positions xvmin, xvmax, w i n ,  yvmax. .4s with the W i o n 6 - 4  
viewing-transformation matrix, we can construct several window-viewport pairs Two-D~mensional Mewing 
and use them for projecting various parts of the scene to different areas of the FUnCL'OnS 

unit square. 
Next, we can store combinations of viewing and window-viewport map- 

pings for various workstations in a viruing tablr with 

setVlewRepresentation (ws, viewIndex, viewMatrlx, 
viewMappingMatrix, xclipmin, xclipmax, yclipmin, 

yclipmax, clipxy) 

where parameter ws designates the output device (workstation), and parameter 
viewIndex sets an integer identifier for this particular window-viewport pair. 
The matrices viewMatrix and viewMappingWatrix can be concatenated and 
referenced by the viewIndex. Additional clipping limits can also be specifled 
here, but they are usually set to coincide with the viewport boundaries. And pa- 
rameter c l  ipxy is assigned either the value rroclrp or the value clip. This allows 
us to turn off clipping if we want to view the parti of the scene outside the view- 
port. We can also select rloclip to speed up processing when we know that all of 
the scene is included within the viewport limits 

The function 

selects a particular set of options from the viewing table. This view-index selec- 
tion is then applied to subsequently specified output primitives and associated 
attributes and generates a display on each of the active workstations. 

At the find stage, we apply a workstation transformation by selecting a 
workstation window-viewport pair: 

setWorkstationWindow (WS, xwswindmir.. xwswixlmax, 
ywswindrnin. ywswindmax) 

setworksrationviewport (ws xwsVPortmin, xwsVPortmax, 
ywsVPortmin, ywsVPortmax) 

where parameter ws gives the workstation number. Windowioordinate extents 
are specified in the range from 0 to 1 (normalized space), and viewport limits are 
in integer devicecoordinates. 

If a workstation viewport is not specified, the unit squaxv of the normalized 
reference frame is mapped onto the largest square area possible on an output de- 
vice. The coordinate origin of normalized space is mapped to the origin of device 
coordinates, and the aspect ratio is retained by transforming the unit square onto 
a square area on the output device. 

Example 6-1 Two-Dimensional Viewing Example 

As an example of the use of viewing functions, the following sequence of state- 
ments sets up  a rotated window in world coordinates and maps its contents to 
the upper right comer of workstation 2. We keep the viewing coordinate origin at  
the world origin, and we choose the view u p  direction for the window as (1,1). 
This gives us  a viewingtoordinate system that is rotated 45" clockwise in the 
world-coordinate refemnce frame. The view index is set to the value 5. 



Chapter b evaluate~iew0rientatlonMatrix (0, 0,  1, 1. 

Two-Oiwenrional V~ewing viewError, viewMat) ; 
evaluate~~ewMappingMatrix ( - 6 0 . 5 ,  4 1 . 2 4 ,  -20 .75 ,  82.5, 0.5. 

0 . 8 .  C.7,  1 . 0 .  viewMapError, viewMapMat); 
setviewRepresentation ( 2 ,  5, viewMat, viewMapMat, 0 . 5 ,  0.8, 

0.7, 1.0, clip); 
setviewlndex L 5 )  : 

Similarly, we could set up an additional transformation with view index 6 that 
would map a specified window into a viewport at the lower left of the screen. 
Two graphs, for example, could then be displayed at opposite screen corners 
with the following statements. 

setViewIndex ( 5 ) ;  
polyline ( 3 ,  axes); 
polyline (15, data11 : 
setVievIndex ( 6 ) ;  
polyline ( 3 ,  axes); 
polyline (25, datz2): 

V~ew index 5 selects a viewport in the upper right of the screen display, and view 
index 6 selects a viewport in the lower left corner. The function p o l y l i n e  ( 3  , 
axes) produces the horizontal and vertical coordinate reference for the data plot 
in each graph. 

6-5 
CLIPPING OPERATIONS 

Generally, any procedure that identifies those portions of a picture that are either 
inside or outside of a specified region of space is referred to as a clipping algo- 
rithm, or simply clipping. The region against which an object is to clipped is 
called a clip window. 

Applications of clipplng include extracting part of a detined scene for v i e w  
ing; identifying visible surfaces in three-dimensiona1 vlews; antialiasing line seg- 
ments or object boundaries; creating objects using solid-modeling procedures; 
displaying a multiwindow environment; and drawing and painting operations 
that allow parts of a picture to be selected for copying, moving, erasing, or dupli- 
cating. Depending on the application, the clip window can be a general polygon 
or it can even have curved boundaries. We first consider clipping methods using 
rectangular clip regions, then we discuss methods for other &p-Agion shapes. 

- 

For the viewing transformation, we want to display only those picture parts 
that are within the window area (assuming that the clipping flags have not been 
set to noclip). Everything outside the window is discarded. Clipping algorithms 
can be applied in world coordinates, so that only the contents of the window in- 
terior are mapped to device coordinates. Alternatively, the ccimplete world-coor- 
dinate picture can be mapped first to device coordinates, or normalized device 
coordinates, then clipped against the viewport boundaries. World-coordinate 
clipping removes those primitives outside the window from further considera- 
tion, thus eliminating the processing necessary to transform those primitives to 
device space. Viewport clipping, on the other hand, can reducd calculations by al- 
lowing concatenation of viewing and geometric transforn>ation matrices. But 



viewport clipping does require that the transformation to device coordinates be section 6-7 
performed for all objects, including those outside the window area. On raster L~neCllpp~ng 
systems, clipping algorithms are often combined with scan conversion. 

In the following sections, we consider algorithms foi clipping the following 
primitive types 

Point Clipping 
Line Clipping (straight-line segments) 
Area Clipping (polygons) 
Curve Clipping 
Text Clipp~ng 

Line and polygon clipping routines are standard components of graphics pack- 
ages, but many packages accommodate curved objects, particularly spline curves 
and conics, such as circles and ellipses. Another way to handle curved objects is 
to approximate them with straight-line segments and apply the line- or polygon- 
clipping procedure. 

6-6 
POINT CLlPPlkG 

Assuming that the clip window is a rectangle in standard position, we save a 
point P = ( x ,  y) for display if the following inequalities are satisfied: 

where the edges of the clip window (numi,, mum,, yw,,, yiu,,,) can be either the 
world-coordinate window boundaries or viewport boundaries. If any one of 
these four inequalities is not satisfied, the point is clipped (not saved for display). 

Although point clipping is applied less often than line or polygon clipping, 
some .applications may require a pointclipping procedure. For example, point 
clipping can be applied to scenes involving explosions or sea foam that are mod- 
eled with particles (points) distributed in some region of the scene. 

LINE CLIPPING 

Figure 6-7 illustrates possible relationships between line positions and a standard 
rectangular clipping region. A lineclipping procedure involves several parts. 
First, we can test a given line segment to determine whether it lies completely in- 
side the clipping window. If it does not, we try to determine whether it lies com- 
pletely outside the window. Finally, if we cannot identify a line as completely in- 
side or completely outside, we must perform intersection calculations with one 
or  more clipping boundaries. We proc&s lines through the "inside-outside'' tests 
by checking the line endpoints. A line with both endpoints inside all clipping 
boundaries, such as the line from P, to P,, i sxved .  A line with both endpoints 
outside any one of the clip boundaries (line P,P, in Fig. 6-7) is outside the win- 



adore Clipping 

(a) 

. .. - - . - - . - - - .. . - -- - 
Fiprre 6-7 
Line clipping against a rectangular shp window. 

After Cl~pping 

lbi 

dow. All other lines cross rwe or more clipping boundaries, and may require cal- 
culation of multiple intmstution points. TCI minimize calculations, we try to de- 
vise clipping algorithms that can efficiently identify ockside lines and redow in- 
tersection calculations. 

For a line segment with endpoints (x,, yl) and (x , .  y:! nnd one o r  both end- 
points outside the clipping rectangle, the parametric reprcwntation 

could be used to determine values of parameter 11 for intersections with the clip- 
ping boundary coordinates. If the value of u for an intersection with a rectangle 
boundary edge is outside the range 0 to 1 ,  the line does not enter the interior of 
thr window ~t that boundarv. I f  the value ol u is witkin the range from 0 to 1 ,  the 
line segment does ~ndeed cross into the clipping area. T h ~ s  method can be ap- 
plied to each clipping boundary edge in turn to determine whether any part of 
the line segment is to be displayed. Line segments that are parallel to window 
edges can be handled as spt-cia1 cases. 

Clipping line segmenls with these parametric tesls requires a good deal of 
computation, and faster approaches to clippng are pms~hle. A number oi effi- 
cient line clippers have been developt,d, and J\,e survey the major algorithms in 
the next sectiim. Some all;orithrns are desipeci explicitl>. for two-dimensional 
pictures and some arc e a d t  adapted to threedimensional applicatiims. 

This is one of the oldest and most popular linc-clipping prcmdures. Generally, 
the method speeds up the pn)cessiug of line segnwnts t;? pvrforming initial tests 
that reduce the nun~hcr (11 intc.rscctions that must he calculated. Everv line end- 



point in a picture is assigned a four-digit binary code, called a region code, that 
identifies the location of the point relative to the boundaries of the clipping rec- 
tangle. Regions are set up in referehce to the boundaries as shown in Fig. 6-8. 
Each bit position in the region code is used to indicate one of the four relative co- 
ordinate positions of the point with respect to the clip window: to the left, right, 
top, or bottom. By numbering the bit positions in the region code as 1 through 
4 from right to left, the coordinate regions can be correlated with the bit posi- 
tions as  

bit 1: left 
bit 2: right 
bit 3: below 
bit 4: above 

A value of 1 in any bit position indicates that the point is in that relative position; 
otherwise, the bit position is set to 0. If a point is within the clipping rectangle, 
the region code is 0000. A point that is below and to the left of the rectangle has a 
region code of 0101. 

Bit values in the region code are determined by comparing endpoint coordi- 
nate values (x, y) to the clip boundaries. Bit 1 is set to 1 if x < nomi,. The other 
three bit values can be determined using similar comparisons. For languages in 
which bit manipulation is possible, region-code bit values can be determined 
with the following two steps: (1) Calculate differences between endpoint coordi- 
nates and clipping boundaries. (2) Use the resultant sign bit of each difference 
calculation to set the corresponding value in the region code. Bit 1 is the sign bit 
of x - bit 2 is the sign bit of xw,,, - x; bit 3 is the sign bit of y - yw,,; and 
bit 4 is the sign bit of yw,, - y. 

Once we have established region codes for all line endpoints, we can 
quickly determine which lines are completely inside the clip window and which 
are clearly outside. Any lines that are completely contained within the window 
boundaries have a region code of 0000 for both endpoints, and we trivially accept 
these lines. Any lines that have a 1 in the same bit position in the region codes for 
each endpoint are completely outside the clipping rectangle, and we trivially re- 
ject these lines. We would discard the line that has a region code of 1001 for one 
endpoint and a code of 0101 for the other endpoint. Both endpoints of this line 
are left of the clipping rectangle, as indicated by the 1 in the first bit position of 
each region code. A method that can be used to test lines for total clipping is to 
perform the logical and operation with both region codes. If the result is not 0000, 
the line is completely outside the clipping region. 

Lines that cannot be identified as completely inside or completely outside a 
clip window by these tests are checked for intersection with the window bound- 
aries. As shown in Fig. 6-9, such lines may or may not cross into the window in- 
terior. We begin the clipping process for a line by comparing an outside endpoint 
to a clipping boundary to determine how much of the line can be discarded. 
Then the remaining part of the Line is checked against the other boundaries, and 
we continue until either the line is totally discarded or a section is found inside 
the window. We set up our algorithm to check line endpoints against clipping 
boundaries in the order left, right, bottom, top. 

To illustrate the specific steps in clipping lines against rectangular bound- 
aries using the Cohen-Sutherland algorithm, we show how the lines in Fig. 6-9 
could be processed. Starting with the bottom endpoint of the line from P, to P,, 

Section 6.7 

Line Clipping 

Figure 6-8 
Binary region codes assigned 
to line endpoints according to 
relative position with respect 
to the clipping rectangle. 



Chapter 6 

Two-D~mensional Wewing 

- - 

Figure 6-9 
Lines extending from one 
coordinate region to another may 
pass through the clip window, 
or they may intersect clipping 
boundaries witho~t entering the 
window. 

we check P, against the left, right, and bottom boundaries in turn and find that 
this point is below the clipping rectangle. We then find the intersection point Pi 
with the bottom boundary and discard the line section from PI to Pi. The line 
now has been reduced to the section from Pi to P,. Since P, is outside the clip 
window, we check this endpoint against the boundaries and find that it is  to the 
left of the window. Intersection point P; is calculated, but this point is above the 
window. So the final intersection calculation yields I"', and the line from Pi to P; 
is saved. This completes processing for this line, so we save this part and go on to 
the next line. Point P3 in the next line is to the left of the clipping rectangle, so we 
determine the intersection P, and eliminate the line section from P3 to P' By 
checking region codes for the line section from Pi to P,, we find that the remain- 
der of the line is below the clip window and can be discarded also. 

Intersection points with a clipping boundary can be calculated using the 
slope-intercept form of the line equation. For a line w ~ t h  endpoint coordinates ( x , ,  
y,) and (x2, y2) ,  they coordinate of the intersection pomt with a vertical boundary 
can be obtained with the calculation 

where the x value is set either to numi, or to XIU,,, and the slope of the line is cal- 
culated as  m = (y2 - y , ) / ( x ,  - x,).  Similarly, if we are looking for the intersection 
with a horizontal boundary, the x coordinate can be calculated as 

with y set either to yw,, or to ywm,. 
The following procedure demonstrates the Cohen-Sutherland line-clipping 

algorithm. Codes for each endpoint are stored as bytes and processed using bit 
manipulations. 

i '  B i t  masks encode a p o i n t ' s  p o s i t i o n  r e l a t i v e  t o  t h e  c l i p  e d g e s .  A 
p o i n t ' s  s ta t .us  is encoded by OR'ing toge ther  a p p r o p r i a t e  b i t  masks 

/ 
Cdefine LEFT-EWE 0x1 



Xdeh~~c RIGH'T EDGE 0x2 
Xdefins BOTTOM-EKE 0x4 
Udefins TOP EDGE 0x8 

, *  Po~ncs encoded as 0000 are completely Inside the clip rectangle; 
all others ere outside at least one edge. If OR'ing two codes is 
FALSE (no bits are set in either code), the line can be Accepted. If 
the .WD operation between two codes is TRUE, the llne defuled by tiose- 
endpoints is completely outside the cllp reqion and can be Rejected. 

./ 
Ydehne INSIDE(a1 (!a) 
Ydcfine REJECT(a, b) (ahb) 
#define ACCEPTIa, b) ( ! (alb) ) 

un:,lyned char encode IwcPti pt, dcPt wlnMln. dcPt winMax) 

unsigneu char code=OxCO: 

i f  ( p t . x  winMin.x) 
<-ode = code / LEFT-EDGE; 

if 1 p t . x  > winMax.x) 
code : code I RIGHT-EKE; 

if (pt .y < winMin. yl 
code = code I BOTTOM-EDGE: 

if (pc .y > winMax. y )  
code = code 1 TOP-EDGE; 

.-eturn icode) ; 
) 

votd swapPts lwcPt2 pl, wcPt2 ' p2) 
( 

wept :  tmp; 

vo-d  swdyi'udcs (unsigned char ' c ? .  cnslgrec char ' c21 

t.mp = 'cl; *c1 = 'c2; *c2 = tmp; 
> 
vo;d clipLlnr IdcPt w i n M i n ,  JcPt winMax, wcFc2 pl, wcPcZ pZ1 
( 
unsigned char codel, code?; 
int done = FALSE, draw = FALSE: 
float m; 

while (!done) ( 
codel = encode ( p l ,  widin, winMax); 
code2 = encode (p2, win~in, winMax); 
if (ACCEPT (codel, code21 ) ( 
done = TRUE; 
draw = TRUE: 

I 
else 

i f  (REJEC T lcodel, code2)) 
done : TRUE; 

else ( 
2' Ensure that pl is outside window ' /  
i f  (INSIDE (codei)) [ 



swapPts (hpl, &p2) ; 
swapcodes (&code:, ccode2); 

1 
/' Use slope (m) to find line-clipEdge intersections * /  
if (p2.x != p1.x) 
m = (p2.y - p1.y: / (p2.x - p1.x); 

if (codel & LEFT-EDGE) ( 
p1.y + =  (winI4in.x - p1.x) m; 
p1.x = v8inMin.x; 

1 
else 

i f  (codel & RIGHT-.EDGE) ( 
p1.y 4: (winMax.x - p1.x) m; 
p1.x = wirG4ax.x; 

) 
else 

i f  (codel & BOTTOMKEDCE) L 
/ *  Need to updace p1.x for non-vertical llnes only * /  

else 
i f  (codel h TOP-EDGE) { 

if (p2.x ! -  p1.x) 
p1.x += ;winMax.y - p1.y) 1 n, 

p1.y = winf4ax.y; 
1 

) 
1 
if (draw) 

lineFDA 
1 

Liang-Barsky Line Clipping 

Faster line clippers have been developed that are based on analysis of the para- 
metric equation of a line segment, which we can write in the form 

x = X, + UAX 

y = y, + uby, 0 l u  5 1  

where Ax .= x2 - X,  and h y  = yz - y,. Using these parametric equations, Cyrus 
and Beck developed an algorithm that is generally more efficient than the 
Cohen-Sutherland algorithm. Later, Liang and Barsky independently devised an 
even fister parametric line-clipping algorithm. Following the Liang-Barsky ap- 
proach, we first write the point-clipping conditions 6-5 in the parametric form: 

Each of these four inequalities can be expressed as  



where parameters p and q are defined as 

p1 = - Ax, 9, = x, - x u .  ,,,,, 

Section 6-7 

Lme Clipping 

Any line that is parallel to one oi the clipping boundaries haspk = 0 for the value 
of k  corresponding to that boundary ( k  = 1, 2, 3, and 4 correspond to the left, 
right, bottom, and top boundaries, respectively). If, for that value of k, we also 
find qk < 0, then the line is completely outside the boundary and can be elimi- 
nated from furthcr consideration. If  9, r 0, the line is inside the parallel clipping 
boundary. 

When p, *- 0, the infinite extension of the line proceeds from the outside to 
the ins~de of thc infinite extension of this particular clipping boundary If p, > 0, 
the line proceeds from the inside to the outside. For a nonzero value of pk, we can 
calculate the value of u that corresponds to the ptxnt where the infinitely ex- 
tended line intersects the extension of boundary k as 

For each line, we can calculate values fur parameters u, and u2 that define 
that part of the lint. that lies within the clip rectangle. The value of u, is deter- 
mined by looking at the rectangle edges for which the line proceeds from the out- 
side to the inside (p < 0).  For these edges, we calculnle rk = q , / p , .  The value of u, 
is taken as the largest of the set consisting of 0 and the various values of r. Con- 
versely, the value of u2 is delerrnined by examining the boundaries for which the 
line proceeds from inside to outside ( p  > 0). A vAue ~ ) f  r, is calc-dated for each of 
these boundaries, and the value nf u, is the minimum of the set cons~sting of 1 
and the calculated r  values. I f  u ,  > u2, the line is conrpletely outside the clip win- 
dow and it can be rejected. Otherwise, the endpoints of the chpped line are calcu- 
lated from the two values of parameter 11. 

This algorithm is presented in the following prc~edure. Line intersection 
parameters arc. initialized to the values r r ,  = 0 an6 u 2  = 1. For each clipping 
boundary, the appropriate values for 1) and q are calculated and used by the func- 
tion clipTr~t to determ~ne whether the line can be rejected or whether the intersec- 
tion parameters are tc).be adjusted. When / I  < 0, the p(3rameter r  is used to update 
2 , ;  when p 0, parameter r is used to update u,. I f  updating u ,  or u2 results in 
u, > II?, we reject the line. Otherwise, we update the appropriate u parameter 
only i f  the new value results in a shortening of the line. When p = 0 and q < 0, 
wc can discard the l ~ n c  since i t  is parallel to and out~jide of this boundary. I f  the 
line has not been rejected after all four values of y and q have been tested, the 
endpoints of the clipped line are determined from values of u, and u,. 

- 

lnt clipTest (flodt p,  float q. flodt u!,  fli-a. ' 112) I 



( 
float r ;  
i n t  r e t v a l  = TRUE; 

i f  ( p  < 0 . 0 )  ( 
r  : q i ~ 2 ;  
i f  l r  > ' ~ 2 1  

r e t V a l  = FALSE; 
else 

i t  (1 .  > * ~ 1 )  
'ul = r ,  

1 
e l s e  

i f  ( p  > 0.01 ( 
r = q / p ;  
i f  ( r  < ' ~ 1 )  

r e t V a l  = FALSE; 
e l s e  i f  l r  < * u 2 )  

.u2 = r ;  
1 
else 

/ *  p  = 0, s o  l i n e  is p a r a l l e l  GO c h i s  r i l p p i n g  edge * /  
i f  (q < 0 . 0 )  

/ *  L i n e  is o u t s i d e  c l i p p i n g  edge ' 
r e t V a l  = FALSE; 

r e t u r n  ( r e c v a l )  ; 
h 

void c l i p L i n e  (dcPt  wirNin,  dcPt winMax, w c ~ l 2  p l ,  wcPt2 p2) 
( 

float u l  = 0 0 ,  u2 = 1 . 0 ,  a x  = p2 .x  - p 1 . x  d y ;  

i f  I c l i p T e s t  (-dx, p 1 . x  - winMin.x, hu!, &u>iI 
i f  ( c l i p T e s t  (dx,  wint4ax.x - p l . x ,  h u l ,  h 1 . 1 2 ) )  ( 

p2.x = p 1 . x  + u2 dx: 
p2 .y  = p 1 . y  + u2 * dy;  

1 

In general, the Liang-Barsky algorithm is more efficient than the 
Cohen-Sutherland algorithm, since intersection calculations are reduced. Each 
update of parameters r r ,  and u, requires only one division; and window intersec- 
tions of the line are computed only once, when the final values of u, and u, have 
been computed. In contrast, the Cohen-Sutherland algorithm can repeatedly cal- 
culate intersections along a line path, even though the line may be completely 
outside the clip window. And, each intersection calculation requires both a divi- 
sion and a multiplication. Both the Cohen-Sutherland and the Liang-Barsky al- 
gorithms can be extended to three-dimensional clipping (Chapter 12). 



Nicholl-Lee-Nic-holl Line Clipping %ion 6-7 - 
Line Clipping 

By creating more regions around the clip window, the Nicholl-Lee-Nicholl (or 
NLN) algorithm avoids multiple clipping of an individual line segment. In the 
Cohen-Sutherland method, for example, multiple intersections may be calcu- 
lated along the path of a single line before an intersection on the clipping rectan- 
gle is located or the line is completely repcted. These extra intersection calcula- 
tions are eliminated in the NLN algorithm by carrying out more region testing 
before intersection positions are calculated. Compared to both the Cohen-Suther- 
land and the Liang-Barsky algorithms, the Nicholl-Lee-Nicholl algorithm per- 
forms fewer comparisons and divisions. The trade-off is that the NLN algorithm 
can only be applied to two-dimensional dipping, whereas both the Liang-Barsky 
and the Cohen-Sutherland methods are easily extended to three-dimensional 
scenes. 

For a line with endpoints PI and Pa we first determine the position of point 
P, for the nine possible regions relative to the clipping rectangle. Only the three 
regions shown in Fig. 6-10 need be considered. If PI lies in any one of the other 
six regions, we can move it to one of the three regions in Fig. 6-10 using a sym- 
metry transformation. For example, the region directly above the clip window 
can be transformed to the region left of the clip window using a reflection about 
the line y = - x ,  or we could use a 90" counte~lockwise rotation. 

Next, we determine the position of P2 relative to P,. To do this, we create 
some new regions in the plane, depending on the location of P,. Boundaries of 
the new regions are half-infinite line segments that start at the position of P, and 
pass through the window corners. If PI is inside the clip window and P2 is out- 
side, we set up the four regions shown in Fig. 6-11. The i n t e e o n  with the ap- 
propriate window boundary is then carried out, depending on which one of the 
four regions (L, T, R, orB) contains Pz. Of course, if both PI and P2 are inside the 
clipping rectangle, we simply save the entirr line. 

If PI is in the region to the left of the window, we set up the four regions, L, 
LT, LR, and LB, shown in Fig. 6-12. These four regions determine a unique bound- 
ary for the line segment. For instance, if P2 is in region L, we clip the line at the 
left boundary and save the line segment fmm this intersection point to P2. But if 
P2 is in region LT, we save the line segment fmm the left window boundary to the 
top boundary. If Pz is not in any of the four regions, L, LT, LR, or LB, the entire 
line is clipped. 

P, in Edge Region 

(bl 

P. In Corner Region 

Ic! 

Figure 6-10 
Three possible positions for a line endpoint P, in the NLN line-djpping algorithm. 



- .- 
F~grlrr  6 .  l l  '. LB Fiprr 6-12 
The four clipping regions '. The four clipping regions used in 
used in the N L N  algorithm the NLN algorithm when P, is 
when PI is inside the clip directly left of the  clip window. 
window and P, is outside. 

For the third case, when PI is to the left and above the clip window, we use 
the clipping regions in Fig. 6-13. In this case, we have the two possibilites shown, 
depending on the position of P, relative to the top left corner of the window. If P, 
is in one of the regions T, L, TR, 78, LR, or LB, this determines a unique clip- 
window edge for the intersection calculations. Otherwise, the entire line is re- 
jected. 

To determine the region in which PI is located, we compare the s l o p  of the 
line to the slopes of the boundaries of the clip regions. For example, if PI is left of 
the clipping rectangle (Fig. 6-12), then P, is in region LT if 

-- 
slope < slope P,P, slope PIP,, 

And we clip the entire line if 

The coordinate diiference and product calculations used in the slope tests 
are saved and also used in the intersection calculations. From the parametric 
equations 

an x-intersection posltion on the left window boundary is x = r,, with 11 = 
(xL  - x , ) / ( x ?  - xI), SO that the y-intersection position is 



-~ .. ~ 

Figurr' 6-13 
The two possible sets of cl~pp~ng regions used in theNLN algor~tlm when P, 1s aboveand 
.o the left of the clip wndow. 

And an intersection position on the top boundary has y = yf and u = 
(y, - y,)!(.k - y,), with 

In some applications, it is often necessary to clip lines against arbitrarily shaped 
polygons. Algorithms based on parametric line equations, such as the 
Liang-Barsky method and the earlier Cyrus-Beck approach, can be extended eas- 
ily convex polygon windows. We do this by modifying the algorithm to in- 
clude the parametric equations for the boundaries oi the clip region. Preliminary 
screening of line segments can be accomplished bv processing lines against the 
coordinate extents of the clipping polygon. For concave polygon-clipping re- 
gions, we can still apply these parametric clipping procedures if we first split the 
concave polygon into a set of convex poiygons. 

Circles or other curved-boundary clipping regions are also possible, but less 
commonly used. Clipping algorithms far these areas are slower because intersec- 
tion calculations involve nonlinear curve equations. At the first step, lines can be 
clipped against the bounding rectangle (coordinate extents! of the curved clip 
ping region. Lines that can be identified as completely outside the bounding rec- 
tangle are discarded. To identify inside lines, we can calculate the distance of line 
endpoints from the circle center. If the square of this distance for both endpoints 
of a line 1s less than or equal to the radius squared, we can save the entire line. 
The remaining lines arc then processed through the intersection calculations, 
which must solve simultaneous circle-line equations 

Splitting Concave Polygons 

We can identify a concave polygon by calculating the cross products of succes- 
sive edge vectors in order around the polygon perimeter. If the z component of 



7wo.Dimensional Viewing -7, IE, Y EJ, > 0 IE, x E,), > 0 tE, >: E,), < 0 
IE, r E ~ ) ,  > o 

E6 J V 3  (E, Y E,), > 0 
(E6 >. E,), . 0 

''2 

Figure 6-14 
Identifying a concave polygon by calculating cross 
products of successive pairsof edge vectors. 

some cross products is positive while others have a negative z component, we 
have a concave polygon. Otherwise, the polygon is convex. This is assuming that 
no series of three successive vertices are collinear, in which case the cross product 
of the two edge vectors for these vertices is zero. If all vertices are collinear, we 
have a degenerate polygon (a straight line). Figure 6-14 illustrates the edge- 
vector cross-product method for identifying concave polygons. 

A wctor metltod for splitting a concave polygon In the xy plane is to calculate 
the edge-vector cross products in a counterclockwise order and to note the sign 
of the z component of the cross products. If any z component turns out to be neg- 
ative (as in Fig. 6-141, the polygon is concave and we can split ~t along the line of 
the first edge vector in the crossproduct pair. The following example illustrates 
this method for splitting a concave polygon. 

- - - - -  

Example 6-2: Vector Method for Splitting Concave Polygons 

Figure 6-15 shows a concave polygon with six edges. Edge vector; for this poly- 
gon can be expressed as 

El = (1,0, O), El = (1,1,0) 

E l - 1 0 ,  E4=(0,2,0) 

Es = (-3,0,O), E,, = (0, -2,O) 

where the z component is 0, since all edges are in the xy plane. The cross product 
E , x E i  for two successive edge vectors is a vector perpendicular to the xy plane 

E, with z component equal to E,E,,  - E,,E,.,,. 

E, x E2 = (0,0, I), Ez X Es = (0,0, -2) 

E3 x E4 = (0,O. 2), E, X E5 = (0,0,6) 

E5 x E, = (0,0,6). E, x E, = (O,O, 2) 

Figure 6-15 Since the cross product Ez X E3 has a negative 2 component, we split the polygon 
Splitting a concave polygon along the line of vector E:. The line equation for this edge has a slope of 1 and a y 
using the vector method. intercept of -1. We then determine the intersection of this line and the other 
236 



Section 6.8 
Polygon Cl~pping 

Figure 6-16 
Splitting a concave polygon using 
the ~ntational method. After 
rota-ing V3 onto the x axis, we find 
that V, is below the x axls. So we 
sp!fi&e polygon along the line 
of v,v,. 

polygon edges to split the polygon into two pieces. No other edge cross products 
are negative, so  the two new polygons are both convex. 

We can also split a concave polygon using a rotalional method. Proceeding 
counterclockwise around the polygon edges, we translate each polygon vertex VI 
in turn tn the coordinate origin. We then rotate in ;I clockwise direction so that 
the next vertex V,,, is on the x axis. If the next vertex, Vkqz ,  is below the x axis, the 
polygon is concave. We then split the polygon into two new polygons along the x 
axis and repeat the concave test for each of the two new polygons. Otherwise, we 
continue to rotate vertices on the x axis and to test for negative y vertex values. 
Figure 6-16 illustrates the rotational method for splitting a concave polygon. 

6-8 
POLYGON CLIPPING 

To clip polygons, we need to modify the line-clipping procedures discussed in 
the previous section. A polygon boundary processed with a line clipper may be 
displayed as a series of unconnected line segments (Fig. 6-17), depending on the 
orientation of the polygon to the cIipping window. What we reaIly want to dis- 
play is a bounded area after clipping, as in Fig. 6-18. For polygon clipping, we re- 
quire an algorithm that wiIl generate one or more closed areas that are then scan 
converted for the appropriate area fill. The output of a polygon clipper should be 
a sequence of vertices that defines the clipped polygcm boundaries. q\ I I ",, 

I 
! -. 
I 
I I Figure 6-17 
L -  _ _ - - _ ._ - -I Display of a polygon processed by a 

Before Clipping Aher C l i ~ p ~ n g  line-dipping algorithm 



, _ _ - _ - _ _ - _  + Before Chpping r4 Aher Clipping I ' i q ~ ~ r c  Display polygon (7- -. of IS .--. a correctly clipped 
Sutherland-Hodgenial1 f'olvgon Clipping 

We can correctly clip a polygon by processing the polygon bound jry as  a whole 
against each window edge. This could be accomplished by processing all poly- 
gon vertices against each clip rectangle boundary in turn. Beginning with the ini- 
tial set of polygon vertices, we could first clip the polygon against the left rectan- 
gle boundary to produce a new sequence of vertices. The new set of vertices 
could then k successively passed to a right boundary clipper, a bottom bound- 
ary clipper, and a top boundary clipper, as in Fig. 6-19. At each step, a new se- 
quence of output vertices is generated and passed to the next window boundary 
clipper. 

There are four possible cases when processing vertices in sequence around 
the perimeter of a polygon. As each pair of adjacent polvgon vertices is passed to 
a window boundary clipper, we make the following tests: (1) If the first vertex is 
outside the window boundary and the second vertex is inside, both the intersec- 
tion point of the polygon edge with the window boundary and the second vertex 
are added to the output vertex list. ( 2 )  If both input vertices are inside the win- 
dow boundary, only the second vertex is added to the output vertex list. (3) li the 
first vertex is inside the ~ , i n J o w  boundary and the second vertex is outside, only 
the edge intersection with the window boundary is added to the output vertex 
list. (4) If both input vertices are outside the window boundary, nothing is added 
to the output list. These four cases are illustrated in Fig. 6-20 for successive pairs 
of polygon vertices. Onct. all vertices have been processed for one clip window 
boundary, the output 11st of vertices is clipped against the next window bound- 
ary. 

O r ~ g ~ n a l  
Polygon 

Clip 
Leh 

C l ~ p  Clip 
Right Bonorn 

Clip 
TOP 

- --- -- - - - - - - - -- -- 
Figure. 6-19 
Clipp~ng a polygon against successive window boundaries. 

238 



hepunoq MOPU!M 
n q l  loj 1s!l xauaA l n d m  
aqa u! wqod a y ~  pqq 08 

pasn ale slaqwnu paw11j .L 
WlJJA ql!M %U!JI~?$S 'MOPU!M 

e P Oepunoq PI aq) 
~su!e%e uo%dlod e % u ! d d l ~  

[Z-9 JJII~!,~ 



Window 

I I 

I 

vz Figwe 6-22 
A polygon overlapping a 
restangular clip window. 

' -- Top 
Elipper - Out 

Figure 6-23 
Processing the vertices of the polygon in Fig. 6-22 through a boundary-clipping pipeline. 
After al l  vertices are processed h u g h  the pipeline, the vertex list for the clipped polygon 
is IF, V D  v3 V3I. 

case Left: i f  (p.x < wMCn.x) re tu rn  (FALSE); break; 
case Right: i f  (p .x  > -.x) re tu rn  (FALSE); break; 
case Bottom: i f  (p.y < wMin.y) re tu rn  (FALSE); break; 
case Top: i f  (p.y > w n M . y )  re tu rn  (FALSE): break; 
1 
re turn (TRUE) ; 

1 

i n t  cross  (wcPt2 p l ,  wcPt2 p2, 8dge b, dcPt M i n ,  dcPt wMax) 
( 

i f  ( ins ide  ( p l ,  b,  -in, == inside (p2, b, d n ,  Wax) 
re turn (FALSE) ; 

e l s e  re turn (TRUE); 
1 

wcPt2 in te r sec t  (wcPt2 p l ,  wcPt2 p2, Edge b, dcPt *in, dcPt wMax) 
f 

wcPt2 i P t ;  
float m; 

i f  (p1.x != p2.x) m = (p1.y - p2.y) / (p1.x - p2.x); 
switch (b) ( 
case Left : 

iP t .x  - dl in .x ;  
iP t .y  = p2.y + (wi4in.x - p2.x) m; 
break ; 

case Right: 
i P t . x  = wHax.x; 



i P t . y  = p2.y + (wb4ax.x - p2.x) ' m ;  
break; 

case Bottom: 
iP t .y  = wt4in.y; 
i f  (p1.x  ! =  p2.x) iPt .x  = p2.x + (Wt4in.y - p2.y) / m; 
e l s e  i P t . x  = p2.x: 
break; 

case Top: 
iP t .y  = wMsx.y; 
i f  (p1.x ! =  p2.x) iP t .x  = p2.x + (wMax.y - p2.y) / m; 
e l s e  i P t . x  = p2.x; 
break; 

1 
re turn  ( i P t )  ; 

1 

void c l ippo in t  (wcPt2 p, Edge b, dcPt d i r , ,  dcPt wNax, 
wcPt2 W t ,  i n t  c n t ,  wcPt2 f i r s t [ ] ,  wcPt2 ' s )  

( 
wcPt2 i P t :  

/ *  If  no previous point e x i s t s  fo r  t h i s  edge, save t h i s  po in t .  * /  
i f  ( ! f i r s t tb l )  

l i r s t [b ]  = hp; 
e l s e  

/' Previous point  e x i s t s .  If ' p '  and previous po in t  cross  edge, 
find in t e r sec t ion .  C l ip  against  next boundary, i f  any. I f  
no more edges, add in t e r sec t ion  t o  output l i s t .  * /  

i f  ( c r o s s  (p, s l b l ,  b, d i n ,  wMax)) ( 
i P t  = i n t e r sec t  (p,  a rb ] .  b, wMin. !Max); 
i f  (b < Top) 

c l ippo in t  ( i P t ,  b t l ,  wMin, Max,  W t ,  cn t ,  f irst , ,  s )  ; 
e l s e  ( 

pOut[*cntl = . iPt;  ( * c n t ) + + ;  
1 

1 

s [b l  = P ;  I' Save ' p '  a s  most recent  point  f o r  t h i s  edge ' /  

/ *  For a l l ,  i f  point  is ' i n s ide '  proceed t o  next c l i p  edge, i f  any * /  
i f  ( i n s i d e  (p,  b, wMin, d a x ) )  

i f  ( b  < Top) 
c l ippo in t  (p ,  b + l ,  d i n ,  f i x ,  pout,  c n t ,  f irst ,  s ) ;  

e l s e  ( 
pout ['cntl = p; I*cnt)++;  

1 
1 

void c l o s e c l i p  (dcPt wMin, dcPt wMax, wcPt2 ' pout,  
i n t  cnt ,  wcPt2 f i r s t [ ] ,  wcPt2 S )  

( 
wcPt2 i ;  
Edge b; 

f o r  ( b  = Left:  b  <= Top; b++)  ( 
i f  ( c ross  ( s [ b l ,  *frrst[bI,  b, d i n .  Max) ) ( 

i = i n t e r s e c t  ( s [ b ] ,  * f i r s t [b l ,  b, M i n ,  wMax!; 
i f  ( b  < Top) 

c l ippo in t  ( i ,  b+ l ,  wMin, Max,  pout,  cnt .  f irst .  s); 
e l s e  ( 

pOutI'cnt1 = i; (*cn t )++ ;  
f 

1 



i n t  cl ipPolygon (dcPt wMin, dcPt wKax, Lnt n ,  wcPtZ ' pIn,  wc?t2 ' pout) 

/ *  ' f i rs t '  holds pointer  t o  f irst  point proce5sed agalnst  a c l i p  
edge. ' s  holds most recent ~ o i n t  proce+sed against  an edge ' /  

wcPt2 ' first1N-EmEl = ( 0 ,  0. 0 ,  0 ) SIN-E:DGEl: 
i n t  i ,  cnt = 0 ;  

for  (i.0; i < n ;  i + + )  
c l ippoint  ( p I > [ i J .  L e f t ,  wMin, wMax, pout hcnt ,  f i rs t ,  s l ;  

c:oseClip IwMin, wMax, p3ut. Lcnt., f i rs t ,  s !  . 
return Lcnt); 

) 

Convex polygons are correctly clipped by the Sutherland-Hodgeman algo- 
rithm, but concave polygons may be displayed with extraneous lines, as demon- 
strated in Fig. 6-24. This occurs when the clipped polygon should have two or 
more separate sections. But since there is only one output vertex list, the last ver- 
tex in the list is always joined to the first vertex. There are several things we 
could d o  to correctly display concave polygons. For one, we could split the con- 
cave polygon into two or more convex polygons and process each convex poly- 
gon separately. Another possibility is to modify the Sutherland-Hodgeman ap- 
proach to check the final vertex list for inultiple vertex points along any clip 
window boundary and mrrectly join pairs of vertices. Finally, we could use a 
more general polygon clipper, such as either the Weiler-Atherton algorithm or 
the Weiler algorithm described in the next section. 

Wc~ler-Athcrton Polygc:n Clipping 

Here, the vertex-processing procedures for window boundaries are modified so 
that concave polygons are displayed correctly. This clipping procedure was de- 
veloped as a method for identifying visible surfaces, and so it can be applied 
with arbitrary polygon-clipping regions. 

The basic idea in this algorithm is that instead of always proceeding around 
the polygon edges as vertices are processed, we sometimes want to follow the 
window boundaries. Wh~ch path we follow depends on the polygon-processing 
direction (clockwise or counterclockwise) and whether tile pair of polygon ver- 
tices currently being processed represents an outside-to-inside pair or an inside- 

Window 
I - - - - - - - - - - - - I  
I  I 
I  I 

I  
I  
I 
I  
I  

- - - -- 
Frgurc 6-24 
C11ppmg the concave polygon III (al 

I  
I wlth the Sutherland-Hodgeman 



Sedion 6-8 
Polygon Clipping 

Figure 6-25 
Clipping a concave polygon (a) with the Weiler-Atherton 
algorithm generates the two separate polygon areas 
in (3). 

to-outside pair. For clockwise processing of polygon vertices, we use the follow- 
ing rules: 

For an outside-to-inside pair of vertices, follow the polygon boundary. 
For an inside-to-outside pair of vertices,. follow the window boundary in 
a clockwise direction. 

In Fig. 6-25, the processing direction in the Weiler-Atherton algorithm and the re- 
sulting clipped polygon is shown for a rectangular clipping window. 

An improvement on the Weiler-Atherton algorithm is the Weiler algorithm, 
which applies constructive solid geometry ideas to clip an arbitrary polygon 
against any polygondipping region. Figure 6-26 illustrates the general idea in 
this approach. For the two polygons in this figure, the correctly dipped polygon 
is calculated as the intersection of the clipping polygon and the polygon object. 

Other Polygon-Cli pping Algorithms 

Various parametric line-clipping methods have also been adapted to polygon 
clipping. And they are particularly well suited for clipping against convex poly- 
gon-clipping windows. The Liang-Barsky Line Clipper, for example, can be ex- 
tended to polygon clipping with a general approach similar to that of the Suther- 
land-Hodgeman method. Parametric line representations are used to process 
polygon edges in order around the polygon perimeter using region-testing pmce- 
dures simillr to those used in line clipping. 

F~grrre 6-26 
Cllpping a polygon by determining 
the intersection of two polygon 

area areas 



I/ 
Before Chpping 

After Cl~ppmg 

- - - - . . . 
Fipw 6-28 
Text clipping using .J 
bounding rectangle about the 
entire string. 

Areas with curved boundaries can be clipped with methods similar to those dis- 
cussed in the previous .sections. Curve-clipping procedures will involve nonlin- 
ear equations, however, and this requires more processing than for objects with 
linear boundaries. 

The bounding rectangle for a circle or other curved object can be used first 
to test for overlap with a rectangular clip window. If the bounding rectangle for 
the object is completely inside the window, we save the object. I f  the rectangle is 
determined to be completely outs~de the window, we discard the object. In either 
case, there is no further computation necessary. But if the bounding rectangle test 
fails, we can l w k  for other computation-saving approaches. For a circle, we can 
use the coordinate extents of individual quadrants and then octants for prelimi- 
nary testing before calculating curve-window intersections. For an ellipse, we can 
test the coordinate extents of individual quadrants. Figure 6-27 illustrates circle 
clipping against a rectangular window. 

Similar procedures can be applied when clipping a curved object against a 
general polygon clip region. On the first pass, we can clip the bounding rectangle 
of the object against the bounding rectangle of the clip region. I f  the two regions 
overlap, we will need to solve the simultaneous line-curve equations to obtain 
the clipping intersection points. 

TEXT CLIPPING 

There are several techniques that can be used to provide text clipping in a graph- 
ics package. Thc clipping technique used will depend on the methods used to 
generate characters and the requirements of a particular application. 

The simplest method for processing character strings relative to a window 
boundary is to use the all-or-none string-clipping strategy shown in Fig. 6-28. If  all 
of the string is inside a clip window, we keep it. Otherwise, the string is dis- 
carded. This procedure is implemented by considering a bounding rectangle 
around the text pattern. The boundary positions of the rectangle are then com- 
pared to the window boundaries, and the string is rejected if there is any overlap. 
This method produces the fastest text clipping. 

An alternative to rejecting an entire character string that overlaps a window 
boundary is to use the all-or-none chnracter-clipping strategy. Here we discard only 
those characters that are not completely inside thc window (Fig. 6-29). In this 
case, the boundary limits of individual characters are compared to the window. 
Any character that either overlaps or is outside a window boundary is clipped. 

A final method for handling text clipping is to clip the components of indi- 
vidual characters. We now treat characters in much the same way that we treated 
lines. If an individual character overlaps a clip window boundary, we clip off the 
parts of the character that are outside the window (Fig. 6-30). Outline character 
fonts formed with line segments can be processed in this way using a line- 
clipping algorithm. Characters defined with bit maps would be clipped by com- 
paring the relative position of the individual pixels in the character grid patterns 
to the clipping boundaries. 



6-1 1 
EXTERIOR CLIPPING Summary 

So far, we have considered only procedures for clipping a picture to the interior 
of a r e e n  by eliminating everything outside the clipping region. What is saved 
by these procedures is inside the region. In some cases, we want to do the reverse, 
that is, we want to clip a picture to the exterior of a specified region. The picture 
parts to be saved are those that are outsrde the region. This is referred to as exte- 
rior clipving. 

A typical example of the application of exterior clipping is in multiple- 
window systems. To correctly display the screen windows, we often need to 
apply both internal and external clipping. Figure 6-31 illustrates a multiple- 
window display. Objects within a window are clipped to the interior of that win- 
dow. When other higher-priority windows overlap these objects, the objects are 
also clipped to the exterior of the overlapping windows. 

Exterior cfipping is used also in other applications that require overlapping 
pictures. Examples here include the design of page layouts in advertising or pub- 
lishing applications or for adding labels or design patterns to a picture. The tech- 
nique can also be used for combining graphs, maps, or schematics. For these ap- 
plications, we can use exterior clipping to provide a space for an insert into a 
larger picture. 

Procedures for clipping objects to the interior of concave p o h o n  windows 
can also make use of external clipping. Figure 6-32 shows a line P,P, that is t& 
clipped to the interior of a concave window with vertices V,V,V,V,V,. tine PIP2 
can be clipped in two passes: (1) First, P,P, is clipped to the interior of the convex 
polygon V,V,V,V,o yield the clipped segment P;P, (Fig. 6-32(b)). (2) Then an 
external clip of PiP', is performA against the convex polygon V,V,V, to yield 
the final clipped line segment P;'P2. 

RING 3 

STRING 4 

I 

Before Clipping 

After Clipping 

I igrrre 6-29 
Text clipping using a 
bounding &tangle about 
individual characters. 

SUMMARY 1 ' 
In this chapter, we have seen how we can map a two-dimensional world- 
coordinate scene to a display device. The viewing-transformation pipeline in- 

Before Clipping 

I I 
After Clipping 

Figure 6-31 
A multiple-window screen display 6-30 
showing examples of both interior ~ext  clipping performed on 
and exterior clipping. (Courts!/ of the components of individual 
Sun Micmystems). characten. 



l a )  Interior Cl$ 

(b) 

Exterior Clip 

(c) 

,. . 

F i p w  6-32 
C h p p n g  line Fr, to the interior of a concave polygon M.IIII \vrtices VlV,V3V,V, (a), using 
ccrn\.rx pnlvgons V,V,V,V, (b) and V,V,V, (c), to products the  clipped lirrc PYP:. 

cludes constructing the \\.orld-coord~natc scene using modeling transformations 
transferring wortci-coordinates to \ziewing coordinates, ]napping the vieiving- 
coordinate descriptions r , f  objects to normalized de\.ice aordinates, and finally 
mapping to device coordinates. Normalized coord~nates 'Ire specified in the 
range from 0 to 1, and tliev are used to make viewing pxkages independent ot 
particular output device5 

Viewing coordinates are specified by giving the \txvlcl-coordinate pos~t~on  
of the viewing origin and the view up vector that delmes the direction ot the 
viewing y axis. These parameters are used to construct t:le viewing transforma- 
tion niatrix that maps world-coordinate object descriptions to viewing coordi- 
nates. 

A window is then 21-t up  in viewing coordinates, and a vie;vport is specilicd 
in normalized device co~vdinates. Typically, the window and \.iewport are rcc- 
tangles in standard posit~on (rectangle boundaries are parallel to the coordinatc 
axes). The mapping from \.iewing coordinates to normallzed device coordinates 
is then carried out so that relative positions in the window are maintained in the 
viewport. 

Viewing functions In graphics programming package are used to create 
one or more sets of v ~ e ~ i n g  parameters. One function is typically provided tu 
calculate the elements of the niatrix for transforming world coordinates to view- 
ing coordinates. Anothcr function is used to set up the window-to-viewport 
transformation matrix, and a third function can be used to specify combinations 
of viewing transformations and window mapping in a viewing table. We can 



Illcn s c l ~ i t  ditfvrcnt \-icwing combin,~tion> L,): y~~"a i t \  111g p.irticular view indices 
listed in the \.ic,wing table. ~umni,wv 

Wlicn objects arc displayed on the o~l tput  li,-v~cc', all parts of a scene out- 
side the \\-indow (and the ;iewport) ,Ire clipped oil unless \\.c set clip parameters 
to turn ofi clipping. I11 many packages, clipping I >  aone  in normalized device co- 
ordinates so  t h ~ t  all transforniations can be concatenated into a single transfor- 
niatiun operation before applying the clipping algc)r~thms. The clipping region IS 
commonly referred to as the clipping window, or iii the clipping rectangle when 
the window and viewport are standard rectangles 3evcral algorithnls have bwn  
developed for clipping objects against the clip-winciow boundaries. 

Line-clipping algorithms include the Cohen-Sutherland method, the 
Liang-Barsky method, and the Nicholl-Let-Nichc~ll method. The Cohen-Suther- 
land method is widely u s d ,  since it was one of t l ic  first line-clipping algorithms 
to b r  developed Region codes are used to idtmliiv tlic position of line endpoints 
relativc to the rectangular, c l~pping window bouncl,~ric.s. Lines that cannot be ini- 
mediately identified as conipletely insdc  the winciuw or completely outside arc 
then clipped against window boundaries. Liang and Barsky use a parametric line 
representation, similar to that of the earlier Cyril--Beck algorithm, to set u p  a 
more cfficicnt II~P-clipping p roced~~re  that red~lre.; intersrction calculations. The 
Nicholl-LecNicholl algorithm uses more region testing in the sy plane to reduce 
~nterseclion calculations even further. Paranictric 11 :ic clipping is easily extended . .  . 
to convex clipping windows and to three-dimens~o~~,il  clipping windows. 

Line clipping can also be carried out for concave, polygon clipping win- 
dows and for clippirig windows with curved h~undar ies .  With concave poly- 
gons, we can use either the vector niethod or the r11:ational method to split a con- 
cave polygon into a number of convex polygons. \I1ith curved clipping windows, 
we calculate line intersections using the curve equ,itions. 

Polygon-clipp~ng algorithms include the Sulhcrland-Hodgeman method. 
the Liang-Barsky method, and the Wcilcr-Athcrtc,ri nwthod. In the Suther- 
land-Hodgeman clipper, vertices 0 1  a convex polygon a r t  processed in order 
against the four rectangular \ v~ndow boundaries t,, product. a n  output vertex list 
for thcb clipped polygon. Liang and Barsky use para~;irtric line equations to repre- 
sent the con\?ex polygon edges, and they use simihr testing to that performed :n 
line clipping t o  produce an outpuc \,ertex list for the clipped polygon. Both the 
Weiler-Atherland niethod and the We~ler method c,orrectly clip both convex ar.d . - 
concave polygons, and these polygon clippers also allow the clipping window to 
be a general polygon. The Weiler-Atherland algorithm processes polygon ver- 
tices in order to produce one or more lists of output polygon vertices. The Weilrr 
method performs dipping by finding the intersectwn region of the two polygons. 

Objects with curved boundaries are procesjai against rectangular clipping 
w ~ n d o w s  by calculating intersections using the curve equations. These clipping 
procedures are slower than I ~ n e  clippers or polyp(m clippers, because the curve 
equations are nonlmear. 

The fastest text-clipping method is to complctelv clip '1 string if  any part of 
the string 1s outside any  window boundary. Another niethod for text clipping is 
to use the all-or-none approach with the individual characters in a string. A third 
method is to apply either point, line, polygon, or u r v e  clipping to the individual 
characters in a string, depending on whether characters are defined as point 
grids or as outline fonts. 

In somc applicat~ons, such as creating picture insets and managing multi- 
ple-screen windows, exterior clipping is performed. In this case, all parts of 
scene that arc inside a window are clipped and the exterior parts are saved. 



Chapter 6 

Two-Dlrnenslonal bewing 
REFERENCES 

Line-cllpping algorithms arc? discussed in Sproull and Sutherland (19681, Cyrus and Beck 
[1978), and Liang and Barsky (1984). Methods for improving the speed of the 
Cohen-Sutherland lineclippi ng algorithm are given in Duvanenko (1 990). 

Polygon-clipping methods are presented in Sutherland and Hodgeman (1974) and in Liang 
and Barsky (1983). General techniques for clipping arbitrarily shaped polygons against 
each other are given in Weiler and Atherton (1977) and in Weiler (19801. 

Two-dimensional viewing operations in PHlGS are discussed in Howard et al. (1991), Gask- 
Ins (1 992), Hopgood and Duce (1991 ), and Blake (1 993). For information on GKS viewing 
operations, see Hopgood et al. (1983) and Enderle et al. (1984). 

EXERCISES 

6-1. Write a procedure to to implement the evaluateViewOrientationMatrix func- 
tion that calculates the elements of the matrix for transforming world coordinates to 
viewing coordinates, given the viewing coordinate origln Po and the view up vector V. 

6-2. Derive the window-to-viewpon transformation equations 6-3 by f~rst scaling the win- 
dow to the slze of the viewpon and then translating the scaled window to the view- 
port position. 

6-3. Write a procedure to ~mplement the evaluateViewMappinqMatrix function that 
calculates the elements of a marrix for performing the window-to-viewport transforma- 
tion. 

6 4 .  Write a procedure to implement the setViewRepresencation function to concate- 
nate v i ewMat r i x  and viewMappingMatrix and to store the result, referenced by 
a spe(iiied view index, in a viewing table. 

6-5. Write a set of procedures to implement the viewing pipeline without clipp~ng and 
without the workstat1011 transformation. Your program should allow a scene to be con- 
structed with modeling-coordinate transformations, a specified viewing system, and a 
specified window-vewport pair. As an option, a viewing table can be implemented to 
store different sets of viewing transformalion parameters. 

6-6. Derive the matrix representation for a workstation transformation. 

6-7. Write a set of procedures to implement the viewing pipeline without clipping, but in- 
cluding the workstation transformation. Your program should allow a scene to be con- 
structed with modeling-coordinate transformations, a specified viewing system, a 
specified window-viewport pair, and workstation transformation parameters. For a 
given world-coordinate scene, the composite viewing transformation matrix should 
transform the scene to an output device for display. 

6-8. Implement the Cohen-Sutherland line-clipplng algorithm. 

6-9. Carefullydiscuss the rarionale behind the various tests and methods for calculating the 
intersection parameters u, and u, in the Liang-Barsky line-cllpping algorithm. 

6-10. Compare the number of arithmetic operations performed in the Cohen-Sutherland 
and the Liang-Barsky I~ie-clipping algorithms for several different line orientations rel- 
ative to a clipping window. 

6-1 1. Write a procedure to ~niplement the Liang-Barsky line-clipping algorithm. 

6-12. Devise symmetry transformations for mapping the inlersec:tion calculations for the 
three regions in Fig. 6-1 0 to the other six regons of the xy p l~ne.  

6-1 3.  Set up a detailed algorithm for the Nicholl-Lee-Nicholl approach to line clipping for 
any input pair of line endpo~nts. 

6-14. Compare the number ol arithmetic operations performea in NLN algor~thm to both the 
Cohen-Sutherland and the Liang-Barsky line-clipping algorithms for several different 
line orientations relatlve to a clipping window. 



6-1 5. Write a routine to identify concave p$ygons by calculating cross products of pairs of 
edge vectors. Exercises 

6-1 6. Write a routine to split a concave polygon using the vector method. 

6-1 7 .  Write a routine to split a concave polygon using the rotational method 

6-1 8 Adapt the Liang-Barsky line-clipping algorithm to polygon clipping. 

6-19. Set up a detaled algorithm for Weiler-Atherton polygon clipping assuming that the 
clipping w~ndow is a rectangle in standard position. 

6-20. Devise an algorithm for Weiler-Atherton polygon clipping, where the clipping win 
dow can be any specified polygon. 

6-21 Write a routine to c l ~ p  an ell~pse against a rectangular window. 

6-22. Assuming that all characters in a text strlng have the same width, develop a text-clip- 
ping algor~thm that cl~ps a string according to the "all-or-none character-clipping" 
strategy. 

6-23. Develop a text-clipping algorithm that clips ind~vidual characters assuming that the 
characters aredefined in a pixel grid of a specified w e .  

6-24. Wr~te a routine to implement exterior clipping on any part of a defined picture using 
any specified window. 

6-25 Wr~te a routine to perform both interior and exterior clipping, given a particular win- 
dow-system display. Input to the routine is a set of window positions on the screen, 
the objects to be displayed in each w~ndow, and the window priorities. The individual 
objects are to be clipped to fit into their respective windows, then clipped to remove 
parts with overlapping windows of higher display pr~ority. 





F or a great many applications, it is convenient to be able to create and ma- nipulate individual parts of a picture without affecting other picture parts. 
Most graphics packages provide this capability in one form or another. With the 
ability to define each object in a picture as a separate module, we can make modi- 
fications to the picture more easily. In design applications, we can try out differ- 
ent positions and orientations for a component of a picture without disturbing 
other parts of the picture. Or we can take out parts of the picture, then we can 
easily put the parts back into the display at a later time. Similarly, in modeling 
applications, we can separately create and position the subparts of a complex ob- 
ject or system into the overall hierarchy. And in animations, we can apply trans- 
formations to individual parts of the scene so that one object can be animated 
with one type of motion, while other objects in the scene move differently or re- 
main stationary. 

7-1 
STRUCTURE CONCEPTS 

A labeled set of output primitives (and associated attributes) in PH1GS is called a 
structure. Other commonly used names for a labeled collection of primitives are 
segme~rls (GKS) dnd ohlects (Graphics Librar) on S~Licon Graphics systems). In this 
section, we consider the basic structure-managing functions in PHIGS. Similar 
operations are available in other packages for handling labeled groups of priml- 
tives in a picture. 

Basic S I r i ~ c l ~ ~ r e  Functions 

When we create a structure, the coordinate positions and attribute values specl- 
fied for the structure are stored as a labeled group in a system structure list called 
the central structure store. We create a structure with the function 

The label for the segment is the positive Integer assigned to parameter id.  In 
PHIGS+, we can use character strings to label the st~uctures instead of using inte- 
ger names. This makes i t  easier to remember the structure identifiers. After all 
primitives and attributes have been listed, the end of the structure is signaled 
with the c l o s e S t r u c t u r e  statement. For example, the following program 



Chapter 7 statements define structure 6 as the line sequence specit'ied in polyline with the 
Struclures 2nd Hierzrchical designated line type and color: 

Modelmg 

openstrucrure i c ) :  
ser;Llnetypc (It); 
setPolylin~ColourIndex ( l c ) ;  
polyline (i?, pts): 

closebtructure; 

Anv number of structures can be created for a picture, but only one structure can 
be open (in the creation process) at a time. Any open structure must be closed be- 
fore a new structure can be created. This requirement eliminates the need for a 
structure identification number in the c1oseStruct:lre statement. 

Once a structure has been created, it can be displayed on a selected output 
device with the function 

poststrucrure (ws, id, priority) 

where parameter ws i s  the workstation identifier, id  is the structure name, and 
p r i o r i t y  is assigned a real value in the range from 0 to I .  Parameter p r i o r i  ty 
sets the display priority relative to other structures. When two structures overlap 
on an output display device, the structure with the higher priority will be visible. 
For example, if structures 6 and 9 are posted to workstation 2 with the following 
priorities 

then any parts of structure 9 that overlap structure 6 will be hidden, since struc- 
ture 6 has higher priority If two structures are assigned the same priority value, 
the last structure to be posted is given display precedence 

When a structure is posted to an active workstation, the primitives in the 
structure are scanned and interpreted for display on the selected output device 
(video monitor, laser printer, etc.). Scanning a structure list and sencling the 
graphical output to a workstation is called traversal. A list of current attribute 
values for primitives is stored in a data structure called a traversal state list. As 
changes are made to posted structures, both the system structure list and the tra- . 

versa1 state list are updated. Th~s  automatically modiiies the display of the 
posted structures on the workstation. 

To remove the display of a structure from a part~cular output device, we in- 
voke the function 

unpostStructure lws, id) 

This deletes the structure from the active list of structures for the designated out- 
put device, but the system structure list is not affected. On a raster system, a 
structure is removed from the display by redrawing the primitives in the back- 
ground color. This process, however, may also affect the display of primitives 
from other structures that overlap the structure we want to erase. To remedy this, 
we can use the coordinate extents o f  the various structures in a scene to deter- 



mine which ones overlap the structure we are erasing. Then we can simply re- ~~ 7-1 
draw these overlapping structures after we have erased the shucture that is to be Structure Concepts 
unposted. A11 structures can be removed from a selected output device with 

If we want to remove a particular structure from the system structure list, 
we accomplish that with the function 

Of course, this also removes the display of the structure h m  all posted output 
devices. Once a structure has been deleted, its name can be reused for another set 
of primitives. The entire system structure list can be cleared with 

It is sometimes useful to be able to relabel a structure. This is accomplished 
with 

One reason for changing a structure label is to consolidate the numbering of the 
structures after several structures have been deleted. Another is to cycle through 
a set of structure labels while displaying a structure in multiple locations to test 
the structure positioning. 

Setting Structure Attributes 

We can set certain display characteristics for structures with workstation filters. 
The three properties we can set with filters are visibility, highlighting, and the ca- 
pability of a structure to be selected with an interactive input device. 

Visibility and invisibility settings for structures on a particular workstation 
for a selected device are specified with the function 

where parameter i n v i s s e t  contains the names oi structures that will be invisi- 
ble, and parameter v i s s e t  contains the names of those that will be visible. With 
the invisibility filter, we can turn the display of structures on and off at selected 
workstations without actually deleting them from the workstation lists. This al- 
lows us, for example, to view the outline of a building without all the interior de- 
tails; and then to reset the visibility so that we can view the building with all in- 
ternal features included. Additional parameters that we can specify are the 
number of structures for each of the two sets. Structures are made invisible on a 
raster monitor using the same procedures that we discussed for unposting and 
for deleting a structure. The difference, however, is that we d o  not remove the 
structure from the active structure list for a device when we are simply making i t  
invisible. 

Highlighting is another convenient structure characteristic. In a map dis- 
play, we could highlight all cities with populations below a certain value; or for a 



Chapter 7 landxape layout, we could highlight certain varieties of shrubbery; or in a circuit 
Strucrures and Hierarchlcal diagram, we could highlight all components within a specific voltage range. This 

is done with the function 

set~ighiigh~ingfilter (ws, devcode, highlighcset, 
nohighlightset) 

Parameter h i g h l i g h t s e t  contains the names of the structures that are to be 
highlighted, and parameter n o h i g h l  i g h t S e  t contains the names of those that 
are not to be highlighted. The kind of highlighting used to accent structures de- 
pends on the type and capabilities of the graphics system. For a color video mon- 
itor, highlighted structures could be displayed in a brighter intensity or in a color 
reserved for highlighting. Another common highlighting implementation is to 
turn the visibility on and off rapidly so that blinking structures are displayed. 
Blinlung can also be accomplished by rapidly alternating the intensity of the 
highlighted structures between a low value and a high value. 

The third display characteristic we can set for structures is pickubility. This 
refers to the capability of the structure to be selected by pointing at it or position- 
ing the screen cursor over i t .  If we want to be sure that certain structures in a dis- 
play can never be selected, we can declare them to be nonpickable with the pick- 
ability filter. In the next chapter, we take up the topic of Input methods in more 
detail. 

7-2 
EDITING STRUCTURES 

Often, we would like to modify a structure after it has bren created and closed. 
Structure modification is needed in design applications to try out different graph- 
ical arrangements, or to change the design configuration In response to new test 
data. 

If additional primitives are lo be added to a structure, this can be done by 
simply reopening the structure with the openscruc ture .  :.nc::hn and append- 
ing the required statements. As an example of simple appending, the following 
program segment first creates a structure with a single f i l l  area and then adds a 
second fill area to the structure: 

openstructure (shape) ; 
setInteriorStyle (solid) ; 
setInteriorColourIndex 1 4 1 ,  
fillArea (n:, vertsl); 

~1oseStructure; 

openstructure (skdpe) ; 
setIntericrCty1~ (hollow). 
flllArea ( n 2 .  verts21; 

closeStructure; 

This sequence of operatwns is equivalent to initially cre'lting the structure with 
both fill areas: 



openstructure (shape) ; 
setInteriorStyle (solid); 
setInteriorColourIndex ( 4 ) ;  
fi11Area (nl, vertsl); 
setInteriorStyle (hollow): 
fi11Area (n2, v e r t s 2 )  ; 

closeStructure; 

In addition to appending, we may also want sometimes to delete certain 
items in a structure, to change primitives or attribute settings, or to insert items at 
selected positions within the structure. General editing operations are carried out 
by accessing the sequence numbers for the individual components of a structure 
and setting the edit mode. 

Structure Lists and the Element Pointer 

Individual items in a structure, such as output primitives and attribute values, 
are referred to as structure elements, or simply elements Each element is as- 
signed a reference position value as it IS entered into the structure. Figure 7-1 
shows the storage of structure elements and associated position numbers created 
by the following program segment. 

openstructure (gizmo): 
set~inetype (ltl) ; 
set~olylineColaurIndex ( 1 ~ 1 ) :  
polyline (nl, ptsl); 
setLinetype (lt2) ; 
set~olylineColourIndex ( 1 ~ 2 ) :  
polyline (n2, pts2); 

closestructure: 

Structure elen~ents are numbered consecutively with integer values starting 
at 1, and the value 0 indicates the position just before the first element. When a 
structure is opened, an element pointer is set up and assigned,a position value 
that can be used to edit the structure. If the opened structure is new (not already 
existing in the system structure list), the element pointer is set to 0. If the opened 
structure does already exist in the system list, the element pointer is set to the po- 
sition value of the last element in the structure. As elements are added to a struc- 
ture, the element pointer is incremented by 1. 

We can set the value of the element pointer to any position within a struc- 
ture with the function 

n aim0 structure 

Section 7-2 

tdlr~ng Structures 

l i p  rlP 7- 1 
Element position values for 
stnlcture gizmo. 



Chapter 7 where parameter k can be assigned any integer value from O to the maximum 
Structures dnd Hierarchical number of elements in the structure. It is also possible to position the element 

Modeling pointer using the following offset function that moves the pointer relative to the 
current position: 

w ~ t h  dk assigned a positwe or negative integer offset from the present position of 
the pointer. Once we have positioned the element pointer, we can edit the struc- 
ture at that position. 

Setting the Ed~t Modt3 

Structures can be modified in one of two possible modes. This is referred to as 
the edit mode of the slnlcture. We set the value of the edit mode with 

setEd;tMode (mode) 

where parameter mode is assigned either the value inserl, or Ihe value replalace. 

Inserting Structure Elenicnts 

When the edit mode is set to irisat, the next item entered into a structure will be 
placed in the position immediately following the element pointer. Elements in 
the structure list following the inserted item are then automatically renumbered. 

To illustrate the ~nsertion operation, let's change the standard line width 
currently in structure gizmo (Fig. 7-2) to some other value. We can d o  this by in- 
serting a line width statement anywhere before the first polyline command: 

openstructure (gizmo); 
setEditMode ( i n s e r t ) :  
setElemertPointer ( 0 ) ;  
setLinewidt h ( l w )  : 

Figure 7-2 shows the mcdified element list of gizmo, created by the previous in- 
sert operation. After this insert, the element pointer is assigned the value 1 (the 
position of the new line-width attribute). Also, all elements after the line-width 
statement have been renumbered, starting at the value 2. 

element - 
~oin ler  

6 setPolylinecolourIndex (lc21 

7 polyline (n2, pts2) 

Fi,yrrris 7-2 
Modified element list and position 
of the element pomter after 
inserting a line-width attribute 
into structure gizmo. 



When a new structure is created, the edit mode is automatically et to the seaion 7-2 
value insert. Assuming the edit mode has not been changed from this lefault Edil~n~Struclures 
value before we reopen this structure, we can append items at the end of the ele- 
ment list wlthout setting values for either the edit mode or element pointer, as 
demonstrated at the beginning of Section 7-2. This is because the edit mode re- 
mains at the value insert and the element pointer for the reopened structure 
points to the last element in the list. 

Replacmg Structure Elements 

When the edit mode is set to the value replace, the next item entered into a struc- 
ture is placed at the position of the element pointer. The element originally at that 
position is deleted, and the value of the element pointer remains unchanged. - 

As an example of the replace operation, suppose we want to change the 
color of the second polyline in structure gizmo (Fig. 7-1). We can d o  this with the 
sequence: 

openstructure (gizrnc); 
setEditMode (replace) ; 
setElementPointer (5); 
setPolylineColourIndex (lc2New) ; 

Figure 7-3 shows the element list of gizmo with the new color for the second 
polyline. After the replace operation, the element pointer remains at position 5 
(the position of the new line color attribute). 

Deleting Structure Elements 

We can delete the element at the current position of the element pointer with the 
function 

This removes the element from the structure and sets the value of the element 
pointer to the immediately preceding element. 

As an example of element deletion, suppose we decide to have both poly- 
lines in structure gizmo (Fig. 7-1) displayed in the same color. We can accom- 
plish this by deleting the second color attribute: 

0 gizmo structure 

1 setLinetype (ltl) 

2 setPolylineColour1ndew (lcll 
- 

3 polyline in l ,  p t s l )  Fig~rrr. 7-3 

1 c-rr.i?etype (1t2) 
Modified element list and position 
of the element pointer alter ' (lczNw' changing the color of the second 

61 w l v l i n e  in2, ptsZl polyline in structure gizmo. 



Chapter 7 openstructure (sizrno); 
Structures and Hierarchical set~lement~ointer (5); 

Modeling deleteElement; 

The element pointer is then reset to the value 4 and all following elements are 
renumbered, as shown in Fig. 7-4. 

A contiguous group of structure elements can be deleted with the function 

where integer parameter kl gives the beginning position number, and k2 speci- 
fies the ending position number. For example, we can delete the second polyline 
snd associated attributes in structure gizmo with 

And all elements in a structure can be deleted with the function 

Labeling Structure Elenients 

Once we have made a number of modifications to a structure, we could easily 
lose track of the element positions. Deleting and inserting elements shift the ele- 
ment position numbers. To avoid having to keep track of new position numbers 
as modifications are made, we can simply label the different elements in a struc- 
ture with the function 

label (k) 

where parameter k is an integer position identifier. Labels can be inserted any- 
where within the structure list as an aid to locating structure elements without re- 
ferring to position number. The label function creates structure elements that 
have no effect on the structure traversal process. We simply use the labels stored 
in the structure as edit~ng references rather than using th? individual element po- 
sitions. Also, the labeling of structure elements need not be unlque. Sometimes it 
is convenient to give two or more elements the same label value, particularly if 
the same editing operations are likely to be applied to several positions in the 
structure. 

0 gizmo structure 

? I  setLinetvpe Iltl) 1 
F i p r r  7-4 
Modified element list and position 
of the element pointer after deleting 
the tolor-attribute statement for the 
second polyline in structilre gizmo. 



To illustrate the use of labeling, we create structure 1abeledGizmo in the *ion 7-2 
following routine that has theelements and position numbers as shown in Fig. 7-5. Editing Structures 

openstructure (1abeledGizrno); 
label (objectlLinetype) ; 
setLinetype (ltl) ; 
label (objectlColor); 
set~olylineColourIndex (lcl); 
label (object11 ; 
polyline (nl. ptsl); 
label (object2Linetype) ; 
setLinetype (lt2) ; 
label (object2Color); 
setPolylineColourIndex ( 1 ~ 2 ) ;  
label (object2); 
polyline (n2, pts2); 

closeStructure: 

Now if we want to change any of the primitives or attributes in this structure, we 
can do it by referencing the labels. Although we have labeled every item in this 
structure, other labeling schemes could be used depending on what type and 
how much editing is anticipated. For example, all attributes could be lumped 
under one label, or all color attributes could be given the same label identifier. 

A label is  referenced with the function 

which sets the element pointer to the value of parameter k. The search for the 
label begins at the current element-pointer position and proceeds forward 
through the element list. This means that we may have to reset the pointer when 
reopening a structure, since the pointer is always positioned at the last element in 
a reopened structure, and label searching is not done backward through the e l e  
ment list. If, for instance, we want to change the color of the second object in 
structure labeledGizmo, we could reposition the pointer at the start of the ele- 
ment list after reopening the structure to search for the appropriate color at- 
tribute statement label: 

0 1abeledGirmo structure 

1 label LobjectlLlnetype) 

6 polyline (nl, ptsl) 

7 label (obiect2Linet~~el 

Fiprre 7-5 
A set of labeled objects and 

element - associated position numbers stored 
in structure 1abeledGizmo. 



- -r-- 

Structures and H~erarch~cal setElementPointer LO) ; 
setEditMode (replace); 

Deleting an item referenced with a label is similar to the replacement opera- 
tion illustrated in the last o p e n s t r u c t u r e  routine. We first locate the appropri- 
ate label and then offset the pointer. For example, the color attribute for the sec- 
ond polyline in structure 1abeledGizmo can be deleted with the sequence 

openstructure (labeledcizmo); 
setElementPolnter ( 0 ) ;  
setEditMode (replace]; 
setElernentPointerAtLabe1 (object2Color); 
offsetElementPointer (1): 
deleteElement ; 

We can also delete a group of structure elements between specified labels with 
the function 

After the set of elements is deleted, the element pointer is set to position kl 

Copying Elements from One Structure to Another 

We can copy all the entries from a specified structure into an open structure with 

, copyA11ElementsF1omStructure (id) 

The elements from structure i d  are inserted into the open structure starting at 
the position immediately following the element pointer, regardless of the setting 
of the edit mode. When the copy operation is complete, the element pointer is set 
to the position of thelast item inserted into the open structure. 

7-3 
BASIC MODELING CONCEPTS 

An important use of structures is in the design and representation of different 
types of systems. Architectural and engineering systems, such as building lay- 
outs and electronic circuit schematics, are commonly put together using com- 
puter-aided design methods. Graphical methods are used also for representing 
economic, financial, organizational, scientific, social, and environmental systems. 
Representationsfor these systems are often constructed to simulate the behavior 



of a system under various conditions. The outcome of the simulation ran serve as 
an instructional tool or as a basis for malung decisions about the system. To be ef- 
fective in these various applications, a graphics package must possess efficient 
methods for constructing and manipulating the graphical system representations. 

The creation and manipulation of a system representation is termed model- 
ing. Any single representation is called a model of the system. Models for a sys- 
tem can be defined graphically, or they can be purely descriptive, such as a set of 
equations that defines the relationships between system parameters. Graphical 
models are often refemd to as geometric models, because the component parls 
of a system are represented with geometric entities such as lines, polygons, or cir- 
cles. We are concerned here only with graphics applications, so we will use the 
term model to mean a computer-generated geometric representation of a system. 

Model Representations 

Figure 7-6 shows a representation for a logic circuit, ilhstrating the features com- 
mon to many system models. Component parts of the system are displayed as 
geometric structures, called symbols, and relationships between the symbols are 
represented in this example with a network of connecting lines. Three standard 
symbols are used to represent logic gates for the Boolean operations: and, or, and 
not. The connecting lines define relationships in terms of input and output flow 
(from left to right) through the system parts. One symbol, the and gate, is dis- 
played at two different positions within the logic circuit. Repeated positioning of 
a few basic symbols is a common method for building complex models. Each 
such occurrence of a symbol within a model is called an instance of that symbol. 
We have one instance for the or and not symbols in Fig. 7-6 and two instances of 
the and symbol. 

In many cases, the particular graphical symbols choser, to rrprrsent the 
parts of a system are dictated by the system description. For circuit models, stan- 
dard electrical or logic symbols are used. With models representing abstract con- 
cepts, such as political, financial, or economic systems, symbols may be any con- 
venient geometric pattern. 

Information describing a model is usually provided as a combination of 
geometric and nongeometric data. Geometric information includes coordinate 
positions for locating the component parts, output primitives and attribute func- 
tions to define the structure of the parts, and data for constructing connections 
between the parts. Nongeometric information includes text labels, algorithms d e  
scribing the &rating characteristics of the model and rules for det&mining the 
relationships or connections between component parts, if these are not specified 
as geometric data. 

I 

Binary 
Input 

M i o n  7-3 

Basic Modeling Concepts 

Figure 7-6 
Model of a logic circuit. 



Chapter 7 There are two methods for specifying the information needed to construct 
Structures and Hierarchical and manipulate a model. One method is to store the infomation in a data slruc- 

ture, such as a table or linked list. The other method is to specify the information 
in procedures. In general, a model specification will contain both data structures 
and procedures, although some models are defined completely with data struc- 
tures and others use only procedural specifications. An application to perform 
solid modeling of objects might use mostly information taken from some data 
structure to define coordinate positions, with very few procedures. A weather 
model, on the other hand, may need mostly procedures to calculate plots of tem- 
perature and pressure variations. 

As an example of how combinations of data structures and procedures can 
be used, we consider some alternative model specifications for the logic circuit of 
Fig. 7-6. One method is to define the logic components in a data table (Table 7-l), 
with processing procedures used to specify how the network connections are to 
be made and how the circuit operates. Geometric data in this table include coor- 
dinates and parameters necessary for drawing and positioning the gates. These 
symbols could all be drawn as  polygon shapes, or they could be formed as com- 
binations of straight-line segments and elliptical arcs. Labels for each of the com- 
ponent parts also have been included in the table, aithough the labels could be 
omitted if the symbols are displayed as commonly recognized shapes. Proce- 
dures would then be used to display the gates and construct the connecting lines, 
based on the coordinate positions of the gates and a specified order for connect- 
ing them. An additional procedure is used to produce the circuit output (binary 
values) for any given input. This procedure could be set up  to display only the 
final output, or it could be designed to display intermediate output values to il- 
lustrate the internal functioning of the circuit. 

Alternatively, we might specify graphical informat~on for the circuit model 
in data structures. The connecting lines, as well as the sates, could then be de- 
fined in a data table that explicitly lists endpoints for each of the lines in the cir- 
cuit. A single procedure might then display the circuit and calculate the output. 
At the other extreme, we could completely define the model in procedures, using 
no external data structures. 

Symbol Hierarchies 

Many models can be organized as a hierarchy of symbols. The basic "building 
blocks" for the model are defined as simple geometric shapes appropriate to the 
type of model under consideration. These basic symbols can be used to form 
composite objects, called modules, which themselves can be grouped to form 
higher-level modules, and so on, for the various components of the model. In the 

TABLE 7-1 
A DATA TABLE DEFINING THE STRUCTURE AND 
POSITION ( ) F  EACH GATE IN THE CIRCUIT Of FIG. 7 - 6  

Symbol Geornetr~c Identrtyrr~g 
Code Oescrrptron 1 dbel 

Gate 1 ~(Ioord~nates and other paramete-sl dnd 
Gate 2 0 1  
Gate 3 not 
Gate 4 m d  
-- 



simplest case, we can describe a model by a one-level hierarchy of component -ion 7-3 
parts, as in Fig. 7-7. For this circuit example, we assume that the gates are p s i -  Ba5ic Modeling Concepts 
tioned and connected to each other with straight lines according to connection 
rules that are speclfied with each gate description. The basic symbols in this hier- 
archical description are the logic gates. Although the gates themselves could be 
described as hierarchis-formed from straight lines, elliptical arcs, and text- 
that sort of description would not be a convenient one for constructing logic cir- 
cuits, in which the simplest building blocks are gates. For an application in which 
we were interested in designing different geometric shapes, the basic symbols 
could be defined as straight-line segments and arcs. 

An example of a two-level symbol hierarchy appears in Fig. 7-8. Here a fa- 
cility layout is planned as an arrangement of work areas. Each work area is out- 
fitted with a collection of furniture. The basic symbols are the furniture items: 
worktable, chair, shelves, file cabinet, and so forth. Higher-order objects are the 
work areas, which are put together with different furniture organizations. An in- 
stance of a basic symbol is defined by specifymg its size, position, and orientation 
within each work area. For a facility-layout package with fixed sizes for objects, 
only position and orientation need be speclfied by a user. Positions are given as 
coordinate locations in the work areas, and orientations are specified as rotations 
that determine which way the symbols are facing. At the second level up the hi- 
erarchy, each work area is defined by speclfylng its size, position, and orientation 
within the facility layout. The boundary for each work area might be fitted with a 
divider that enclo- the work area and provides aisles within the facility. 

More complex symbol hierarchies are formed by repeated grouping of syrn- 
bol clusters at each higher level. The facility layout of Fig. 7-8 could be extended 
to include symbol clusters that form different rooms, different floors of a build- 
ing, different buildings within a complex, and different complexes at widely s e p  
arated physical locations. 

Modeling Packages 

Some general-purpose graphics systems, GKS, for example, are not designed to 
accommodate extensive modeling applications. Routines necessary to handle 
modeling procedures and data struc&es are often set up as separate modeling 
packages, and graphics packages then can be adapted to interface with the mod- 
eling package. The purpose of graphics routines is to provide methods for gener- 

- 
I iguw ;-; 
A one-level hierarchical description of a circuit formed with logic gates. 



F i p r c  7-8 
A two-level hierarchical description of a facility layout. 

ating and manipulating final output displays. Modeling routines, by contrast, 
provide a means for defining and rearranging model representations in terms of 
symbol hierarchies, wluch are then processed by the graphics routines for dis- 
play. Systems, such as PHIGS and Graphics Library (GL) on Silicon Graphics 
equipment, are designed so that modeling and graphics functions are integrated 
into one package. 

Symbols available in an application modeling package are defined and 
struchmd according to the type of application the package has been designed to 
handle. Modeling packages can be designed for either twedimensional or three- 
dimensional displays. Figure 7-9 illustrates a two-dimensional layout used in cir- 
cuit design. An example of threedimensional molecular modeling is shown in 
Fig. 7-10, and a three-dimensional facility layout is given in Fig. 7-11. Such three- 
dimensional displays give a designer a better appreciation of the appearance of a 
layout. In the following sections, we explore the characteristic features of model- 
ing packages and the methods for interfacing or integrating modeling functions 
with graphics routines. 

F i p r e  7-9 
Two-dimensional modeling layout used in circuit 
design. (Courtesy of Surnmographics) 



Wion 7-4 
Hierarchical Modeling with 

Figure 7-10 
One-half of a stereoscopic image 
pair showing a threedimensional 
molecular model of DNA. Data 
supplied by Tamar Schlick, NYU, 
and Wima K. Olson, Rutgers 
University; visualization by Jeny 
Greenberg, SDSC. (Courtesy of 
Stephanie Sides, San Dicp Supmompurer 
Center.) 

- 

F@rt 7-11 
A three-dimensional view of an office layout. Courtesy of 
Intergraph Corporation. 

7-4 
HIERARCHICAL MODELING WITH STRUCTURES 

A hierarchical model of a system can be created with structures by nesting the 
structures into one another to form a tree organization. As each structure is 
placed into the hierarchy, it is assigned an  appropriate transformation so that it 
will fit properly into the overall model. One can think of setting up an office facil- 
ity in which furniture is placed into the various offices and work areas, which in 
turn are placed into depaments, and so forth on up  the hierarchy. 

Local Coordinates and Modeling Transformations 

In many design applications, models are constructed with instances (transformed 
copies) of the geometric shapes that are defined in a basic symbol set. instances 
are created by positioning the basic symbols within the world-coordinate mfer- 
ence of the model. The various graphical symbols to be used in an application are 
each defined in an independent coordinate reference called the modeling-coordi- 
nate system. Modeling coordinates are also referred to as local coordinates, or 
sometimes master coordinates. Figure 7-12 illustrates local coordinate definitions 



Chapdcr 7 for two symbols that could be used in a two-dimensional facility-layout applica- 
Structures and Hieramhiul tion. 

MOde11n8 . To construct the component parts of a graphical model, we apply transfor- 
mations to the localcoordinate definitions of svmbols to produce instances of the 
symbols in world coordinates. ~ransfonnatio& applied io the modeling-rdi- 
nate definitions of symbols are referred to as modeling transformations. Tpi- 
cally, modeling transformations involve translation, rotation, and scaling to G i -  
tiona symbol in world coordinates, but other transformations might also be used 
in some applications. 

Modeling Transformations 

We obtain a particular modeling-transformation matrix using the geometric- 
transformation functions discussed in Chapter 5. That is, we can set up the indi- 
vidual transformation matrices to accomplish the modeling transformation, or 
we can input the transformation parameters and allow the system to build the 
matrices. In either case, the modeling package toncatenates the individual trans- 
formations to construct a homogeneous-coordinate modeling transformation ma- 
hix, MT. An instance of a symbol in world coordinates is then produced by ap- 
pIying MT to modelingcoordinate positions (P,) to generate corresponding 
world-coordinate positions (P,): 

Structure Hierarchies 

As we have seen, modeling applications typically require the composition of 
basic symbols into groups, called modules; these modules may be combined into 

Arrays for 
WorHable Coordinales 

I x Worktable I vworktsble 
Arrays for Chair 

Coordinates 

1 - 3 

Chair 
-10 - 5  0 5 10 

fal 
WorkMa 

F i p m  7-12 
Ob- defined in local coordinates 



higher-level modules; and so on. Such symbol hierarchies can be created by em- *ction 7-4 
bedding structures within structures at each successive level in the tree. We can H~erarchical Modeling with 
First define a module (structure) as a list of symbol instances and their transfor- S"uc'ures 

mation parameters. At the next level, we define each higher-level module as a list 
of the lower-module instances and their transformation parameters. This process 
1s continued u p  to the root of the tree, which represents the total picture in world 
coordinates. 

A structure is placed within another structure with the function 

executestruccure ( i d )  

To properly orient the structure, we first assign the appropriate local transforma- 
tion to structure id. This is done with 

setLocalTransformation ( m l ~ ,  type) 

where parameter m l t  specifies the transformation matrix. Parameter type is as- 
signed one of the following three values: pre, post, or replace, to indicate the type 
of matrix composition to be performed with the current modeling-transformation 
matrix. If we simply want to replace thecurrent transformation matrix with l m t ,  
we set parameter type to the value replace. If we want the current matrix to be 
premultipled with the local matrix we are specifying in this function, we choose 
pre; and similarly for the value post. The following code section illustrates a se- 
quence of modeling statements to set the first instance of an object into the hier- 
archy below the root node. 

The same procedure is used to instance other objects within structure id0 
to set the other nodes into this level of the hierarchy. Then we can create the next 
level down the tree by instancing objects within structure id1 and the other 
structures that are in idO. We repeat this process until the tree is complete. The 
entire tree is then displayed by posting the root node: structure id0 in the previ- 
ous example. In the following procedure, we illustrate how a hierarchical struc- 
ture can be used to model an object. 

void main I )  
( 
enum { Frame, Wheel, Bicycle ) ;  
int nPcs: 
wcPt2 pts 12\61; 
pMacrix3 m: 
/ '  Roucines to generate geometry ' /  
extern void getwheelvertices (inc ' nPcs. >u:cPc2 ptsl : 
excern void grtFrameVertices line . nPcs, .,:cP~2 pts): 
t'' Make the wheel structure * /  



getwheelvertices tnPts, prsl; 
openstructure ( W e e l )  : 
setLineWidth ( 2  : I :  
polyline (nPts. ~ t s )  : 
closestructure; 
/ *  Make the frame structure * i  
getFrameVertices fnPts. pts); 
openstructure (Frame) ; 
setLineW1dt.h ( : ! . I ) )  : 
polyline (nPts I ts) ; 
closeStructure- 

I *  Make the bicy:le * i  
openstructure iB:cycle) ; 
/ *  Include the flame ', 
exe~'ut2Structure (?:amel: 
' *  Position and :xlude rear wheel " 
matrixSetIdentrtj (m); 
m [ 0 , 2 1  : =  .1.0; - I :  21 :: -0.5: 
setLocalTransfor~iat~onMatr1x (m, REPLACE); 
executestr~cture (Wheel): 
/ *  Position and i~clude front wheel 'I 
m [ 0 , 2 ]  : -  1.0; m [ 1 , 2 1  : =  -0.5; 
setLocalTransformstlonmatrlx (m. REP1,ACE): 
executestructu~e (Xheel); 
clos~Structure; 

> 

We delete a hierdrchy with the function 

deleteStrucru:e?le: work I i d )  

+here pal.arnt.ter i d  refvrences the root stn~cture of the tree. This deletes the root 
node of the hierarchy and all structures that have been placed below the root 
using the e x e c u t e ~ t : . ~ c t u r - e  function, assuming that the hierarchy is orga- 
nized as a tree. 

SUMMARY 

A structure (also called a segment or an object in some systems) is a labeled 
group of output statemcsnts and associated attributes. Fv des~gning pictures as 
sets of stmctures, we can easily add, delete, or manipulate picture components 
independently of each mother. As structures are created. they are entered into a 
central structure store. Structures are then displayed h!. posting them to various 
output devices with a w p e d  priorities. When two structures overlap, the struc- 
ture with the higher prioritv is displayed over the structure with the lower prior- 
ity. 

We can use workst,~twn filtcrs to set attributes, such as visibility and high- 
lighting, for structures. With the visibility iilter, we can turn off the display of a 
structure while retaining i t  in the structure list. The highlighting filter is used to 
emphasize a displayed structure with blinking, color, or high-intensity patterns. 

Various editing o p a t i o n s  can be applied to structures. We can reopen 
structures to carry nut alppend, insert, or delete operat iox Locations in a struc- 
ture are referenced w ~ t h  the element pointer. In addition. we ~ndividually label 
the primitives or attrihut~ts In a structure. 



The term model, in graphics applications, refers to  a graphical representa- 
t ion for some system. Components o f  the system are represented as symbols, de- Exercisrs 
fined i n  l t r a l  (modeling) coordinate reference frames M a n y  models, such as elec- 
trical circuits, are constructed by placing instances o f  the symbols at selected 
locations. 

Many  models are constructed as symbol hierarchies. A bicycle, for instance, 
can be constructed w i th  a bicycle frame and the wheels. The frame can include 
s ~ c h  parts as t h r  handlebars and the pedals. And the wheels contain spokes, 
rims, and tires. We can construct a hierarchial model by nesting structures. For 
example, we can set up a bike structure that contains a frame structure and a 
wheel structure. Both the frame and wheel structures can then contain pr imit ives 
and addit ional structures. We continue this nesting d o w n  to structures that con- 
tain only output  pr imit ives (and attributes). 

As each structure is  nested w i th in  another structure, an associated model- 
ing transformation can be set tor the nested structure. This transformation de- 
scribes the operations necessary to  properly orient and scale the structure to fit 
into the hierarchv. 

REFERENCES 

Structure operation5 and hierarchical modeling in PHIGS are discussed in Hopgood and 
Duce ( 1  491 I ,  Howard et al. (1991 ), Gaskiris ( 1  9921, and Blake (1 993). 

For ~nformat~on on GKS segment nperatinn5 ~ P P  Hopgood I I W 3 )  and Enderle et dl ( I  984) 

EXERCISES 

7 - 1 .  Write a procedure for creating and man~pulating the ~niormation In a central structure 
store. Th~s procedure is to be ~nvoked by funct~ons such as openst ruc ture ,  
d e l e t e S t r x c t u r e .  and changeStructure1dent.ifier. 

. ' - 2 .  Write A roittine for storing inforrnat~on in a traversal stale list. 
;-I. Write a routine tor erasing a specified structure on d raster system, given the coordi- 

nate extents tor all displayed structures in a scene 

7 - 4 .  Writr a procedure to implement the un2osts t rucLure  function on a raster system. 
y . 5 .  Write a procrdure to implement the d e l e t e s t r u c t a r e  function on a raster system. 

T-6 Write a procedure lo implernenl highlighting as a blmuing operation. 
' 7 Write a set of routines for editing structures. Your routlnes should provide tor the fol. 

low~ng types of editing: appending, insening, replacing, and deleting structure ele- 
ments. 

i - P .  r11,cuss model representat~ons that would be appropriate for several distinctly dlfter- 
ent k~nds ot systems. Also discuss how graphical representations might be imple- 
mented for eacb system. 

7-9 Tor a Iog~<-<ir<uit modeling application, such as that tn Fig. 7-6, glve a deta~led graph- 
KIJI descript~on of the standard logic symbols to be u,ed in constructing a display of a 
t i r t u~ l .  

7-10, Develop a modeling package for electrical des~gn that will allow a user to position 
c~lcctrical sy~nhols w~thin a c~rcuit network. Only tranilations need be applied to place 
an Instance of one of the electr~cal menu shapes into the network. Once a componenl 
ha5 been p l xed  in the network, it is to be connected to other specified component5 
with straight linr segments. 

7 - 1  1 Dvvw 'I two-thnlensional facility-layout package. A iienu of furniture shapes is to he 



Chapter 7 provided to a de,.gner, who can place the objects in any location w~thin a single room 
S~ructures and H~erarchical (one-level hierarchy I .  Instance rransformations can be l~~n i t ed  to translations and  rot^- 

Modellng tions. 

7-12 Dev~se a two-d~meris~onal fac~l~ty-layout package that presents a menu of furn~ture 
shapes A two-le~el h~erarchv 1s to be used so that turnlwre Items can be placed Into 
varlous work areas and the work areas can be arranged w ~ t h ~ n  a larger area Instance 
transforrnat~ons may be llrn~ied to translat~ons and rotatlcms, bul scal~ng could be used 
~f furn~ture Items of d~fferent s~zes are to be ava~lable. 





T he humanxomputer intertace for most systems involves extensive graph- ics, regardless ot the application. Tspically, generiil svstems now conslst of 
wmdows, pull-down .rnd pop-up menus, icons, and pointing devices, such as a ~. ~ 
mouse or spaceball, tor positioning the screen cursor. P.>pular graphical user in- 
terfaces include X Windows, Windows, Macintosh, OpenLook, and Motif. These 
interfaces are used in a variety of applications, including word processing, 
spreadsheets, databases and file-management systems, presentation systems, and 
page-layout systems. In graphics packages, specialized interactive dialogues are 
designed for individual applications, such as engineering design, architectural 
design, data visualization, drafting, business graphs, and artist's paintbrush pro- 
grams. For general graphics packages, interfaces are usually prov~ded through a 
standard system. An example is the X Window System ~ntcrface with PHIGS. In 
this chapter, we take a look at the basic elements of graphical user mteriaces and 
the techniques for interactive dialogues. We alsoconsider how dialogues in 
graphics packages, in particular, can allow us to construct and manipulate pic- 
ture components, select menu options, assign parameter values, and select and 
position text strings. h variety of input delrices exists, and general graphics 
packages can be designcbd to interface with various devices and to provide exten- 
sive d ia lope  capabilities. 

THE USER DlALOGlJE 

For a particular application, the rrser's rrroiid serves as the basis for the deslgn ot 
the dialogue. The user's model describes what the svstem is designed to acconi- 
plish and what graphics operations are available. I t  state; the type of objects that 
can be displayed and how the objects can be manipulated. For example, if the 
graphics system is to bc used as a tool for architectural design, the model de- 
scribes how the package can be used to construct and dispIay views of buildings 
by positioning walls, doors, windows, and other buildin!; components. Similarly, 
for a facilitv-layout system, objects could be defined as a set of furniture items 
(tables, cham, etc.), and the available operations would include those for posi- 
tioning and removing different pieces of tcrniture within the fticility layout. And 
a circuit-design progranl might use electrical or logli t ~ l ~ m e n t s  for objects, with 
positic~ningoperations .I ,.adable for adding or drlctirg c~lc~nients within the o\.?r- 
all circuit design 



All information in the user dialogue is then presented in the language of the M i o n  6-1 
application. In an architectural design package, this means that all interactions The User Dialogue 
are described only in architectural terms, without reference to particular data 
structures or other concepts that may be unfamiliar to an architect. In the follow- 
ing sections, we discuss some of the general considerations in structuring a user 
dialogue. 

Windows and Icons 

Figure 8-1 shows examples of common window and icon graphical interfaces. Vi- 
sual representations are used both for obpds to be manipulated in an application 
and for the actions to be performed on the application objects. 

A window system provides a window-manager interface for the user and 
functions for handling the display and manipulation of the windows. Common 
functions for the window system are opening and closing windows, reposition- 
ing windows, resizing windows, and display routines that provide interior and 
exterior clipping and other graphics functions. Typically, windows are displayed 
with sliders, buttons, and menu icons for selecting various window options. 
Some general systems, such as X Widows and NeWS, are capable of supporting 
multiple window managers so that different window styles can be accommo- 
dated, each with its own window manager. The window managers can then be 
designed for particular applications. In other cases, a window system is designed 
for one specific application and window style. 

Icons representing objects such as furniture items and circuit elements are 
often referred to as application icons. The icons representing actions, such as 1-0- 
tate, magnlfy, scale, clip, and paste, are called control icons, or command icons. 

Accommodating ~ u l h l e  Skill Levels 

Usually, interactive graphical interfaces provide several methods for selecting ac- 
tions. For example, options could be selected by pointing at an icon and clicking 
different mouse buttons, or by accessing pull-down or pop-up menus, or by typ- 
ing keyboard commands. This allows a package to accommodate users that have 
different skill levels. 

For a less experienced user, an interface with a few easily understood oper- 
ations and detailed prompting is more effective than one with a large, compre- 

,"/ \Of ICl 

l i p r e  8-1 
Examples of screen layouts using window systems and icons. (Courtesyof(rr) lntergmph 
Corporalron. ( b )  V~slrnl Numencs. lnc , and (c) Sun Micrsysrems.)  



An important design cr~nwieration in ,In ~ntertacr i:. i o r ~ \ ~ > t t ~ n i \ -  For cx~niplc ,  '1 
particular icon shape s h ~ u l d  al\ \ .ay~ have a single me<lr:ing, mther than serving 
t c ~  represent different actions or objects depending on thc context. Sonie other ex- 
amples of consistency alt% alwavs placing menus in the >.inw sclat~\ e,positions so  
that a user does not 1lat.e to hunt for a particular option. ;~I\V,IY< uring a partlcu- 
lar cornhination o l  keyboard keys for the 3amc action, ii~l,.i a l \ ~ ~ , 1 \ ~  color coding so 
that the sanw color Jocr nc)t h a w  diii'ercwt nic.,inings in i ~ f i c r e ~ l t  situ,lt~ons 

Generall\; n conlylic.ltccl, incoiisistcnt model is clifttiult ior user to under- 
s t m d  and to work with 11: an cxttccti\.e M.I?\. The objects , ~ n d  opcr,ltions pro\.ided 
s l ~ n ~ l c l  be designed to 1.lrn1 a m ~ n ~ r n u m  .~nd  consistent -c,t so that the .;ystcni is 
casv to learn, hut not o\ a?rsirnpl~t~ed to the point \vhcre ~t IS d ~ t ( ~ c u l t  to apply. 

Operations i l l  an interl.~ce shot~lcl illso br  structured .;o -tiat thev are easy to un- 
derstand and to renicmxr.  Ohx-urc, compl~cated, incor5istent. and abbreviated 
command forrn'its Ir,ld . o  conlu*~on and reduct~on In tho' c~tlcvt~\,ent~ss ot the u w  
ol  the package. One ke! or  button used lor all delete operat~ons,  for example, is 
easier to remember than a nulnher of ditlerent kevs for different types of delete 
operations. 

Icons and windo\\ systems also aid in minimizing memor~zation. Diiferent 
kinds of information can he separated into d~fferent w~ndows ,  so that we d o  not 
have to rely on meniorization when different information displays overlap. We 
can simply retain the multiple information on the scretbn in different windows, 
and switch bark and forth between windoh areas. Icons .Ire used to reduce mem- 
orizing by displaying easily recognizable shapes for various objects and actions. 
To select a particular action, we simply select the icon thal rrsemhles that action. 



execution is completed, with the system restored to the state it was in before the section 8-1 
operation was started. With the ability to back up ;it any point, we can confi- The User D~alogue 
dently explore the capabilities of the system, knowing that the effects of a mis- 
take can be erased. 

Backup can be provided in many forms. A standard undo key or command 
is used to cancel a single operation. Sometimes a system can be backed u p  
through several operations, allowing us to reset the system to some specified 
point. In a system with extensive backup capabilities, all inputs could be saved 
so that we can back up and "replay" any part of a session. 

Sometimes operations cannot be undone. Once we have deleted the trash in 
the desktop wastebasket, for instance, we cannot recover the deleted files. In this 
case, the interface would ask us to verify the delete operation before proceeding. 

Good diagnostics and error messages are designed to help determine the 
cause of an error. Additionally, interfaces attempt to minimize error possibilities 
by anticipating certain actions that could lead to an error. Examples of this are 
not allowing us to transform an object position or to delete an object when no ob- 
ject has been selected, not allowing us to select a line attribute if the selected ob- 
ject is not a line, and not allowing us to select the pabte operation if nothing is in 
the clipboard. 

Feedback 

Interfaces are designed to carry on a continual interactive dialogue so that we are 
informed of actions in progreis at each step. This is particularly imporcant when 
the response time is high. Without feedback, we might begin to wonder what the 
system is doing and whether the input should be given again. 

As each input is received, the system normally provides some type of re- 
sponse. An object is highlighted, an icon appears, or ;I message is displayed. This 
not only informs us that the input has been received, but it also tells us what the 
system is doing. I f  processing cannot be completed within a few seconds, several 
feedback messages might be displayed to keep us informed of the progress of the 
system. In some cases, this could be a flashing message indicating that the system 
is still working on the input request. It may also be possible for the system to dis- 
play partial results as they are completed, so that the final display is built up  a 
piece at a time. The system might also allow us to input other commands or data 
while one instruction is being processed. 

Feedback messages are normally given clearly enough so that they have lit- 
tle chance of being overlooked, but not so overpowering that our concentration is 
interrupted. With function keys, feedback can be given as an audible click or by 
lighting up  the key that has been pressed. Audio feedback has the advantage that 
it does not use up  screen space, and we do not need to take attention from the 
work area to receive the message. When messages are displayed on the screen, a 
fixed message area can be used so that we always know where to look for mes- 
sages. In some cases, it may be advantageous to place feedback messages in the 
work area near the cursor. Feedback can also be displayed in different colors to 
distinguish it from other displayed objects. 

To speed system response, feedback techniques can be chosen to take ad- 
vantage of the operating characteristics of the type of devices in use. A typical 
raster feedback technique is to invert pixel intensities, particularly when making 
menu selections. Other feedback methods include highlighting, blinking, and 
color changes. 



Chapter 8 Special symbols are designed for different t y p s  of feedback. For example, a 
Graphical User Interfaces and cross, a frowning face, or a thumbs-down symbol is often used to indicate an 

l n t e r a c t l v e  Inputuethods error; and a blinking "at work" sign is us& to indicate that processing is in 
progress. This type of feedback can be very effective with a more experienced 
user, but the beginner may need more detailed feedback that not only clearly in- 
dicates what the system is doing but also what the user should input next. 

With some types of input, echo feedback is desirable. Typed characters can 
be displayed on the scrwn as they are input so that we can detect and correct er- 
rors immediately. Buttm and dial input can be echoed in the same way. Scalar 
values that are selected with dials or from displayed scales are usually echoed on 
the screen to let us check input values for accuracy. Selection of coordinate points 
can be echoed with a cursor or othersymbol that appears at the selected position. 
For more precise echolng of selected positions, the coordinate values can be dis- 
played on the screen. 

8-2 
INPUT OF GRAPHICAL DATA 

Graphics programs use several kinds of input data. I'iclure specifications 
need values for coordinate positions, values for the character-string parameters, 
scalar values for the transformat~on parameters, values specifying menu options, 
and values for identific.hon of picture parts. Any of tht. input devices discussed 
in Chapter 2 can be used to input the various graphical data types, but some de- 
vices are better suited f a x  certain data types than others. To make graphics pack- 
ages independent of the. particular hardware devices used, input functions can he 
shuctured according to the data description to be handled by each Function. This 
approach provides a logical input-device classificatior~ in terms of the kind of 
data to be input by the device. 

The various kinds of input data are summarlzed in the following six logical de- 
vice classifications used hv PHlGS and GKS: 

LOCATOR-a dcl.vce for sperltyinga coordinate posltlon ( x ,  y) 
STROKE-+ dtv1c.e for specifying a series of coordinate positions 
STRING-.. a drviie for specifying text input 
VALUATOR-'1 de\-ice for specifying scalar value: 
CHOICE-a deu~Le for selecting nwnu options 
PICK-a device tc%r selecting picturc components 

In some packages, a single logical device is used iclr both locator and stroke 
operations. Some other mechanism, such as a switch, can then be used to indicate 
whether one coordinntcb position or a "stream" of positions 1s to be input. 

Each of the six logical input device class~ficationh (an be i~nplemented with 
anv of the hardware dcwrrs, but some hardware deiicr., are lnore convenrent for 
certain kinds of data than others. A device that can hc, polnted at a screen posi- 
tion is more convenient tor enterlng coordinate data th.~n <I kevboard, for exam- 
ple. In the following wct~ans, \\.e d~scuss ho\v tlic \ . A ~ O L I S  p l~vs~ral  dewces arc 
uscd to pro\.ide lnput 1% ithin each oi thc logli.)l cIassiiii,!tions. 



Locator Devices Section 8-2 
Input of Graphical Data 

A standard method for interactive selection of a coordinate point is by position- 
ing the screen cursor. We can d o  this with a mouse, joystick, trackball, spaceball, 
thurnbwheels, dials, a digitizer stylus or hand cursor, or some other cursor-posi- 
tioning device. When the screen cursor is at the desired location, a button is acti- 
vated to store the coordinates of that screen point. 

Keyboards can be used for locator input in several ways. A general-purpose 
keyboard usually has four cursor-conhol keys that move the screen cursor up, 
down, left, and right. With an additional four keys, we can move the cursor diag- 
onally as well. Rapid cursor movement is accomplished by holding down the se- 
lected cursor key. Alternatively, a joystick, joydisk, trackball, or thumbwheels can 
be mounted on the keyboard for relative cursor movement. As a last resort, we 
could actually type in coordinate values, but this is a slower process that also re- 
quires us to know exact coordinate values. 

Light pens have also been used to input coordinate positions, but some spe- 
cial implementation considerations are necessary. Since light pens operate by de- 
tecting light emitted from the screen phosphors, some nonzero intensity level 
must be present at the coordinate position to be selected. With a raster system, 
we can paint a color background onto the screen. .4s long as no black areas are 
present, a light pen can be used to select any screen position. When it is not pos- 
sible to eliminate all black areas in a display (such as on a vector system, for ex- 
ample), a light pen can be used as a locator by creating a small Light pattern for 
the pen to detect. The pattern is moved around the screen until it finds the light 
pen. 

Stroke Dev~ces 

This class of logical devices is used to input a sequence of coordinate positions. 
Stroke-device input is equivalent to multiple calls to a locator device. The set of 
input points is often used to display line sections. 

Many of the physical devices used for generating locator input can be used 
as stroke devices. Continuous movement of a mouse, trackball, joystick, or tablet 
hand cursor is translated into a series of input coordinate values. The graphics 
tablet is one of the more common stroke devices. Button activation can be used to 
place the tablet intu "continuous" mode. As the cursor is moved across the tablet 
surface, a stream of coordinate values is generated. This process is used in paint- 
brush systems that allow art~sts to draw scenes on the screen and in engineering 
systems where layouts can be traced and digitized for storage. 

String Osvices 

The primary physical device used for string input is the keyboard. lnput charac- 
ter strings are typically used for picture or graph labels. 

Other physical devices can be used for generating character patterns in a 
"text-writing" mode. For this input, individual characters are drawn on the 
screen with a stroke or locator-type device. A pattern-recognition program then 
rnterprels the characters using a stored dicti~nary of predefined patterns. 

Valuator Devic ?s 

This log~cal class of devices IS employed in graph~cs systems to input scalar val- 
ues. Valuators are used for setting various graphics paramcten, such as rotation 



C h a w  8 angle and sd le  factors, afd for setting physical parameters associated with a par- 
CraphicarUser Intdaces and ticular application (temprature settings, voltage levels, shess factors, etc.). 

lntwact~w Input ~ e m o d s  A typical physical device used to provide valuator input is a set of control 
dials. Floating-point n h b e r s  within any predefined range are input by rotating 
the dials. Dial rotations in one direction increase the numeric input value, and 
opposite rotations decrease the numeric value. Rotary potentiometers convert 
dial rotation into a corresponding voltage. This voltage is then translated into a 
real h ~ b e r  within a defined scalar range, such as -10.5 to 25.5. Instead of dials, 
slide Ftentiometers am sometimes used to convert linear movements into scalar 
values. 

Anp keyboard with a set of numeric keys can be used as a valuator dev~ce. 
A user sihply types the numbers directly in floating-point format, although this 
is a slower hethod than using dials or slide potentiometers 

Joystick, trackbalk, tablets, and other interadive devices can be adapted 
for valuator input by interpreting pressure or movement of the device relative to 
a scalar range. For one direction of movement, say, let3 to right, increasing scalar 
values can be input. Movement in the opposite direction decreases the scalar . - 
input value. 

Another t d n i q u e  for providing valuator input is to display sliders, but- 
tons, rotating scales, and menus on the video monitor. Figure 8-2 illustrates some 
possibilities for scale representations. Locator input from a mouse, joystick, 
spaceball, or other device is used to select a coordinate position on the display, 
and the screen roordinate position is then converted to a numeric input value. As 
a feedback mechanism for the w r ,  the selected position on a scale can be 
marked with some symbol. Numeric values may also be echoed somewhere on 
the screen to confirm the selections. 

- - - - - - - 

Figure 8-2 
Scales displayed on a video monitor for interactive selection of 
parameter values. In this display, sliders are provided for selerting 
scalar values for superellipse parameters, sl and 52, and for individual 
R, G, and B color values. In addition, a small circle can be positioned on 
the color wheel for seleaion of a combined RGB color, and buttons can 
be activated to make small changes in color values. 



C h o ~ c e  Devices Section 8-2 
Input of Graphical Data 

Graphics packages use menus to select programming options, parameter values, 
and object shapes to be used in constructing a picture (Fig. 8-11. A choice device 
IS defined as  one that enters a selection from a list (menu) of alternatives. Com- 
monly used choice devices are a set of buttons; a cursor positioning device, such 
as a mouse, trackball, or keyboard cursor keys; and a touch panel. 

A function keyboard, or "button box", designed as a stand-alone unit, is 
often used to enter menu selections. Usually, each button is programmable, so 
that its function can be altered to suit different applications. Single-purpose but- 
tons have fixed, predefined functions. Programmable function keys and fixed- 
function buttons are often included with other standard keys on a keyboard. 

For screen selection of listed menu options, we can use cursor-contml de- 
vices. When a coordinate position ( x ,  y) is selected, it is compared to the coordi- 
nate extents of each listed menu item. A menu item with vertical and horizontal 
boundaries at the coordinate values xdn, x,, y,,,, and y,, is selected if the 
input coordinates (x, y) satisfy the inequalities 

For larger menus with a few options displayed at a timc, a touch panel is 
commonly used. As with a cursor-control device, such as a mouse, a selected 
screen position is compared to the area occupied by each menu choice. 

Alternate methods for choice input include keyboard and voice entry. A 
standard keyboard can be used to type in commands or menu options. For this 
method of choice input, some abbreviated format is useful. Menu listings can be 
numbered or  given short identifying names. Similar codings can be used with 
voice-input systems. Voice input is particularly useful when the number of op- 
tions is small (20 or less). 

Pick Devices 

Graphical object selection is the function of this logical class of devices. Pick de- 
vices are used to select parts of a scene that are to be transformed or edited in 
some way. 

Typical devices used for object selection are the same as those for menu se- 
lection: the cursor-positioning devices. With a mouse or joystick, we can position 
the cursor over the primitives in a displayed structure and press the selection 
button. The position of the cursor is then recorded, and several levels of search 
may be necessary to locate the particular o b p t  (if any) that is to be selected. 
First, the cursor position is compared to the coordinate extents of the various 
structures in the scene. If the bounding rectangle of a structure contains the cur- 
sor coordinates, the picked structure has been identified. But if two or more 
structure areas contain the cursor coordinates, further checks are necessary. The 
coordinate extents of individual lines in each structure can be checked next. If the 
cursor coordinates are determined to be inside the coordinate extents of only one 
line, for example, we have identified the picked object. Otherwise, we need addi- 
tional checks to determine the closest line to the cursor position. 

One way to find the closest line to the cursor position is to calculate the dis- 
tance squared from the cursor coordinates (x, y) to each line segment whose 
bounding rectangle contains the cursor position (Fig. 8-31. For a line with end- 
points ( x , ,  y,) and (x,, y,), distance squared from ( x ,  y) to the line is calculated as 



Figure 8-3 
Distances to line segments from the 
pick position. 

where Ax= x,-r,, and Ay=yz - y, Various approximations can be used to speed 
u p  this distance calculation, or other identification schemes can be used. 

Another method for finding the closest line to the cursor position is to spec- 
ify the size of a pick window. The cursor coordinates are centered on  this win- 
dbw and the candidate lines are clipped to the window, as shown in Fig. 8-4. By 
making the pick window small enough, we can ensure that a single line will 
cross the window. The method for selecting the size of a pick window is de- 
scribed in Section 8-4, where we consider the parameters associated with various 
input functions. 

A method for avoiding the calculation oi pick distances or window clipping 
intersections is to highlight the candidate structures and allow the user to resolve 
the pick ambiguity. One way to do this is to highlight [he structures that overlap 
the cursor position one bv one. The user then signals when the desired structure 
is highlighted. 

An alternative to cursor positioning is to use button input to highlight suc- 
cessive structures. A second button is used to stop the process when the desired 
structure is highlighted. I t  very many structures are to he searched in this way, 
the process can be speeded up  and an additional button is used to help identify 
the structure. The first button can initiate a rapid successive highlighting of struc- 
tures. A second button call again be used to stop the process, and a third button 
can be used to back up  more slowly if the desired structure passed before the o p  
erator pressed the stop button. 

Finally, we could use a keyboard to type in structure names. This is a 
straightforward, but less interactive, pick-selection method. Descriptive names 
can be used to help the user in the pick process, but the method has several 
drawbacks. It is generally slower than interactive picking on the screen, and a 
user will probably need prompts to remember the various structure names. In 
addition, picking structure subparts from the keyboard can be more difficult than 
picking the subparts on thescreen. 

I 
I - - - -- - - . - - . - . - - .- 1 1  I r ; , q t r r c ~ - ~  : A p~ck w~ndow, centered on p d  

I 
cmrd~nates (1,. y,). used t3 resolve 

w --I I pick object overlap 



8-3 sedan 8-3 
INPUT FUNCTIONS lnput Functions 

Graphical input functions , , I .  be set up  to allow users to specify the following 
options: 

Which physlcal devices are to provide input within a particular logical clas- 
sification (for example, a tablet used as a stroke device). 
How the graphics program and devices are to interact (input mode). Either 
the program or the devices can initiate dat.. entry, or both can operate si- 
multaneously. 
When the data are to be input and which device is to be used at that time to 
deliver a particular input type to the specified data variables. 

lnput Modes 

Functions to provide input can be structured to operate in various input modes, 
which specify how the program and input devices interact. Input could be initi- 
ated by the program, or the program and input devices both could be operating 
simultaneously, or data input could be initiated by the devices. These three input 
modes are referred to as request mode, sample mode, and event mode. 

In request mode, the application program initiates data entry. lnput values 
are requested and processing is suspended until the required values are received. 
This input mode corresponds to typical input operation in a general program- 
ming language. The program and the input devices operate alternately. Devices 
are put into a w?t state until an input request is made; then the program waits 
until the data are delivered. 

In sample mode, the application program and input devices operate inde- 
pendently. Input devices may be operating at the same time that the progtam is 
processing other data. New input values from the input devices are stored, re- 
placing previously input data values. When the program requires new data, it 
samples the current values from the input devices. 

In event mode, the input devices initiate datd input to the application pro- 
gram. The program and the input devices again operate concurrently, but now 
the input devices deliver data to an input queue, All input data are saved. When 
the program requires new data, it goes to the data queue. 

Any number of devices can be operating at the same time in sample and 
event modes. Some can be operating in sample mode, while others are operating 
in event mode. But only one device at a time can be providing input in request 
mode. 

An input mode within a logical class for a particular physical device operat- 
ing on a specified workstation is declared with one of six input-class functions of 
the form 

set . . .  Moce (us, devlceCode, inputMode. echoclag) 

where devicecode is a pos~tive integer; inputMode is assigned one of the val- 
ues: request, .;ample, or everrt; and parameter echoFlag is assigned either the 
value echo or the value noecho. How input data will be echoed on the display de- 
vice IS determined by parameters set in other input functions to be described 
later in this section. 



G t d p h d  U w  ln~erfdces and 
Interact~ve Input Methods 

TABLE 8-1 
ASSIGNMENT O F  INPUT-DtVICF 
CODES 

D e w e  Code Physical Devlce Type 

1 Keyboard 
2 Graph~cs Tablet 
3 Mouse 
4 lovsllch 
5 Trackball 
6 Button 

Device code assignment is installation-dependent. One possible assignment 
of device codes is shown in Table 8-1. Using the ass~gnments in this tahle, we 
could make the following declarations: 

s e t ~ o c a t o r M o d e  ( 1 ,  2 ,  sample, noecho)  
setTextMode ( 2 ,  1,  r e q u e s t .  echo)  
s e t ~ i c k M o d e  ( 4 ,  3 ,  e v e n t ,  e c h o )  

Thus, the graphics tablet is declared to be a locator device in sample mode on 
workstation 1 with no input data feedback echo; the keyboard IS a text device in 
request mode on workstation 2 with input echo; and the mouse is declared to be 
a pick device in event mode on workstation 1 with input echo. 

Request Mode 

Input commands used in this mode correspond to standard input functions in a 
high-level programming language. When we ask for an input in request mode, 
other processing is suspended until the input is received. After a device has been 
assigned to request mode. as discussed in the preceding section, input requests 
can be made to that device using one of the six logical-class functions represented 
by the following: 

r e q u e s t  . . .  ( w s ,  d e v i c e c o d e ,  s t a c u s .  . . .  1 

Values input with this function are the workstation code and the device code. Re- 
turned values are assigned to parameter status and to the data parameters cor- 
responding to the requested logical class. 

A value of ok or nonc is returned in parameter status, according to the va- 
lidity of the input data. A value of none indicates that the input device was acti- 
vated so as to produce invalid data. For locator input, this could mean that the 
coordinates were out of range. For pick input, the device could have been acti- 
vated while not pointing at a structure. Or a "break" button on the input device 
could have been pressed. A returned value of none can be used as an end-of-data 
signal to terminate a programming sequence. 

Locator and Stroke Input in Request Mode 

The request functions for these two logical input classes art. 

r e q u e s t L o c a t o r  (wi, devcode,  s t a t u s ,  viewIr;dex, p t )  
r e q u a s t s t r o k e  (ws, devCcde, nNax, s t a t u s ,  vlewTndex, n ,  p:s) 



For locator input, p t  is the world-coordinate position selected. For stroke input, w i o n 8 - 3  
pts is a list of n coordinate positions, where parameter *-ax gives the maxi- W u t  Functions 
mum number of points that can go in the input list. Parameter viewIndex is as- 
signed the two-dimensional view index number. 

Determination of a world-coordinate position is a two-step process: (1) The 
physical device selects a point in device coordinates (usually from the video-dis- 
play screen) and the inverse of the workstation transformation is performed to 
obtain the corresponding point in normalized device coordinctes. (2) Then, the 
inverse of the window-to-viewport mapping is carried out to get to viewing co- 
ordinates, then to world coordinates. 

Since two or more views may overlap on a device, the correct viewing 
transformation is identified according to the view-transformation input priority 
number. By default, this is the same as the view index number, and the lower the 
number, the higher the priority. View index 0 has the hghest priority. We can 
change the view priority relative to another (reference) viewing transformation 
with 

where viewIndex identifies the viewing transformation whose priority is to be 
changed, refViewIndex identifies the reference viewing transformation, and 
parameter p r i o r i t y  is assigned either the value lower or the value higher. For 
example, we can alter the priority of the first fnur viewing transformations on 
workstation 1, as shown in Fig. 8-5, with the sequence of functions: 

setVie~ransformationInputPriority :; 3, 1, higher) 
setVie\*rransformationInputPriority !l, 0, 2, lower) 

String Input in Request Mode 

Here, the request input function is 

requeststring (ws, devcode, status, nChars, s t r )  

Parameter s t r  in this function is assigned an input string. The number of charac- 
ters in the string is given in parameter nChars. 

Original 
Psiority Ordering 

Fmal 
Priority Ordering 

Figlrrr 8-5 
Rearranging viewing priorities 



Chapter 8 Valuator lnput in Request Mode 
Graphical User Interfaces and 

lrlreractive Input Methods A numerical value is input in request mode with 

requestvaluator (ws, devcode, status, value) 

Parameter value cal be assigned any real-number value. 

Choice lnput in Request Mode 

We make a menu selection with the following request function: 

requestchoice (ws, devCode, status, itemNum) 

Parameter itemNum is assigned a positive integer value corresponding to the 
menu item selected. 

Pick lnput in Request Mode 

For this mode, we obtain a structure identifier number with the function 

requestpick (ws, devCode, maxPathDepth, stacus. pathDepth, 
pickpath) 

Parameter pickpath is a list of information identifying the primitive selected. 
This list contains the structure name, pick identifier for the primitive, and the ele- 
ment sequence number. Parameter pickDepth is the number of levels returned 
in pickpath, and maxPathDepth is the specified maxlmum path depth that 
can be included in pickpath. 

Subparts of a structure can be labeled for pick input with the following 
function: 

An example of sublabeling during structure creation is given in the following 
programming sequence: 

openstructure (id); 
for (k = 0; k < n; k++) ( 

set~ickIdentifier (k); 

Picking of structures and subparts of structures is also contmlled by some work- 
station filters (Section 7-1) Objects cannot be picked if the); are invisible. Also, we 
can set the ability to pick objects mdependently of their visrbility. This is accom- 
plished with the pick filter: 

setplckFilter (ws, devcode, pickables, nonplckablesl 



where the set pi c k a b l e s  contains the names of objects (structures and primi- Sec'ione3 
tives) that we may want to select with the spec~fied p c k  devlce. Similarly, the set Input Functions 
n o n p i c k a b l e s  contains the names of objects that we do not want to be avail- 
able for picking with this input device. 

Sample Mode 

Once sample mode has been set for one or more physical devices, data input be- 
;ins without waiting for program direction. If a joystick has been designated as  a 
ocator device in sample mode, coordinate values for the current position of the 
activated joystick are immediately stored. As the activated stick position changes, 
the stored values are continually replaced with thtr coordinates of the current 
stick position. 

Samphng of the current values from a physical device in this mode begins 
when a sample command is encountered in the application program. A locator 
device is sampled with one of the six logical-class functions represented by the 
following: 

sample . . .  (ws, devicecode, . . .  

Some device classes have a status parameter in sample mode, and some d o  not. 
Other input parameters are the same as in request mode. 

As an example of sample input, suppose we want to translate and rotate a 
selected object. A final translation position for the object can be obtained with a 
locator, and the rotation angle can be supplied by a valuator device, as demon- 
strated in the following statements. 

samplelocacor (wsl, devl, viewIndex, p:) 
sampleval t~ator  (ws2. dev2, angle) 

Event Mode 

When an input device is placed in event mode, the program and device operate 
simultaneously. Data input from the device is accumulated in an event queue, or 
input queue. All input devices active in event mode can enter data (referred to as 
"events") into this single-event queue, with each device entering data values as 
they are generated. At any one time, the event queue can contain a mixture of 
data types, in the order they were input. Data entered into the queue are identi- 
fied according to logical class, workstation number, and physical-device code. 

An application program can be directed to check the event queue for any 
input with the function 

awaitEvent I t ime,  ws, devic.eClass, devicecode) 

Parameter t i m e  is used to set a maximum waiting time for the application pro- 
gram. I t  the queue happens to be empty, processing is suspended until either the 
number of seconds specified in time has elapsed or an input arrives. Should the 
waiting time nln out before data values are input, the parameter a e v i c e c l a s s  
is assign?d the value tlone. When t i m e  is given the value 0, the program checks 
the queue and immediately returns to other processing if thequeue is empty. 



Chapter 8 If processing is directed to the event queue with the awai tEvent  function 
Graphical User Interfaces and and the queue is not empty, the first event in the queue is transferred to a current 

Interactive Input Methods event record. The particular logical device class, such as locator or stroke, that 
made this input is stored in parameter d e v i c e c l a s s .  Codes, identifying the 
particular workstation and physical device that made the input, are stored in pa- 
rameters ws and devicecode,  respectively. 

To retrieve a data input from the current event record, an event-mode input 
function is used. The functions in event mode are similar to those in request and 
sample modes. However, no workstation and device-code parameters are neces- 
sary in the commands, since the values for these parameters are stored in the 
data record. A user retrieves data with 

get . . .  ( . . .  
For example, to ask for locator mput, we invoke the function 

In the following program section, we give an example of the use of the 
awaitEvent  and get functions. A set of points from a tablet (device code 2) on 
workstation 1 is input to plot a series of straight-line segments connecting the 
input coordinates: 

setStrokeMode (1, 2 ,  event, noecho); 

do ( 
awaitEvent (0, ws, deviceclass, devicecode) 

) while IdeviceClass ! =  stroke); 
getstroke (Max, viewIndex, n, pts); 
polyline (n, pts); 

The r e p e a t - u n t i l  loop bypasses any data from other devices that might be in 
the queue. If the tablet is the only active input device in event mode, this loop is 
not necessary. 

A number of devices can be used at the same time in event mode for rapid 
interactive processing of displays. The following statements plot input lines from 
a tablet with attributes specified by a button box: 

setPoly1;neIndex (1) ; 
/ *  set tablet to stroke device, event mode * /  
setStrokeMode (1, 2, event. noecho) ; 

I / *  set buttons to choice device. event mode * /  1 setChoiceMode (1, 6 .  event, noechol ; 
do ( 
awaitEvent (60, vs, deviceclass, devicecode): 
if (deviceclass == choice) { 
getchoice (status, option) ; 
setPolylineIndex (option); 

) 
else 

if (deviceclass == stroke) ( 
getstroke (Wax, viewIndex. n, pts); 
polyline (n, pts); 

) 
) while ldeviceclass ! =  none); 



Some additional housekeeping functions can be used in event mode. Func- Sectim8-4 
tions for clearing the event queue are useful when a process is terminated and a lnltlal Values for Input-Devlce 
new application is to begin. These functions can be set to clear the entire queue or 
to clear only data associated with specified input de\wes and workstations. 

Concurrent Use of Input Modes 

An example of the simultaneous use of mput devices in different modes is given 
in the following procedure. An object is dragged around the screen with a 
mouse. When a final position has been selected, a button is pressed to terminate 
any further movement of the ~bject.  The mouse positions are obtained in sample 
mode, and the button input is sent to theevent queue 

/ '  drags object in response to mouse Cnput ' /  
/ *  terminate processing by button press " 
setLocatorMode (1, 3, sample, echo) ; 
setChoiceMode (1, 6, event, noecho); 
do ( 

sanplelocator (1, 3, viewIndex, pt) ; 

/ '  translate object centroid to position pt and draw ' /  

awaitEvent (0, ws, class, code); 
) while (class ! =  choice); 

8-4 

INITIAL VALUES FOR INPUT-DEVICE PARAhlETERS 

Quite a number of parameters can be set for input devices using the i n i t i a l -  
i ze function for each logical class: 

initialize . . .  (ws, devicecode, . . .  , p e ,  coordExt, dataRec) 

Parameter pe is the prompt and echo type, parameter coordExt  is assigned a 
set of four coordinate values, and parameter dataRec is a record of various con- 
trol parameters. 

For locator input, some values that can be assigned to the prompt and echo 
parameter are 

pe = 1: installation defined 
pe = 2: crosshair cursor centered at current position 
pe = 3: line from initial pusition to current position 
pe = 4: rectangle defined by current and initial points 

Several other options are also available. 
For structure picking, we have the following options: 

pe = 1: highlight picked primitives 
pe = 2: highlight all primitives with value of pick id 
pe = 3: highlight entire structure 

as well as several others. 



Chapter 8 When an echo of the input data is requested, it is displayed within the 
Graphical User Interfaces and bounding rectangle specified by the four coordinates in parameter coordExt. 

lnteractive InpMMethods Additional options can also be set in parameter dataRec. For example, we can 
set any of the following: 

size of the pick window 
minimum pick distance 
type and size of cursor display 
type of structure highlighting during pick o p a t i o n s  
range (min and rnax) for valuator input 
resolution (scale) for valuator input 

plus a number of other options. 

8-5 
INTERACTIVE PICTURE-CONSTRUCTION TECHNIQUES 

There are several techniques that are incorporated into graphics packages to aid 
the interactive construction of pictures. Various input options can be provided, so 
that coordinate information entered with locator and stroke devices can be ad- 
justed or interpreted according to a selected option. For example, we  can restrict 
all lines to be either horizontal or vertical. Input coordinates can establish the po- 
sition or boundaries for o b ~  to be drawn, or they can be used to rearrange pre- 
viously displayed objects. 

Basic Positioning Methods 

Coordinate values supplied by locator input are often used with positioning 
methods to speclfy a location for displaying an object or a character string. We in- 
teractively select coordinate positions with a pointing device, usually by p s i -  
tioning the screen cursor. Just how the object or text-string positioning is pe- 
formed depends on the selected options. With a text string, for example, the 
screen p i n t  could be taken as the center string position, or the start or end p s i -  
tion of the string, or any of the other string-positioning options discussed in 
Chapter 4. For lines, straight line segments can be displayed between two se- 
lected screen positions. 

As an aid in positioning objects, numeric values for selected positions can 
be echoed on the screen. Using the echoed coordinate values as a guide, we can 
make adjustments in the selected location to obtain accurate positioning. 

Constraints 

With some applications, certain types of prescribed orientations or object align- 
ments are useful. A constraint is a rule for altering input-coordinate values to 
produce a specified orientation or alignment of the displayed cocmdinates. There 
are many kinds of constraint functions that can be specified, but the most com- 
mon constraint is a horizontal or vertical alignment of straight lines. This type of 
constraint, shown in Figs. 8-6 and 8-7, is useful in forming network layouts. With 
this constraint, we can create horizontal and vertical lines without worrying 
about precise specification of endpoint coordinates. 



Select Fira 
Endpoint Position 

Select 
Second Endpoint 

Posit~on Along 
Approximate 

Horizontal Path 

Figure 8-6 
Horizontal line constraint. 

Stctiion 8-5 

lnreractive Picture-Conmaion 
Techniques 

Seb* First 
Endpoint Position 

Select 
Second Endpoint 

Position Along 
Approximate 
Vert~cel Path 

-- 
Fig~rr t  5-7 
Vertical line constraint. 

A horizontal or vertical constraint is implemented by determining whether 
any two input coordinate endpoints are more nearly horizontal or more nearly 
vertical. If the difference in the y values of the two endpoints is smaller than the 
difference in x values, a horizontal line is displayed. Otherwise, a vertical line is 
drawn. Other kinds of constraints can be applied to input coordinates to produce 
a variety of alignments. Lines could be constrained to have a particular slant, 
such as 45", and input coordinates could be constrained to lie along predefined 
paths, such as circular arcs. 

Grids 

Another kind of constraint is a grid of rectangular lines displayed in some part of 
the screen area. When a grid is used, any input coordinate position is rounded to 
the nearest intersecton of two grid lines. Figure 8-8 illustrates line drawing with a 
grid. Each of the two cursor positions is shiged to the nearest grid intersection 
point, and the line is drawn between these grid points. Grids facilitate object con- 
structions, because a new line can be joined easily to a previously drawn line by 
selecting any position near the endpoint grid intersection of one end of the dis- 
played line. . 

Select a Position 
Near a Second 

Grid Intersection 

-- - -. - 
I r , q l l l ~ c ~  S-S 
Line drawing using a grid. 



Chapter 8 Spacing between grid lines is often an option that can he set by the user. 
Graphical User Interfaces and Similarly, grids can be turned on and off, and it is sometimes possible to use par- 

Interactive Input Methods tial grids and grids of different sizes in different screen areas. 

1.. Gravity Field 
In the construction of figures, we sometimes need to connect lines at pos~t~ons  be- 
tween endpoints. Since exact positioning of the screen cursor at the connecting 
point can be difficult, graphics packages can be des~gned to convert any mput 
position near a line to a position on the line. 

This conversion of input position is accomplished by creating a gravity field 
Figure 8-9 area around the line. Any selected position within the gravity field of a line is 
Gravib' fieldaroundaline. moved ("gravitated") to the nearest position on the line. A gravity field area 
Any point in the around a line is illustrated with the shaded boundary shown in Fig. &9. Areas 
shaded area to a around the endpoints are enlarged to make it easler lor us to connect lines at 
position on the line. their endpoints. Selected positions in one of the circular areas of the gravity field 

are attracted to the endpoint in that area. The size oi gravity fields is chosen large 
enough to aid positioning, but small enough to reduce chances of overlap with 
other lines. If many lines are displayed, gravity areas can overlap, and it may be 
difficult to speclfy points correctly. Normally, the boundary for the gravity field is 
not displayed. 

Kubber-Band Method. 

Straight lines can be constructed and positioned using rrtbbcr-band methods, 
which stretch out a line from a starting position as the screen cursor is moved. 
Figure 8-10 demonstrates the rubber-band method. We first select a screen posi- 
tion for one endpoint of the line. Then, as the cursor moves around, the line is 
displayed from the start position to the current position of the cursor. When we 
finally select a second screen position, the other line endpoint IS set. 

Rubber-band methods are used to construct and position other objects bc- 
sides straight lines. Figure 8-11 demonstrates rubber-band construction of a rec- 
tangle, and Fig. 8-12 shows a rubber-band circle construction. 

Select 
First 
Line 

Endpoint 

As the Cursor 
Moves, A Line 
Stretches out 

from the Initial 
Point 

Line Follows 
Cursor Position 

unril the Second 
Endpoint Is 

Selected 

Figure 8-10 
Rubber-band method for drawing and posit~oning a straight line 
segment. 



Select Rectangle Select Final 
Position Stretches Out Position for 

for One Corner As Cursor Moves Opposite Corner 
of the Rectangle of the Rectangle 

Figure 8-11 
Rubber-band method for conslructing a rectangle. 

Dragging 

A technique that is often used in interactive picture construction is to move ob- 
jects into position by dragging them with the screen cursor. We first select an ob- 
ject, then move the cursor in the d i~c t ion  we want the object to move, and the se- 
lected object follows the cursor path. Dragging obpcts to various positions in a 
scene is useful in applications where we might want to explore different possibil- 
ities before selecting a final location. 

Painting and Drawing 

Options for sketching, drawing, and painting come in a variety of forms. Straight 
lines, polygons, and circles can be generated with methods discussed in the pre- 
vious sections. Curvedrawing options can be p v i d e d  using standard curve 
shapes, such as circular arcs and splines, or with freehand sketching procedures. 
Splines are interactively constmcted by specifying a set of discrete screen points 
that give the general shape of the curve. Then the system fits the set of points 
with a polynomial curve. In freehand drawing, curves are generated by follow- 
ing the path of a stylus on a graphics tablet or the path of the screen cursor on a 
video monitor. Once a curve is displayed, the designer can alter the curve shape 
by adjusting the positions of selected points along the curve path. 

Select Position 
for the Circle 

Center 

Circle Stretches 
Out as the 

Cursor Moves 

Select the 
Final Radius 
of the Circle 

Figure 8-12 
Constructing a circle using a rubber-band method. 



Chapter 8 

Craohical User Interfaces and - -  - 
Interactive Input Methods 

" 
A screen layout showing one type 
of interface to an artist's painting 

Line widths, line styles, and other attribute options are also commonly 
found in -painting and drawing packages. These options are implemented with 
the methods discussed in Chapter 4. Various brush styles, brush patterns, color 
combinations, objed shapes, and surface-texture pattern.; are also available on 
many systems, particularly those designed as artist's H orkstations. Some paint 
systems vary the line width and brush strokes according to the pressure of the 
artist's hand ,on the stylus. Fimre 8-13 shows a window and menu system used 
with a painting padage that k o w s  an artist to select variations of a specified ob- 
ject shape, different surface texhrres, and a variety of lighting conditions for a 
scene. 

8-6 
VIRTUAL-REALITY ENVIRONMENTS 

A typical virtual-reality environment is illustrated in Fig. 8-14. lnteractive input 
is accomplished in this environment with a data glove (Section 2-5), which is ca- 
pable of grasping and moving objects displayed in a virtual scene. The computer- 
generated scene is displayed through a head-mounted viewing system (Section 
2-1) as a stereoscopic projection. Tracking devices compute the position and ori- 
entation of the headset and data glove relative to the object positions in the scene. 
With this system, a user can move through the scene and rearrange object posi- 
tions with the data glove. 

Another method for generating virtual scenes is to display stereoscopic pro- 
jections on a raster monitor, with the two stereoscopic views displayed on alter- 
nate refresh cycles. The scene is then viewed through stereoscopic glasses. Inter- 
active object manipulations can again be accomplished with a data glove and a 
tracking device to monitor the glove position and orientation relative to the p s i -  
tion of objects in the scene. 



Summary 

- 

Figurn 8-14 
Using a head-tracking stereo 
display, called the BOOM (Fake 
Space Labs, Inc.), and a Dataglove 
(VPL, lnc.), a researcher 
interactively manipulates 
exploratory probes in the unsteady 
flow around a Harrier jet airplane. 
Software dwebped by Steve 
Bryson; data from Harrier. (Courfrjy 
of E m  Uselfon, NASA Ames Rexnrch 
Ccnler.) 

SUMMARY 

A dialogue for an applications package can be designed from the user's model, 
which describes the tifictions of the applications package. A11 elements of the di- 
alogue are presented in the language of the applications. Examples are electrical 
and arrhitectural design packages. 

Graphical interfaces are typically designed using windows and icons. A 
window system provides a window-manager interface with menus and icons 
that allows users to open, close, reposition, and resize windows. The window 
system then contains routines to carry out these operations, as well as the various 
graphics operations. General window systems are designed to support multiple 
window managers. Icons are graphical symbols that are designed for quick iden- 
tification of application processes or control processes. 

Considerations in user-dialogue design are ease of use, clarity, and flexibil- 
ity. Specifically, graphical interfaces are designed to maintain consistency in user 
interaction and to provide for different user skill levels. In addition, interfaces are 
designed to minimize user memorization, to provide sufficient feedback, and to 
provide adequate backup and errorhandling capabilities. 

Input to graphics programs can come fropl many different hardware de- 
vices, with more than one device providing the same general class of input data. 
Graphics input functions can be designed to be independent of the particular 
input hardware in use, by adopting a logical classification for input devices. That 
is, devices are classified according to the type of graphics input, rather than a 



~ ~ ~ a r ) ~ e r  8 hardware des~gnation, such as mouse or tablet. The six logical devices in com- 
Gr . lph~td  I : w  irl~rrfdte> and mon use are locator, stroke, string, valuator, choice, and pck.  Locator devices are 

I n t e r a i l i V e  Inpu' Me'hodS any devices used by a program to input a single coordinate position. Stroke de- 
vices input a stream of coordinates. String devices are used to input text. Valuator 
devices are any input devices used to enter a scalar value. Choice devices enter 
menu selections. And pick devices input a structure name. 

Input functions available in a graphics package can be defined In three 
input modes. Request mode places input under the control of the application 
program. Sample mode allows the input devices and program to operate concur- 
rently. Event mode allows input devices to initiate data entry and control pro- 
cessing of data. Once a mode has been chosen for a logical device class and the 
particular physical devicc to be used to enter this class of data, Input functions in 
the program are used to enter data values into the progrilm. An application pro- 
gram can make simultaneous use of several physical input devices operating in 
different modes. 

Interactive picture-construction methods are commcinly used in a variety of 
applications, including design and painting packages. These methods provide 
users with the capability to position objects, to constrain figures to predefined 
orientations or alignments, to sketch figures, and to drag objects around the 
screen. Grids, gravity fields, and rubber-band methods ,Ire used to did in posi- 
tioning and other picture.construction operations. 

REFERENCES 

Guidelines ior uwr ~nteriacc. design are presented in Appk ilW7). Hleher (1988;. Digital 
(IW91, and 0SF.MOTIF 1989). For inlormation on the X \\.rndow Svstem, see Young 
(1090) and Cutler (;illy. ~ r i d  Reillv (10921. Addit~onal discu5c1~1ns oi inreriace dwgn can 
be iound in Phill~ps (19i7) .  Goodmari dnd Spt.rice (19781, Lotlcliilg 19831, Swezey dnd 
Davis (19831, Carroll and ( arrithers (1984). Foley, Wallace. a17d Clwn 1984). and Good er 
id.  (19841, 

The evolution oi thr concept oi logical (or virtuali input de\,ic~.b i 5  d15cusbed In Wallace 
(1476) and in Roienthal er al. (1982). An earlv discussion oi ~nput-debice classifications is 
to be found in Newman (1068). 

Input operdtions in PHICS ' .an he found in Hopgood a n d  Chte (19911, Howard el al .  
(1491). Gaskins (1992), .111d Blake (1993). For intormat~on un GKS :nput functions, see 
Hopgood el 31. (19831 anti Enderle, Kansy, and Piaii i1984). 

- 
EXERCISES 

8-1 Select smir g~apti~c* ,tppl~cation with which you drc lainil~,ir ,ant1 set up a user model 
that will serve as thcal),~sis k ~ r  the design of a user inlericire tor grdphi~s applications in 
that ,>red. 

8-2.  L~st ~ O S S ~ D ~  help facillrie that can be probided i n  a user ~ntrrface and discuss which 
types of help would hr appropriate ior different level5 ct user\. 

8-3 Summar~ze the ~wssibl'r ways oi handling backup and error< 5tar \vhich approaches 
are more suitatde ior the beginner and whicli are better wrt(~1 to the experienced user. 

8-4. L~st the possible iorm,ir5 ior presenting menus to a user ,ird explain uder what cir- 
cumstances each mi$t be appropriate. 

8-5. Disc~~ss dltcwu~ivrs 'or fepdbac-k in term5 of the variou5 le\c,I5 ot users 
8-6. List the tunctlons that  iiust bc periormed b) a windo.\ m:!nager in handling scwen 

idyout9 wth niultiplv t>.,erldppng \vindows. 



t%7. Set up a deslgn for a window-manager package. 

8-8. Design d user ~nleriace for a painting program. 

8-9. Design a user interface for a two-level hierarchical model~n): package. 

8-1 0. For any area with which you are familiar, design a c umplete user interiace to a  graph^ 
ics package providing capabilities to any users in that area. 

0-1 I .  Develop a program that allows objects to be positicmed on the screen uslng a locator 
device. An object menu of geometric shapes is to be presented to a user who is to se- 
lect an object and a placement position. The program should allow any number of ob- 
jects to be positioned until a "terminate" signal is givt.ri. 

8-1 2. Extend the program of the previous exercise so that wlected objects can be scaled and 
rotated before positioning. The transformation chc& cts and transformation parameters 
are to be presented to the user as menu options. 

8-1 3 Writp a program that allows a user to interactlvelv sketch pictures using a stroke de- 
vice. 

8-14. Discuss the methods that could be employed in a panern-recognition procedure to 
match input characters against a stored library of shapes. 

8-15. Write a routine that displays a linear scale and a sllder on the screen and allows nu- 
meric values to be selected by positioning the slider along the scale line. The number 
value selected is to be echoed in a box displayed near the linear scale. 

8-16. Write a routine that displays a circular scale and d pointer or a slider that can be 
moved around the circle to select angles (in degrees). The angular value selected is to 
be echoed in a box displayed near the circular scale. 

8-1 7. Write a drawing program that allows users to create a picture as a set of line segments 
drawn between specified endpoints. The coordinates of the individual line segments 
are to be selected with a locator device. 

0-1 0. Write a drawing package that allows pictures to be created with straight line segments 
drawn between specified endpoints. Set up a gravity field around each line in a pic- 
ture, as an aid in connecting new lines to existing lines. 

8-19. Moddy the drawing package in the previous exercise that allows lines to be con- 
strained horizontally or vertically. 

8-20. Develop a draming package that can display an optlonal grid pattern so that selected 
screen positions are rounded to grid intersections. The package is to provide line- 
drawing capabilities, wjlh line endpoinb selected with a locator device. 

8-2 1. Write a routine that allows a designer to create a picture by sketching straight lines 
with a rubber-band method. 

8 - 2 2 .  Writp a drawing package that allows straight lines, rectangles, and circles to be con- 
structed with rubber-band methods. 

8-23. Write a program that allows a user to design a picture from a menu of bas~c shapes bv 
dragging each selected shape into position with a plck device. 

8-24. Design an implementation of the inpu: functions for request mode 

8-25. Design an implementation of the sample,mode input functions. 

8-26. Design an implementation of the input functions for event mode. 

8-27. Set up a general implementation of the input functions for request, sample, and event 
modes. 





w hen we model and display a three-dimensional scene, there are many more considerations we must take into account besides just including 
coordinate values for the third dimension. Object boundaries can be constructed 
with various combinations of plane and curved surfaces, and we soniet~mes need 
to specify information about object interiors. Graphics packages often provide 
routines for displaying internal components or cross-sectional views of solid ob- 
jects. Also, some geometric transformations are more involved in three-dimen- 
sional space than in two dimensions. For example, we can rotate an object about 
an axis with any spatial orientation in three-dimensional space. Two-dimensional 
rotations, on the other hand, are always around an axis that is perpendicular to 
the xy plane. View~ng transformations in three dimensions are much more corn- 
plicated because we have many more parameters to select when specifying how 
a three-dimensional scene is to be mapped to a display device. The scene descrip- 
tion must be processed through viewing-coordinate transformations and projec- 
tion routlnes that transform three-dinrensional viewing coordinates onto two-di- 
nlensional device coordinates. Visible parts of a scene, for a selected \,iew, n ~ s t  
he identified; and surface-rendering algorithms must he applied if a realist~c ren- 
dering oi the scene is required. 

J m 

THRFF-DIMENSIONAL DISPLAY METHODS 

To obtain A display of a three-dimensional scene Lhat has been modeled in world 
coordinates. we must first set up a coordinate reference for the "camera". This co- 
ordinate reference defines the position and orientation for the plane ot the carn- 
era film (Fig. %I) ,  which is the plane we !\!ant to u w  to display a view of the ob- 
jects in the scenc. Object descriptions are then translcrred to the camera reference 
coordinates and projected onto the sclectcd displav pldnr We can then displajf 



Chaptw9 the objects in wireframe (outline) form, as in Fig. 9-2, or we can apply lighting 
Three-Dimensional Concepts and surfamnendering techniques to shade the visible surfaces. 

Parallel Projection 

One method for generating a view of a solid object is to project points on the o b  
ject surface along parallel lines onto the display plane. By selecting different 
viewing positions, we can project visible points on the obpct onto the display 
plane to obtain different two-dimensional views of the object, as in Fig. 9-3. In a 
prallel projection, parallel lines in the world-coordinate scene projed into parallel 
lines on the two-dimensional display plane. This technique is used in engineer- 
ing and architectural drawings to represent an object with a set of views that 
maintain relative proportions of the object. The appearance of the solid object can 
then be reconstructured from the mapr views. 

Figure 9-2 
Wireframe display of three obpcts, 
with back lines removed, from a 
commercial database of object 
shapes. Each object in the database 
is defined as a grid of coordinate 
points, which can then be viewed in 
wireframe form or in a surface- 
rendered form. (Coudesy of Viewpoint 
Lhtahbs.) 

Figurc 9-3 
Three parallel-projection views of an object, showing relative 
proportions from different viewing positions. 



Perspective Projection Section 9-1 
Three-Dimens~onal Display 

Another method for generating a view of a three-dimensionaiscene is to project Methods 
points to the display plane along converging paths. This causes objects farther 
from the viewing position to be displayed smaller than objects of the same size 
that are nearer to the viewing position. In a perspective projection, parallel lines in 
a scene that are not parallel to the display plane are projected into converging 
lines. Scenes displayed using perspective projections appear more realistic, since 
this is the way that our eyes and a camera lens form images. In the perspective- 
projection view shown in Fig. 94, parallel lines appear to converge to a distant 
point in the background, and distant objects appear smaller than objects closer to 
the viewing position. 

Depth Cueing 

With few exceptions, depth information is important so that we can easily iden- 
tify, for a particular viewing direction, which is the front and which is the back of 
displayed objects. Figure 9-5 illustrates the ambiguity that can result when a 
wireframe object is displayed without depth information. There are several ways 
in which we can include depth information in the two-dimensional representa- 
tion of solid objects. 

A simple method for indicating depth with wireframe displays is to vary 
the intensity of objects according to their distance from the viewing position. Fig- 
ure 9-6 shows a wireframe object displayed with depth cueing. The lines closest to 

Fiprrr 9-4 
A perspective-projection view of an airport scene. (Courtesy of Evans 6 Sutherlund.) 

299 



Figure 9-5 
Thc wireframe 
representation of the pyramid 
in (a) contains no depth 
information to indicate 
whether the viewing 
direction is (b) downward 
from a position above the 
apex or (c) upward from a 
position below the base. 

the viewing position are displayed with the highest intensities, and lines farther 
away are displayed with decreasing intensities. Depth cueing is applied by 
choosing maximum and minimum intensity (or color) values and a range of dis- 
tances over which the intensities are to vary. 

Another application of depth cueing is modeling the effect of the atmos- 
phere on the perceived intensity of objects. More distant objects appear dimmer 
to us than nearer objects due to light scattering by dust particles, haze, and 
smoke. Some atmospheric effects can change the perceived color of an object, and 
we can model these effects with depth cueing. 

Visible Line and Surface Identification 

We can also clarify depth  lat ti on ships in a wireframe display by identifying visi- 
ble lines in some way. The simplest method is to highlight the visible lines or to 
display them in a different color. Another technique, commonly used for engi- 
neering drawings, is to display the nonvisible lines as dashed lines. Another ap- 
proach is to simply remove the nonvisible lines, as in Figs. 9-5(b) and 9-5(c). But 
removing the hidden lines also removes information about the shape of the back 
surfaces of an object. These visible-line methods also identify the visible surfaces 
of objects. 

When objects are to be displayed with color or shaded surfaces, we apply 
surface-rendering procedures to the visible surfaces so that the hidden surfaces 
are obscured. Some visiblesurface algorithms establish visibility pixel by pixel 
across the viewing plane; other algorithms determine visibility for object surfaces 
as a whole. 

Surface Rendering 

Added realism is attained in displays by setting the surface intensity of objects 
according to the lighting conditions in the scene and according to assigned sur- 
face characteristics. Lighhng speclhcations include the intensity and positions of 
light sources and the general background illumination required for a scene. Sur- 
face properties of obpds include degree of transparency and how rough or 
smooth the surfaces are to be. Procedures can then be applied to generate the cor- 
rect illumination and shadow regions for the scene. In Fig. 9-7, surface-rendering 
methods are combined with perspective and visible-surface identification to gen- 
erate a degree of realism in a displayed scene. 

Exploded and Cutaway Views 

- - - -  

Figure 9-6 
A wireframe object displayed 
with depth cueing, so that the 
intensity of lines decreases 
from the front to the back of 
the object. 

300 

Many graphics packages allow objects to be defined as hierarchical structures, so 
that lntemal details can be stored. Exploded and cutaway views of such objects 
can then be used to show the internal structure and relationship of the object 
parts. Figure 9-8 shows several kinds of exploded displays for a mechanical de- 
sign. An alternative to exploding an obpd into its component parts is the cut- 
away view (Fig. 9-91, which removes part of the visible surfaces to show internal 
structure. 

Three-Dimensional and Stereoscopic Views 

Another method for adding a sense of realism to a computer-generated scene is 
to display objects using either three-dimensional or stereoscopic views. As we 
have seen in Chapter 2, three-dimensional views can be obtained by reflecting a 



h I -2.- Figure 9-7 A realistic room display achieved 
with stochastic ray-tracing 
methods that apply a perspective 

'I projection, surfacetexhm 
I mapping, and illumination models. 

(Courtesy of lohn Snyder, led Lngycl, 
Deandm ffilm, Pnd A1 &In, Cd~foli~bmm 
Instihrte of Technology. Copyright 8 1992 
Caltech.) 

r i , ~ ~ ~ ~ ~ ,  9-8 
A fully rendered and assembled turbine displiy (a) can also be viewed 
as (b) an exploded wireframe display, (c) a surfacerendered exploded 
display, or (d) a surface-rendered, color-coded exploded display. 
(Courtesy of Autodesk, 1nc.l 

raster image from a vibrating flexible mirror. The vibrations of the m i m r  are syn- 
chronized with the display of the scene on the CRT. As the m i m r  vibrates, the 
focal length varies so that each point in the scene is projected to a position corre- 
sponding to its depth. 

Stereoscopic devices present two views of a scene: one for the left eye and 
the other for the right eye. The two views are generated by selecting viewing po- 
sitions that correspond to the two eye positions of a single viewer. These two 
views then can be displayed on  alternate refresh cycles of a raster monitor, and 
viewed through glasses that alternately darken first one.lens then the other in 
synchronization with the monitor refresh cycles. 

Section 9-1 

Three-Dimensional Display 
Methods 



Figure 9-9 
Color-coded cutaway view of a lawn mower engine showing the 
structure and relationship of internal components. (Gurtesy of 
Autodesk, Inc.) 

9-2 
THREE-DIMENSIONAL GRAPHICS PACKAGES 

Design of threedimensional packages requires some considerations that are not 
necessary with two-dimensional packages. A significant difference between the 
two packages is that a three-dimensional package must include methods for 
mapping scene descriptions onto a flat viewing surface. We need to consider im- 
plementation procedures for selecting different views and for using different pro- 
jection techniques. We also need to consider how surfaces of solid obpds are to 
be modeled, how visible surfaces can be identified, how transformations of ob- 
jects are performed in space, and how to describe the additional spatial proper- 
ties introduced by three dimensions. Later chapters explore each of these consid- 
erations in detail. 

Other considerations for three-dimensional packages are straightforward 
extensions from two-dimensional methods. World-coordinate descriptions are 
extended to three dimensions, and users are provided with output and input rou- 
tines accessed with s@cations such as 

polyline3 (n, wcpoints) 
f illarea3 (n, wcpoints) 
text3 (wcpoint, string) 
getLocator3 (wcpoint) 
translate3(translateVector, rnatrixTranslate) 

where points and vectors are specified with three components, and transforma- 
tion matrices have four rows and four columns. 

Two-dimensional attribute functions that are independent of geometric con- 
siderations can be applied in both two-dimensional and three-dimensional appli- 
cations. No new attribute functions need be defined for colors, line styles, marker 



- - - -  

Figure 9-10 
Pipeline for transforming a view of a world-coordinate scene to device coordinates. 

attributes, or text fonts. Attribute procedures for orienting character strings, how- 
ever, need to be extended to accommodate arbitrary spatial orientations. Text-at- 
tribute routines associated with the up vector require expansion to include z-co- 
ordinate data so that strings can be given any spatial orientation. Area-filling 
routines, such as those for positioning the pattern reference point and for map- 
ping patterns onto a fill area, need to be expanded to accommodate various ori- 
entations of the fill-area plane and the pattern plane. Also, most of the two-di- 
mensional structure operations discussed in earlier chapters can be carried over 
to a three-dimensional package. 

Figure 9-10 shows the general stages in a three-dimensional transformation 
pipeline for displaying a world-coordinate scene. After object definitions have 
been converted to viewing coordinates and projected to the display plane, scan- 
conversion algorithms are applied to store the raster image. 





G raphics scenes can contain many different kinds of objects: W s ,  flowers, clouds, rocks, water, bricks, wood paneling, rubber, paper, marble, steel, 
glass, plastic, and cloth, just to mention a few. So it is probably not too surprising 
that there is no one method that we can use to describe objects that will include 
all characteristics of these different materials. And to produce realistic displays of 
scenes, we need to use representations that accurately model object characteris- 
tics. 

Polygon and quadric surfaces provide precise descriptions for simple Eu- 
clidean objects such as polyhedrons and ellipsoids; spline surfaces end construc- 
tion techniques are useful for designing air&aft wings, gears, and other engineer- 
ing structures with curved surfaces; procedural methods, such as fractal 
constructions and particle systems, allow us to give accurate representations for 
clouds, clumps of grass, and other natural objects; physically based modeling 
methods using systems of interacting forces can be used to describe the nonrigid 
behavior of a piece of cloth or a glob of jello; octree encodings are used to repre- 
sent internal features of objects, such as those obtained from medical CT images; 
and isosurface displays, volume renderings, and other visualization techniques 
are applied to three-dimensional discrete data sets to obtain visual representa- 
tions of the data. 

Representation schemes for solid objects are often divided into two broad 
categories, although not all representations fall neatly into one or the other of 
these two categories. Boundary representations (B-reps) describe a three-dimen- 
sional object as  a set of surfaces that separate the object interior from the environ- 
ment. Typical examples of boundary representations are polygon facets and 
spline patches. Space-partitioning representations are used to describe interior 
properties, by partitioning the spatial region containing an object into a set of 
small, nonoverlapping, contiguous solids (usually cubes). A common space-par- 
titioning description for a three-dimensional object is an odree representation. In 
this chapter, we consider the features of the various representation schemes and 
how they are used in applications. 

10-1 
POLYGON SURFACES 

The most commonly used boundary =presentation for a three-dimensional 
graphics object is a set of surface polygons that enclose the object interior. Many 
graphics systems store all object descriptions as sets of surface polygons. This 
simplifies and speeds up the surface rendering and display of objects, since all 
surfaces are described with linear equations. For this reason, polygon descrip- 



tions are often referred to as "standard graphics objects." In some cases, a polyg- 
onal representation is the only one available, but many packages allow objects to 
be described with other schemes, such as spline surfaces, that are then converted 
to polygonal represents tions for prwessing. 

A polygon representation for a polyhedron precisely defines the surface fea- 
tures of the object. But for other objects, surfaces are tesst~lated (or tiled) to produce 
the polygon-mesh approximation. In Fig. 10-1, the surface of a cylinder is repre- 
sented as a polygon mesh. Such representations are common in design and solid- 

Figure 10-1 modeling applications, since the wireframe outline can be displayed quickly to 
Wireframe representation of a give a general indication of the surface structure. Realistic renderings are pro- 
cylinder with back (hidden) duced by interpolating shading patterns across the polygon surfaces to eliminate 
hnes removed. or reduce the presence of polygon edge boundaries. And the polygon-mesh ap- 

proximation to a curved surface can be improved by dividing the surface into 
smaller polygon facets. 

Polygon Tables 

We specify a polygon suriace with a set of vertex coordinates and associated at- 
tribute parameters. As information for each polygon is input, the data are placed 
into tables that are to be used in the subsequent'processing, display, and manipu- 
lation of the objects in a scene. Polygon data tables can be organized into two 
groups: geometric tables and attribute tables. Geometric data tables contain ver- 
tex coordinates and parameters to identify the spatial orientation of the polygon 
surfaces. Attribute intormation for an object includes parameters specifying the 
degree of transparency of the object and its surface reflectivity and texture char- 
acteristics. 

A convenient organization for storing geometric data is to create three lists: 
a vertex table, an edge table, and a polygon table. Coordinate values for each ver- 
tex in the object are stored in the vertex table. The edge table contains pointers 
back into the vertex table to identify the vertices for each polygon edge. And the 
polygon table contains pointers back into the edge table to identify the edges for 
each polygon. This scheme is illustrated in Fig. 10-2 far two adjacent polygons on 
an object surface. In addition, individual objects and their component polygon 
faces can be assigned object and facet identifiers for eas) reference. 

Listing the geometric data in three tables, as in Flg. 10-2, provides a conve- 
nient reference to the individual components (vertices, edges, and polygons) of 
each object. Also, the object can be displayed efficiently by using data from the 
edge table to draw the component lines. An alternative '~rrangement is to use just 
two tables: a vertex table and a polygon lable. But this scheme is less convenient, 
and some edges could get drawn twice. Another possibility is to use only a poly- 
gon table, but this duplicates coordinate information, since explicit coordinate 
values are listed for each vertex in each polygon. Also edge Information would 
have to be reconstructed from the vertex listings in the polygon table. 

We can add extra information to the data tables of Fig. 10-2 for faster infor- 
mation extraction. For instance. we could expand the edge table to include for- 
ward pointers into the polygon table so that common edges between polygons 
could be identified mow rapidly (Fig. 10-3). This is particularly useful for the ren- 
dering procedures that must vary surface shading snloothly across the edges 
from one polygon to the next. Similarly, the vertex table could be expanded so 
that vertices are cross-referenced to corresponding edge.; 

Additional geomctr~c information that is usually stored In the data tables 
includes the slope for each edge and the coordinate extents for each polygon. As 
vertices are input, we can calculate edge slopes, and w r  can scan the coordinate 



Polygon Surfaces 

S , :  E l .  El.E, 
S ,  : E,. E4. E,. E, 

Figrrrr 10-2 
Geometric data table representation for two adjacent polygon 
surfaces, formed with six edges and five vertices. 

values to identify the minimum and maximum x, y, and z values for individual 
polygons. Edge slopes and bounding-box information for the polygons are 
needed in subsequent processing, for example, surface rendering. Coordinate ex- 
tents are also used in some visible-surface determination algorithms. 

Since the geometric data tables may contain extensive listings of vertices 
and edges for complex objects, it is important that the data be checked for consis- 
tency and completeness. When vertex, edge, and polygon definitions are speci- 
fied, it is possible, parhcularly in interactive applications, that certain input er- 
rors could be made that would distort the display of the object. The more 
information included in the data tables, the easier it is to check for errors. There- 
fore, error checking is easier when three data tables (vertex, edge, and polygon) 
are used, since this scheme provides the most information. Some of the tests that 
could be performed by a graphics package are (1) that every vertex is listed as an 
endpoint for at least two edges, (2) that every edge is part of at least one polygon, 
(3) that every polygon is closed, (4) that each polygon has at least one shared 
edge, and (5) that if the edge table contains pointers to polygons, every edge ref- 
erenced by a polygon pointer has a reciprocal pointer back to the polygon. 

Plane Equations 

To produce a display of a three-dimensional object, we must process the input 
data representation for the object through several procedures. These processing 
steps include transformation of the modeling and world-coordinate descriptions 
to viewing coordinates, then to device coordinates; identification of visible sur- 
faces; and the application of surface-rendering procedures. For some of these 
processes, we need information about the spatial nrientation of the individual 

FIXMC 10-3 
Edge table for the surfaces of 
Fig. 10-2 expanded to include 
pointers to the polygon table. 



Ct.apfrr 10 surface components or t h ~  object. This information Is ihtained from the vertex- 
ilirre i)memlonal Ohlerl coordinate valucs and Ine equations that describe the pcllygon planes. 

Krl~rc'~enlal~or~\ The equation for 'I plane surface can be expressed In the form 

where (r, y, z )  i h  any p ) ~ n t  on the plane, and the coettiiients A, B, C, and D are 
constants descr~bing tht, spatla1 properties of the plane. We can obtain the values 
oi A ,  B, C, and 1> by sc~lving a set of three plane equatmns using the coordinatc 
values for lhree noncollinear points in the plane. For this purpose, w e  can select 
threc successive polygon vertices, ( x , ,  y,, z,), (x?, y2, z? ) ,  ,rnJ (:,, y,, z,), and solve 
thc killowing set of simultaneous linear plane equation5 for the ratios A I D ,  B/D, 
and ClD: 

The solution ior this set ot equations can be obtained in determinant form, using 
Cramer's rule, a s  

Expanding thc determinants, we can write the calculations for the plane coeffi- 
c~ents in the torm 

As vertex values and other information are entered into the polygon data struc- 
ture, values tor A, 8, C'. and D are computed for each polygon and stored with 
the other polygon data. 

Orientation of a plane surface in spacc. can bc described with the normal 
vector to the plane, as shown in Fig. 10-4. This surface normal vector has Carte- 
sian components (A ,  8, C), where parameters A, 8, and C are the plane coeffi- 
c~enta calculated in Eqs. 10-4. 

Since we are usuaily dealing witlr polygon surfaces that enclose an object 
interlor, we need to dishnguish bftween the two sides oi the surface. The side of 
the planc that faces thc object mterior is called the "inside" face, and the visible 
or outward side is the "outside" face. I f  polygon verticeh are specified in a coun- 
terclockwise direction \\.hen viewing the outer side of thv plane in a right-handed 
coordinate system, the direction of the normal vector will be from inside to out- 
side. This isdcnonstratrd for one plane of a unit cube in Fig. 10-5. 



To determine the components of the normal vector for the shacled surface 
shown in Fig. 10-5, we select three of the four vertices along the boundary of the 
polygon. These points are selected in a counterclockwise direction as we view 
from outside the cube toward the origin. Coordinates for these vertices, in the 
order selected, can be used in Eqs. 10-4 to obtain the plane coefficients: A = I, 
B = 0, C = 0, D = -1. Thus, the normal vector for this plane is in the direction of 
the positive x axis. 

The elements ofthe plane normal can also be obtained using a vector cross- 
p d u d  calculation. We again select three vertex positions, V1, V, and V3, taken 
in counterclockwise order when viewing the surface from outside to inside in a 
right-handed Cartesian system. Forming two vectors, one h m  V1 to V2 and the 
other from V, to V, we calculate N as the vector cross product: 

This generates values for the plane parameters A, B,  and C .  We can then obtain 
the value for parameter D by substituting these values and the coordinates for 
one of the polygon vertices in plane equation 10-1 and solving for D. The plane 
equation can be expmsed in vector form using the normal N and the position P 
of any point in the plane as 

Plane equations are used also to identify the position of spatial points rela- 
tive to the plane surfaces of an object. For any point (x, y, z) not on a plane with 
parameters A, B, C, D, we have 

We can identify the point as either inside or outside the plane surface according 
to the sign (negative or positive) of Ax + By + Cz + D: 

if Ax + By + Cz + D < 0, the point (x, y, z) is inside the surface 

if Ax + By + Cz + D > 0, the point (x, y, z) is outside the surface 

These &quality tests are valid in a right-handed Cartesian system, provided the 
plane parameters A, B, C, and D were calculated using vertices selected in a 
counterclockwise order when viewing the surface in an outside-to-inside direc- 
tion. For example, in Fig. 1&5, any point outside the shaded plane satisfies the in- 
equality x - I > 0, while any point inside the plane has an xcoordinate value 
less than 1. 

Polygon Meshes 

Some graphics packages (for example, PHlCS) provide several polygon functions 
for modeling o b F .  A single plane surface can be specified with a hnction such 
as f illArea. But when object surfaces are to be tiled, it is more convenient to 
specify the surface facets with a mesh function. One type of polygon mesh is the 
triangle strip. This function produces n - 2 connected triangles, .as shown in Fig. 
10-6, given the coordinates for n vertices. Another similar function is the quadri- 
laferal mesh, which generates a mesh of (n - I) by (m - 1) quadrilaterals, given 

. 

F i g u r e  10-5 
The shaded polygon surface 
of the unit cube has plane 
equation x - 1 = 0 and 
normal vector N = (1,0,0). 

- 
I ~ p r e  10-6 
A triangle strip formed with 
11 triangles connecting 13 
vertices. 



the coordinates for an n by m array of vertices. Figure 10-7 shows 20 vertices 
forming a mesh of 12 quadrilaterals. 

When polygons are specified with more than three vertices, it is possible 
that the vertices may not all Lie in one plane. This can be due to numerical errors 
or errors in selecting coordinate positions for the vertices. One way to handle this 
situation is simply to divide the polygons into triangles. Another approach that is 

- . _  sometimes taken is to approximate the plane parameters A, B, and C. We can do 
Figure 10-7 this with averaging methods or we can propa the polygon onto the coordinate 
A quadrilateral mesh planes. Using the projection method, we take A proportional to the area of the 
containing 12quadrilaterals polygon pro$ction on the yz plane, B proportionafto the projection area on the xz 
construded from a 5 by 4 plane, and C proportional to the propaion area on the xy plane. 
input vertex array. Highquality graphics systems typically model objects with polygon 

meshes and set up a database of geometric and attribute information to facilitate 
processing of the polygon facets. Fast hardware-implemented polygon renderers 
are incorporated into such systems with the capability for displaying hundreds 
of thousands to one million br more shaded polygonbper second (u&ally trian- 
gles), including the application of surface texture and special lighting effects. 

10-2 
CURVED LINES A N D  SURFACES 

Displays of threedimensional curved lines and surfaces can be generated from 
an input set of mathematical functions defining the objects or hom a set of user- 
specified data points. When functions are specified, a package can project the 
defining equations for a curve to the display plane and plot pixel positions along 
the path of the projected function. For surfaces, a functional description is often 
tesselated to produce a polygon-mesh approximation to the surface. Usually, this 
is done with triangular polygon patches to ensure that all vertices of any polygon 
are in one plane. Polygons specified with four or more vertices may not have all 
vertices in a single plane. Examples of display surfaces generated from hnctional 
descriptions include the quadrics and the superquadrics. 

When a set of discrete coordinate points is used to specify an object shape, a 
functional description is obtained that best fits the designated points according to 
the constraints of the application. Spline representations are examples of this 
class of curves and surfaces. These methods are commonly used to design new 
object shapes, to digitize drawings, and to describe animation paths. Curve-fit- 
ting methods are also used to display graphs of data values by fitting specified 
q r v e  functions to the discrete data set, using regression techniques such as the 
least-squares method. 

Curve and surface equations can be expressed in either a parametric or a 
nonparamehic form. Appendix A gives a summary and comparison of paramet- 
ric and nonparametric equations. For computer graphics applications, parametric 
representations are generally more convenient. 

10-3 
QUADRIC SUKFAC'ES 

A frequently used class of objects are the quadric surfaces, which are described 
with second-degree equations (quadratics). They include spheres, ellipsoids, tori, 



paraboloids, and hyperboloids. Quadric surfaces, particularly spheres and ellip- 10-3 
soids, are common elements of graphics scenes, and they are often available in Quadric Surfaces 
graphics packages as primitives horn which more complex objects can be con- 
structed. 

axis t 1 rA P - (x ,  v, Z) 
Sphere 

In Cartesian coordinates, a spherical surface with radius r centered on the coordi- ,#+ y axis 
nate origin is defined as the set of points (x, y, z) that satisfy the equation x axis 

- 
Parametric coordinate 

We can also describe the spherical surface in parametric form, using latitude and poiition (r, 0,6) on the 
longitude angles (Fig. 10-8): surface of a sphere with 

radius r. 
X = T C O S ~ C O S O ,  - ~ / 2 s 4 s ~ / 2  

y = rcost#~sinO, - n 5  0 5  TI ( 1  6-8) z axis f 

The parametric representation in Eqs. 10-8 provides a symmetric range for 
the angular parameters 0 and 4. Alternatively, we could write the parametric 

colatitude (Fig. 10-9). Then, 4 is defined over the range 0 5 4 5 .rr, and 0 is often 
equations using standard spherical coordinates, where angle 4 is specified as the ,,is 

taken in the range 0 5 0 27r. We could also set up the representation using pa- 
rameters u and I, defined over the range fmm 0 to 1 by substituting 4 = nu and spherical coordinate 
0 = 2nv. parameters (r, 8, 6)) using 

colatitude for angle 6 

Ellipsoid 

An ellipsoidal surface can be described as an  extension of a spherical surface, 
where the radii in three mutually perpendicular directions can have different val- 
ues (Fig. 10-10). The Cartesian representation for points over the surface of an el- 
lipsoid centered on the origin is 

(10-9) , 

And a parametric representation for the ellipsoid in terms of the latitude angle 4 Figure 10-10 
and the longitude angle 0 in Fig. 10-8 is An ellipsoid with radii r,, r,, 

and r: centered on the 

x=r , cvs~c0s0 ,  -7 r /25457r /2  coordinate origin. 

Torus 

A torus is a doughnut-shaped object, as  shown in Fig. 10-11. It can be generated 
by rotating a circle or other conic about a specified axis. The Cartesian represen- 



x axis 4 

Figure 10-11 
A torus with a circular cmss section 
centered on the coordinate origin. 

tation for points over the surface of a torus can be written in the form 

where r is any given offset value. Parametric representations for a torus are simi- 
lar to those for an ellipse, except that angle d extends over 360". Using latitude 
and longitude angles 4 and 8, we can describe the toms surface as  the set of 
points that satisfy 

z = r, sin C#J 

This class of objects is a generalization of the quadric representations. Super- 
quadrics are formed by incorporating additional parameters into the quadric 
equations to provide increased flexibility for adjusting object shapes. The number 
of additional parameters used is equal to the dimension of the object: one para- 
meter for curves and two parameters for surfaces. 

Supclrell ipse 

We obtain a Cartesian representation for a superellipse from the corresponding 
equation for an ellipse by allowisg the exponent on the x and y terms to be vari- 



able. One way to do this is to write the Cartesian supemllipse equation ir, the M i o n  10-4 
form juperquadrics 

where parameter s can be assigned any real value. When s = 1, we get an ordi- 
nary ellipse. 

Corresponding parametric equations for the superellipse of Eq. 10-13 can be 
expressed as 

Figure 10-12 illustrates supercircle shapes that can be generated using various 
values for parameters. 

Superellipsoid 

A Cartesian representation for a superellipsoid is obtained from the equation for 
an ellipsoid by incorporating two exponent parameters: 

For s, = s2 = 1, we have an ordinary ellipsoid. 
We can then write the corresponding parametric representation for the 

superellipsoid of Eq. 10-15 as 

Figure 10-13 illustrates supersphere shapes that can be generated using various 
values for parameters s, and s2. These and other superquadric shapes can be com- 
bined to create more complex structures, such as furniture, threaded bolts, and 
other hardware. 

Figrrrc 10-12 
Superellipses plotted with different values for parameter 5 and with 
r , = r .  



Three-Dimensional Object 
Reprerentations 

Figure 10-14 
Molecular bonding. As two 
molecules move away from 
each other, the surface shapes 
stretch, snap, and finally 
contract into spheres. 

Figrrre 70-15 
Blobby muscle shapes in a 
human arm. 

Figure 10-13 
Superellipsoids plotted with different values for parameters 
s, and s, and with r, = r, = r,. 

1 n-5 - -  - 
BLOBBY OBJECTS 

Some obpcts d o  not maintain a fixed shape, but change their surface characteris- 
tics in certain motions or when in proximity to other obpcts. Examples in this 
class of objects include molecular structures, water droplets and other liquid ef- 
fects, melting objects, and muscle shapes in the human body. These objects can be 
described as exhibiting "blobbiness" and are often simply referred to as blobby 
objects, since their shapes show a certain degree of fluidity. 

A molecular shape, for example, can be described as spherical in isolation, 
but this shape changes when the molecule approaches another molecule. This 
distortion of the shape of the electron density cloud is due to the "bonding" that 
occurs between the two molecules. Figure 10-14 illustrates the stretching, s n a p  
ping, and contracting effects on m o l d a r  shapes when two molecules move 
apart. These characteristics cannot be adequately described simply with spherical 
or elliptical shapes. Similarly, Fig. 10-15 shows muscle shapes in a human a m ,  
which exhibit similar characteristics. In this case, we want to model surface 
shapes so that the total volume remains constant. 

Several models have been developed for representing blobby objects as dis- 
tribution functions over a region of space. One way to do this is to model objects 
as combinations of Gaussian density functions, or "bumps" (Fig. 1&16). A sur- 
face function is then defined as 

where r i  = vxi + 3, + zt, parameter 7 is some specified threshold, and parame- 
ters a and b are used to adjust the amount of blobbiness of the individual object.. 
Negative values for parameter b can be used to produce dents instead of bumps. 
Figure 10-17 illustrates the surface structure of a composite object modeled with 
four Gaussian density functions. At the threshold level, numerical root-finding 



techniques are used to locate the coordinate intersection values. The cross sec- 
tions of the individual objects are then modeled as circles or ellipses. If two cross 
sections zie near to each other, they are m q e d  to form one bIobby shape, as in 
Figure 10-14, whose structure depends on the separation of the two objects. 

Other methods for generating blobby objects use density functions that fall 
off to 0 in a finite interval, rather than exponentially. The "metaball" model de- 
scribes composite o b j j  as combinations of quadratic density functions of the 
form 

fir) = - 1  - r d ,  if d / 3  < r s d 1; 
And the "soft object" model uses the function 

Some design and painting packages now provide blobby function modeling 
for handling applications that cannot be adequately modeled with polygon or 
spline functions alone. Figure 10-18 shows a ujer interface for a blobby object 
modeler using metaballs. 

10-6 
SPLINE REPRESENTATIONS 

In drafting terminology, a spline is a flexible strip used to produce a smooth 
curve through a designated set of points. Several small weights are distributed 
along the length of the strip to hold it in position on the drafting table as the 
curve is drawn. The term spline curve originally referred to a curve drawn in this 
manner. We can mathematically describe such a curve with a piecewise cubic 

t'ig~rrr 10- 18 
A screen layout, used in the Blob 
Modeler and the Blob Animator 
packages, for modeling o b j s  with 
metaballs. (Carrhy of Thornson Digital 
I n q r  ) 

section 1M 
Spline Representations 

Figure 10-16 
A three-dimensional 
Gaussian bump centered at 
position 0, with height band 
standard deviation a. 

Figure 10-17 
A composite blobby objxt 
formed with four Gaussian 
bumps. 



Chapter 10 

Three-D~rnens~onal Object 
R~ptesentations 

~ -. 

F i p r c  10-19 
A set of six control point> 
interpolated with piecesn.~se 
contmuous polvnornial 
sections. 

.- . -- 
F ~ p r r ~  10-20 
A set of six control points 
approximated w ~ t h  piecewise 
continuous polynom~al 
sectlons 

polynomial function whose first and second derivatives are continuous across 
the various curve sect~ons. In computer graphics, the term spline curve now 
refers to any composite :urve formed with polynomial sect~ons satisfying speci- 
fied continuity conditions at the boundary of the pieces. A spline surface can be 
described with two sets of orthogonal spline curves. There are several different 
kinds of spline specifications that are used in graphics applications. Each individ- 
ual specification simply refers to a particular type of polynomial with certain 
specihed boundary conditions. 

Splines are used ,n graphm applications to design curve a.:d surface 
shapes, to digitize drawings for computer storage, a ~ d  to specify animation 
paths for the objects o r  the camera in a scene. Typical CAD applications for 
sphnes include the dcs.1~1 of automobile bodies, aircraft and spacecraft surfaces, 
and s h ~ p  hulls. 

We specify a spline curLC by giv~ng a set of coordinate positions, called control 
points, which indicates the general shape of the curve Thest, control points are 
then fitted with pircewi.e cont i~~uous pdrarnetric poly nomial functions in one of 
two ways. When polync:mlal sectlons are fitted so that the curve passes through 
each control point, as in Fig. 10-19, the resulting curve is said to interpolate the 
set of control points. On the other hand, when the polynomials are fitted to the 
general control-point path without necessarily passing through any control point, 
the resulting curve is said to approximate the set of control points (Fig. 14-20), 

interpolation curves are commonly used to digitize drawings or to specify 
animation paths. Appwximation curves are primarily used as design tools to 
structure object surfaces F~gure 10-21 shows an appreximation spline surface 
credted for a design appl~iation. Straight lines connect the control-point positions 
above the surface. 

A spline curve 1s cleiined, modified, and manipulated with operations on 
the control points. By ~nteractwely selecting spatial positions for the control 
points, a designer can set up an initial curve. After the polynomial fit is displayed 
for a given set of control points, the designer can then reposition some or all of 
the control points to restructure the shape of the curve. In addition, the curve can 
be translated, rotated, or scaled with transformations applied to the control 
points. CAD packages can also insert extra control points to aid a designer in ad- 
justing the curve shapes. 

The convex polygon boundary that encloses a set of control points is called 
the convex hull. One w a y  to envision the shape of a convex hull is to imagine a 
rubber band stretched around the positions of the control points so that each con- 
trol point is either on the perimeter of the hull or inside it (Fig. 10-22). Convex 
hulls provide a measure for the deviation of a curve or surface from the region 
bounding the control points. Some splines are bounded by the convex hull, thus 
ensuring that the polyncmials smoothly follow the control points without erratic 
oscillations. Alsn. the polygon region inside the convex hull is useful in some al- 
gorithms as a clipping ri-glen. 

A polyline connecting the scqucnce of cnntrol points for an approximation 
spline is usually displaved to remind a designer of the control-point ordering. 
This set of connected line segments is often referred to as the control graph of the 
curve. Other names for the series of straight-line sections connecting the control 
points in the order specified are control polygon and characteristic polygon. Fig- 
ure 10-23 s h o w  the s h q x  of the control graph for the control-point sequences in 
Fig. 10-22 



Section 10-6 

Spline Repmentations 

fiprrr 10-21 
An approximation spline surface for a CAD application 
in automotive design. Surface contours are plotted with 
polynomial curve sections, and the surface control points are 
connected with straight-line segments. (Courtesy of Ewns  & 
Sutherlnnd.) 

Parametric Continuity Conditions 

To ensure a smooth transition from one section of a piecewise parametric curve 
to the next, we can impose various continuity conditions a t  the connection 
points. If each section of a spline is described with a set of parametric coordinate 
functions of the form 

. -. - - - - -. -. - - - - - 
F ~ x u r e  10-22 
Convex-hull shapes (dashed lines) for two sets of control 
points 



'Cl 

- -  -- 

F W U W  70-24 
Piecewise construction of a 
curve by joining two curve 
segments using different 
orders of continuity: (a) zero- 
order continuity only, 
(b) first-order continuity, 
and (c) second-order 
continuity. 

Figrrr? 10-23 
Control-graph shapes (dashed lines) for two different sets of 
control points. 

we set parametric continuity by matching the parametric derivatives of adjoin- 
ing curve sections at their common boundary. 

Zero-order parametric continuity, described as C1 continuity, means simply 
that the curves meet. That is, the values of x, y, and z evaluated at u, for the first 
curve section are equal, respectively, to the values of x, y, and z evaluated at u ,  
for the next curve sect~on. First-order parametric continuity, d e r r e d  to as C1 
continuity, means that the first parametric derivatives (tangent lines) of the coor- 
dinate functions in Eq. 10-20 for two successive curve sections are equal at their 
joining point. Second-order parametric continuity, or C2 continuity, means that 
both the first and second parametric derivatives of the two curve secttons are the 
same at the intersection, Higher-order parametric continuity conditions are de- 
fined similarly. Figure 10-24 shows examplesof C", C1, and C2 continuity. 

With second-order continuity the rates of change of the tangent vectors for 
connecting sections are equal at their intersection. Thus, the tangent line transi- 
tions smoothlv from one section of the curve to the next Wig. 10-24(c)). But with 
first-order continuity, the rates of change of the tangent vectors for the two sec- 
tions can be quite different (Fig. 10-24(b)), so that the genc:ral shapes of the two 
adjacent sections can change abruptly, First-order continuitv is often sufficient for 
digitizing drawings and some design applications, while second-order continuity 
is useful for setting up animation paths for camera mot~on and for many preci- 
sion CAD requirements A camera traveling along the curve path In Fig. l0-24(b) 
with equal steps in parameter u would experience an abrupt change in accelera- 
tion at the boundary of the two sections, producing a discontinuity in the motion 
sequence. But i f  the camera were traveling along the path in Fig. 10-24(c), the 
frame sequence for the motion would smoothlv transition across the boundary. 

Geometric Continuity Condi!ions 

An alternate method for jolning two successive curve sectwns is to specify condi- 
tions for geometric continuity. In this case, we only require parametric deriva- 
tives of the two sections to be proportional to each other at their comnwn bound- 
ary instead of equal to each other. 

Zero-order geometric continuity, described as Go cont~nuity, is the same as 
zero-order parametric continuity. That is, the two curves sections must have the 



same coordinate position at the boundary point. First-order geometric continu- kction 10-6 
ity, or G' continuity, means that the parametric first derivatives are proportional 511l1ne Representations 
at the intersection of two successive sections. If we denote the parametric posi- 
tion on the curve as P h ) ,  the direction of the tangent vector P'(u) ,  but not neces- 
sarilv its magnitude, will be the same for two successive curve sections at their ., 
joining point under GI continuity. Second-order geometric continuity, or G2 con- 
tinuitv. means that both the first and second ~arametr ic  derivatives of the two 

2. 

curve sections are proportional at their boundary. Under GL continuity, curva- 
tures of two curve sections will match at the joining position. 

A curve generated with geometric continuity conditions is similar to one 
generated with parametric continuity, but with slight differences in curve shape. 
Figure 10-25 provides a comparison of geometric and parametric continuity. With 
geometric continuity, the curve is pulled toward the section with the greater tan- 
gent vector. 

Spline Specificalions 

There are three equivalent methods for specifying a particular spline representa- 
tion: (1) We can state the set of boundary conditions that are imposed on the 
spline; or (2) we can state the matrix that characterizes the spline; or (3) we can 
state the set of blending functions (or basis functions) that determine how spec- 
ified geometric constraints on the curve are combined to calculate positions along 
the curve path. 

To illustrate these three equivalent specifications, suppose we have the fol- 
lowing parametric cubic polynomial representation for the x coordinate along the 
path of a spline section: 

Boundary conditions for this curve might be set, for example, on the endpoint co- 
ordinates x (0 )  and x(l)  and on the parametric first derivatives at the endpoints 
x'(0) and ~ ' ( 1 ) .  These four boundary conditions are sufficient to determine the 
values of the four coefficients a,, b,, c,, and d,. 

From the boundary conditions, we can obtain the matrix that characterizes 
this spline curve by first rewriting Eq. 10-21 as the matrix product 

- - - -. - . 

Figrrrc. 10-25 
Three control points fitted with two curve sections jo~ned with 
(a) parametric continuity and (b) geometric continuity, where the 
tangent vector of curve Cj at p in t  p, has a greater magnitude than the 
tangent vector of curve CI at p,. 



Chapter 10 
Three-Dirnens~onal Object 

Representations 

where U is the row matrix of powers of parameter u, and C is the coefficient col- 
umn matrix. Using Eq. 10-22, we can write the boundary conditions in matrix 
form and solve for the coefficient matrix C as 

where M,,, is a four-element column matrix containing the geometric constraint 
values (boundary condihons) on the spline; and Yph is the 4-by-4 matrix that 
transforms the geomebic constraint values to the polynomial coefficients and 
provides a characterization for the spline curve. Matrix Mgmm contains control- 
point coordinate values and other geometric constrzints that have been specified. 
Thus, we can substitute the mahix representation for C into Eq. 10-22 to obtain 

The matrix, Msphl characterizing a spline representation, sometimes called the 
basis matrix, is parhcularly useful for transforming from one spline representation 
to another. 

Finally, we can expand Eq. 10-24 to obtain a poly~iomial representation for 
coordinate x in terms oi the geometric constraint parameters 

where gt are the constraint parameters, such as the control-point coordinates and 
slope of the curve at the control points, and BFk(u) are the polynomial blending 
functions. In the following sections, we discuss some commonly used splines and 
their matrix and blending-function specifications. 

10-7 
ClJBlC SPLlhE INTI 4POLAT ION hlETHOD5 

This class of splines is most often used to set up paths for object motions or to 
provide a representation for an existing object or drawing, but interpolation 
splines are also used sometimes to design object shapes. Cubic polynomials offer 
a reasonable compromlje between flexibility and speed of computation. Com- 
pared to higher-order polynomials, cubic splines require less calculations and 
memory and they are more stable. Compared to lower-order polynomials, cubic 
splmes are more flexible for modeling arbitrary curve shapes. 

Given a set of control points, cubic interpolation splines are obtained by fit- 
ting the input points t.%.ith a piecewise cubic polynomial curve that passes 
through every control point. Suppose we have n + 1 control points specified with 
coordinates 



A cubic interpolation fit of these points is illustrated in Fig. 10-26. We can de- 
scribe the parametric cubic polynomial that is to be fitted between each pair of section 10-7 
control points with the following set of equations: Cubic Spline Interpolation 

Methods 

X ( U )  = a,u3 + b,u2 + C,U + d ,  
y ( u ) = a y u 3 + b y u 2 + c y u + d y r  ( 0 1 ~ 5 1 )  ( 10-26) 

Z ( U )  = a,u3 + b.u2 + C,U + d, 

For each of these three equations, we  need to determine the values of the four co- 
efficients a, b, c, and d  in the polynomial representation for each of the n curve 
sections between the n + 1 control points. We do this by setting enough bound- 
ary conditions at the "joints" between curve sections so  that we can obtain nu- 
merical values for all the coefficients. In the following sections, we discuss com- 
mon methods for setting the boundary conditions for cubic interpolation splines. 

Natural Cubic Splines 

One of the first spline curves to be developed for graphics applications is the nat- 
ural cubic spline. This interpolation curve is a mathematical representation of 
the original drafting spline. We formulate a natural cubic spline by requiring that 
two adjacent curve sections have the same first and second parametric deriva- 
tives at their common boundary. Thus, natural cubic splines have C2 continuity. 

If we have n + 1 control points to fit, as in Fig. 10-26, then we have n curve 
sections with a total of 4n polynomial coefficients to be determined. At each of 
the n - 1 interior control points, we have four boundary conditions: The two 
curve sections on either side of a control point must have the same first and sec- 
ond parametric derivatives at that control point, and each curve must pass 
through that control point. This gives us 4 n  - 4 equations to be satisfied by the 
4n polynomial coefficients. We get an additional equation from the first control 
point p ,  the position of the beginning of the curve, and another condition from 
control point p,, which must be the last point on the curve. We still need two 
more conditions to be able to determine values for all coefficients. One method 
for obtaining the two additional conditions is to set the second derivatives at po 
and p, to 0. Another approach is to add two extra "dummy" control points, one 
at each end of the original control-point sequence. That is, we add a control point 
p-I and a control point p,,,, Then all of the original control points are interior 
points, and we  have the necessary 4 n  boundary conditions. 

Although natural cubic splines are a mathematical model for the drafting 
spline, they have a major disadvantage. If the position of any one control point is 
altered, the entire curve is affected. Thus, natural cublc splines allow for no "local 
control", so that we cannot restructure part of the curve without specifying an 
entirely new set of control points. 

Figure 70-26 
A piecewise continuous cubic-spline interpolation of n + 1 control 

.points. 



Chapter 10 rterrnile I n t ~ r p o l a t w ~  
Thrre-Ddmpnsional Object 

~ ~ ~ , ~ ~ ~ ~ ~ ~ ~ ~ i ~ ~ ~  4 Hemite  spline ( n a n d  after the French mathematiclan Charles Hermite) is an 
interpolating piecewist cubic polynomial with a specitied tangent at each control 
point. Unlike the natural cubic splines, Hermite splints can be adjusted locally 
because each curve section is only dependent on its endpoint constraints. 

If P(L) represents a parametric cubic point function for the curve section be- 
tween control points pi and pk, ), as  shown in Fig. 10-2". then the boundary con- 
ditions that define this Hermite cunrr  section are 

with Dpk and Dpk+, spcitying the values for the parametric derivatives (slope of 
the curve) a t  control polnts pk and p k + ~ ,  respectively. 

We can write thr \.ector equivalent of Eqs. 10-26 for this Hermite-curve sec- 
tion a s  

where the x component of P is r(u) = a$ + b,u2 + - d,, and similarly for the 
y and z components. The matrix equ~valent ot Eq. 10-28 1s 

and the derivative of thin point function can be expressed as 

Substituting endpoint v;~lues 0 and 1 for parameter u Into the previous two equa- 
tions, we can express the Hermite boundary conditions 10-27 i n  the matrix form: 

Hermite cuive section helween 
control pmnts ph and p,.  : 



Solving this equation for the polynomial coefficients, we have Seclion 10-7 
Cubic Spline Interpolallon 
Methods 

where M,, the Hermite matrix, is the inverse of the boundary constraint matrix. 
Equation 10-29 can thus be written in terms of the boundary conditions as 

Finally, we can determine expressions for the Hermite blending funct'ions 
by carrying out the matrix multiplications in Eq. 10-33 and collecting coefficients 
for the boundary constraints to obtain the polynomial form: 

The polynomials Hh(u) for k = 0, 1, 2, 3 are referred to as blending functions be- 
cause they blend the boundary constraint values (endpoint coordinates and 
slopes) to obtain each coordinate position along the curve. Figure 10-28 shows 
the shape of the four Hermite blending functions. 

Hermite polynomials can be useful for some digitizing applications where 
it may not be too difficult to specify or approximate the curve slopes. But for 
most problems in computer graphics, it is more useful to generate spline curves 
without requiring input values for curve slopes or other geometric information, 
in addition to control-point coordinates. Cardinal splines and Kochanek-Bartels 
splines, discussed in the following two sections, are variations on the Hermite 
splines that d o  not require input values for the curve derivatives at the control 
points. Procedures for these splines compute parametric derivatives from the co- 
ordinate positions of the control points. 

As with Hermite splines, cardinal splines are interpolating piecewise cubics with 
specified endpoint tangents at the boundary of each curve section. The difference 



la) 

Figure 10-28 
Thc Hermite blending functions. 

-- -- . - .- - - - . .- - 
I i,prrv 10-29 
Parametric paint function 
P(u) for a cardinal-spline 
sectior. between control 
points p, and PA., . 

is that we  do not have to give the values for the endpoint tangents. For a cardinal 
spline, the value for the slope at a control point is calndated from the coordinates 
of the two adjacent control points. 

A cardinal spline section is completely specified with four consecutive con- 
trol points. The middle two control points are the section endpoints, 'and the 
other two points are used in the calculation of the endpoint slopes. If we take 
P(u) as the representation for the parametric cubic point function for the curve 
section between control points pt and ot+,, as in Fig. 10-29, then the four control 
points from pi-, to pi+, are used to set the boundary conditions for the cardinal- 
spline section as 

Thus, the slopes at contnd points pk and p,,, are taken to be proportional, respec- 
tively, to the chords p;_,p,,, and pl Fig. 10-30). Parameter t is called the 
tension parameter since it controls how loosely or tightly the cardinal spline fits 



the input control points. Figure 10-31 illustrates the shape of a cardinal curve for 
very small and very large values of tension t .  When t  = 0, this class of curves is 
referred to as ~atmdl- om splines, or Overhauser splines. 

Using methods similar to those for Hennite splines, we can convert the 
pb_: * P ~ . z  

boundary conditions 10-35 into the matrix form -- 
k - i p r c  10-30 
Tangent vectors at the 
endpoints of a cardinal-spline 

'10-30' section am proportional to 
the chords formed with 
neighboring control points 
(dashed lines). 

where the cardinal matrix is 

Mc = 

with s = ( 1  - t ) / 2 .  
Expanding matrix equation 10-36 into polynomial form, we have 

where the polynomials CARk(u) for k = 0, 1, 2, 3 are the cardinal blending func- 
tions. Figure 10-32 gives a plot of the basis functions for cardinal splines with 
t = 0. 

Kochanek-Bartels Splines 

These interpolating cubic polynomials are extensions of the cardinal splines. Two 
additional parameters are introduced into the constraint equations defining 
Kochanek-Bartels splines to provide for further flexibility in adjusting the shape 
of curve sections. 

Given four consecutive control points, labeled pr-,, pk, pk+,, and Pk+2, we 
define the boundary conditions for a Kochanek-Bartels curve section between pk 
and pk+ I as 

P(0) = p, 

P(1) = Pk+l 

P'(O), = + ( I  - t ) [ ( l  + b)(l - c)(pk - pk-I) 

where t  is the tension parameter, b is the bias parameter, and c is the continuity 
parameter. In the Kochanek-Bartels formulation, parametric derivatives may not 
be continuous across section boundaries. 



Figure 10-31 
Effect of the tension parameter on 

t  < o t > o  the shape of a cardinal spline 
(Lwwr Curve) (Tighter Curve1 section. 

Tension parameter t has the same interpretation as in the cardinal-spline 
formulation; that is, it controls the looseness or tightness of the curve sections. 
Bias (b)  is used to adjust the amount that the curve bends at each end of a section, 
so that curve sections can be skewed toward one end or the other (Fig. 10-33). Pa- 
rameter c controls the continuity of the tangent vector across the boundaries of 
sections. If c is assigned a nonzero value, there is a discontinuity in the slope of 
the curve across section boundaries. 

Kochanek-Bartel splines were designed to model animation paths. In par- 
ticular, abrupt changes in motion of a object can be simulated with nonzero val- 
ues for parameter c. 

Figure 10-32 
The cardinal blending functions for t = 0 and s = 0.5 

326 



Section 10-8 

Bkzier Curves and Surfaces 

Figure 10-33 
Effect of the bias parameter on the shape of a 
Kochanek-Bartels spline section. 

10-8 
BEZIER C U R V E S  AND SURFACES 

This spline approximation method was developed by the French engineer Pierre 
Mzier for use in the design of Renault automobile bodies. BCzier splines have a 
number of properties that make them highly useful and convenient for curve and 
surface design. They are also easy to implement. For these reasons, Wzier splines 
are widely available in various CAD systems, in general graphics pckages  (such 
as  GL on Silicon Graphics systems), and in assorted drawing and painting pack- 
ages (such as Aldus Superpaint and Cricket Draw). 

In general, a Bezier curve section can be fitted to any number of control points. 
The number of control points to be approximated and their relative position de- 
termine the degree of the BCzier polynomial. As with the interpolation splines, a 
Wzier curve can be specified with boundary conditions, with a characterizing 
matrix, or with blending functions. For general Bezier curves, the blending-func- 
tion specification is the most convenient. 

Suppose we are given n + 1 control-point positions: pk = (xk, yk, zk), with k 
varying from 0 to n. These coordinate points can be blended to produce the fol- 
lowing position vector P(u), which describes the path of an approximating BCzier 
polynomial function between p, and p,. 

The Bezier blending functions BEZk,, , (u) are the B e m s t e i n  polynomials: 

where the C(n,  k) are the binomial coefficients: 

Equivalently, we can define Bezier blending functions with the recursive calcula- 
tion 



- Cha~ter 10 with BE& = u A ,  and BEZOL = (1 - 1 4 ) ~ .  Vector equation 10-40 represents a set of 
~hree-~trnensional Object three parametric equations for the individual curve coorclinates~ 

Representations 
8 

X ( U )  = 1 x A  BEZi.,.(u) 
k-0  

As a rule, a Wzier curve is a polynomial of degree cne less than the number 
of control points used: Three points generate a parabola, four points a cubic 
curve, and so forth. Figure 10-34 demonstrates the appearance of some Bezier 
curves for various selections of control points in the ry plane ( z  = 0). With certain 
control-point placements, however, we obtain degenerate Bezier polynom~als. 
For example, a Bezier curve generated with three collinear control points is a 
straight-line segment. And a set of control points that are all at the same coordi- 
nate position produces a B6zier "curve" that is a single point. 

Bezier curves are commonly found in painting and drawing packages, as 
well as CAD systems, since they are easy to implement and they are reasonably 
powerful in curve design. Efficient methods for determining coordinate positions 
along a Bezier curve can be set up using recursive calculations. For example, suc- 
cessive binomial coefficients can be calculated as 

- - 

F~gure 10-34 
Examples of two-dimens~onal Bher curves generated from three, four, 
and five control points. Dashed lines connect the control-polnt 
pos~tions 



n-k+  l Section 10-8 
C(n ,  k) = - 

k 
C(n,  k-- 1) (10-i5' Bhier Curves and Surfacer 

for n 2: k. The following example program illustrates a method for generating 
Mzier curves. 

void ~omputecoefficients (int n, int ' c )  

int k, i; 

for (k=O; k<=n: k++) ( 
/ *  Compute n! / (k! (n-k) ! )  'I 
c[kl = 1; 
for (i=n; ~>=k+l; i--1 
cIkl " =  i; 

f ~ r  (i=n-k: i>=2; i--) 
c[kl I= i; 

1 
1 

/ void computepoint 
(float u, wcPt3 * pt, int ncontrols, wcPt2 controls, int cl i 

1 . inc k, n = nzontrols - 1; 
! float blend; 

: / '  Add in influence of each control point ./ 
for (k=O: kcncontrols; k++) { 

I blend = clkl ' powâ€¢â€™ (u,k) ' powâ€¢â€™ (1-u,n-k); 
I pt->x + =  controlslk) . x  blend: 

pt->y += controls[kl . y  ' blend; 
pt->z += controls[k].z blend; ! 1 

void bezier (wcPt3 controls, int ncontrols, int m, wcPt3 ' curve) 
{ 
I' Allocate space for the coefficients * /  
int ' c = (int ') malloc (ncontrols sizeof (int)); 
int i; 

computeCoefficients (ncontrols-I, c); 
for (i=O; iem; i++) 
computepoint (i / (float) m, Lcurvelil, ntontrols, controls, c); 

i free (c): 
I )  

Properties of B k i e r  Curves 

A very useful property of a Wzier curve is that it always passes through the first 
and last control points. That is, the boundary conditions at the two ends of the 
curve are 



Chapter 10 Values of the parametric first derivatives of a Mzier curve at the endpoints 
Thrvr-Dimensional O h w t  can he calculated trom co~~trol-point coordinates as 

Representdtions 

P'(0) = -np, + np, 
P'(I) = - np,., + np,, 

Thus, the slope at the beg~nning of the curve is along the line joining the first two 
control points, and the slope at the end of the curve is along the line joining the 
last two endpoints. Similarly, the parametric second der~vatives of a B6zier curve 
at the endpoints are calculated as 

Another important property of any Wzier c u n e  is that ~t lies within the 
convex hull (convex polygon boundary) of the control points. This follows from 

P3 the properties of Bkzier blending functions: They are all positive and their sum is 
' always 1 ,  

,' I 
I I 

\ so that any curve position is simply the weighted sum of the control-point p s i -  P? * I tions. The contrex-hull property lor a Bezier curve ensures that the polynomial 
/ mioothly follows the cuntn)l points without erratic oscillations. 

/ 

* - - -  . * . . - - -  4 
P. PO = PG P. Dtwgn Tec t1niqut.s U~II?.  RG;.ler Curves 

Clcised Bezier curves are generated hv spec~fying the first and last control points 
at the same position, as in the example shown in Fig. 10-35. Also, specifying mul- 
tiple control points at a single coordinate position gives nI,>re weight to that posi- 
tion. In Fig. 10-36, a single coorclin,\te position is input as two control points, and 
the resulting curve is pullt:d nearer to this position. 

We can fit A Rezirr curve to any number of control points, but this requires 
the calculation of polynonlial functions of higher deg1.x. When complicated 
curves are to be generated, they can be formed by piecing several Bezier sections 
of lower degree tugether I'iecing together smaller sertiuns also gives us better 
control over the shape oi the curve in small regions. Since Bezier curves pass 
through endpoints, i t  1s easy to match cunfe sections (zero-order continuity). 
A h ,  BC~ier curws ha\c the important property that the tangent to the curve at 
an endpoint IS along the line joining that endpoint to the ,adjacent control point. 
Therefore, to obtam first-order continuity between curve sections, we can pick 
control points p',, and p' (> t  a new section to be along the same straight line as 
control points p,, and p ,  of the previous section (Fig. 10-37). When the two 
curve sections have thc .;.lnIe number of control points, \\.r obtain C1 continuity 
by choosing the f m t  control point of the new section as the last control point of 
the previous section and Iw positioning the second cuntrol point of the new sec- 
tion at position 



Scctmn 10-8 

Bezier Curves and Surfacer 

Figure 10-37 
Piecewise approximation curve formed with two Mzier sections. Z e n  
order and first-order continuity are attained between curve sections by 
setting pb = p, and by making points p,, p2 and pi collinear. 

Thus, the three conhol points are collinear and equally spaced. 
We obtain C continuity between two Bezier sections by calculating the po- 

sition of the third control point of a new section in terms of the positions of the 
last three control points of the previous section as 

Requiring second-order continuity of Mzier curve sections can be unnecessarily 
restrictive. This is especially true with cubic curves, which have only four control 
points per section. In this case, second-order continuity fixes the position of the 
first three control points and leaves us only one point'that we can use to adjust 
the shape of the curve segment. 

Cubic Berier Curves 

Many graphics packages provide only cubic spline functions. This gives reason- 
able design flexibility while avoiding the increased calculations needed with 
higher-order polynomials. Cubic Bbzier curves are generated with four control 
points. The four blending functions for cubic Kzier curves, obtained by substi- 
tuting 11 = 3 into Eq. 10-41 are 

Plots of the four cubic Mzier blending functions are given in Fig. 10-38. The 
form of the blending functions determine how the control points influence the 
shape of the curve for values of parameter u over the range from 0 to 1. At u = 0, 



- - - - . . . -. . - - .. - - . - - - - - - - . . . . . . -. . . . . . . 
Figlrrt, 70- 3<S 
The four Bezier blending funct~ons for cubic curves (n 3) 

theonly nonzero blending function is BEZ,,, which has the vatue 1 .  At u = 1, the 
only nonzero function is BEZ3,,, .with a value of 1 at that point. Thus, the cubic 
Bezier curve will always pass through control points p,, .lnd p,. The other furw 
tions, BEZ,,, and BEZ?,, ~nfluence the shape of the curve, a t  intermediate values 
of parameter u, so that the resulting curve tends toward ~wints  p, and p,. Blend- 
ing function BEZl,3 is maximum at 11 = 1 /3, and REZ,,! IS maximum at I(  = 2/3. 

We note in Fig. 10-38 that each of the four blend~ng functions is nonzero 
over the entire range of parameter u .  Thus, Bezier curvei do not allow for locnl 
conrrol of the curve shape. If we decide to reposition ,in!, one of the control 
points, the entire curve will be affected. 

At the end positionc of the cubic Bez~er curve, the parnnictric first dcri\fa- 
tives (slopes) are 

And the parametric second derivatives are 

We can use these expresstons for the parametric derlvati\.cs to ccmsbuct piere- 
wise curves with C' or C7 c.ontinuitv between sections. 



By expanding the polynomial expressions for the blending functions, we 10-8 
can write the cubic Bezier point function in the matr~x form Bez~er Curves and Surfaces 

where the Bkzier matrix is 

We could also introduce additional parameters to allow adjustment of curve 
"tension" and "bias", as we did with the interpolating splines. But the more use- 
ful B-splines, as well as p-splines, provide this capability. 

Bez i t r  Surtaces 

Two sets of orthogonal Bkzier curves can be used to design an object surface by 
specifying by an input mesh of control points. The parametric vector function for 
the Bkzier surface is formed as the Cartesian product of Bezier blending func- 
tions: 

with p,,, specifying the location of the (m + 1 )  by (n + I )  control points. 
Figure 10-39 illustrates two Mzier surface plots. The control points are con- 

nected by dashed lines, and the solid lines show curves of constant u and con- 
stant v. Each curve of constant u is plotted by varying v over the interval from 0 
to 1, with u fixed at one of the values in this unit interval. Curves of constant v 
are plotted similarly 

Figure 10-39 
Bezier surfaces constructed tor (a) in = 3,11 = 3, and (b) m = 4, n = 4. Dashed lines connect 
the control points. 



Chapter 10 

Three43imensional Object 
Representat ~ons 

-- 
Figure 10-40 
A composite Wier surface constructed with two Kzier sections, 
joined at the indicated boundary line. The dashed lines connect 
specified control points. First-order continuity is established by 
making the ratio of length L ,  to length L, constant for each collinear 
line of control points across the boundary between the surface 
sections. 

Bezier surfaces have the same properties as Bezier curves, and they provide 
a convenient method for interactive design applications. For each surface patch, 
we can select a mesh of control points in the xy "ground" plane, then we choose 
elevations above the ground plane for the z-coordinate values of the control 
points. Patches can then be pieced together using the boundary constraints. 

Figure 10-40 illustrates a surface formed with two Bkzier sections. As with 
curves, a smooth transition from one section to the other is assured by establish- 
ing both zero-order and first-order continuity at the boundary line. Zero-order 
continuity is obtained by matching control points at the boundary. First-order 
continuity is obtained bv choosing control points along a straight line across the 
boundary and by maintaining a constant ratio of collinear line segments for each 
set of specified control points across section boundaries. 

10-9 
B-SPLINE CURVES AUD SURFACES 

These are the most widely used class of approximating splines. B-splines have 
two advantages over B6zier splines: (1) the degree of a B-spline polynomial can 
be set independently of the number of control points (with certain limitations), 
and (2) B-splines allow local control over the shape of a spline curve or surface 
The trade-off is that &splines are more complex than Wzier splines. 



B-Spline Curves 5ection 10-9 
B.Spline Curves and Surfaces 

We can write a general expression for the calculation of coordinate positions 
along a B-spline curve in a blending-function formulation as 

where the pk are an input set of n + 1 control points. There are several differences 
between this B-spline formulation and that for Bezier splines. The range of para- 
meter u now depends on how we choose the Bspline parameters. And the B- 
spline blending functions Bbd are polynomials of degree d - 1, where parameter 
d can be chosen to be any integer value in the range from 2 up  to the number of 
control points, n + 1. (Actually, we can also set the value of d at 1, but then our 
"curve" is just a point plot of the control points.) Local control for Bsplines is 
achieved by defining the blending functions over subintervals of the total range 
of u. 

Blending functions for B-spline curves are defined by the Cox-deBoor re- 
cursion formulas: 

where each blendjng function is defined over d subintervals of the total range of 
u.  The selected set of subinterval endpoints u, is referred to as a knot vector. We 
can choose any values for the subinterval endpomts satisfying the relation 
u I  4 u,+,. Values for u,,, and u,,, then depend on  the number of control points 
we select, the value we choose for parameter d, and how we set u p  the subinter- 
vals (knot vector). Since it is possible to choose the elements of the knot vector so 
that the denominators in the previous calculations can have a value of 0, this for- 
mulation assumes that any terms evaluated as  0/0 are to be assigned the value 0. 

Figure 10-41 demonstrates the local-control characteristics of Bsplines. In 
addition to local control, B-splines allow us to vary the number of control points 
used to des~gn a curve w~thout changing the degree of the polynomial. Also, any 
number of control points can be added or modified to manipulate curve shapes. 
Similarly, we can increase the number of values in the knot vector to aid in curve 
design. When we do this, however, we also need to add control points since the 
size of the knot vector depends on parameter n. 

B-spline curves have the following properties. 

The polynomial curve has degree d - 1 and C"? continuity over the range 
of u.  
For n + 1 rmtrol points, the curve is described with ti + 1 blending func- 
tions. 
Each blending function Bk,, is defined over d subintervals of the total range 
of u, starting at knot value ul. 
The range of parameter u 1s divided into n + d subintervals by the n + d + 
1 values specified in the knot vector. 



Chapter 10 
Three-Dimensional Ob~ect 

- 
Figure 10-41 
Local modilkation of a B-spline curve. Changing one of the control points in (a) produces 
curve (b), which is modified only in the neighborhood of the altered control point. 

With knot values labeled as [u,, u,, . . . , it,,,,], the resulting B-spline curve is 
defined only in the interval from knot value u,, . , up to knot value u,,-,. 
Each section of the spline curve (between two successive knot values) is in- 
fluenced by d control points. 
Any one control point can affect the shape of at most d curve sections. 

In addition, a B-spline curve lies within the convex hull of at most d + 1 control 
points, so that B-splines are tightly bound to the input positions. For any value of 
u in the interval from knot value u,-,  to u,,,, the sum over all basis functions is 1: 

(10-56) 
k = O  

Given the control-point positions and the value ot parameter d, we then 
need to specify the knot values to obtain the blending functions using the recur- 
rence relations 10-55. There are three general ctassitications for knot vectors: uni- 
form, open uniform, and nonuniform. B-splines are commonly described accord- 
ing to the selected knot-vector class. 

Uniform, Periodic B-Splines 

When the spacing between knot values is constant, the r~sulting curve is called a 
uniform B-spline. For example, we can set up a uniform knot vector as 

Often knot values are normalized to the range between 0 and 1, as in 

It is convenient in many applications to set u p  uniform knot values with a sepa- 
ration of 1 and a starting value of 0. The following knot vector is an example of 
this specification scheme. 



Figure 10-42 
Periodic B-spline blending functions for 11 = d = 3 and a uniform, integer knot vector. 

Uniform B-splines have periodic blending functions. That is, for given val- 
ues of n and d, all blending functions have the same shape. Each successive 
blending function is simply a shifted version of the previous function: 

where Au is the interval between adjacent knot values. Figure 10-42 shows the 
quadratic, uniform B-spline blending functions generated in the following exam- 
ple for a curve with four control points. 

Example 10-1 Uniform, Quadratic B-Splines 

To illustrate the calculation of Rspline blending functions for a uniform, integer 
knot vector, w e  select parameter values d = 17 = 3. The k3ot vector must then 
contain n + d + 1 = 7 knot values: 

and the range of parameter u is from 0 to 6, with n + d = 6 subintervals. 



Chapter l o  Each of the four blending functions spans d = 3 subintervals of the total range of 
Three-Dimensional Object U .  Using the recurrence relations 10-55, we obtain the first blending function as 

Representations 

f o r O < u < l  

B,,,(u) = f u12 - u) + i ( u  - 1)(3 - u),  for 1 5 u < 2 I 
We obtain the next periodic blending function using relat~onship 10-57, substitut- 
ing u - 1 for u in BOA, and shifting the starting positions up  by 1: 

Similarly, the remaining two periodic functions are obtained by successively 
shifting B13 to the right: 

A plot of the four periodic, quadratic blending functions is given in Fig. 10-42, 
which demonstrates the local feature of 8-splines. The first control point is multi- 
plied by blending function B,&). Therefore, changing the position of the first 
control point onlv affects the shape of the curve up  to I' = 3. Similarly, the last 
control point influences the shape of the spline curve in thc interval where B3,, is 
defined. 

Figure 10-42 also illustrates the limits of the B-spline curve for this example. All 
blending functions are present in the interval from r i d .  , -= 2 to u,,, , = 4. Below 2 
and ahove 4, not all blending functions are prescnt. This i s  the range of the poly- 



Figure 10-43 
Quadratic, periodic Bspline fitted 
to four control points in the xy 
plane. 

nornial curve, and the interval in which Eq. 10-56 is valid. Thus, the sum of all 
blending functions is 1 within this interval. Outside this interval, we cannot sum 
all blending functions, since they are not all defined below 2 and above 4. 

Since the range of the resulting polynomial curve is fmm 2 to 4, we can deter- 
mine the starting and ending positions of the curve by evaluating the blending 
functions at these points to obtain 

Thus, the curve start. at the midposition between the first two control points and 
ends at the midposition between the last two control points. 

We can also determine the parametric derivatives at the starting and ending posi- 
tions of the curve. Taking the derivatives of the blending functions and substitut- 
ing the endpoint values for parameter u, we find that 

The parametric slope of the curve at the start position is parallel to the line join- 
ing the first two control points,and the parametric slope at the end of the curve is 
parallel to the h e  joining the last two control points. 

An example plot of the quadratic periodic B-spline c u b e  is given in Figure 10-43 
lor four control points selected in the xy plane. 

In the preceding example, we noted that the quadratic curve starts between 
the first two control points and ends at a position between the last two control 
points. This result is valid for a quadratic, periodic B-spline fitted to any number 
of distinct control points. In general, for higher-order polynomials, the start and 
end positions are each weighted averages of d - 1 control points. We can pull a 
spline curve closer to any control-point position by specifying that position mul- 
tiple times. 

General expressions for the boundary conditions for periodic B-splines can 
be obtained by reparameterizing the blending functions so that parameter u is 
mapped onto the unit interval from 0 to 1. Beginning and ending conditions are 
then obtained at  u = 0 and u = 1. 

Cubic, Period~c- K-Splines 

Since cubic, periodic 8-splines are commonly used in graphics packages, we con- 
sider the fornlulation for this class of splines. Periodic splines are particularly 
useful for generating certain closed curves. For example, the closed curve in Fig. 
10-44 can be generated in sections by cyclically specifying four of the six control 



Chapter 10 PI PY 

Three-Dimensional Obiect 
Representations I ;(---> \ 

,. P3 Figure 10-44 
\ 

'\ 
A closed, period~c, piecewise, cubic 

\ / B.-spline constructed with cyclic 
L- - -  ------- -4 specification of the six control 
P5 Pa points. 

points shown at each step. If any three consecutive control points are identical, 
the curve passes through that coordinate position. 

For cubics, d = 4 and each blending function spans four subintervals of the 
total range of u. If we are to fit the cubic to four control points, then we could use 
the integer knot vector 

and recurrence relations 10-55 to obtain the periodic blending functions, as we 
did in the last section for quadratic periodic B-splines. 

In this section, we consider an alternate formulation for periodic cubic B- 
splines. We start with the boundary conditions and obtain the blending functions 
normalized to the interval 0 I u 5 1. Using this formulation, we can also easily 
obtain the characteristic matrix. The boundary conditions for periodic cubic B- 
splines with four consecutive control points, labeled po, p,, p,, and ps, are 

These boundary conditions are similar to those for cardinal splines: Curve sec- 
tions are defined with four contro1 points, and parametric derivatives (slopes) at 
the beginning and end of each curve section are parallel to the chords joining ad- 
jacent control points. The 8-spline curve section starts at a position near p, and 
ends at a position near p2. 

A matrix formulation for a cubic periodic B-splines with four control points 
can then be written as 

where the B-spline matrlx for periodic cubic polynomials is 



Section 10-9 

B-Spline Curves and Surfaces 
(70-60) 

This matrix can be obtained by solving for the coefficients in a general cubic 
polynomial expression using the specified four boundary conditions. 

We can also modify the B-spline equations to include a tension parameter t 
(as in cardinal splines). The periodic, cubic B-spline with tension matrix then has 
the form 

which reduces to M B  when t = 1. 
We obtain the periodic, cubic B-spline blending functions over the parame- 

ter range from 0 to 1 by expanding the matrix representation into polynomial 
form. For example, for the tension value t = 1, we have 

Open Uniform B-Splines 

This class of B-splines is a cross between uniform B-splines and nonuniform B- 
splines. Sometimes it is treated as a special type of uniform 8-spline, and some- 
times it is considered to be in the nonuniform B-spline classification. For the 
open uniform B-splines, or simply open B-splines, the knot spacing is uniform 
except at the ends where knot values are repeated d times. 

Following are two examples of open uniform, integer knot vectors, each 
with a starting value of 0: 

We can normalize these knot vectors to the unit interval from 0 to 1: 

10,0,0.33,0.67,1, 1,); for o' = 2 and 11 = 3 

l0,0,0,0,0.5,1,1,1,1t, f o r d = 4 a n d n = 4  



Chapter 10 

Three-D~mensional Object 
Represemat~onr 

For any values of paranreters d and n, we can generate an open uniform knot 
vector with integer valucs using the calculations 

f o r O S j < d  

1, f o r d s j S r i  ( I O - ( I  ;) 

n d + 2 ,  f o r j > n  

for values of] ranging from 0 to n + d .  With this assignment, the first d knots are 
assigned the value 0, and the last d knots have the value n - d + 2. 

Open uniform B-splines have characteristics that are very similar to Bezier 
splines. In fact, when d = tr + 1 (degree of the polynomial 1s n )  open B-splines re- 
duce to Bezier splines, and all knot valucs are either O or 1. For example, with a 
cubic, open B-spline ( d  = 4) and four control points, the knot vector is 

The polynomial curve ior an open B-spline passes through the iirst and last con- 
trol points. Also, the slope of the parametric curves at the first control point is 
parallel to the line connecting the first two control points. And the parametric 
slope at the last control point is parallel to the line connecting the last two control 
points. So geometric constraints for matching curve sections are the same as for 
Kzier curves. 

As with Bbzier cuncs, specifying multiple control points at the same coor- 
dinate position pulls ans B-spline curve cioser to that position. Since open B- 
splines start at the first control point and end at the last specified control point, 
closed curves are generated by specifyng the first and last control points at the 
same position. 

Example 10-2 Open Uniform, Quadratic B-Splines 

From conditions 10-63 with 11 = 3 and ir = 1 (five control points), we obtain the 
following eight values for the knot vector: 

The total rangeof u is divided into seven subintervals, and each of the five blend- 
ing functions BkJ is defined over three subintervals, starting at knot position 11,. 
Thus, is defined from u, = 0 to 11, = 1, R , ,  is defined from u, = 0 to u4 = 2, 
and Big is defined from 14, = 2 to u7 = 3. Explicit polynomial expressions zre ob- 
tained for the blending functions from recurrence relations 10-55 as 



Figure 10-45 shows the shape of the these five blending functions. The local fea- 
tures of B-splines are again demonstrated. Blending function Bo,, is nonzero only 
in the subinterval from 0 to I, so the first control point influences the curve only 
in this interval. Similarly, function BdZ3 is zero outside the interval from 2 to 3, and 
the position of the last control point does not affect the shape 3 f  the begrnning 
and middle parts of the curve. 

Matrix formulations for open B-splines are not as conveniently generated as 
they are for periodic, uniform B-splines. This is due to the multiplicity of knot 
values at the beginning and end of the knot vector. 

For this class of splines, we can specify any values and intervals for the knot vec- 
tor. With nonuniform B-splines, we can choose multiple internal knot values and 
unequal spacing between the knot values. Some examples are 

Nonuniform B-splines provide increased flexibility in controlling a curve 
shape. With unequally spaced intervals in the knot vector, we obtain different 
shapes for the blending functions in different intervals, which can be used to ad- 
just spline shapes. By increasing knot multiplicity, we produce subtle variations 
in curve shape and even introduce discontinuities. Multiple knot values also r e  
duce the continuity by 1 for each repeat of a particular value. 

We obtain the blending functions for a nonuniform B-spline using methods 
similar to those discussed for uniform and open B-splines. Given a set of n + I 
control points, we set the degree of the polynomial and select the knot values. 
Then, using the recurrence relations, we could either obtain the set of blending 
functions or evaluate curve positions directly for the display of the curve. Graph- 
ics packages often restrict the knot intervals to be either 0 or 1 to reduce compu- 
tations. A set of characteristic matrices then can be stored and used to compute 

Section 10-9 

8-Splme Curves and Surfaces 



la) (bl 

id) 

Figzrrr 10-45 
Opn,  uniform 6-spline blending functions for n = 4 and d = 3 

values along the spline curve without evaluatmg the recurrence relations for each 
curve point to be plotted. 

6-.Spline Surfaces 

Formulation of a B-spline surface is similar to that for B6zier splines. We can ob- 
tain a vector point function over a B-spline surface using the Cartesian product of 
B-spline blending functions in the form 



-- 

Section 10-10 

- - 

Figure 10-46 
A prototype helicopter, designed and modeled by 
Daniel Langlois of SOFTUIAGE, Inc., Montreal, 
using 180,000 Bspline surface patches. The scene 
was then rendered using ray tracing, bump 
mapping, and reflection mapping. (Coudesy  silicon 
Graphics, Inc.) 

Beta-Splines 

whew the vector values for P ~ , , ~ ,  specify positions of the (n, + I )  by (n2 + 1) con- 
trol points. 

B-spline surfaces exhibit the same properties as those of their component B- 
spline curves. A surface can be constructed from selected values for parameters 
d, and d ,  (which determine the polynomial degrees to be used) and from the 
specified knot vector. Figure 10-46 shows an object modeled with 8-spline sur- 
faces. 

10-10 
BETA-SPLINES 

A generalization of Bsplines are the beta-splines, also referred to as psplines, 
that are formulated by imposing geometric continuity conditions on the first and 
second ,parametic derivatives. The continuity parameters for beta-splines are 
called /3 parameters. 

Beta-Spline Continuity Conditions 

For a specified knot vector, we can designate the spline sections to the left and 
right of a particular knot ui with the position vectors P,-,(u) and PJu) (Fig. 10-47). 
Zero-order continuity (positional continuity), Go, at u, is obtained by requiring 

~osition vectors along curve 
First-order continuity (unit tangent continuity), G1, is obtained by requiring sections to the left right 

tangent vectors to be proportional: of knot u,. 
345 



Chapter 10 DIP;- ~(u:) = P;(u,), P I  > 0 (10-hb)  
Three-Dimensronal Objcc! 

Representaliom Here, parametric first derivatives are proportional, and the unit tangent vectors 
are continuous across the knot. 

Second-order continuity ( c u m t u r e  vector continuity), G2, is imposed with the 
condition 

where 6 can be assigned any real number, and pl > 0. The curvature vector pro- 
vides a measure of the amount of bending of the curve at position u,. When Pi = 
1 and & = 0, beta-splines reduce to B-splines. 

Parameter is called the bins parameter since it controls the skewness of the 
curde. For PI > 1, the curve tends to flatten to the right in the direction of the unlt 
tangent vector at the knots. For 0 < p, < 1, the curve tends to flatten to the left. 
The effect of 0, on the shape of the spline curve is shown in Fig. 10-48. 

Parameter is called the tension parameter since it controls how tightly or 
loosely the spline fits the control graph. As /3, increases, the curve approaches the 
shape of the control graph, as shown in Fig. 10-49. 

Cubic, Period~c Beta-Spline Matrix Representation 

Applying the beta-spline boundary conditions to a cubic polynomial with a uni- 
form knot vector, we obtain the tollowing matrix representation for a periodic 
beta-spline: 

- 
F i p r t  10-48 
Effect of parameter /3, on the shape of a beta-spline curve. 

. - -- 
Figrrrr 10-49 
Effect of parameter & on the shape of a beta-spline curve. 



-2& 2(P2 + P: + P: + PJ -2(P2 + P: + PI + 1 )  Section 10-1 1 
3(& + 2P:) Rational Splmes 

6(P? - P:) 681 

where S = p2 + 2fi: + 4lj: + 401 + 2 .  
We obtain the B-spline matrix M, when /3, = 1 and = 0. And we get the 

8-spline with tension matrix MB,when 

10-1 1 
RATIONAL SPLINES 

A rational function is simply the ratio of two polynomials. Thus, a rational 
spline is the ratio of two spline functions. For example a rational B-spline curve 
can be described with the position vector: 

where the pk are a set of n + 1 control-point positions. Parameters q are weight 
factors for the control points. The greater the value of a particular o,, the closer 
the curve 1s pulled toward the control point pk weighted by that parameter. 
When all weight factors are set to the value 1, we have the standard 8-spline 
curve since the denominator in Eq. 10-69 is 1 (the sum of the blending functions). 

Rational splines have two important advantages compared to nonrational 
splines. First, they provide an exact representation for quadric curves (conics), 
such as circles and ellipses. Nonrational splines, which are polynomials, can only 
approximate conics. This allows graphics packages to model all curve shapes 
with one representation-rational splines-without needing a library of curve 
functions to handle different design shapes. Another advantage of rational 
splines is that they are invariant with respect to a perspective viewing transfor- 
mation (Section 12-3). This means that we can apply a perspective viewing trans- 
formation to the control points of the rational curve, and we will obtain the cor- 
rect view of the curve. Nonrational splines, on the other hand, are not invariant 
with respect to a perspective viewing transformation. Typically, graphics design 
packages usc nonuniform knot-vector representations for constructing rational B- 
splines. These splines are referred to as NURBs (nonuniform rational B-splines). 

Homogeneous coordinate representations are used for rational splines, 
since the denominator can be treated as the homogeneous factor in a four-dimen- 
sional representation of the control points. Thus, a rational spline can be thought 
of as the projection of a four-dimensional nonrational spline into three-dimen- 
sional space. 

Constructing a rational 8-spline representation is carried out with the same 
procedures for constructing a nonrational representation. Given the set of control 
points, the degree of the polynomial, the weighting factors, and the knot vector, 
we apply the recurrence relations to obtain the blending functions. 



- Chapter 10 To plot conic sections with NURBs, we use a quadratic spline function ( d  = 
Three-Dlmensional Object 3) and three control points. We can d o  this with a B-spline function defined with 

Representat~ons the open knot vector: 

which is the same as a quadratic Bezier spline. We then set the weighting func- 
tions to the following values: 

and the rational B-spline representation is 

We then obtain the various conics (Fig. 10-50) with the following values for para- 
meter r: 

r > 1/2, w, > 1 (hyperbola section) 
r = 1 /2, o, = 1 (parabola section) 
r < 1 /2,  o, < 1 (ellipse section) 
r = 0, w ,  = 0 (straight-line segment) 

We can generate a one-quarter arc of a unit circle in the first quadrant of the 
xy plane (Fig. 10-51) by setting w, = cosdand by choosing the control p i n t s  as 

-. 

Figure 70-50 
Conic sections generated with various values of the r.1tional-spline 
wei5hting factor w , .  



I p2= (1, O) of the xy plane. 

Other sections of a unit circle can be obtained with different control-point posi- 
tions. A complete circle can be generated using geometric transformation in the 
xy plane. For example, we can reflect the one-quarter circular arc about the x and 
y axes to produce the circular arcs in the other three quadrants. 

In some CAD systems, we construct a conic section by specifying three 
points on an arc. A rational homogeneous-coordinate spline representation is 
then determined by computing control-point positions that wouId generate the 
selected conic type. As an example, a homogeneous representation for a unit cir- 
cular arc in the first quadran I[ of the xy plane-is 

10-1 2 
CONVERSION BETWEEN SPLINE REPRESENTATIONS 

Sometimes it is desirable to be able to switch from one spline representation 10 
another. For instance, a Bezier representation is the most convenient one for sub- 
dividing a spline curve, while a B-spline representation offers greater design flex- 
ibility. So we might design a curve using B-spline sections, then we can convert 
to an equivalent Bezier representation to display the object using a recursive sub- 
d~vision procedure to locate coordinate positions along the curve. 

Suppose we have a spline description of an object that can be expressed 
with the following matrix product: 

where M,,l,,el is the matrix characterizing the spline representation, and M,,,, 1s 
the column matrix of geometric constraints (for example, control-point coordi- 
nates). To transform to a second representation with spline matrix MrpllnrZ, we 
need to determme the geometric constraint matrix Mgwm2 that produces the same 
vector point function for the object. That is, 



Three-D~mens~onal Object Or 
Representations 

Solving for MRPOm2, we have 

and the required transformation matrix that converts from the first spline repre- 
sentation to the second is then calculated as 

A nonuniform B-spline cannot be characterized ivith a general splme ma- 
trix. But we can rearrange the knot 5equence to change the nonuniform B-spline 
to a Bezier representation. Then the Bezier matrix could be converted to any 
other form. 

The following example calculates the transformation matrix tor conversion 
from a periodic, cubic B-spline representation to a cub~c, Bezier spline representa- 
tion. 

And the the hansformaticm matrix for converting from a cubic Bezier representa- 
tion to a periodic, cubic B-spline representation is 



10-13 Section 10-13 

DISPLAYING SPLINE CURVES AND SURFACES [lisplaying Spline Curves and 
Surfaces 

To display a spline curve or surface, we must determine coordinate positions on 
the curve or surface that project to pixel positions on the display device. This 
means that we  must evaluate the parametric polynomial spline functions in cer- 
tain increments over the range of the functions. There are several methods we 
can use to calculate positions over the range of a spline curve or surface. 

Horner's Rule 

The simplest method for evaluating a polynomial, other than a brute-force calcu- 
lation of each term in succession, is Horner's rule, which performs the calculations 
by successive factoring. This requires one multiplication and one addition at each 
step. For a polynomial of degree n, there are n steps. 

As an example, suppose we have a cubic spline representation where coor- 
dinate positions are expressed as 

with similar expressions for they and z coordinates. For a particular value of pa- 
rameter u,  we evaluate this polynomial in the following factored order: 

The calculation of each x value requires three multiplications and three additions, 
so  that the determination of each coordinate position (x, y, 2 )  along a cubic spline 
curve requires nine multiplications and nine additions. 

Additional factoring tricks can be applied to reduce the number of compu- 
tations required by Homer's method, especially for higher-order polynomials 
(degree greater than 3). But repeated determination of coordinate positions over 
the range of a spline function can be computed much faster using forward-differ- 
ence calculations or splinesubdivision methods. 

Forward-Difference Calculations 

A fast method for evaluating polynomial functions is to generate successive val- 
ues recursively by incrementing previously calculatd values as, for example, 

Thus, once we  know the increment and the value of xk at any step, we get the 
next value by adding the increment to the value at that step. The increment Axk at 
each step is called the forward difference. If we divide the total range of u into 
subintervals of fixed size 6, then two successive x positions occur at x, = x(uk) 
and xk+, = x ( u ~ + , ) ,  where 

and uo = 0. 



Chapter 10  To illustrate the method, suppose we have the lineiir spline representation 
Three-D~mensional Object x(u) = n,,u + h,. TWO surc15sive x-coordinate positions are represented as 

Reprcrentationr 

Subtracting the two equations, we obtain the forward difference: Axk = a,& In 
this case, the forward difference is a constant. With higher-order polynomials, the 
forward difference is itself a polynomial function of parameter u with degree one 
less than the original pol\:nomial. 

For the cubic spline representation in Eq. 10-78, two successive x-coordinate 
positions have the polynomial representations 

The forward difference now evaluates to 

which is a quadratic function of parameter uk. Since AxL is a polynomial function 
j f  11, we can use the same incremental procedure to obtain successive values of 
Ax,. That is, 

where the second forward difference IS the linear function 

Repeating this process once more, we can write 

with the third forward ditference as the constant 

Equations 10-80, 10-85, 111-87, and 10-88 provide an incremental forward-differ- 
ence calculation of point5 along the cubic curve. Starting at u ,  = 0 with a step size 
6, we obtain the initial values for the x coordinate and its iirst two forward differ- 
ences as 

x o =  d ,  

Ax, = n,63 + bra2 + c,6 
A2x,, = 6n,S3 + 2b,tj2 

Once these initial values have been computed, the calculation for each successive 
r-coordinate position requires onlv three additions. 



We can apply forward-difference methods to determine positions along w.bn10-13  
spline curves of any degree n. Each successive coordinate position (x,  y, z) is Displaying Spline Curves and 
evaluated with a series of 3 n  additions. For surfaces, the incremental calculations Surfaces 

are applied to both parameter u and parameter v. 

Subdivision Methods 

Recursive spline-subdivision procedures are used to repeatedly divide a given 
curve section in half, increasing the number of control points at each step. Subdi- 
vision methods are useful for displaying approximation spline curves since we 
can continue the subdivision process until the control graph approximates the 
curve path. Control-point coordinates then can be plotted as curve positions. An- 
other application of subdivision is to generate more control points for shaping 
the curve. Thus, we could design a general curve shape with a few control points, 
then we could apply a subdivision procedure to obtain additional control points. 
With the added control pants, we can make fine adjustments to small sections of 
the curve. 

Spline subdivision is most easily applied to a Bezier curve section because 
the curve passes through the first and last control points, the range of parameter 
u is always between 0 and 1, and it is easy to determine when the control points 
are "near e n o u g h  to the curve path. Ezier subdivision can be applied to other 
spline representations with the following sequence of operations: 

1 .  Convert the spline representation in use to a Bezier representation. 
2. Apply the Ezier  subdivision algorithm. 
3. Convert the Kzier representation back to the original spline representation. 

Figure 10-52 shows the first step in a recursive subdivision of a cubic Bezier 
curve section. Positions along the Bbzier curve are described with the parametric 
point function P(u) for 0 5 u 5 1. At the first subdivision step, we use the 
halfway point P(0.5) to divide the original curve into two sections. The first sec- 
tion is ihen described 
with Pz(t), where 

with the point ?unction P,(s), and the section is described 

s = 2u. for 0 5 u 5 0.5 

1 ~ 2 ~ - I ,  f o r 0 . 5 5 u c I  

Each of the two new curve sections has the same number of control points as  the 
original curve section. Also, the boundary conditions (position and parametric 

Before 
Subdivision 

Aher 
Subdivision 

F i p r c  10-52 
Subdividing a cubic Bezier curve section into two 
sections, each with four control points. 



Chapter 10 

Three-Dimensional 0bjw1 
Representations 

slope) at the two ends of each new curve section must match the position and 
slope values for the original curve PW. This g v e s  us four conditions for each 
curve section that we can use to determine the control-point positions. For the 
first half of the curve, the four new control points are 

And for the second half of the curve, we obtain the four control points 

An efficient order for con~yuting the new control points can be set up  with only 
add and shift (division by 2)  operations as 



These steps can be repeated any number of times, depenaing on whether Section 10-14 
we are subdividing the curve to gain more control points or whether we are try- Sweep Representat~ons 
ing to locate approximate curve positions. When we are subdividing to obtain a 
set of display points, we can terminate the subdivision procedure when the curve 
sections are small enough. One way to determine this is to check the distances 
between adjacent pairs of control points for each section. If these distances are 
"sufficiently" small, we can stop subdividing. Or we could stop subdividing 
when the set of control points for each section is nearly along a straight-line 
path. 

Subdivision methods can be applied to Bezier curves of any degree. For a 
Bezier polynomial of degree n - 1, the 2n control points for each half of the curve 
at the first subdivision step are 

where C(k, i) and C ( n  - k, n - i) are the binomial coefficients. 
We can apply subdivision methods directly to nonuruform Bsplines by 

adding values to the knot vector. But, in general, these methods are not as effi- 
cient as B6zier subdivision. 

10-1 4 
SWEEP REPRESENTATIONS 

Solid-modeling packages often provide a number of construction techniques. 
Sweep representations are useful for constructing three-dimensional obpcts that 
possess translational, rotational, or other symmetries. We can represent such ob- 
jects by specifying a twodimensional shape and a sweep that moves the shape 
through a region of space. A set of two-dimensional primitives, such as circles 
and rectangles, can be provided for sweep representations as menu options. 
Other methods for obtaining two-dimensional figures include closed spline- 
curve constructions and cross-sectional slices of solid objects. 

Figure 10-53 illustrates a translational sweep. The periodic spline curve in 
Fig. 10-53(a) defines the object cross section. We then perform a translational 

Figurr 10-53 
Constructing a solid with a translational sweep. Translating the 
control points of the periodic spline curve in (a) generates the solid 
shown in (b), whose surface can be described with pqint function 
PW). 



Figun 10-54 
Constructing a solid with a 
rotational sweep Rotating the 
control points of the periodic spline 
curve in (a) about the given rotation 
axis generates the sohd shown in 
(b), whose surface can be described 
with pomt function P(u,v). 

sweep by moving the control points p, through p3 a set distance along a straight- 
line path perpendicular to the plane of the cross section. At intervals along this 

we replicate the cross-sectional shape and draw a set of connecting lines in 
the direction of the sweep to obtain the wireframe representation shown in Fig. 
10-53(b). 

An example of object design using a rotational sweep is given in Fig. 10-54. 
This time, the periodic spline cross section is rotated about an axis of rotation 
specified in the plane of the cross section to produce the wireframe representa- 
tion shown in F&. 10-54(b). Any axis can be chosen for a rotational sweep. If  we 
use a rotation axis perpendicular to the plane of the spline cross section in Fig. 
10-54(a), we generate a two-dimensional shape. But if the cross section shown in 
this figure has depth, then we are using one three-dimensional object to generate 
another. 

In general, we can specify sweep constructions using any path. For rota- 
tional sweeps, we can move along a circular path through any angular disfance 
from 0 to 360'. For noncircular paths, we can specify the curve function describ- 
ing the path and the distance of travel along the path. In addition, we can vary 
the shape or size of the cross section along the sweep path. Or we could vary the 
orientation of the cross section relative to the sweep path as we move the shape 
through a region of space. 

10-15 
CONSTRUCTIVE SOI-ID-GEOMETRY METtIODS 

Another technique for solid modeling is to combine the vdumes occupied by 
overlapping three-dimensional objects using set operations. This modeling 
method, called constructive solid geometry (CSG), creates a new volume by ap- 
plying the unlon, intersection, or difference operation to two specified volumes. 



Figures 10-55 and 10-56 show examples for forming new shapes using the 
set operations. In Fig. 10-55(a), a bIock and pyramid are placed adjacent to each 
other. Specifying the union operation, we obtain the combined object shown in 
Fig. 10-55(b). Figure 10-%(a) shows a block and a cylinder with overlapping vol- 
umes. Using the intersection operation, we obtain the resulting solid in Fig. 10- 
%(b). With a difference operation, we can get the solid shown in Fig. 10-%(c). 

A CSG application-starts with an k t i a l  set of three-dirne&nal objects 
(primitives), such as blocks, pyramids, cylinders, cones, spheres, and closed 
spline surfaces. The primitives can,be provided by the CSG package as menu se- 
lections, or the primitives themselves could be formed using sweep methods, 
spline constructions, or other modeling procedures. To create a new three-dimen- 
sional shape using CSG methods, we-first select two primitives and drag them 
into position in some region of space. Then we select an operation (union, inter- 
section, or difference) for c o r n b i g  the volumes of the two primitives. Now we 
have a new object, in addition to the primitives, that we can use to form other ob- 
jects. We continue to construct new shapes, using combinations of primitives and 
the objects created at each step, until we have the final shape. An object designed 
with this procedure is represented with a binary tree. An example tree represen- 
tation for a CSG object is given in Fig. 10-57. 

Ray-casting methods are commonly used to implement constructive solid- 
geometry operations when objects are described with boundary representations. 
we  apply ray casting by constructing composite objects in world ckrdinates 
with the xy plane corresponding to the pixeI plane of a video monitor. This plane 
is then referred to as the "firing plane" since we fire a ray from each pixel posi- 
tion through the objects that are to be combined (Fig. 10-58). We then determine 
surface intersections along each ray path, and sort the intersection points accord- 
ing to the distance from the firing The surface limits for the composite ob- 
ject are then determined by the specified set operation. An example of the ray- 
casting determination of surface limits for a CSG object is given in Fig. 10-59, 
which shows yt cross sections of two primitives and the path of a pixel ray per- 
pendicular to the firing plane. For the union operation, the new volume is the 
combined interior regions occupied bv either or both primitives. For the intersec- 
tion operation, the new volume is the-interior region common to both primitives. 

. . . - . -- - - - - . . 
I ' i ~ ~ l r c  10-56 
(a) Two overlapping objects. (b) A wedge-shaped CSG object 
formed with the intersection operat~on. (c )  A CSG object 
formed with a difference operation by subtracting the 

Section 10-1 5 

Construcrive Solid-Geometry 
Methods 

la) (b) 

Figure 10-55 
Combining two objects 
(a) with a union operation 
produces a single, composite 
solid object (b). 

overlapping volume of the-cylinder from the block volume 



Object ( csG ) 

- - 

Figure 10-57 
A CSG tree representation for an 
object. 

Operation ' Surface Limits 

Union I A, D 
Intersection c. 0 
Difference 8. D 

(obi, - obi,) ; 
i 

Figirrc 10-58 Figure 10-59 
Implementing CSG Determining surface limits along a pixel ray. 
operations using ray casting. 

And a difference operation subtracts the volume of one primitive from the other. 
Each primitive can be defined in its own local (modeling) coordinates. 

Then, a composite shape can be formed by specifying the rnodeling-transforma- 

Firing 

tion matrices that would place two in an overlapping position in 
world coordinates. The inverse of these modeling matrices can then be used to 
transform the pixel rays to modeling coordinates, where the surface-intersection 
calculations are carried out for the individual primitives. Then surface intersec- 
tions for the two objects are sorted and used to determine the composite object 

Pla limits according to the specified set operation. This procedure is Apeated for 
each pair of objects that are to be combined in the CSG tree for a particular object. 

Once a CSG object has been designed, ray casting is used to determine 

, physical properties, such as volume and mass. To determine the volume of the 
object, we can divide the firing plane into any number of small squares, as shown 
in Fig. 10-60. We can then approximate the volume V., of the object for a cross- 
sectional slice with area A,, along the path of a ray from the square a t  position (i, 

fi,yur-r 10-60 j )  as 

Determining object volume 
along a ray path for a small V,, - A,j hz,, 11 0-953 
area A,, on the firing plane. 

where Az,, is the depth of the object along the ray from position ( i ,  j). I f  the object 
has internal holes, Az;, is the sum of the distances between pairs of intersection 

358 points along the ray. The total volume of the CSG object is then calculated as 



( J  11-96) Section 10-16 
Ocrrees 

Given the density function, p(x, y, z ) ,  for the object, we can approximate the 
mass along the ray from position (i, j )  as 

where the one-dimensional integral can often be approximated without actually 
carrying out the integration, depending on the form of the density function. The 
total mass of the CSG object is then approximated as 

Other physical properties, such as center of mass and moment of inertia, can be 
obtained with similar calculations. We can improve the approximate calculations 
for the values of the physical properties by taking finer subdwisions in the firing 
plane. 

If object shapes are reprewllled with octrees, we can implement the set op- 
erations in CSG procedures by scanning the tree structure describing the contents 
of spatial octants. This procedure, described in the following section, searches the 
octants and suboctants of a unit cube to locate the regions occupied by the two 
objects that are to be combined. 

10-16 
OCTREES 

Hierarchical tree structures, called octrees, are used to represent solid objects in 
some graphics systems. Medical imaging and other applications that require dis- 
plays of object cross sections commonly use octree representations. The tree 
structure is organized so that each node corresponds to a region of three-dimen- 
sional space. This representation for solids takes advantage of spatial coherence 
to reduce storage requirements for three-dimensional objects. It also provides a 
convenient representation for storing information about object interiors. 

The octree encoding procedure for a three-dimensional space is an exten- 
sion of an encoding scheme for two-dimensional space, called quadtree encod- 
ing. Quadtrees are generated by successively dividing a two-dimensional region 
(usually a square) into quadrants. Each node in the quadtree has four data ele- 
ments, one for each of the quadrants in the region (Fig. 10-61). If all pixels within 
a quadrant have the same color (a homogeneous quadrant), the corresponding 
data element in the node stores that color. In addition, a flag is set in the data ele- 
ment to indicate that the quadrant is homogeneous. Suppose all pixels in quad- 
rant 2 of Fig. 10-61 are found to be red. The color code for red is then placed in 
data element 2 of the node. Otherwise, the quadrant is said to be heterogeneous, 
and that quadrant is itself divided into quadrants (Fig. 10-62). The corresponding 
data element in the node now flags the quadrant as heterogeneous and stores the 
pointer to the next node in thequadtree. 

An algorithm for generating a quadtree tests pixel-intensity values and sets 
up the quadtree nodes accordingly. If each quadrant in the original space has a 



Chapter 10 

Three-D~niensional Object 
Quadranl Quadrant I 
Q u a d r a n  1 Qua:ral 1 Data Elements 

3 in the Representative 
Ouadtree Node 

Region of a 
Two-Dmensional 

Space 

F@rc 70-6 1 
Region of a two-dimensional space divided intu numbered 
quadrants and the associated quadtree node with four 
data elements. 

single color specification, the quadtree has only one node. For a heterogeneous 
region of space, the suc.cessive subdivisions into quadrants continues until all 
quadrants are homogeneous. Figure 10-63 shows a quadtree representation for a 
region containing one area with a solid color that is different from the uniform 
color specified for all other areas in the region. 

Quadtree encodings prowde considerable savings in storage when large 
color areas exist in a region of space, since each single-color area can be repre- 
sented with one node. For an area containing 2'' by 2" pixels, a quadtree repre 
sentation contains at n~c~st  11 levels. Each node in the quadtree has at most four 
immediate descendants 

An octree encoding scheme divides regions of three-dimensional space 
(usually cubes) into octants and stores eight data elements in each node of the 
tree (Fig. 10-64). Individual elements of a three-dimensional space are called vol- 
ume elements, or voxels. When all voxels in an octant are of the same type, this 

Reg~on ol  a 
Two-Dimensional 

Space 

Quadtree 
Representation 

- - . - . -- -, . - . . - - - -- - - - - . . - -. - 
Fisrris. 10-62 
Region of a two-d~mensional space with two levels ot quadrant 
divisions and the .issociated quadtree representation 



Figure 10-63 
Quadtree representation for a region containing one foreground-color 
pixel on a solid background. 

type value is stored in the corresponding data element of the node. Empty re- 
gions of space are represented by voxel type "void." Any heterogeneous octant is 
subdivided into octants, and the corresponding data element in the node points 
to the next node in the octree. Procedures for generating octrees are similar to 
those for quadtrees: Voxels in each octant are tested, and octant subdivisions con- 
tinue until the region of space contains only homogeneous octants. Each node in 
the octree can now have from zero to eight immediate descendants. 

Algorithms for generating octrees can be structured to accept definitions of 
objects in any form, such as a polygon mesh, curved surface patches, or solid- 
geometry constructions. Using the minimum and maximum coordinate values of 
the object, we can define a box (parallelepiped) around the object. This region of 
three-dimensional space containing the object is then tested, octant by octant, to 
generate the o&ee representation. 

Once an octree representation has been established for a solid object, vari- 
ous manipulation routines can be applied to the solid. An algorithm for perform- 
ing set operations can be applied to two octree representations for the same re- 
gion of space. For a union operation, a new octree is wnstrncted with the 
combined regions for each of the input objects. Similarly, intersection or differ- 

Region of a 
Three-Dimensional 

Space 

Data Elements 
in the Representative 

Octree Node 

Section 10-16 

Ocrrees 

Figure 10-64 
Region of a three-dimensional space divided mto numbered 
octants and the associated octree node with eight data elements 



Chapter 10 ence operations are perfonned by looking for regions of overlap in the two oc- 
Three-Dimensional Object trees. The new octree is then formed by either storing the octants where the two 

Reprerentat'ons objects overlap or the region occupied by one object but not the other. 
Three-dimensonal octree rotations are accomplished by applying the trans- 

formations to the occupied octants. Visible-surface identification is carried out by 
searching the octants from front to back. The first object detected is visible, so 
that info-mation can be transferred to a quadtree representation for display. 

10-17 
BSP TREES 

This representation scheme is similar to octree encoding, except we now divide 
space into two partitions instead of eight at each step. With a binary space-parti- 
tioning (BSP) tree, we subdivide a scene into two sections at each step with a 
plane that can be at any position and orientation. In an octree encoding, the scene 
is subdivided at each step with three mutually perpendicular planes aligned with 
the Cartesian coordinate planes. 

For adaptive subdivision of space, BSP trees can provide a more efficient 
partitioning since we can position and orient the cutting planes to suit the spatial 
distribution of the objects. This can reduce the depth of the tree representation for 
a scene, compared to an octree, and thus reduce the time to search the tree. In ad- 
dition, BSP trees are useful for identifying visible surfaces and for space parti- 
tioning in ray-tracing algorithms. 

10-1 8 
FRACTAL-GEOMETRY METHODS 

All the object representations we have considered in the previous sections used 
Euclidean-geometry methods; that is, object shapes were described with equa- 
tions. These methods are adequate for describing manufactured objects: those 
that have smooth surfaces and regular shapes. But natural objects, such as moun- 
tains and clouds, have irregular or fragmented features, and Euclidean methods 
do not realisticalIy model these objects. Natural objects can be realistically de- 
scribed with fractal-geometry methods, where procedures rather than equations 
are used to model objects. As we might expect, procedurally defined objects have 
characteristics quite different from objects described with equations. Fractal- 
geometry representations for objects are commonly applied in many fields to de- 
scribe and explain the features of natural phenomena. In coinputer graphics, we 
use fractal methods to generate displays of natural objects and visualizations of 
various mathematical and physical systems. 

A fractal object has two basic characteristics: infinite detail at every point 
and a certai~. self-similnrity between the object parts and the overall features of the 
object. The self-similarity properties of ,an object can take different forms, de- 
pending on the choice of fractal representation. We describe a fractal object with a 
procedure that specifies A repeated operation for producing the detail in the ob- 
ject subparts. Natural objects are represented with procedures that theoretically 
repeat an infinite number of times. Graphics displays of natural objects are, of 
course, generated witha f nite number of steps. 

If we zoom in on .I continuous Euclidean shape, no matter how compli- 
cared, we can eventually get the zoomed-in view to smooth out. But if we zoom 



Section 10-1 8 

~ractalGeometr~ Methods 

Distant 
Mountain 

Closer 
View 

Closer 
Yet 

Figure 10-65 
The ragged appearance of a mountain outline at different levels of 
magnification. 

in on a fractal object, we  continue to see as much detail in the magnification as 
we did in the original view. A mountain outlined against the sky continues to 
have the same jagged shape as we view it from a closer and closer position (Fig. 
10-65). As we near the mountain, the smaller detail in the individual ledges and 
boulders becomes apparent. Moving even closer, we see the outlines of rocks, 
then stones, and then grains of sand. At each step, the outline reveals more twists 
and turns. If w e  took the grains of sand and put them under a microscope, we 
would again see the same detail repeated down through the molecular level. 
Similar shapes describe coastlines and the edges ofplants and clouds. 

Zooming in on a graphics display of a fractal object is obtained by selecting 
a smaller window and repeating the fractal procedures to generate the detail in 
the new window. A consequence of the infinite detail of a fractal object is that it 
has no definite size. As we consider more and more detail, the size of an object 
tends to infinity, but the coordinate extents of the object remain bound within a 
finite region of space. 

We can describe the amount of variation in the object detail with a number 
called the fractal dimension. Unlike the Euclidean dimension, this number is not 
necessarily an integer. The fractal dimension of an object is sometimes referred to 
as the fractional dimension, which is the basis for the name "fractal". 

Fractal methods have proven useful for modeling a very wide variety of 
natural phenomena. In graphics applications, fractal representations are used to 
model terrain, clouds, water, trees and other plants, feathers, fur, and various 
surface textures, and just to make pretty patterns. In other disciplines, fractal pat- 
terns have been found in the distribution of stars, river islands, and moon craters; 
in rain fields; in stock market variations; in music; in traffic flow; in urban prop- 
erty utilization; and in the boundaries of convergence regions for numerical- 
analysis techniques. 

Fractal-Generation Procedures 

A fractal object is generated by repeatedly applying a specified transformation 
function to points within a region of space. If Po = (xO, yo, zo) is a selected initial 
point, each iteration of a transformation function F generates successive levels of 
detail with the calculations 



Chapter 10 In general, the transformation funct~on can be applied to a specified point 
ThreeDimensional Object set, or we could apply the transformation function to an initial sel of primitives, 

Represenrations such as straight lines, curves, color areas, surfaces, and solid objects. Also, we can 
use either deterministic or random generation procedures at each iteration. The 
transformation function may be defined in terms of geometric transformations 
(scaling, translation, rotation), or it can be set u p  with nonlinear coordinate trans- 
formations and decision parameters. 

Although fractal objects, by definition, contain infinite detail, we apply the 
transformation function a finite number of times. Therefore, the objects we dis- 
play actually have finite dimensions. A procedural representation approaches a 
"true" fractal as the number of transformations is increased to produce more and 
more detail. The amount of detail included in the final graphical display of an ob- 
jed depends on the number of iterations performed and the resolut~on of the dis- 
play system We cannot display detail variations that are smaller than the size of 
a pixel. To see more of the object detail, we zoom in on selected sections and re- 
peat the transformation function iterations. 

Classification 01 Fractals 

Self-similar fractals have parts that are scaled-down versions of the entire object. 
Starting with an initial shape, we construct the object subparts by apply a scaling 
parameter s to the overall shape. We can use the same scaling factors for all sub- 
parts, or we can use different scaling factors for different scaled-down parts of 
the object. If we also apply random variations to the scaled-down subparts, the 
fractal is said to be statistically sey-similar. The parts then have the same statistical 
properties. Statistically self-similar fractals are commonly used to model trees, 
shrubs, and other plants. 

Self-afSine fractals have parts that are formed with different scaling para- 
meters, s,, sy, s,, in different coordinate directions. And we can also include ran- 
dom variations to obtain statistically self-afine fractals. Terrain, water, and clouds 
are typically nde lec l  u.ith statistically self-affine fractal construction methods. 

Invariant fractal sets are formed with nonlinear transformations. This class 
of fractals includes selj-squaring fractals, such as the Mandelhrot set, which are 
formed with squaring functions in complex space; and sclf-irrverse fractals, 
formed with inversion procedures. 

Fractal Dimension 

The detail variation in a fractal object can be described with a number D, called 
the fractal dimension, which is a measure of the roughness, or fragmentation, of 
the object. More jagged-looking objects have larger fractal dimensions. We can set 
up some iterative procedures to generate fractal objects using a given value for 
the fractal dimension D. With other procedures, we may be able to determine the 
fractal dimension from the properties of the constructed object, although, in gen- 
eral, the fractal dimension is difficult to calculate. 

An expression for the fractal dimension of a self-similar fractal, constructed 
with a single scalar factor s, is obtained by analogy with the subdivision of a Eu- 
clidean object. Figure 10-66 shows the relationships between the scaling factor r; 
and the number of subparts n for subdivision of a unit straight-line segment, A 
square, and a cube. With s = 1 /2, the unit line segment (Fig. 10-&(a)) is divided 
into two equal-length subparts. Similarly, the square in Fig. 10-6Hb) is divided 
into four equal-area subparts, and the cube (Fig. 10-66(c)) is divided into eight 
equal-volume subparts. For each of these objects, the relationship between the 



Fisprr 10-66 
Subdividing objects with Euclidean dimensions 
(a) DE = I ,  (b) DC = 2, and (c) D, = 3 using scaling 
factors = 1 / 2 .  

number of subparts and the scaling factor is n . sDr = 1. In analogy with Euclid- 
ean objects, the fractal dimension D for self-similar objects can be obtained from 

Solving this expression for D, the fractal similarity dimension, we have 

For a self-similar fractal constructed with different scaling factors for the different 
parts, the fractal similarity dimension isobtained from the implicit relationship 

where sk is the scaling factor for subpart number k. 
In Fig. 10-66, we considered subdivision of simple shapes (straight line, rec- 

tangle, box). I f  we have morecomplicated shapes, including curved lines and ob- 
jects with nonplanar surfaces, determining the structure and properties of the 
subparts is more difficult. For general object shapes, we can use topological rover- 

Section 10-1 8 

Fraaal.Cmmetry Methods 



Chapter 10 ing methods that approximate obpct subparts with simple shapes. A subdivided 
Three-Dimensional Object curve, for example, can be approximated with straight-line sections, and a subdi- 

Repesenrations vided polygon could be approximated with small squares or rectangles. Other 
covering shapes, such as circles, spheres, and cylinders, can also be used to ap- 
proximate the features of an object divided into a number of smaller parts. Cov- 
ering methods are commonly used in mathematics to determine geometric prop- 
erties, such as  length, area, or volume, of an object by summing the properties of 
a set of smaller covering objects. We can also use covering methods to determine 
the fractal dimension D of some objects. 

Topological covering concepts were originally used to extend the meaning 
of geometric properties to nonstandard shapes. An extension of covering meth- 
ods using circles or spheres led to the notion of a Hausdorff-Besicovitch dimension, 

-. 
or fracfional dimension. The Hausdorff-Besicovitch dimension can be used as the 
fractal dimension of some objects, but, in general, it is difficult to evaluate. More 

Figure 10-67 commonly, the fractal dimension of an object is estimated with box-covering meth- 
Box covering of an irregularly 
shaped object. ods using redangles or parallelepipeds. Figure 10-67 illustrates the notion of a 

box cwering. Here, the area inside the large irregular boundary can be approxi- 
mated by the sum of the areas of the small covering rectangles. 

We apply box-covering methods by first determining the coordinate extents 
of an object, then we subdivide the object into a number of small boxes using the 
given scaling factors. The number of boxes n that it takes to cover an obpct is 
called the box dimension, and n is related to the fractal dimension D of the object. 
For statistically self-similar objects with a single scaling factors, we can cover the 
object with squares or cubes. We then count the number u of covering boxes and 
use Eq. 10-101 to estimate the fractal dimension. For self-affine objects, we cover 
the object with rectangular boxes, since different directions are scaled differently. 
In this case, the number of boxes n is used with the affine-transformation para- 
meters to estimate the fractal dimension. 

The fractal dimension of an object is always greater than the corresponding 
Euclidean dimension (or topological dimension), which is simply the least num- 
ber of parameters needed to specify the object. A Euclidean curve is one-dimen- 
sional, a Euclidean surface is two-dimensional, and a Euclidean solid is three-di- 
mensional. 

For a fractal curve that lies completely within a two-dimensional plane, the 
fractal dimension D is greater than 1 (the Euclidean dimension of a curve). The 
closer D is to 1, the smoother the fractal curve. If D = 2, we have a Penno crrrzr; 
that is, the "curve" completely fills a finite region of two-dimensional space. For 
2 < D < 3, the curve self-intersects and the area could be covered an infinite 
number of times. Fractal curves can be used to model natural-object boundaries, 
such as shorelines. 

Spatial fractal curves (those that do not lie completely within a single plane) 
also have fractal dimension D greater than 1, but D can be greater than 2 without 
self-intersecting. A curve that fills a volume of space has dimension D = 3, and a 
self-intersecting space curve has fractal dimension D > 3. 

Fractal surfaces typically have a dimension within the range 2 < D 5 3. If 
D = 3, the "surface" fills a volume of space. And if D > 3, there is an overlapping 
coverage of the volume. Terrain, clouds, and water are typically modeled with 
fractal surfaces. 

The dimension of a fractal solid is usually in the range 3 < D 5 4. Again, if 
D > 4, we have a self-overlapping o b j .  Fractal solids can be used, for example, 
to model cloud properties such as water-vapor density or temperature within a 
region of space. 



Initiator 

Section 10-18 

F racral-Geomerw Methods 

Generator 

Fipuri. 70-68 
Initiator and generator for the Koch curve. 

Geometric Construction of Deterministic Self-Similar Fractals 

To geometrically construct a deterministic (nonrandom) self-similar fractal, we 
start with a given geometric shape, called the initiator. Subparts of the initiator 
are then yeplaced with a pattern, called thegenerator. 

As an example, if we use the initiator and generator shown in Fig. 10-68, we 
can construct the snowflake pattern, or Koch curve, shown in Fig. 10-69. Each 
straight-line segment in the initiator is replaced with four equal-length line seg- 
ments at each step. The scaling factor is 1/3, so the fractal dimension is D = In 
4/In 3 -- 1.2619. Also, the length of each line segment in the initiator increases by 

- -- - - - - - - - - - 
I ! p i t 2  l&h9 
F1r4 three ~terat~ons In the generation of the Koch 
curve 



Segment Length = 1 Segment Length = 1 Segment Length - 1 
9 

Length = 1 Lengrh = 4 
3 

Length = 1-6 
9 

- - - - - - - - -- - - - - . -.- - - 

F ~ g u r c  10-70 
Length of each side of the Koch curve increases by a factor of 4/3 at 
each step, w l l e  the 11r1e-segment lengths are reduced by a factor of 1 / 3  

Segment Segment Segment 
Length = 11, 7 Length = 114 Length = 116 

-- . . - 
1 I y l w  10-71 
Self-sirndar curie cvnstruct~ons and assoc~ated fractal dlmens~ons 

Segment Segment 
Length = 1/3 Length = 118 

Segment 
Length = 118 

- - . - - . . . . - . . - - . . 
Fr,quu' 10- 7 2  
Generators with multiple, disloint parts. 

a factor of 4 / 3  at each $tep, so that the length of the fractal curve tends to infinity 
as more detail is added to the curve (Fig. 10-70). Examples of other self-similar, 
fractal-curve constntctions are shown in Fig. 10-71. These examples illustrate the 
more jagged appearance of objects with higher fractal dimensions. 

We can also use generators with multiple disjoint components. Some exani- 
ples of compound generators are shown in Fig. 10-72. Using random variations 

, -- -- ~~ - - with compound generators, wc can model various natural objects that have com- 
f i ' p i ~ r  70-73 pound parts, such as island distributions along coastlines. 
A snowflake-filling Peano Figure 10-73 shows an example of a self-simdar construction using multi- 
curve. ple scaling factors. Tht. fractal dimension of this object is determined from Eq. 

10-102. 
As an example of self-similar fractal construction !or a surface, we scale the 

regular tetrahedron shown in Fig. 10-74 by a factor of 1 /2, then place the scaled 



Sedion 10-18 

Fractal-Geometry Methods 

Front Face 

/-.. Scaled Copy 
,/' ' . of Tetrahedron - . 

/' ' . A (a)  A (bl 
Figure 10-74 
Scaling the tetrahedron in (a) by a factor of 1 /2 and positioning the 
scaled version on one face of the original tetrahedron produces the 
fractal surface (b). 

object on each of the original four surfaces of the tetrahedron. Each face of the 
original tetrahedron is converted to 6 smaller faces and the original face area is 
increased by a factor of 3/2. The fractal dimension of this surface is 

which indicates a fairly fragmented surface. 
Another way to create selfsimilar fractal objects is to punch holes in a given 

initiator, instead of adding more surface area. Fig. 10-75 shows some examples of 
fractal objects created in this way. 

Geometric Construction of Statistically Self-Similar Fractals 

One way we can introduce some randomness into the geometric construction of a 
self-similar fractal is to choose a generator randomly at each step from a set of 
predefined shapes. Another way to generate random self-similar objects is to 
compute coordinate displacements randomly. For example, in Fig. 10-76 we cre- 
ate a random snowflake pattern by selecting a random, midpointdisplacement 
distance at each step. 

- .  

1 ~ s ~ i r c  10-75 
-Self-similar, three-dimensional tractals formed with 
generators that subtract suhpam from an initiabr. 
(C'orrrftru ollohn C Hart. Wmhin,pn Slalc Uni;mi?/) 



Figure 10-76 
A modified "snowflake" pattern using random midpoint displacement. 

Displays of trees and other plants can be constructed with similar geometric 
methods. Figure 10-77 shows a self-similar construction for a fern. In (a) of this 
figure, each branch is a scaled version of the total object, and (b) shows a fully 
rendered fern with a twist applied to each branch. Another example of this 
method is shown in Fig. 10-78. Here, random scaling parameters and branching 
directions are used to model the vein patterns in a leaf. 

Once a set of fractal objects has been mated, we can model a scene by plac- 
ing several transformed instances of the fractal objects together. Figure 10-79 il- 
lustrates instancing with a simple fractal tree. In Fig. 10-80, a fractal forest is dis- 
played. 

To model the gnarled and contorted shapes of some trees, we can apply 
twisting functions as well as scaling to create the random, self-similar branches. 

Figure 10-77 
Self-similar conshuctions for a fern. 
(Courtesy of Peter Oppnheimrr, Computer 
Gmpfiics h b ,  N m  York Imtilufe of 
Tdnology.) 

~ i & r c  10-78 
Random, self-similar construction 
of vein branching in a fall leaf. 
Boundary of the leaf is the limit of 
the vein growth. (Courtay of Peter 
Oppnheimer, Computer Graphics Lnb, Nno 
York lrlslitute of Technology.) 



Figure 10-79 
Modeling a scene using multiple object instancing. Fractal leaves arqattached to a tree, 
and several instances of the tree are used to form a grove. The grass is modeled with 
multiple instances of green cones. (Courfesy of john C. Hurl, Washingfan Stale Uniiierwly.) 

This technique is illustrated in Fig. 10-81. Starting with the tapered cylinder on 
the left of this figure, we can apply transformations to produce (in succession 
from left to right) a spiral, a helix, and a random twisting pattern. A tree modeled 
with random twists is shown in Fig. 10-82. The tree bark in  this display is mod- 
eled using bump mapping and fractal Brownian variations o n  the bump patterns, 
as  discussed in the following section. 

. - - - - - . - . - . . -. 

1 r - ( t t j , ,  I I J - \ / ~  1 r p r r  10-SI  
A fraclal forest created with multiple instances oi Mdeling t r c ~  branches with y r a l ,  helical, and 
leaves, pinr ne~dlrs, grass, and tree bark. I C o u r r ~ w  ,,I random twists. (Courtmy o(i lrfrr  0ppwhrrrnr.r. rompurt.r 
I .~hrr  i Hxt .  fibshngtan Sfote  U n i ~ r r s t l , ~ . )  Graphrn -5h. W m  Yorl lns t~lutr .  o' Tcclrt1n1r~y.l 371 



Figure 10-82 
Tree branches modeled with 
random squiggles. (Courtesy of Peter 
Oppenheimn, Computer Graphics l n b ,  N m  
York Institute ~{Technology.) 

Affine Fractal-Construction Methods 

We can obtain highly realistic representations for terrain and other natural objects 
using affine fractal methods that model object features as fractioml Brownian mo- 
tion. This is an extension of standard Brownian motion, a form of "random 
walk", that describes the erratic, zigzag movement of particles in a gas or otner 
fluid. Figure 10-83 illustrates a random-walk path in the xy plane. Starting from a 
given position, we generate a skaight-line segment in a random direction and 
with a random length. We then move to the endpoint of the first line segment 

Figure 10-83 and repeat the process. This procedure is repeated for any number of line seg- 
An example of Brownian ments, and we can calculate the statistical properties of the line path over any 

(random in the time interval 1. Fractional Brownian motion is obtained by adding an additional 
nj plane. parameter to the statistical distribution describing Brownian motion. This addi- 

tional parameter sets the fractal dimension for the "motion" path. 
A single fractional Brownian path can be used to model a fractal curve. 

With a two-dimensional array of random fractional Brownian elevations over a 

Figure 70-84 
A Brownian-motion planet observed from the surface 
of a fractional Brownian-motion planet, with added 
craters, in the foreground. (Courtesy of R. V. Voss and B B. 
Mnndelbml, orkrptedfmm The Fractal Geometry of Nature by 
k o i t  B. Mandrlbrol INN, York. W. H .  Freerrmi~ and Co., 19831.) 



ground plane grid, we can model the surface of a mountain by connecting the el- w o n  10-16 
evations to form a set of polygon patches. If random elevations axe generated on Fractal-Geometry Methods 
the surface of a sphere, we can model the mountains, valleys, and oceans of a 
planet. In Fig. 10-84, Brownian motion was wd to create the elevation variations 
on the planet surface. The elevations were then color coded so that lowest eleva- 
tions were painted blue (the oceans) and the highest elevations white (snow on 
the mountains). Fractional Brownian motion was used to create the terrain fea- 
tures in the foreground. Craters were mated with random diameters and ran- 
dom positions, using affine fractal pmxdures that closely describe the distribu- 
tion of observed craters, river islands, rain patterns, and other similar systems of 
objects. 

By adjusting the fractal dimension in the fractional Brownian-motion calcu- 
lations, we can vary the ruggedness of terrain features. Values for the fractal di- 
mension in the neighborhood of D -- 2.15 produce realistic mountain features, 
while higher values close to 3.0 can be used to ma te  unusual-looking extrater- 
restrial landscapes. We can also scale the calculated elevations to deepen the val- 
leys and to increase the height of mountain peaks. Some examples of terrain fea- 
tures that can be modeled with fractal procedures are given in Fig. 10-85. A scene 
modeled with fractal clouds over a fractal mountain is shorn  in Fig. 10-86. 

Random Midpoint-Displacement Methods 

Fractional Brownian-motion calculations are time-consuming, because the eleva- 
tion coordinates of the terrain above a ground plane are calculated with Fourier 
series, which are sums of cosine and sine'terms. Fast Fourier transform (FFT) 
methods are typically used, but it is still a slow process to generate fractal-mom- 
tain scenes. Therefore, faster random midpoint-displacement methods, similar 
to the random displacement methods used in geometric constructions, have been 
developed to approximate fractional Brownian-motion representations for terrain 
and other natural phenomena. These methods were originally used to generate 
animation frames for science-fiction films involving unusual terrain and planet 
features. Midpoint-displacement methods are now commonly used in many ap- 
plications, including television advertising animations. 

Although random midpointdisplacement methods are faster than frac- 
tional Brownian-motion calculations, they produce less realistic-looking terrain 
leatures. Figure 10-87 illustrates the midpointdisplacement method for generat- 
ing a random-walk path in the xy plane. Starting with a straight-line segment, we 
calculate a displaced y value for the midposition of the line as the average of the 
endpoint y values plus a random offset: 

To approximate fractional Brownian motion, we choose a value for r from a 
Gaussian distribution with a mean of 0 and a variance proportional to I b - a 1 2H, 
where H = 2 - D and D > 1 is the fractal dimension. Another way to obtain a 
random offset is to take r = sr, 1 b - a I ,  with parameter s as a selected "surface- 
roughness" factor, and r$ as a Gaussian random value with mean 0 and variance 
1. Table lookups can be used to obtain the Gaussian values. The process is then 
repeated by calculating a displaced y value for the midposition of each half of the 
subdivided line. And we continue the subdivision until the subdivided line sec- 
tions are less than some preset value. At each step, the value of the random vari- 



la:  

Figure 10-85 
Variations in terrain features modeled with fractional 
Brownian motion. (Courtmyoffa) R. V. Vossand B. B. h n d d b m t ,  
adnpted fmm The Ftsdal Gometry of Nature by h i t  B. Mrmdelbmt 
(New Yo*. W. H.  F m n  and G., 1983); and (b) and (c) Km 
Musgmw and Benoif B. MPndelbmt, MPfhmtcltics and Compuln 
Science, Yale Uniarsify.)  



Section 10-18 

Fractal-Geometry Methods 

Figure 10-86 
A scene modeled with fractal clouds and mountains. 
(Courtby of Ken M u s p v e  and & M i l  0. Manddbrot, 
MathrmPtics and Computer Scimu, Yale UniwrsiIy.) 

Figrrre 10-87 
Random midpointdisplacement of a straight-line segment. 

able r decreases, since it is proportional to the width I b - a I of the line section to 
be subdividd.  Figure 10-88 shows a kactal curve obtained with this method. 

Terrain features are generated by applying the random midpointdisplace 
ment procedures to a rectangular ground plane (Fig. 10-89). We begin by assign- 
ing an elevation z value to each of the four corners (a, b, c, and d in Fig. 10-89) of 
the ground plane. Then we  divide the ground plane at the midpoint of each edge 
to  obtain the five new grid positions: e, f, g, h, and m. Elevations at midpositions 

Fig~rm 10-89 
A rectangular ground plane (a) is subdivided into four 
equal grid sections Cb) for the first step in a random 
midpoint4isplacement piocedure to calculate terrain 
elevations. 

Figrrrc 10-88 
A random-walk path 
generated from a straight-line 
segment with four iterations 
of the random midpoint- 
displacement procedure. 



Chapter 10 

Three-Dirnens~onal Objm 
Representations 

Flgrrrc 10-90 
Eight surface patches formed 
over a gmund plane at the 
first step of a random 
midpointdisplacement 
pro=edure for generating 
terrain features 

e, f, g, and h of the ground-plane edges can be calculated as the average eleva- 
tion of the nearest two vertices plus a random offset. For example, elevation z ,  at 
midposition e is cal~ulated using vertices a and b, and the elevation at midposi- 
tion f is calculated using vertices b and c: 

Random values re and 1 ,  can be obtained hom a Gaussian distribution with mean 
0 and variance proportional to the grid separation raised to the W power, with H 
= 3 - D, and where D > 2 is the fracta! Jimension for the surface. We could also 
calculate random oflsets as  the product of a surface-roughness factor times the 
grid separation times a table lookup value for a Gaussian value with mean 0 and 
variance 1. The elevahon z ,  of the ground-plane midposition m can be calculated 
using positions e and g, or positions f and h. Alternatively, we could calculate z, 
using the assigned elevations of the four ground-plane corners: 

This process is repeated for each of the four new grid sections at each step until 
the grid separation becomes smaller than a selected value. 

Triangular surface patches can be formed as  the elevations are generated. 
Figure 10-90 shows the eight surface patches formed at the first subdivision step. 
At each level of recursion, the triangles are successively subdivided into smaller 
planar patches. When the subdivision process is completed, the patches are ren- 
dered according to the position of the light sources, the values for other illumina- 
tion parameters, and the selected color and surface texture for the terrain. 

The random midpoint-displacement method can be applied to generate 
other components of a scene besides the terrain. For instance, we could use the 
same methods to obtain surface features for water waves or cloud patterns above 
a ground plane. 

Controlling Terrain ?.,pography 

One way to control the placement of peaks and valleys in a fractal terrain scene 
modeled with a midpoint-displacement method is to constrain the calculated ele- 
vations to certain intervals over different regions of the ground plane. We can ac- 
complish this by setting up  a set of control surfaces over the ground plane, as ilius- 
trated in Fig. 10-91. Then we calculate a random elevation at each midpoint grid 
position on the ground plane that depends on the difference between the control 
elevation and the average elevation calculated for that position. This procedure 
constrains elevations to be within a preset interval about the control-surface ele- 
vations. 

. - . . - - -- - 
1 &urv 70-'?I 
Control surfaces over a ground plane. 



Control surfaces can be used to model existing terrain features in the Rocky s+caion lo18 
Mountains, or some other region, by constructing the plane facets using the e le  FraRalCeom*ry M e i w d s  
vations in a contour plot for a particular +on. Or we could set the elevations 
for the vertices of the control polygons to design our own terrain features. Also, 
control surfaces can have any shape. Planes are easiest to deal with, but we could 
use spherical surfaces or other curve sha'pes. 

We use the random midpoint-displaewent method to calculate grid eleva- 
tions, but now we select random values frronr a Gaussian distribution where the 
mean p and standard deviation o are functions of the control elevations. One 
way to set the values for fi  and o is to make them both pportional to the differ- 
ence between the calculated average elevation and the predefined control eleva- 
tion at each grid position. For example, for grid position e in Fig. 10-89, we set 
the mean and standard deviation as 

where rc, is the control elevation for ground-plane position t, and 0 < s < 1 is a 
p m t  scaling factor. Small values for s (say, s < 0.1) produce tighter conformity 
to the terrain envelope, and larger values of s allow greater fluctuations in terrain 
height. 

To determine the values of the control elevations over a p l a d  control sur- 
face, we first calculate the plane parameters A, B, C, and D. For anjr $round-plane 
position (x ,  y), the elevation in the plane containing that control polygon is then 
calculated as 

Incremental methods can then be used to calculate control elevations over ps i -  
tions in the ground-plane grid. To efficiently carry out these calculations, we first 
subdivide the ground plane into a mesh of xy positions, as shown in Rg. 10-92. 
Then each polygon control surface is projected onto the ground plane. We can 
then determine which grid positions are within the projection of the control poly- 
gon using procedures similar to those in scan-line area filling. That is, for each y 
"scan Line" in the ground-plane mesh that crosses the polygon edges, we o h -  
late xan-line intersections and determine which grid positions are in the interior 
of the projection of the control polygon. Calculations for the control elevations at 
those grid positions can then be performed incrementally as 

. ! 
Figure 10-92 
Projection of a hiangular control 
surface onto the ground-plane grid. 



Figure 10-93 
A composite scene modeled with a random 
midpoint-displacement method and planar control 
surfaces over a ground plane. Surface features for 
the terrain, water, and clouds were modeled and 
rendered separately, then combined to form the 
composite picture. (burlay of Eng-Ut Koh, lnfonnotion 
Tmhndogy Instilute, RrpuMic of Singpporr.) 

with Ax and Ay as the grid spacing in the x and y directions. This procedure is 
particularly fast when parallel vector methods are applied to process the control- 
plane grid positions. 

Figure 1&93 shows a scene constructed using control plares to structure the 
surfaces for the terrain, water, and clouds above a ground plane. Surface-render- 
ing algorithms were then applied to smooth out the polygon edges and to pro- 
vide the appmpriate surface colors. 

Self-squaring Fractals 

Another method for generating fractal obpcts is to repeatedly apply a transfor- 
mation function to points in complex space. In two dimensions, a complex num- 
ber can be represented as z = x + iy, where x and y are real numbers, and i 2  = 
-1. In three-dimensional and fourdimensional space, points are represented 
with quatemions. A complex squaring function f l z )  is one that involves the calcu- 
lation of z2, and we can use some self-squaring functions to generate fractal 
shapes. 

Depending on the initial position selected for the iteration, repeated appli- 
cation of a self-squaring function will produce one of three possible results (Fig. 
1&94): 

The transformed position can d~verge to infinity. 
The transformed position can converge to a finite limit point, called an at- 
tractor. 
The transformed position remains on the boundary of some object. 

As an example, the nonfractal squaring operation flz) = z2 in the complex plane 
transforms points according to their relation to the unit circle (Fig. 10-95). Any 



-- 
Figure 10-94 
Possible outcomes of a self- 
squaring transformation f(z) in the 
complex plane, depending on the 
position of the selected initial 
position. 

point z whose magnitude I z 1 is greater than 1 is transformed through a sequence 
of positions that tend to infinity. A point with 1 z I < 1 is transformed toward the 
coordinate origin. Points on the circle, I z  1 = 1, remain on the circle. For some 
functions, the bounda~y between those points that move toward infinity and 
those that tend toward a finite limit is a fractal. The boundary of the fractal object 
is called the julia set. 

In general, we can locate the fractal boundar~es by testing the behavior of 
selected positions. If a selected position elther diverges to infinity or converges to 
an attractor point, we can try another nearby position. We repeat this process 
until we eve&ually locate a position on the fractal boundary. Then, iteration of 
the squaring transformation generates the fractal shape. For simple transforma- 
tions in the complex plane, a quicker method for locating positions on the fractal 
curve is to use the inverse of the transformation function. An initial point chosen 
on the inside or outside of the curve will then converge to a position on the frac- 
tal curve (Fig. 10-96). 

A function that is rich in fractals is the squaring transformation 

where A is assigned any constant complex value. For this function, we can use 
the inverse method to locate the fractal curve. We first rearrange terms to obtain 
the quadratic equation: 

The inverse transformation is then the quadratic formula: 

Using complex arithmetic operations, we solve this equation for the real and 
imaginary parts of z as 

Figure 10-95 
A unit circle in the complex 
plane. The nonfractal, 
complex squaring function 
f ( z )  = z2 moves points that 
are inside the circle toward 
the origin, while points 
outside the circle are moved 
farther away from the circle. 
Any initial point on the circle 
remains on the circle. 

- :a a Fi,yrrw Locating 10-96 the fractal boundary with 
theinverse, self-squaring function 
=' = f- '(2). 



Chapter 10 

Three.Dimens~onal Oblecr 
Reprerentations 

+ 1 I discr 1 - Re(discr) y =- Im(z) = - 
2 i 2 

with the discriminant of the quadratic formuIa as Jiscr = 1 - &'/A. A few initial 
values for x and y (say, 10) can be calculated and discarded before we b e p  to 
plot the fractal curve. AIso, since this function yields two possible transformed 
(x ,  y) positions, we can randomly choose either the plus or the minus sign at each 
step of the iteration as long as Im(disa) 2 0. Whenever Im(discr) < 0, the two 
possible positions are in the second and fourth quadrants. In this case, x and y 
must have opposite signs. The following procedure gives an implementation of 
this self-squaring function, and two example curves are plotted in Fig. 10-97. 

typedef struct { 
float x ,  y; 

) Complex; 

void calculatepoint (Complex lambda. Complex 2 )  
i 
float lambdaMagSq, discrMag; 
Complex discr: 
static Complex fourOverLambda = ( 0. 0 1 ;  
static firstpomt = TRUE; 

l f  (firstYoint) I 
/ ^  c'ompute 4 divided by lambda ' /  
lam5daMagSq = 1ambda.x ' lamh5a.x + 1ambda.y ' 1ambda.y; 
four0verLambda.x = 4 ' 1ambda.x / 1ambdaMagSq; 

- - . - . - - -- - -- - - . -- - - . . - - . - . - - -- - 
F ~ p r i ~  10-97 
Two fractal curves generated with the inverse of the funchon f(z) = &(I -2) by procedure 
self Square. (a) A = 3 and (b) A = 2 + i. Each curve is plotted with 10,000 points. 



four0veriambda.y = - 4  * 1ambda.y / lambdaMagSq: 
firstPoin: : FALSE; 

discr .x  = 1.0 - (z->x ' four0verLambda.x - z->y four(hrer- 
 lambda.^); 

discr .y  = z->x four0verLambda.y + z->y fourGverLamWa.x: 
dlscrMag = sqr t  ( d i s c r . ~  d i sc r .x  + discr .y  ' disc r .y ) ;  

/ '  Update z ,  checking to avoid the sq-rt of a negative number * /  
i f  ( d i s c r a g  + d i s c r . x  < 0) 

z->x = 0; 
else  

z->x = sqrt  ((discrMag + disc r .x )  / 2 .0) ;  
i f  (discrMag - di5cr .x c 0) 

z-=-y = 0: 
e lse  

z->y = 0 . 5  sqr t  ((discrMag - d i s c r . ~ )  / 2.0):  
/ '  For half the points,  use negative root, placing point in  quad- 

rant 3 ./ 
i f  (random0 .c MAXINT/2) ( 

z->X = -2->x:  
z->y = - 2 - > y ;  

1 

/ +  When imaginary part of discriminant i s  negative, point 
should l i e  i n  quadrant 2 or 4 ,  so reverse sign of x ' /  

if (d i sc r .y  < 0) z->x = -2-zx; 

/ *  Finish up calculation for  the real  part of z * /  
z->x = 0.5 (1 - z->XI; 

void selfsquare (Compiex lambda, Complex z, i n t  count) 
( 

i c t  k: 

I *  Skip the first few points * /  
fcr  (k=O; kclo; k + + )  

calculatepoint (lambda, &zi ; 
for (k=O; kccount; k + + )  ( 

calculatepoint (lambda, &z1; 
I *  Scale point t o  fit window and draw * /  
p o i n t  (Z.x*WINDOW-WIDTH, O.S*WINDOW-HEIGHT+t.y*WIND0WWHEIGHT); 

1 

A three-dimensional plot in variables x, y, and A of the self-squaring func- 
tion f(z) = Az(1 -z), with I A ] = 1, is given in Fig. 10-98. Each cross-sectional slice 
of this plot is a fractal curve in the complex plane. 

A very famous fractal shape is obtained from the Mandelbrot set, which is 
the set of complex values z that do not diverge under the squaring transforma- 
tion: 

That is, we first select a point z in thecomplex plane, then we compute the trans- 
formed position z2 + z. At the next step, we square this transformed position and 
add the original z value. We repeat this procedure until we can determine 



Figure 10-98 
The function f ( z )  = b ( 1 - 2 )  
plotted in three dimensions 
with normalized A values 
plotted as the vertical axis. 
(Courtesy of Ahn Norton, IBM Research.) 

whether or not the transformation is diverging. The boundary of the convergence 
region in the complex plane is a hactal. 

To implement transformation 10-109, we first choose a window in the com- 
plex plane. Positions in this window are then mapped to color-coded pixel posi- 
tions in a selected screen viewport (Fig. 10-99). The pixel colors are chosen ac- 
cording to the rate of divergence of the corresponding point in the complex plane 
under transformation 10-109. If the magnitude of a complex number is greater 
than 2, then it will quickly diverge under this self-squaring operation. Therefore, 
we can set up a loop to repeat the squaring operations until either the magnitude 
of the complex number e x c d s  2 or we have reached a preset number of itera- 
tions. The maximum number of iterations is usually set to some value between 
100 and 1000, although lower values can be used to speed up the calculations. 
With lower settings for the iteration limit, however, we do tend to lose some de- 
tail along the boundary (Julia set) of the convergence region. At the end of the 
loop, we select a color value according to the number of iterations executed by 
the loop. For example, we can color the pixel black if the iteration count is at the 

Viewpan 

. - - -. - -. - - - . . . 
Figrrre 10-99 
Mapping positions i n  the complex plane to color-coded pixel positions 
on a video monitor. 

/ 

\ 
/ pixel 

0 
1 

real < 

imaginary 

Window- 

i 



maximum value, and we can color the pixel red if the iteration count is near 0. Section 10-18 
Other color values can then be chosen according to the value of the iteration Fractal-Geometrv Method, 
count within the interval from 0 to the maximum value. By choosing different 
color mappings, we can generate a variety of dramatic displays for the Mandel- 
brot set. One choice of color coding for the set is shown in Fig. 10-100(a). 

An algorithm for displaying the Mandelbrot set is given in the following 
procedure. The major part of the set is contained within the following region of 
the complex plane: 

We can explore the details along the boundary of the set by choosing succ&sively 
smaller window regions s o  that we can zoom in on selected areas of the display. 
Figure 10-100 shows a color-coded display of the Mandelbrot set and a series of 
zooms that illustrate some of the features of this remarkable set. 

Figrire 10-100 
Zooming in on the Mandelbrot set. Starting with a display of the Mandelbrot set (a), we 
zoom in on selected regions (b) through (0. The white box outline shows the window area 
selected for each successive zoom. (Courtesy of Brian Evans. Vanderbill University.) 



typedef strust ( float A .  y; ) Lomplex, 

csp.x = c.x ' ; .x - . . y  ' c . y ;  
csq.y = 2 ' c . x  ' i..:.: 
return (cSq); 

, I 

int iterate ICom;)lex :.nit, int rnaxIter) 
( 
Complex z = zI?.ic: 
int cnt = 0; 

I *  Quit when z ' z - 4 ' I  
while ((2.x ' z . x  :.y ' z . y  < =  1 . 0 1  h h  (cnt < maxIter)) ( 

2 = comp1exS:jllare 2 1 ;  
z.x += z1nlt.x: 
z.y += z1nlc.y; 
cnt++; 

) 

recurn (cnt) ; 
1 

void mandelbrot (int nx, int ny, inc maxIter, float realMin, 
float realMax, float imagMin, floac imagMax) 

( 
float realInc = (realnax - realMln) 1 nx; 
float imagInc = (imagkax - imagMin) i ny; 
Complex z; 
 in^ x,  y; 
int cnt; 

for (x=O, z.x=realKi~; x < m ;  x++, z.x+=realInc) 
for (y=O, z.y=imaqYin; y<r,y: y++, z.y+=imagIncl ( 
cnt = itesdte ( z  rnaxTteri, 
if (cnt == maxIterl 
setcolos ( B L A C K ) ;  

else 
setColor (cnt I . 

pPoint (x, y ) ;  
) 

I 

Complex-function transformations, such as Eq. 10-105, can be extended to 
produce fractal surfaces and fractal solids. Methods for generating these objects 
use quaternion representations (Appendix A) for transforming points in three- 
dimensional and fourdimensional space. A quaternion has four components, 
one real part and three imaginary parts, and can be represented as an extension 
of the concept of a number i n  the complex plane: 

wherei2 = j2 = k2 = -1 . Th e real part sisalsoreferred toas thescalarpart of thequater- 
nion,and the imaginary terms arecalled thequaternion vector part v = (0, b,c). 



Using the rules for quatemion multiplication and addition discussed in Ap- seaion 10-16 
pendix A, we can apply selfsquaring functions and other iteration methods to Fractal-Geometry Methods 
generate surfaces of fractal obiects instead of fractal curves. A basic procedure is 
Fb start with a position inside A fractal object and generate successive'points from 
that position until an exterior (diverging) point is idenaed.  The previous inte- 
rior point is then retained as a surface point. Neighbors of this surface point are 
then tested to determine whether they are inside (converging) or outside (diver- 
rging). Any inside point that connects to an outside point is asurface point. In 
this way, the procedure threads its way along the fractal boundary without gen- 
erating points that axe too far from the surface. When four-dimensional fractals 
are generated, three-dimensional slices are p r o w  onto the two-dimensional 
surface of the video monitor. 

Procedures for generating self-squaring fractals in four-dimensional space 
require considerable computation time for evaluating the iteration function and 
far '  testing points. Each pdint on a surface can be represented as a small cube, giv- 
ing the inner and outer limits of the surface. Output from such programs for the 
three-dimensional projections of the fractal typically contain over a million ver- 
tices for the surface cubes. Display of the fractal o b e s  is performed by applying 
illumination models that determine the lighting and color for each surface cube. 
Hidden-surface methods are then applied so that only visible surfaces of the ob- 
pcts are displayed. Figures 10-101 and 10-102 show examples of self-squaring, 
four-dimensional fractals with propctions into three-dimensions. 

Self-Inverse Fractals 

Various geometric inversion transformations can be used to create fractal shapes. 
Again, we start with an initial set of points, and we repeatedly apply nonlinear 
inversion operations to transform the initial points into a fractal. 

As an example, we consider a two-dimensional inversion transformation 
with respect to a circle with radius r and center at position Po = (xo, yo). Any 
point P outside the circle will be inverted to a position P' inside the circle (Fig. 
10-103) with the transformation 

-- 
(PoP)(Pd") = 9 

? -- 

Figrm 10-101 
Threedimensional projections of fourdimensional fractals generated with the self- 
squaring, quatemion function f(q) = Aq(1 -q): (a) A = 1.475 + 0.906li,and (b) A = -0.57 + i .  
(Courtcry of Alan Norlnn, IBM Rrsmrch.) 385 



Chapter 10 

Three-Dimensional Object 
Representations 

Figure 10-102 
A threedimensional surface propction of a four- 
dimensional object generated with the self- 
squaring, quatemion function f(q) = q2 - 1. 
(Courtesy of Alrrn Norton, IBM Research.) 

Figure 10-1 03 
Inverting point P to a position P' 
inside a circle with radius r. 

Reciprocally, this trai~sformation inverts any point inside the circle to a point out- 
side the drcle. Both P and P' lie on a straight line passing through the drcle cen- 
ter P,. 

If the coordinates of the two points are P = ( x ,  y) and P' = (Y, y'), we can 
write Eq. 10.111 as 

Also, since the two points lie along a line passing through the circle center, we 
have (y - yo)/(x - q) = (y' - yo)/(i - x0) Therefore, the transformed coordi- 
nate values are 

Figure 10-104 illustrates the inversion of points along another circle bound- 
ary. As long as the circle to be inverted does not pass through Po, it will transform 
to another circle. But if the circle chmference passes through Po, the circle 



Invemd 
Circle L.. 

. Original 
Circb 

Figum 10-104 
Inversion of a circle with respect to 
another circle. 

transforms to a straight line. Conversely, points along a straight line not passing 
through Po invert to a circle. Thus, straight lines are invariant under the inversion 
transformation. Also invariant under this transformation are circles that are or- 
thogonal to the ~ f m n c e  drcle. That is, ihe tangents of the two cifiles are perpen- 
dicular at the intersection points. 

We can create various fractal shapes with this inversion transformation by 
starting with a set of &les and repeatedly applying the transformation using 
different reference circles. Similarly, we can apply &le inversion to a set of 
straight lines. Similar inversion methods can be developed for other ob-. And, 
we can generalize the procedure to spheres, planes, or other shapes in three-di- 
mensional space. 

10-1 9 
SHAPE GRAMMARS AND OTHER P~OCEDURAL METHODS 

A number of other procedural methods have been developed for generating ob- 
pct details. Shape grammars are sets of production rules that can be applied to 
an initial obpct to add laye& of detail that are harmonious with the original 
shape. Transformations can be applied to alter the geometry (shape) of the object, 
or the transformation rules can be applied to add surface-color or surfacetexture 
detail. 

Given a set of productioll rules, a shape designer can then experiment by 
applying different rules at each step of the transformation from a given initial ob 
Wt to the final structure. Figure 10-105 shows four geometric substitution rules 
for altering triangle shapes. The geometry transformations for these rules can be 

Rule 1 Rule 2 Rula 4 

~ ' I S I I ~ P  10- 105 
Four geomehic substitution rules for subdividing and altering the shape 
of an equilateral triangle. 



Chapter 10 written algorithmically by the system based on an input picture drawn with a 
Three-Dimensional Objecr production-rule editor. That is, each rule can be described graphically by show- 

Representations ing the initial and fmal shapes. lmplementations can then be set up  in Mathemat- 
ica or some other programming langirage with graphics capability. 

An application of the geometric substitutions in Fig. 10-105 is given in Fig. 
10-206, where Fig. 10-106(d) is obtained by applying the four rules in succession, 
starting with the initial hiangle in Fig. 10-106(a). Figure 10-107 shows another 
shape created with triangle substitution rules. 

figure 10-106 
An equilateral hiangle (a) is 
converted to shape (bl using 
substitution rules 1 and 2 in Fig. 
10-105. Rule 3 is then used to 
convert (b) into shape (c), which in 
hun is transformed to (d) using 
rule 4. (Copyright 8 1992 Andmo 
Ghssner, Xerox PARC (Palo Alto Restnrch 

Fiprre 10-107 
A design created with geometric 
substitution rules for altering 
hiangle shapes. (Copyright 0 1992 
Andrew Gbssner, X m x  PARC (Palo Alto 
Rrsnrrch CmtrrJ.) 



Figure 10-108 
A design created with geomehic 
substitution rules for altering prism 
shapes. The initial shape for this 
design was a representation of 
Rubik's Snake. (Copyright O 1992 
Andrew Clnssner, Xnox PARC (Palo Allo 
Resmrch Center).) 

Thmedimensional shape and surface features are transformed with similar 
operations. F i p  10-108 shows the results of geometric substitutions applied to 
polyhedra. The initial shape for the objects shown in Figure 10-109 is an icosahe- 
dron, a polyhedron with 20 faces. Geometric substitutions were applied to the 
plane faces of the icosahedron, and the resulting polygon vertices were projected 
to the surface of an enclosing sphere. 

Another example of using production rules to describe the shape of objects 
is L-grammars, or gratals. These rules provide a method for describing plants. For 
instance, the topology of a tree can be described as a trunk, with some attached 
branches and leaves. A tree can then be modeled with rules to provide a particu- 
lar connection of the branches and the leaves on the individual branches. The 
geometrical description is then given by placing the object stmctures at particular 
coordinate positions. 

Figure 10-110 shows a scene containing various plants and trees, con- 
structed with a commercial plant-generator package. Procedures in the software 
for constructing the plants are based on botanical laws. 

Figure 10- 109 
Designs created on the surface of a 
sphere using triangle substitution 
rules applied to the plane faces of 
an icosahedron, followed by 
projections to the sphere surface. 
(Copyright 8 1992 Andrtw Glassner, Xertn 
PARC (Palo Allo Rwarch Cnller).) 



-- - - -  

Figure 10-110 
Realistic scenery generated with the TDI-AMAP software package, which can generate 
over 100 varieties of plants and trees using procedures based on botanical laws. (Courtesy of 
nMnson Digit01 lmpge.) 

10-20 
PARTICLE SYSTEMS 

A method for modeling natural objects, or other irregularly shaped objects, that 
exhibit "fluid-like" properties is particle systeqs. This method is particularly 
good for describing objects that change over time by flowing, billowing, spatter- 
ing, or expanding. Objects with these characteristics include clouds, smoke, fire, 
fireworks, wateifalls, water spray, and dumps of grass. For example, particle sys- 
tems were used to model the planet explosion and expanding wall of fire due to 
the "genesis bomb" in the motion picture Star Trek 11-The Wrath of Khnn. 

Random processes are used to generate objects within some defined region 
of space and to vary their parameters over time. Ai some random time, each ob- 
ject is deleted. During the lifetime of a particle, its path and surface characteris- 
tics may be color-coded and displayed. 

Particle shapes can be small spheres; ellipsoids, boxes, or other shapes. The 
size and shape of particles may vary randomly over time. Also, other properties 
such as transparency, color, and movement all can vary randomly. In 
some applications, particle motion may be controlled by specified forces, such as 
a gravity field. 

As each particle moves, its path is plotted and displayed in a particular 
color. For example, a fireworks pattern can be displayed by randomly generating 
particles w i t h i  a spherical region of space and allowing them to mo;e radially 



Figure 10-111 
Modeling fireworks as a particle system with particles 
traveling radially outward from the center of the 
sphere. 

outward, as in Fig. 10-111. The particle paths can be color-coded from red to yel- 
low, for instance, to simulate the temperature of the exploding particles. Simi- 
larly, realistic displays of grass clumps have been modeled with "trajectory" par- 
ticles (Fig. 10-112) that are shot up from the ground and fall back to earth under 
gravity. In this case, the particle paths can originate withn a tapered cylinder, 
and might be color-coded from green to yellow. 

Figure 10-113 illustrates a particle-system simulation of a waterfall. The 
water particles fall from a fixed elevation, are deflected by an obstacle, and then 
splash up from the ground. Different colors are used to distinguish the particle 

Figurr  10-112 
Modeling a clump of grass by firing 
particles upward within a tapered 
cylinder. The particle paths are 
parabolas due to the downward 
force of gravity. 

Figure 10-113 
~imulatidn of the behavior of a 
waterfall hitting a stone 'circle). The 
water particles are deflected by the 
stone and then splash up from the 
ground. (Courtesy of M. B r m b  and T. L. 
1. H o w r d ,  Dpprrtmmt ofcomputer 
Scrence, Uniwrsi ty  ofManchesler.) 

Sedan 10-21 

Partical Systems 



Chapter 10 

Three-D~mennonal Object 
Represenlal~ons 

paths at each stage. An example of an animation simulating the disintegration of 
an object is shown in Fig. 10-114. The object on  the left disintegrates into the par- 
ticle distribution on the right. A composite scene formed with a variety of repre- 
sentations is given in Fig. 10-115. The scene is modeled using particle-system 
grass, fractal mountains, and texture mapping and other surface-rendering pro- 
cedures. 

Figwe 10- 114 
An object disintegrating into a cloud of particles. (Courtesy of 
Autodesk, lnc.) 

Figure 10-115 
A scene, entitled Road to Point Reyes, showing particle-system grass, 
fractal mountains, and texture-mapped surfaces. (Courtesy of Pirar. 
Copyright 0 1983 Pimr.) 



10-21 
PHYSICALLY BASED MODELING 

A nonrigid object, such as a rope, a piece of cloth, or a soft rubber ball, can be 
represented with physically based modeling methods that describe the behavior 
of the object in terms of the interaction of external and internal forces. An accu- 
rate discription of bhe shape of a terry cloth towel drapped over the back of a 
chair is obtained by considering the effect of the chair on the fabric loops in the 
cloth and the interaction between the cloth threads. 

A common method for modeling a nonrigid object is to approximate the ob- 
ject wjth a network of point nodes with flexible connections between the nodes. 
One simple type of connection is a spring. F~gure 10-116 shows a section of a two- 
dimensional spring network that could be used to approximate thc behavior of a 
sheet of rubber. Similar spring networks can be set u p  in three dimensions to 
model a rubber ball or a block of jello For a homogeneous object, we can use 
identical springs throughout the network. If  we want the object to have different 
properties in different directions, we can use different spring properties in differ- . - .  
ent directions. When external forces are applied to a spring network, the amount 
of stretching or compression of the individual springs depends on the value set 
for the spring conslnnf k, also called the force cot~stant for the spring. 

Horizontal displacement x of a node position under the influence of a force 
F, is illustrated in Fig. 10-117. If the spring is not overstretched, we can closely 
approximate the amdunt of displacemmt x from the equilibrium position using 
Hooke's law: 

where F, is the equal and opposite restoring force of the spring on the stretched 
node. This relationship holds also for horizontal compression of a spring by an 
amount x,  and we have similar relationships for displacements and force compo- 
nents in they and z directions. 

If objects are completely flexible, they return to their original configuration 
when the external forces are removed. But if we want to model putty, or some 
other deformable object, we need to modify the spring characteristics so that the 
springs do not return to their original shape when the evternal forces are re- 
moved. Another set of applied forces then can deform the object In some other 
way. 

/ I k 1 
A (unstretched positon) 
/- 1 

Fw 10-11 7 
A n  external force F ,  pulling on one 
end of a spring, with the other end 
rigidly fixed. 

~ -- 

I ;;;It#,* I l l - I l l ~  
A two-d~mens~onal spring 
network, constructed w t h  
Identical spring constants A 



Chapter 10 Instead of using springs, we can also model the connections between nodes 
Three-Dimensional Object with elastic materials, then we minimize shain-energy functions to determine ob- 

Represen(a'ions ject shape under the influence of external forces. This method provides a better 
model for cloth, and various energy functions have been devised to describe the 
behavior of different cloth materials. 

To model a nonrigid object, we first set up the external forces acting on the 
object. Then we consider the propagation of the forces throughout the network 
representing the object. This leads to a set of simultaneous equations that we 
must solve to determine the displacement of the nodes throughout the network. 

Figure 10-118 shows a banana peel modeled with a spring network, and the 
scene in Fig. 10-119 shows examples of cloth modeling using energy functions, 
with a texturemapped pattern on one cloth. By adjusting the parameters in a 
network using energy-function calculations, different kinds of cloth can be mod- 
eled. Figure 10-120 illushates models for cotton, wool, and polyester cotton mate- 
rials draped over a table. 

Physically based modeling methods are also applied in animations to more 
accurately describe motion paths. In the past, animations were often specdied 
using spline paths and kinematics, where motion parameters are based only on 

Figure 10-118 
Modeling the flexible behavior of a 
banana peel with a spring network. 
(Copyrighl O 1992 David Laidlaw, Iohn 
Snyder, Adam Woodbury. and Alan Ban, 
Computer Graphics Lab, California Institute 
o f  Technology.) 

Fixrrt, 10-119 
Modeling the flexible behavior of 
cloth draped over furniture using 
energy-function minimization. 

G (Copyriglil O 1992 Gene Grcger and David 
E .  Brtvn, Design Remrch Center, 
Rensdaer Polytechnic Inslitule.) 



Sertion 10-22 

7. Visualization of Data Sets 

Figure 10-120 
Modeling the characteristics of (a) cotton, (b) wool, and (r )  polyester 
cotton using energy-function minimization. (Copyright 0 1992 David E.  
Brmn and Donald H. Housc, Design Rewrch Center, Rcnwlncr Polyfechnic Institute.) 

position and velocity. Physically based modeling describes motion using dynam- 
ical equations, involving forces and accelerations. Animation descriptions based 
on the equations of dynamics produce more realistic motions than those based on 
the equations of kinematics. 

10-22 
VISUALIZATION OF DATA SETS 

The use of graphical methods as an aid in scientific and engineering analysis is 
commonly r e f e d  to as scientilic visualization. This involves the visualization 
of data sets and processes that may be difficult or impossible to analyze without 
graphical methods. For example, visualization techniques are needed to deal 
with the output of high-volume data sources such as supercomputers, satellite 
and spacecraft scanners, radio-astronomy telescopes, and medical scanners. Mil- 
lions of data points are often generated from numerical solutions of computer 
simulations and from observational equipment, and it is difficult to determine 
trends and relationships by simply scanning the raw data. Similarly, visualization 
techniques are useful for analyzing processes that occur over a long time period 
or that cannot be observed directly, such as quanhun-mechanical phenomena 
and special-relativity effects produced bypbpds traveling near the speed of 
light. Scientific visualization uses methods from computer graphics, image p n  
cessing, computer vision, and other areas to visually display, enhance, and ma- 
nipulate information to allow better understanding of the data. Similar methods 
employed by commerce, industry, and other nonscientific arras an? sometimes re- 
ferred to as business visualization. 

Data sets are classifkl according to their spatial distribution and according 
to data type. Two-dimensional data sets have values distributed over a surface, 
and three-dimensional data sets have values distributed over the interior of a 
cube, a sphere, or some other region of space. Data types include scalars, vectors, 
tensors, and multivariate data. 

Visual Representations for Scalar Fields 

A scalar quantity is one that has a single value. Scalar data sets contain values 
that may be distributed in time, as well as over spatial positions. Also, the data 



Chaper 10 values may be functions of other scalar parameters. Some examples of physical 
Three-Dimens~onal Object scalar quantities are energy, density, mass, temperature, pressure, charge, resis- 

Representations tance, reflectivity, frequency, and water content. 
A common method for visualizing a scalar data set is to use graphs or 

charts that show the distribution of data values as a function of other parameters, 
such as position and time. If the data are distributed over a surface, we could plot 
the data values as vertical bars rising up  from the surface, or we can interpolate 
the data values to display a smooth surface. Pseudo-color methods are also used 
to distinguish different values in a scalar data set, and color-coding techniques 
can be combined with graph and chart methods. To color code a scalar data set, 
we choose a range of colo& and map the range of data values to the color range. 
For example, blue could be assigned to the lowest scalar value, and red could be 
assigned to the highest value. Figure 10-121 gives an example of a color-coded 
surface plot. Color coding a data set can be tricky, because some color combina- 
tions can lead to misinterpretations of the data. 

Contour plots are ;sed to display isolines (lines of constant scalar value) for 
a data set distributed over a surface. The isolines are spaced at some convenient 
interval to show the range and variation of the data values over the region of 
space. A typical application is a contour plot of elevations over a ground plane. 
Usually, contouring methods are applied to a set of data values that is distributed 
over a regular grid, as in Fig. 10-122. Regular grids have equally spaced grid 
lines, and data values are known at the grid intersections. Numerical solutions of 
computer simulations are usually set up to produce data distributions on a regu- 
lar grid, while observed data sets are often irregularly spaced. Contouring meth- 
ods have been devised for various kinds of nonregular grids, but often nonregu- 
lar data distributions are converted to regular grids. A two-dimensio&d 
contouring algorithm traces the isolines from cell to cell within the grid by check- 
ing the four corners of grid cells to determine which cell edges are crossed by a 

Fig~tri. 10-121 
A financial surface plot showing 
stock-growth ptential during the 
October 1987 stock-market crash. 
Red indicates high returps, and the 
plot shows that low-growth stocks 
performed better in the crash. 
(Courtesy of EngXmf Koh. lnformlrot~ 
Technology Institute, Repirblic of 
Singapore.) 

Fi~irrv 10-122 
A regular, two-dimensional grid 
with data values at the intersection 
of the grid lines. The r grid lines 
have a constant Ax spacing, and the 
y grid lines have a constant Ay 
spacing, where the spacing in the x 
and y directions may not be the 
same. 



particular isoline. The isolines are usually plotted as straight-line sechons across M i o n  10-22 
each cell, as illustrated in Fig. 10-123. Sometimes isolines are plotted with spline wsual~zat~on of Data Sets 
curves, but spline fitting can lead to inconsistencies and misinterpretation of a 
data set. For example, two spline isolines could m, or curved isoline paths 
might not be a true indicator of the data trends since data values are known only 
at the cell comers. Contouring packages can allow interactive adjustment of iso- 
lines by a researcher to correct any inconsistencies. An example of three, overlap 
ping, colorcoded contour plots in the xy plane is given in Fig. 10-124, and Fig. 
10-125 shows contour lines and color coding for an irregularly shaped space. 

For three-dimensional scalar data fields, we can take cms-sectional slices 
and display the two-dimensional data distributions over the slices. We could ei- 

@ 
ther color code the data values over a slice, or we could display isolines. Visual- Figure 10-123 
ization packages typically provide a slicer routine that allows cross sections to be The path of an isoline across 

five grid cells. 

I 
I 

Figure 10-124 
Color-cded contour plots for three 1 data sets within the sm region of 
the xy h e .  (Courtesy ofthe National 1 Center; ~ u p y r p u t i n ~  t$#icutims, 
unimmity o f n l r n ~ ~  at urkna- 

' Ckumplign.) 

Figurn 10-125 
Color-coded contour plots over the surface of an 
apple-coreshaped region of space. (Courtesy of Grrg 
Nrlson, Deprlrnent of Computer Scicnce and Engmnm~ng, 
Arizona Slate Unitfcrsity.) 



, dimensional data set. (Courlesy of 
I_- - Spyglass, Inc.) 

Figure 10-126 
Cross-sectional slices of a three- 

taken at any angle. Figure 10-126 shows a display generated by a commercial 
slicerdicer package. 

Instead of looking at two-dimensional cross sections, we can plot one or 
more isosurfaces, which are simply three-dimensional contour plots (Fig. 10- 
127). When two overlapping isosurfaces are displayed, the outer surface is made 
transparent so that we can view the shape of both isosurfaces. Constructing an 
isosurface is similar to plotting isolines, except now we have three-dimensional 
grid cells and we need to check the values of the eight comers of a cell to locate 
sections of an isosurface. Figure 10-128 shows some examples of isosurface inter- 
sections with grid cells. Isosurfaces are modeled with triangle meshes, then sur- 
face-rendering algorithms are applied to display the final shape. 

F i p r r  10-127 
An isosurface generated from a set 

-ehvad-content values obtained 
from a numerical model of a 
thunderstorm. (Courr~sy bf Bob 
Wilhelmson. D q r f m e n t  of Almospher~r 
Sciences ami Nalional Center for 
Supercomp~rting Applicol~ons, Uniivrsity of  
lllinois at Urbana Champaign.) 

F i p r v  10- 12s 
lsosurface intersections with grid cells, modeled with triangle patches 



Volume rendering, which is often somewhat like an X-ray picture, is an- 
other method for visualizing a three-dimensional data set. The interior informa- 
tion about a data set is projected to a display screen using the ray-casting meth- 
ods introduced in Section 10-15. Along the ray path from each screen pixel (Fig. 
70-1291, interior data values are examined and encoded for display. Often, data 
values at the grid positions. are averaged so that one value is stored for each 
voxel of the data space. How the data are encoded for display depends on the ap- 
plication. Seismic data, for example, is often examined to find the maximum and 
minimum values along each ray. The values can then be color coded to give in- 
formation about the width of the interval and the minimum value. In medical ap- 
plications, the data values are opacity factors in the range from 0 to 1 for the tis- 
sue and bone layers. Bone layers are completely opaque, while tissue is 
somewhat transparent (low opacity). Along each ray, the opacity factors are accu- 
mulated until either the total is greater than or equal to 1, or until the ray exits at 
the back of the three-dimensional data grid. The accumulated opacity value is 
then displayed as a pixel-intensity level, which can be gray scale or color. Figure 
10-130 shows a volume visualization of a medical data set describing the struc- 
ture of a dog heart. For this volume visualization, a color-coded plot of the dis- 
tance to the maximum voxel value along each pixel ray was displayed. 

Section 10-22 

Virualiralion ol Dala Sets 

F i p r c  10-129 
Volume visualization of a regular, Cartesian data grid using 
ray cashng to examine interior data values. 

Volume visualization of a data 
set for a dog heart, obtained by 
plotting the color-coded distance to 
the maximum voxel value for each 
pixel. (Courfesy of Patrick Moran arid 
Cltnte~i Potter, Naliorrl Crnter for 
Sii~rcorrrputiii~ Applicatiorts, Univrrsity 
rif Illrrias rl Urhrrnn-Chntnpip 



Chapla 10 Visual Representations for Vector Fields 
Three-Dimensional Object 

~~~~~~~~~~i~~ A vector quantity V in three-dimensional space has three scalar values ( V ,  V,, 
V J ,  one for each coordinate direction, and a two-dimensional vector has two 
components (V,, V,). Another way to describe a vector quantity is by giving its 
magnitude I V  I and its d M o n  as a unit vector u. As with scalars, vector quan- 
tities may be functions of position, time, and other parameters. Some examples of 
physical vector quantities are velocity, acceleration, force, electric fields, magnetic 
fields, gravitational fields, and electric current. 

One way to visualize a vector field is to plot each data point as a small 
armw that shows the magnitude and direction of the vector. This method is most 
often used with cross-sectional slices, as in Fig. 10-131, since it can be difficult to 
see the data trends in a three-dimensional region cluttered with overlapping ar- 
rows. Magnitudes for the vector values can be shown by varying the lengths of 
the arrows, or we can make all arrows the same size, but make the arrows differ- 
ent colors according to a selected color coding for the vector magnitudes. 

Figure 10-131 
Arrow representation for a vector field over cross- 
sectiod slices. (Courfcry of the Nationul Cmln for 
Supmmrputing AppIimtions, U n i a i t y  of Illinois at Urhnu-  
C h m p i g n . )  

We can also represent vector values by plotting field lines or streamlines. 
Field lines are commonly used for electric, magnetic, and gravitational fields. The 
magnitude of the vector values is indicated by the spacing between field lines, 
and the direction is the tangent to the field, as shown in Fig. 10-132. An example 
of a streamline plot of a vector field is shown in Fig. 10-133. Stmadines can be 
displayed as wide arrows, particularly when a whirlpool, or vortex, effect is pre 
sent. An example of this is given in Fig. 10-134, which displays swirling airflow 
patterns inside a thunderstorm. For animations of fluid flow, the behavior of the 
vector field can be visualized by tracking particles along the flow direction. An 

Figure 10-132 
t higher Field-line representation for a 

lower vector data set. 



Seclion 10-22 

Vnsualizat~on of Data Sets 

-- 

Fyrrrr. 10-1.33 
Visualizing airflow around a 
cylinder with a hemispherical cap 
that is tilted slightly relative to the 
incoming direction of the airflow. 
(Courtesy of M. Gerald-Yomasaki, 1. 
Huiltquist,and Sam Uselfon, NASA Ames 
Resemch Center.) 

F i g w e  10-134 
Twisting airflow patterns, 
visualized with wide strezmlines 
inside a transparent isosurface plot 
of a thunderstorm. (Courtesy of Bob 
Wilhelmson, Drplrlment of Alrnosphtrac 
Sciencesand Natioml Cmterfor 
Supermmputing Appl~cotions, Uniwrsily 
of lllinois at Urhm-Chompign . )  

- 
F i p r e  10-135 
Airflow patterns, visualized with 
both streamlines and particle 
motion inside a transparent 
isosurface plot of a thunderstorm. 
Rising sphere particles are colored 
orange, and falling sphere particles 
are blue. (Courtesy of Bob W ~ l h e l m ~ n ,  
Dcprrmnmr oJ Ahnospheric Sciences and 
Natioml h f e r  (or Supercomputing 
Applicatwns, U n k i l y  of Illinois at 
UrtsmChampign.) 

example of a vector-field visualization using both streamlines and particles is 
shown in Fig. 10-135. 

Sometimes, only the magnitudes of the vector quantities are displayed. This 
is often done when multiple quantities are to be visualized at  a single position, or 
when the directions d o  not vary much in some region of space, or when vector 
directions are of less interest. 

Visual Representations ior Tensor Fields 

A tensor quantity in three-dimensional space has nine components and can be 
represented with a 3 by 3 matrix. Actually, this representation is used for a sec- 
ond-order tensor, and higher-order tensors d o  occur in some applications, particu- 
larly general relativity. Some examples of physical, second-order tensors are 



Chapter 10 stress and s h a h  in a material subjected to external forces, conductivity (or resis- 
Three-Dimensional Object tivity) of an electrical conductor, and the metric tensor, which gives the proper- 

Representations ties of a particular coordinate space. The stress tensor in Cartesian coordinates, 
for example, can be represented as 

Tensor quantit~rs rlre frequently encountered in anisotropic materials, 
which have diiferent properties in different'directions. The x, xy, aid  xz elements 
of the conductivity tensor, for example, describe the contributions of electric field 
components in the x, y, and z diredions to the current in the x direction. Usually, 
physical tensor quantities are symmetric, so that the tensor has only six distinct 
values. For instance, the xy and yx components of the stress tensor are the same. 

Visualization schemes for representing all six components of a second-order 
tensor quantity are based on devising shapes that have six parameters. One 
graphical representation for a tensor is shown in Fig. 10-136. The three diagonal 
elements of the tensor are used to construct the magnitude and direction of the 
arrow, and the three offdiagonal tenns are used to set the shape and color of the 
elliptical disk. 

Instead of trying to visualize all six components of a tensor quantity, we can 
reduce the tensor to a vector or a scalar. Using a vector representation, we can 
simply display a vector representation for the diagonal elements of the tensor. 
And by applying tmsorumtraction operations, we can obtain a scalar reprpsenta- 
tion. For example, stress and strain tensors can be contracted to generate a scalar 
strain-energy density that can be plotted at points in a material subject to external 
forces (Fig. 10-137). 

Visual Representations for Multivariate Data Fields 

In some applications, at each grid position over some region of space, we may 
have multiple data values, which can be a mixture of scalar, vector, and even ten- 

F i p r c  10-136 
Representing shf fs  and strain tensors with an 
elliptical disk and a rod over the surface of a 
stressed material. (Courtesy ofBob tlp&r, N a t i o ~ l  h t n  for 
Supmumpuling Appliations, U n b i t y  o~1llinois at Urtum- 
Chanqmign.) 



I 
Visualization of Data Sets 

Figure 10-137 
Representing stress and strain tensors with a strain- 
energy density plot in a visualization of crack 
propagation on the surface of a stressed material. 
(Courtesy of Bob Hok,  National Center for Supncomputing 
Applicutions, University of  Jllinois at U r h n a - C h a m p i p . )  

sor values. As an example, for a fluid-flow problem, we may have fluid velocity, 
temperature, and density values at each three-dimensional position. Thus, we 
have five scalar values to display at each position, and the situation is similar to 
displaying a tensor field. 

A method for displaying multivariate data fields is to construct graphical 
objects, sometimes referred to as glyphs, with multiple parts. Each part of a 
glyph represents a physical quantity. The size and color of each part can be used 
to display information about scalar magnitudes. To give directional information 
for a vector field, we can use a wedge, a cone, or some other pointing shape for 
the glyph part representing the vector. An example of the visualization of a mul- 
tivahite d k a  fieid using a-glyph structure at selected grid positions is shown in 
Fig. 10-138. 

One frame from an animated visualization of a 
multivariate data field using glyphs. The wedge- 
shaped part of the glyph indicates the directionof a 
vector quantity at each point. (Courtesy of the Nrtlionol 
Crllter for Supercornpsling A}ydicn!rons, Uniwrcrly of lllh,nrs a t  
llrbnrra-Champnip.) 



Chapter 10 op 

Three-D~nimvonal Ob~ect 
S U M M A R Y  

~, 
Rqxewntamn, 

M a n y  representations have  been deve loped  for  m o d e l i n g  the  w i d e  variety o f  ob-  
jects that  m i g h t  b e  d isp layed  in a g raph ics  scene. "Standard g r a p h ~ c s  objects" arc 
those represented w i t h  a sur face m e s h  o f  p o l y g o n  facets. Polygon-mesh represen- 
tat ions are tvp ica l ly  d e r w e d  f r o m  o ther  representations. . .  - 

Surface functions, such  as t h e  quadrics, are used t o  d e s c r ~ b e  spheres a n d  
other smooth  surfaces. F o r  des ign  applications, w e  can  use superquadrics, 
sphnes, o r  b l o b b y  objects t o  represent smooth  surface shapes. In a d d i t ~ o n ,  con-  
s tntc t lon techniques, such as CSG a n d  sweep representations, are usefu l  f o r  de-  
s ign ing  c o m p o u n d  object shapes that  a re  b u i l t  up f r o m  a set o f  s impler  shapes. 
A n d  interior, as w e l l  as surface, i n f o r m a t i o n  can  be stored in octree representa- 
tions. 

Descr ipt ions fo r  n, l tural objects, such as trees and  ~:louds, and other  i r regu-  
lar ly  shaped objects can h e  specif ied w i t h  fractals, shape grammars, and part icl t-  
systems Finally, v isual izat ion techniques use graphic rt tprescntations to d isp lay  
numer ica l  o r  o t h e r  types o f  data sets. The  var ious types o f  numer ica l  data in- 
c lude scalar, vector, and  tensor values. A l s o  m a n y  s c i e n t ~ t ~ c  visual izations requi re 
methods ior representi l ip mu l tn8ar ia te  data sets, that conta in c o m b i n a t ~ o n  of 
the va r ious  data types. 

REFERENCES 

A dr la~let l  tl izcuwon ol r~)wrquadr i rc  I.; tont,iined in Barr (1981 1 For more informal~on on 
blobhy objrtr niodrllng -ee B l l m  ,1982) The metaball mod21 1s discussed in N h m u r , ~  
(1 985); and the soft-oblrt t lnodel is d~scussed In M'yville, Wvi ~ l le ,  ,111d h4rPheetrr5 1 1  487) 

Source5 o i  iniormation on paramelric curve .ind wriace represent,ltlons include Bezim 
(19721, Hurt m d  Adrl50-i (198.1). Barsky (19113, 19811, Korhanek ,ind Bartel5 11984) 
F,lrouki ~ n d  Hinds (IORi!. Huitric 'ind Nahas 119851. Morlenzon I 19851. F x i n  (198ll1. and 
Rugers <IIIJ Atidnlr (10901 

Ortree< and qundlrees >rtT d ~ s t  ucsed h i  Doctor (19811, Yamaguclii Kunli, .ind Fullniur<~ 
(1984), and in C.drlboni. C'hakravarc\ and V.intierschel (19851. Solic-modeling leirrenc-c,, 
~ncl t~c le ( a,alr ~ l l d  S t ~ i t ~  , n  t140S1 snd Reqi~~cha and Roh~gli.tt ~19'321. 

For iunher ~ntornml~un or1 ~rdc'tal representations see M a n r l ~ l l ~ r o l  (1977 ic ) t l? l  Fnirrl~~cv. 
F d d ,  , ~ n d  ( arpenter (1  98L), Norton (1982). Peitgen and Richter ( 1  %O) ,  Pcvtrcn m t l  
5 u p e  (1'3881 Koh and Hearn (1992). and Barnslev (19931. Shape grammars are d~cc u5wtl 
In Llassner I 19921, and particle systems are discussed in Rewes (19811 A cliccus51on or 
phy51callu I ) , ~ w i  rnoddinq is given In Barzel (19921. 

A grnt.ral introduction to wsualization method, 1s given ir H w r n  and Bakrr (1991 t .  Aiitli 
tion'il ~n iorn~, j t~on on sl)ecific vlsuallza~lon methods can be found In Sahm 1198i l .  
lorensen m d  Cline (i W7), Drebin, Carpenter, and Iidnr.~li,~n ( 19881, S,lbella 11  98Hi. 
I phon dund Ktic.lw (19881, Frenke (1989). Nielson, Shri:,er. .ind Rosenblum I 19401 ,lnd 
Xlelion ( 1  09 3 1  Guidelines tor vlsual displavs o i  information ,Ire given In T& t 198 1. 
19901. 

EXERCISES 

10-1 St31 uk; ~ T ~ I I I ( ~ ~ I I (  :hla 1 h k 5  a s  111 F I ~ ,  l ( b2  lor a LII~I: ( 111 1, 

iW.! Sc.1 up gronic~tr~c : tad tal~les lor a uriii c-uhe uung o111\ ,I' tc'rtw L~~i(l ~ ) o l v ~ , ( ~ ~ i  t c> l> l~~> .  
,ant1 ( I I ~  .i 31nglr polvgon table Compare thr two nwthotls lor rcprrwnl~ l i<  Ilw untr 
c-uht, ) ! t i  CI r ~ L ~ ) r t ~ ~ 2 1 ~ l ~ i l l t ~ r ~  uwns ~ l i r c e  d,it,~ L I I ~ ~ ~ .  .mcl t..!~lnm2 ,1or,lgt, r c ~ p ! ~ t v ~ t ~ l t ~  
t o r  t ~ a .  t i  



10-3. Deflne dn efficient polygon representation for a cylinder. justify your choice of repre- 
sentatlon. Exercises 

10-4. Set up a procedure for establishing polygon tables for any input set of data points 
defining an object. 

10-5. Dev:,e routines for checking the data tables in Fig. 10-2 for consistency and com- 
pleteness. 

10-6. Write a program that calculates parameters A, 8, C, and D for any set of three-di- 
mensional plane surfaces defining an object. 

10-7. Given the plane parameters A, 6, C, and D for all surfaces of an object, devise an al- 
gorithm to determine whether any specified point is inside or outside the object. 

i0-8. How would the values for parameters A, 6, C, and D in the equation of a plane sur- 
face have to be altered if the coordinate reference is changed from a right.handed 
system to a lefi-handed system? 

'10-9. Se: up an algorithm for converting any specified sphere, ellipsoid, or cylinder to a 
polygon-mesh representation. 

10-10. Sat up an algorithm for converting a specified superellipsoid to a polygon-mesh rep 
resentation. 

10-1 1. Set up an algorithm for converting a metaball representation to a polygon-mesh rep- 
resentation. 

10-12. Write a routine to display a two-dimensional, cardinal-spline curve, given an input 
set of control points in the xy plane. 

10-1 3 .  Write a routine to display a two-dimensional, Kochanek-Bartels curve, given an Input 
set of control points in the xy  plane. 

10-14. Determine thequadratic Bkzier blending functions for three control points. Plot each 
function and'label the maximum and minimum values. 

10-1 5. Determine the Bezier blending functions for five control points. Plot each function 
and label the maximum and minimum values. 

10-16. Write an efficient routine to display two-dimensional, cubic Bezier curves, given a 
set of four control points in the xy  plane. 

10-1 7. Write a routine to design two-dimens~onal, cubic BPzier curve shapes that have first. 
order piecewise continuity. Use an interactive technique for selecting control-point 
positions in the xy plane for each section of the cume. 

10-1 8 .  Write a routine to design two-dimensional, cubic BCzier curve shapes that have sec- 
ond-order piecewise continuity. Use an interactive technique for selecring control- 
point positions in the xy  plane for each section of the curve. 

10-19. Write a routine to display a cubic Bezier curve using a subdivision method. 

10-20. Determine the blending functions for uniform, periodic B-spline curves for d = 5. 

10-21. Determine the blending functions for uniform, periodic B-spline curves for d = 6. 

10-22. Write a program using forward differences to calculate points along a two-dlmen. 
sional, uniform, periodic, cubic B-spline curve, given an input set of control polnts. 

10-23. Write a routine to display any specified conic in the xy plane using a ratioiial BCzier 
spline representation. 

10-24. Write a routine to display any specified conlc in  the x y  plane using a rational 
B-spline representation. 

10-25. Develop an algorithm for calculating the normal vector to a Bezier surface at the 
point P(u, v). 

10-26. Write a program to display any specified quadrat~c curve using forward differences to 
calculate points along {hecurve path. 

10-27. Wr~te a program to display any specified cubic curve using forward differences to 
calculate points along the curve path. 

10-28. Derive expressions for calculating the forward differences for any specdied qtlddratir 
C U N C  



C h a m  10 10.19. Derive expresslow rot ca lcuat~ng the inrward difierentes for any spec~fied cubic 
Thrcc~D~mena~onal Object curve. 

Repre5erfari0ns 10-30. Set up procedurr, rcr generating the description o l  a 1hr.e-dimensconal object trom 
input parameters thai define the object in terms of a translational sweep. 

10-31. Develop procedurr> for generating the description o i  .3 three-dimensional object 
using input parameters that def~ne the object in terms of a rotational sweep. 

10-32. Devise an algorithni for generating sol~d objects as cornbinations of three-d~men- 
sional p r ~ m ~ t i v e  \h.ipes. each def~ned as a set of surfaces, using constructive solid- 
geometry methods. 

10-33. Develop an algorithm ior performing constructive solid-geometry modeling using a 
primitive set of solidi defined ~n octree struclures 

10-34. Develop an d lgor~thn for encoding a two-dimensional scene as a quadtree represen- 
tation. 

10-35. Set up an algorithm for loadmg a quadtree representatlon of a scene into a irarne 
buffer ior d~bplav of t i e  scene. 

10-36. Write a routlne to rc'nged the polygon deiln~tton of a three-dimensional object into 
an octree representatlon. 

10-37. Using the random, nlidpoint-d~splacement method, write J routine to create a moun- 
tain outl~ne, darting with a horirontal line in the ~y plane 

10-38. Write a routlne to calculate elevat~ons above a ground p lme using the random. rnid- 
point-displacement method. 

10-39. Write a program lor ;:eneraling a fractal snowilake (lioch curve) for any given num- 
ber of iterations. 

10-40. Write a program to generate a fractal curve for a speclfied number o i  iterations using 
one of the generatori In Fig. 10-71 or 1C'-72. What is the fractal dimension o i  vour 
curve? 

10-41 Wr~te a pcogram 18, generate fractal curves using  he self-squarmg iunct~on 
i'k) = z '  + A whe.e 4 IS any selected complex constant 

10.42. Write a program I ~ J  genetate frdctal cur\,es usir~g lhe seli-,quaring iunclton 
i ( x )  = + I ) ,  wher? I = m. 

10.43. Write c routlne to rnteractivelv select different color comhnat~ons for d~spldylng the 
Mandelbrot set. 

10-44. Write z program to ~nteractively select arv rectangular reglon of the Mandelbrot set 
and to zoom in on the selected region. 

10-45. Write a routine to ~ni l~ lement  point Inversion, Eq. 10-1 I?,  ior any speclfied c~rcle and 
any given potnt poslllon. 

10-46. Devise a set of geomt?tric-substitution rules ior altering the shape o i  an equildteral tri: 
angle. 

10.47. Write a program to display the stages in the conversion of an equilateral triangle ~ n t o  
another shape, glven a set of geometric-substitution rules. 

10.48. Write a program to model an exploding firecracker in thv x y  plane using a particle 
system. 

10-49. Devise an algor~thm for modeling a rectangle as a non:igid body, using identical 
springs for the iour sides of the rectangle. 

10-50. Write a routlne to v~jual ize a two-diniens~onal, scalar d . m  set using pseudo-color 
methods. 

10-51 Write a routlne to wiual~ze a two~d~mens~onal, scalar datd 5el using tontour hnes 

10-52. Write a routlne to t ,~wal l re a two-dimensional, vector data set using an arrow repre- 
sentation for the vect.3r values. Make all arrows the same length, but displav the ar- 
rows with dtfierent cclors to represent the different vector magnitudes. 





M ethods for geometric transformations and object modeling in three di- mensions are extended from two-dimensional methods by including 
considerations for the z coordinate. We now translate an object by specifying a 
three-dimensional translation vector, which determines how much the object is to 
be moved in each of the three coordinate directions. Similarly, we scale an object 
with three coordinate scaling factors. The extension for three-dimensional rota- 
tion is less straightforward. When we discussed two-dimensional rotations in the 
xy plane, we needed to consider only rotations about axes that were perpendicu- 
lar to the xy plane. In three-dimensional space, we  can now select any spatial ori- 
entation for the rotation axis. Most graphics packages handle three-dimensional 
rotation as a composite of three rotations, one for each of the three Cartesian axes. 
Alternatively, a user can easily set up a general rotation matrix, given the orienta- 
tion of the axis and the q u i r e d  rotation angle. As in the two-dimensional case, 
we express geometric transformations in matrix form. Any sequence of transfor- 
mations is then represented as,a single matrix, formed by concatenating the ma- 
trices for the individual transf@mations in the sequence. 

1 1 - 'I  
TRANSLATION 

In a three-dimensional homogeneous coordinate representation, a point is trans- 
lated (Fig. 17-1) from position P = (x ,  y, z )  to position P' = ( x ' ,  y', z') with the ma- 
trix operation 

Parameters t,, t,, nnd t,, specifying translation distances for the coordinate direc- 
tions x ,  y, and z, are asslgned any real values. The matrix representation in Eq. 
11-1 is equivalent to the three equations 

x ' = : r  + t,, y ' = y  + f,, z ' = z  + t ;  111 . ; I  



Section 11 -2 

Rotation 

Figure 11 -1 
a point with translation 

z axis vector T = (I,, t,, f , ) .  

Figirrr 11-2 
Translating an object with e -- 

I ~ X I S  translation vector T. 

An object is translated in three dimensions by transforming each of the 
defining points of the object. For an object represented as  a set of polygon sur- 
faces, we translate each vertex of each surface (Fig. 11-21 and redraw the polygon 
facets in the new position. 

We obtain the inverse of the translation matrix in Eq. 11-1 by negating the 
translation distances t,, t,, and t,. This produces a translation in the opposite di- 
rection, and the product of a translation matrix and its inverse produces the iden- 
tity matrix. 

11 -2 
ROTATION 

To generate a rotation transformation for an object, we must designate an axis of 
rotation (about which the object is to be rotated) and the amount of angular rota- 
tion. Unlike two-dimensional applications, where all transformations are carried 
out in the xy plane, a three-dimensional rotation can be specified around any line 
in space. The easiest rotation axes to handle are those that are parallel to the coor- 
dinate axes. Also, we can use combinations of coordinateaxis rotations (along 
with appropriate translations) to specify any general rotation. 

By convention, positive rotation angles produce counterclockwise rotations 
about a coordinate axis, if we are looking along the positive half of the axis to- 
ward the coordinate origin (Fig. 11-3). This agrees with our earlier discussion of 
rotation in two dimensions, where positive rotations in the xy plane are counter- 
clockwise about axes parallel to the z axis. 

Coordinate-Axes Rotations 

The two-dimensional z-axis rotation equations are easily extended to three di- 
mensions: 



Chaorer 1 1  

Figure 11 - l 
Positive rotation directions 
about th(. coordinate axes are 
coun~erclockw~se, when looking 
toward the origm from a positive 
coordinate position on each azis. 

x' = .x cos 0 - y sin 0 

y' = x s i n 0  + y c o s 0  
t' = 2 

Parameter 8 specifies the rotation angle. In homogeneous coordinate form, the 
three-dimensional z-axis rotation equations are expressed as 



Section 11 -2 
Kotat~on 

. 

Rotatlon of an object about the z 

which we can write more compactly as 

Flgure 11-4 iilustrates rotation of an object about the z axis. 
Transformation equations for rotations about the other two coordinate axes 

can be obtained with a cyclic permutation of the coordinate parameters x,  y, and 
: in Eqs. 11-4. That is, we use the replacements 

as illustrated in Fig. 11-5. 
Substituting permutations 11-7 in Eqs. 11-4, we get the equations for an  

x-axis rotation: 

y' = ycos0 - zsin B 

z' = y sin0 + z cose 
I' = X 

which can be written in the homogeneous coordinat~? form 

Cycllc permutat~on of the Cartesian-coordinate axes to prsduce the 
three sets of coordinate-axis rotahon equations. 



Three-Dimensional Gecmetrlc 
and Modellng lranslormat~ons 

I 

z -- - 
Figure 11-6 
Rotation of an  object about the 

x x axis. 

Rotation of an object around the x axis is demonstrated in Fig. 11.6. 
Cyclically permuting coordinates in Eqs. 11-8 give us the transformation 

equations for a y-axis rotation: 

The matrix representation for y-axis rotation is 

An example of y-axis rotation is shown in Fig. 11-7. 

. -  - . ~ ~ 

I'iyrltn. 11-7 
A ' Rotation ot a n  object ahout the 
1 y axis. 



An inverse rotation matrix is formed by replacing the rotation angle 0 by kc'io" "-* 
- 8. Negative values for rotation angles generate rotations in a clockwise direc- RotJt lOn 
tion, so the identity matrix is produced when any rotation matrix is multiplied by 
~ t s  inverse. Since only the sine function is affected by the change in sign of the ro- 
tation angle, the inverse matrix can also be obtained by interchanging rows and 
columns. That is, we can calculate the inverse of any rotation matrix R by evalu- 
ating its transpose (R-I = RT). This method for obtaining an inverse matrix holds 
also for any composite rotation matrix. 

General Three-D~mensional Rotations 

A rotation matrix for any axis that does not coincide with a coordinate axis can 
be set up as a composite transformation involv~ng combinations of translations 
and the coordinate-axes rotations. We obtain the required composite matrix by 
first sett~ng u p  the transformation sequence that moves the selected rotation axis 
onto one of the coordinate axes. Then we set u p  the rotation matrix about that co- 
ordinate axis for the specified rotation angle. The last step is to obtain the inverse 
transformation sequence that returns the rotation axis to its original position. 

In the special case where an object is to be rotated about an axis that is par- 
allel to one of the coordinate axes, we can attain tht! desired rotation with the fol- 
lowing transformation sequence. 

1. Translate the object so that the rotation axis coincides with the parallel coor- 
dinate axis. 

2. Perform the specified rotation about that axls. 
3. Translate the obiect so that the rotation axis is moved back to its original po- 

sition. 

The steps in this sequence are illustrated in Fig. 11-8. Any coordinate position P 
on the object in this figure is transformed with the sequence shown as 

\\,here the composite matrix for the transformntion is 

which is of the same iorm as the two-dimensional transformation sequence for 
rotation about an arbitrary pivot point. 

When an object is to be rotated about an axis that i s  not parallel to one of 
the coordinate axes, we need to perform some additional transformations. In this 
case, vre also need rotations lo align the axis with a selected coordinate axis and 
to bring the axis hack to its original orientation. Given the specificat~ons for the 
rotatton axis and the rotation angle, we can accomplish the required rotation in 
fi\'e s t e p  

1 Translate the object so that the rotation axis pass= through the coordinate 
orlgin. 

2 .  Rotate the object so that the axis of rotation ccincides with one of the coor- 
dinate axes. 

3. I'erform thc specified rotation about that coordinate axis. 



Chapter 1 1  

Three-D~rnensional Ceometr~c 
and Model~ng Transformations 

(a) 
Original Position o' Object (cl 

Rotate 0b~ei:t Through Angle 8 

(bl 
Translare Rotation A m  onto xAxis Id) 

Translate Rotation 
Axis to Original Position 

- . . .. . . -. - -- . 
Figurc ? 1 -S 
Sequence o f  transformations for rotat ing a n  object about a n  axis that  i s  
parallel to the x axis 

4. Apply inverse rotat~ons to bring the rotation axls back to ~ t s  original orien- 
tation. 

5. Apply the inverse translation to bring the rotahon axis back to its original 
position. 

We can transform the rotation axis onto any of the three coordinate axes. The z 
axis is a reasonable cho~ce, and the following discussion shows how to set u p  the 
transformation matrices for getting the rotation axis onto the z axis and returning 
the rotation axis to its original position (Fig. 11-9). 

A rotation axis can be defined with two coordinate positions, as in Fig. 11- 
10, or with one coordinate point and direction angles (or direction cosines) be- 
tween the rotation axis and two of the coordinate axes. We will assume that the 
rotation axis is defined by two points, as illustrated, and that the direction of ro- 
tation is to be counterclockwise when looking along the axis from P, to P,. An 
axis vector is then defined by the two points as 

v = P, - P, 
= ( x ,  - x , ,  y> - YI, 22 - 21' 

A unit vector u is then defined along the rotation axis as 

v 
u = ---- - (a ,  h, C) 

Ivl - 



Step 3 
Rotate the 

Object Around the 
z Axts 

Step 1 
Translate 

P,  to the Or~gin 

Step 4 
Rotate the Axis 
to the Original 

Orientation 

P;' 
,/- 
I 

Step 2 
Rotate Pi 

onto the r Axis 

z 
Step 5 

Translate the 
Rotation Axis 

to the Original 
Position 

Section 1 1  -2 

Figure 11 -9 
Five transformation steps for obtaming a composite matrix for rotation 
about an arb~trary axis, w~th the rotation axis projected onto the z axis. 

where the components a, b, and c of u n ~ t  vector u are the direction cosines for the 
rotation axis: 

i f  the rotation is to be in the opposite direction (clockwise when viewing from P, 
to PI), then we would reverse axis vector V and unit vector u so  that they point 
from P2 to P,. 

The first step in the transformation sequence for the desired rotation is to 
set up the translation matrix that repositions the rotation axis so that it passes 
through the coordinate origin. For the desired direction of rotation (Fig. 11-10), 
we accomplish this by moving point PI to the origin. ( I f  the rotation direction had 

] f 
been specified in the opposite direction, we ivould move P, to the origin.) This / :  X translation matrix is 

1 0  0 Figure 11-10 
An axis of rotation (dashed 

0 0 1  
(71 -7  7 )  line) defined with points 

PI and P2. The direction for 
0 0 0  the unit axis vector u is 

determined by the specified 
which repositions the rotation axis and the object, as shown in Fig. 11-11. rotation direction. 



Flgtrrr 11-11 
Translation of the rotation 
axis to the coordinate ongin 

Figrrrc 11-13 
Rotation of u around the x 
axis into the rz plane is 
accomplished by rotating u' 
(the projection of u in the yz 
plane) through angle or onto 
the z axis. 

- - . . - . - . . - - . . . - . - . . . . - - 
Ficyttrts 11. 12 
Unit vector u is rotated about the x axis to bring it 
mto the xz plane (a), then it is rotated around they 
axis to align it with the z axis (b) 

Now we need the transformations that will put the rotation axis on the z 
axis. We can use the coordinate-axis rotations to accomplish this alignment in 
two steps. There are a number of ways to perform the two steps. We will first ro- 
tate about the x axis to transform vector u into the xz plane. Then we swing u 
around to the z axis using a y-axis rotation. These two rotations are illustrated in 
Fig. 11-12 for one possiblc orientation of vector u. 

Since rotation calculations involve sine and cosine functions, we can use 
standard vector operations (Appendix A) to obtain elements of the two rotation 
matrices. Dot-product operations allow us to determine the cosine terms, and 
vector cross products provide a means for obtaining the sine terms. 

We establish the transformation matrix for rotation around the x axis by de- 
termining the values for the sine and cosine of the rotation angle necessary to get 
u into the xz plane. This rotation angle is the angle between the projection of u in 
the yz plane and the positive z axis (Fig. 11-13), I f  we designate the projection of u 
in the yz plane as the vector u' = (0, b, c), then the c ~ s i n e  of the rotation angle a 
can be determined from the dot product of u' and @e unit vector u, along the z 
axis: 

where d is the magnitude of u': 

Similarly, we can determine the sine of cr from the cross product of u' and u,. The 
coordinate-independent form of this cross product is 

and the Cartesian torm for the cross product gives us 

Equating the right sides of Eqs. 11-20 and 11-21, and notlng that 1 u, I = 1 and 
1 u' 1 = d, we have 

t i  sinn = h 



sina = - 
d 

Sedion 1 1-2 

Rotation 

Now that we have determined the values for cosa and sina in terms of the com- 
ponents of vector u, we can set up the matrix for rotation of u about the x axis: 

This matrix rotates unit vector u about the x axis into the xz plane. 
Next we need to determine the form of the transformation matrix that will 

swing the unit vector in the rz plane counterclockwise around they axis onto the 
positive z axis. The orientation of the unit vector in the xz plane (alter rotation 
about the x axis) is shown in Fig. 11-14. This vector, labeled u", has the value a for 
its x component, since rotation about the x axis leaves the x component un- 
changed. Its z component is d (the magnitude of u'), because vector u' has been 
rotated onto the z axis. And they component of u" is 0, because it now lies in the 
xz plane. Again, we can determine the cosine of rotation angle P from expres- 
slons for the dot product of unit vectors u" and u,: 

u" ' U, 
cosp = = d I ulI I I U, I 

since 1 ui ! = 1 u" 1 = 1. Comparing the coordinate-independent form of the cross 
product 

with the Cartesian form 

we find that 

Thus, the transforn~ation matrix for rotation of u" about they axis is 

W~th transformat~on matrices 11-17, 11 -23, anti 11-28, we have aligned the 
rotation axls wlth the positive z axis. The specifled rotation angle e can now be 
applied as a rotation about the z axis: 

- -.. .- - - - - - - . . . 
F r p r c  11- 1-1 
Rotation of unit vector u" 
(vector u after rotation into 
the xz plane) about they axis. 
Positive rotation angle 13 
aligns u" with vector u,. 



Chapter 11 

Three Dimensional Geometric 
and Modeling Transformalions 

- . - - . - . . - . . - - - 
Qgii n. 17 - I .i 
Local coordinate system for a 
rotation axls defined hv unit 
vedor U. 
4 18 

To complete the required rotation abou! the given axis, we need to trans- 
form the rotation axis back to its original position. This is done by applying the 
inverse of transformations 11-17,ll-23, and 11-28. The tr;rnsformation matrix for 
rotation about an arbitrary axis then can be expressed as the composition of these 
-,even individual transformations: 

A somewhat quicker, but perhaps less intuitive, method for obtaining the 
composite rotation matrix R,(a) is to take advantage pt the form of the 
composite matrix for any sequence of three-dimensional rotations: 

The upper left 3 by 3 submatrix of this matrix is orthogonal. This means that the 
rows (or the columns) of this submatrix form a set of orthogonal unit vectors that 
are rotated by matrix R onto the x, y, and z axes, respectively: 

Therefore, we can consider a loczl coordinate svstem defined bv the rotation 
axis and simply form a matrix whose columns are ;he local unit cokdinate vec- 
tors. Assuming that the rotation axis is not parallel to anv coordinate axis, we can 
form the following local set of unit vectors (Fig. 11-15): 

If we express the elements of the local unit vectors for the rotation axis as 

then the required composite matrix, equal to the product R,(P) . R,(u), is 



Section 11.2 

-35) Rotallon 

This matrix transforms the unit vectors u;, ui,  and ui onto the x, y, and z axes, re- 
spectively. Thus, the rotation axis is aligned with the z axis, since u; = u. 

Kotations with Quaternions 

A more efficient method for obtaining rotation about a specified axis is to use a 
quaternion representation for the rotation transformation. In Chapter 10, we dis- 
cussed the usefulness of quaternions for generating three-dimensional fractals 
using self-squaring procedures. Quaternions are useful also in a number of other 
computer graphics procedures, including three-dimensional rotation calcula- 
tions. They require less storage space than 4-by-4 matrices, and it is simpler to 
write quaternion procedures for transformation sequences. This is particularly 
important in animations that require complicated motion sequences and motion 
interpolations between two given positions of an object. 

One way to characterize a quaternion (Appndix A) is as an ordered pair, 
consisting of a scalar part and a vector part: 

We can also think of a quaternion as a higher-order complex number with one 
real part (the scalar part) and three complex parts (the elements of vector v). A 
rotation about any axis passing through the coordinate origin is performed by 
f~rst setting u p  a unit quaternion with the following scalar and vector parts: 

where u is a unit vector along the selected rotation axis, and B is the specified ro- ,/ x 
tation angle about this axis (Fig. 11-16). Any point position P to be rotated by this 2 
quaternion can he represented in quaternion notation as 

I igrrre 11-16 

P = (0, p) Ln~t quaternion parameters 
band u for rotation about 

with the coordinates of the point as the vector part p = (x, y, z ) .  The rotation of "'IJecified axis. 

the point is then carried out with the quaternion operation 

where q ' = ( 5 , - V )  IS the lnverse of the unit quaternion q with the scalar and vec- 
tor parts given In Eqs 11-36. This transformation produces the new quaternion 
~\.ith scalar part equal to 0: 

and the vector part 1s calculated with dot and cross productsas 



Tlirw-D~rnenrlo~ial Geomelr~c 
and ~ ~ d ~ l ~ ~ ~  ~ r , , n i f o r m a t i o n s  Parameters .; and v 1xn.c. the rotation values given in Eqs. 11-36. Many computer 

graphics systems use eiticient hardware implementations of these vector calcula- 
tions to perform rapid three-din~ensional object rotat~ons 

Transforn~ation 11-37 is equivalent to rotation about an axis that passes 
through the coordinate origin. This is the same as the sequence of rotation trans- 
formations in Eq. 11-30 that aligns the rotation axis with the z axis, rotates about 
z ,  and then returns the rotation axis to its original position. 

Using the definition for quaternion multiplication given in Appendix A. 
and designating the components of the vector part of y as v = (a, b, c), we can 
evaluate the terms in Eq 11-39 to obtain the elements for the composite rotation 
matrix R;'(a) . R b l ( P )  R,(0)  . R,(O) . R , b )  in a 3 by 3 form as 

To obtain the complete general rotation equation 11-30, we need to include the 
translations that move the rotation axis to the coordinate axis and back to its orig- 
inal position That is, 

As an example, we can perform a rotation a h u t  the z axis by setting the 
unit quaternion parameters as 

. - . - 0 0 COS - 2 '  v = (0,  0, 1 ) sin - 2 

where the quaternion iwtor elements are R = b = 0 and 1- = sin(0/2). Subst~tut- 
ing these values into matrix 11-40, and using the following trigonometric identi- 
ties 

B 
C0S2 - -. ' 0 B 0 0 s~n';  - 1 - 2sin2- = cost), 2cos-- sin- = sin0 

2 2 2 2 

we get the 3 by 3 version of the z-axis robtion matrix R,(O) in transformation 
equation 11-5. Similarly, substituting the unit quaternion rotation values into the 
transformation equation 11-37 produces the rotated coordinate values in Eqs. 
11 -4.  

11-3 
SCALING 

The matrix expression tor the scaling transformation of a position P = (x, y, z )  rel- 
ative to the coordinate origin can be written as 



Figure 11-17 
Doubling the sue of anobject with 
transformation 11-42 also moves the 
object farther from the origin 

where scaling parameters s ,  sy, and s, are assigned any positive values. Explicit 
expressions for the coordinate transformations for scaling relative to the origin 
a re 

z'  = 2 .  S, (11-44)  x' = x  . s,, y' = y . s,, 

Scaling an object with transformation 11-42 changes the size of the object 
and repositions the object relative to the coordinate origin. Also, if the transfor- I 
mation parameters are not all equal, relative dimensions in the object are A\ 
changed: We preserve the original shape of a n  object with a uniform scaling (s, = 
s, = s,).  The result of scaling an object uniformly with each scaling parameter set 
to 2 is shown in Fig. 11-17. 

Scaling with respect to a selected fixed position ( x ,  y,, 2,) can be represented 
with the following transformation sequence: 

I C l  

1. Translate the fixed point to the origin. 
2. Scale the object relative to the coordinate origin using Eq. 11-42. 
3. Translate the fixed point back to its original position. 

This sequence of transformations is demonstrated in Fig. 11-18. The matrix reprc- 
sentation for an arbitrary fixed-point scaling can then be expressed as the con- 
catenation of these translate-scale-translate transformations as 

*elected fixed point is 
We form the inverse scaling matrix for either Eq. 11-42 or Eq. 11-45 by re- equivalent to thesequence of 

placing the scaling parameters s,, 5 ,  and s: with their reciprocals. The inverse ma- :ransformations shown 



Chapter 11 trix generates an opposite scaling transformation, so the concatenation of any 
Three-Dimensional Geometric scaling matrix and its inverse produces the identity matrix. 
and Madellng Transformations 

11-4 
OTHER TRANSFORMATIONS 

In addition to translahon, rotation, and scaling, there are various additional 
transformations that are often useful in three-dimensional graphics applications. 
Two of these are reflection and shear. 

Reflections 

A three-dimensional retlection can be performed relative to a selected reflection 
axis or with respect to a selected repection plane. In general, three-dimensional re- 
fledion matrices are set up  similarly to those for two dimensions. Reflections rel- 
ative to a given axis are equivalent to 18@" rotations about that axis. Reflections 
with respect to a plane are equivalent to 160' rotations in four-dimensional space. 
When the reflection plane is a coordinate plane (either xy, xz, or yz), we can think 
of the transformation as a conversion between Left-handed and right-handed sys- 
tems. 

An example of a reflection that converts coordinate specifications from a 
right-handed system to a left-handed system (or vice versa) is shown in Fig. 
11-19. This transformation changes the sign of the z coordinates, Ieaving the x-  
and y-coordinate values unchanged. The matrix representation for this reflection 
of points relative to the xy plane is 

Transformation matrices for inverting x and y valces are defined similarly, 
as reflections relative to the yz plane and xz plane, respectively Reflections about 
other planes can be obtained as a combination of rotations and coordinate-plane 
reflections. 

fleflect~on 
Relative lo the Y 

x y  Plane 
--r 

Convers~on of coordinate specifications from a r~ght- 
handed to a left-handed system can be carried out with 
the reflection transformation 11-46, 



She~ ls  Section 11 -5 
('omposte Trandormationc 

Shearing transformations can he used to modify ohjrvt shapes. They are also use- 
ful in three-dimensional viewing for obtainuig grneral projectio-n transforma- 
tions. In two climensions, we discussed tranformat~c~ns relative to the x or y axes 
to produce distortions in the shapes of objects. In three dimensions, we can also 
generate shears relative to the z axis. 

As an example of three-dimensional shearing. the following transformation 
produces a z-axis shear: 

Parameters a and b can be assigned any real values. The effect of this transforma- 
tion matrix is to alter x- and y-coordinate values hy an amount that is propor- 
tional to the 2 value, while leaving the z coordinate unchanged. Boundaries of 
planes that are perpendicular to the z axis are thus shifted by an amount propor- 
tional to z .  An example of the effect of this shearing matrix on a unit cube is 
shown in Fig. 11-20, for shearing values a = b = !. Shearing matrices for the r 
axis and y axis are defined similarly. 

11-5 . . - - - - . . - . . - - - - 

COMPOSITE TRANSFORMATIONS I i,vurt, 11-20 
4 unit cube (a) is sheared 
a:b) by transformation matrix 

As with two-dimensional transforrnat~ons. we torn1 a composite three-dimen- w i t h a  - b - 
sional transformation by multiplying the matrix representations for the individ- 
ual operations in the transformation sequence. 1-h~c concatenation is carried out 
from right to left, where the rightmost matrlx is the first transformation to be ap- 
plied to an object and the leftmost matrix is the las: transformation. The follow- 
ing program provides an example for implementing, a composite transformation. 
A sequence of  basic, three-dimensional geometric transformations are combined 
to produce a single composite transformation, which is then applied to the coor- 
dinate definition of an object. 

typedef flodt M a t r l ~ 4 ~ 4 I 4 1  i 4 1 :  

void matrlx4~4Setldenrity (Marrix4x4 mt 
I 

: l n r  I , ? ,  



I *  Multiplies matrix a times b, putting resulL in b * /  
void matrlx4~4PreMultlply (Matrix4xl a, Matrix4x4 b) 
l 
int r,c; 
Matrix4x4 tmp; 

for (r.0; r<4; r + + )  
for 1c.O; C<4; c++I 
trnp[r] [cl = a[rl IOI'blO] [cl + alrl Ill'b !I [cl + 
a[r] [2Ji1[2! Ic] + alr] [3l*b[3] [cl: 

for (r=O; r<4: r*+l 
for (c=S: cc4; c*+) 
blri lc: = tmplrl i c l ;  

) 

w i d .  Erar.slace3 (float tx, float ty, float tz) 

motrix4~4SecIder.city (rn) : 
m(01 [ 3 ]  = tx; mI11 [3] = ty: m121 (31 = tz; 
matrix4~4PreMultiply (m, theMatrix); 

) 

void scale3' [float sx, float s y ,  float sz, wcPt3 .:enter) 
( 
Matrlx4x4 m; 

matrix4~4SetIdentity ( m i ,  
m[O] I01 = sx: 
m(01 131 = ( 1  - sx)  center.^; 
mllllll = sy; 
m[11 1 3 1  = ( 1  - syl ' cencer.~; 
m[21 [21 = s z :  
m[?] 131 = I1 szl ' center.2; 
matrix4~4PreMultiply (m, theMatrix) ; 

1 

void rotate? (wcPt3 pl, wcPt3 p2, float radiadu~glel 
{ 
float 1engt.h = sqrt I (p2.x - g1.x) . (p2.x - 1,l.x) + 

(p2.y - p1.y) ' (p2.y - p1.y) + 
(p2.z - pl.21 ' ( ~ 2 . 2  - p1.z)); 

floac cosA.2 = cosf (radianAngle 1 2.0); 
float six42 = sinf IradianAngle / 2.0): 
float a = s inA2 * (p2.x - p1.x) / length; 
float b = sinA2 ' (p2.y - pl.yl / length; 
float c = sinAZ (p2.z - p1.z) / length; 
Macrix4x4 m, 



void transfonnP3ints3 (int nPts, wcPt3 ' pts) 
{ 

int t ,  j; 
float tmpl31 ; 

for (k=O; k<nPts; k++) ( 
for (j=0; j < 3 ;  j++) 
mp[jl = theMatrix[j][Ol ptsik1.x + thetlatrixljl [ll ' pts1kl.y 4 
theMatrix[j][21 pts(k1.z + theMatrix[jl(3]; 

sezWcPt3 (&pts[kl, tmpI01, tmp[ll. tmpI21); 

void maln (int argc, char * '  argv) 
( 
wcPt3 ptsl51 = ( 10,lO.O. 100,10,0, 125,5[1,0, 35 50.0, 10.10,O ) :  
wcpt3 pl = 1 10.10.0 ) ,  p2 = ( 10,10,10 I :  
wcPt3 refPt = ( 68.0,30.0,0.0 1; 

long windowID = openGraphics ('argv, 200, 200); 
setBackground (WHITE) : 
setcolor (BLUE): 
pPolyline3 (5, pts) ; 

matrix4~4SetIdentity (theMatrix); 
rotate3 (pl, p2. PI/4.0); 
scale3 (0.75, 0.75. 1.0. refPtl; 
translate3 (25, 40, 0); 
transformPoints3 (5. pts) ; 
setcolor (RED) ; 
pPolyline3 ( 5 ,  p t s ) ;  

sleep (10); 
' closeCraphics (windowlD): 

THREE-DIMENSIONAL TRANSFORMATION FUNCTIONS 

We set up matrices for modeling and other transformations with functions siml- 
lar to those given in Chapter 5 for two-dimensional transformations. The major 
difference is that we can now specify rotations around any coordinate axis. These 
functions are 



Chapter 1 1  Each of these functions produces a 4 by 4 transformation matrix that can then be 
Three-Dirnens~onal Ceornerr~c used to transform conrdinate positions expressed as homogeneous column vec- 
and M*c'lng TransformatlOns tors. Parameter t r a n s  1 at evector is a pointer to the list of translation distances 

t,. :,, and t,. Similarly, parameter scalevector specifies the three scaling para- 
meters s,, sy, and s,. Rotate and scale matrices transfomi objects with respect to 
the coordinate origin. 

And we can constmct composite transformations w ~ t h  the functions 

wh~ch have parameters similar to two-dimensional transformation functions for 
setting up composite matrices, except we can now specify three rotation angles. 
The order of the transformation sequence for the b u i  ldTrar,sf ormationMa- 
trix3 and composeTransf ormationMatrix3 functions is the same as in two 
dimensions: (1) scale, (21 rotate, and (3) translate. 

Once we have specified a transformation matrix, we can apply the matrix to 
specified points with 

~ransformFoin t?  I i n P o i c t .  matrix, outP0:n:) 

In addition, we can set the transformations for hierarhcal constructions using 
structures with the f u n c t ~ ~ w  

s e t L o c a 1 T r ; m s t o l  r-dr ion3 [matrix, C y p e l  

where parameter mat r l x  specifies the elements of a 4 by 4 transformation ma- 
trix, and paranreter type can be assigned one of the following three values: prr- 
c-oi~c~~tt'nnlr, ~ a s t c o r r c n t r ~ r ~ ~ l ~ ~ ,  or rrplnrr. 

11 -7 
MODELIKG A \ [ )  C'( I (  IKDINATE TRANSFORLI-41 IONS 

So far, we have discusseil three-dimensional transfor~lialions a5 operations that 
mm'e objects from one pis~tion to another within a single reference frame. There 
are many tinies, however. \\.hen we are interested in swt<hing coordinates from 
cme svstem tu nnc ther. Gcneral three-dimensional ~~ie\ving procedures, for exam- 
ple involve an initial t~insformation of world-coordinate descriptions to a view- 
ing-coordinate vstem. Then viewing coordinates are transformed to device coor- 
dinates. And in modehng, objects are often described in a local (modeling) 
coordinate reference framtr, then the objects are repositioned into a world-coordi- 
nate scene. For example, tables, chairs, and other furniture, each defined in a 
locdl (modeling) coordinate system, can be placed into the descr~ption of a room, 
defined in another refertvice frame, by transforming the turniture coordinates to 
room coordinates. Then the room might be transformed illto a largcr scene, con- 
structed in world coordin,rte$. 

An example of the use af nlultiple coordinate systems and hierarchical 
n i o d t h g  with tlirt*r-d~nre~rsii>ndI objects is given in Fig. 11-21. This tigure illus- 
trates simulation ut tract<>- nrovement. As the tractor moves, the tractor coordi- 
nate .;!stem and fro~rt-\vIii=t~I coordinate svstenl niovs Ln the world-coordinate 



Section 11-7 

 odel ling and Coordinate 

L Transformations 
I, World x, 

Svstern 

Figurc 11-27 
Possible coordinate systems used in simulating *actor movement. 
Wheel rotations are described in the front-wheel system. T i g  of 
the tractor is described by a rotation of the front-wheel system in 
the tractor system. Both the wheel and tractor reference frames 
move in the world-coordinate system. 

system. The front wheels rotate xi the wheel system, and the wheel system ro- 
tates in the tractor system when the tractor turns. 

Three-dimensional objects and scenes are C O ~ S ~ N C ~ ~  using structure (or 
segment) operations similar to those discussed in Chapter 7. Modeling transfor- 
mation functions can be applied to create hierarchical representation for three-di- 
mensional objects. We can define three-dimensional object shapes in local (mod- 
eling) coordinates, then we construct a scene or a hierarchical representation with 
instances of the individual objects. That is, we transform object descriptions from 
modeling coordinates to world coordinates or to another system in the hierarchy. 
An example of a PHlGS structure hierarchy is shown in Fig. 11-22. This display 
was generated by the PHIGS Toolkit software, developed at the University of 

I F i p r e l l - 2 2  
Displaying an object hierarchy 
using the PHlGS Toolkit package 

I developed at the University of : Manchester. The displayed obpct 
tree is itself a PHIGS structure. 

I (Courtesy of T. L. 1. Houmrd, 1. G. Williams, 
and W. T. Hnuitt, Dcprrlment ofcomputer 
Scrcnce, Un~vprsify  of Manchester, United 
Kingdom.) 



-- 

Figure 11 -23 
Threedimensional modeling: (a) A 
ball-and-stwk representation for 
key amino acid residues interacting 
with the natural substrate of 
Thymidylate Sy nthase, modeled 
and rendered by Julie Newdoll, 
UCSF Computer Grapliics Lab. (b) A 
CAD model showing individual 
engine components, rendered by 
Ted Malone, FTI/3D-Magic. 
(Courtesy of Silicon Gmylrics. Inc.) 

Manchester, to provide an editor, windows, menus, and other interface tools for 
PHIGS applications. Figure 11-23 shows two example applications of three- 
dimensional modeling. 

Coordinate descriptions of objects are transferred from one system to an- 
other with the same procedures used to obtain two-dimensional coordinate 
transformations. We need to set u p  the transformation matrix that brings the two 
coordinate systems into alignment. First, we set up  a translation that brings the 
new coordinate origin to the position of the other coordinate origin. This is fol- 
lowed by a sequence of rotations that corresponding coordinate axes. If different 
scales are used in the two coordinate systems, a scaling transformation may also 
be necessary to compensate 'or the differences in coordinate intervals. 

If a second coordinate system is defined with origin (x,,, yo, z,) and unit axis 
vectors as  shown in Fig. 11-24, relative to an existing Cartesian reference frame, 
we  first construct the translation matrix T(-xO, -ya. -i$. Next, we  can use the 
unit axis vectors to form the coordinate rotation matrix 

Figure 11-24 
Transformation of an object 
description from one coordinate 
system to another. 



which transforms unit vectors u:, u;, and u: onto the x, y, and z axes, respec- 
tively. The complete coordinate-transfornation sequence is then given by the 
composite mahix R . T. This matrix correctly transforms coordinate descriptions 
from one Cartesian system to another even if one system is left-handed and the 
other is right-handed. 

SUMMARY 

Three-dimensional transformations useful in computer graphics applications in- 
clude geometric transformations within a single coordinate system and hanfor- 
mations between different coordinate systems. The basic geomehic transforma- 
tions are translation, rotation, and scaling. Two additional object transformations 
are reflections and shears. Transformations between different coordinate systems 
are common elements of modeling and viewing routines. In three dimensions, 
transformation operations are represented with 4 by 4 matrices. As in two-di- 
mensional graphics methods, a composite transformation in three-dimensions is 
obtained by concatenating the mahix representations for the individual compo- 
nents of the overall transformation. 

Representations for translation and scaling are straightforward extensions 
of two-dimensional transformation reuresentations. For rotations, however, we 
need more general representations, since objects can be rotated about any speci- 
fied axis in space. Any three-dimensional rotation can be represented as a combi- 
nation of basic rotations around the x, y, and z axes. And many graphics pack- 
ages provide functions for these three rotations. In general, however, it is more 
efficient to set up a three-dimensional rotation using either a local rotation-axis 
reference frame or a quaternion representation. Quaternions are particularly use- 
ful for fast generation of repeated rotations that are often required in animation 
sequences. 

Reflections and shears in three dimensions can be carried out relative to any 
reference axis in space. Thus, these transformations are also more involved than 
the corresponding transformations in two dimensions. Transforming object de- 
scriptions from one coordinate system to another is equivalent to a transforma- 
tion that brings the two reference frames into coincidence. Finally, object model- 
ing often requires a hierarchical transformation structure that ensures that the 
individual components of an object move in harmony with the overall structure. 

REFERENCES 

For additional techniques involving matrices, modeling, and three-dimensional transforma- 
tions, see Glassner (19901, Arvo (1991 ), and Kirk (1992). A detailed dixussion of quater- 
nion rotations is given in Shoemake (1985). Three-dimensional PHlGS and PHIGS + trans- 
formation functions are diqcussed in Howard et al. (1991). Gaskins (1992), and Blake 
11993). 



Chapter 11 

Three.D~rnens~onal Ceometrrc 
and MCdehng Transformat~ons 

EXERCISES 

11-1. Prove that tht. ni~rltiplication of three-d~niens~onal trdlsformation matrices for each 
of the iollowmg ~ i q u e n c e  of operations 15 commu:ativt.: 
la) Any two successwe translations. 
(b) Any two sucrtwive scaling operations. 
(c) Any two succt.ssive rotations about any one of the coord~nate axes 

1 1  -2. Using either Eq. 1 1-30 or Eq. 11 -41, prove that any hv.3 successive rotations about a 
given rotallon axis 1s commutative. 

I 1-3. By evaluat~ng tht. term, in  Eq.  1 1  -39, derive the eements for general rotation matrix 
given ~n E q .  11 40 .  

11-4. Show that rotdtlc~rr mdtrlx 11-35 is equal to the cornpus~te lnatrix R,(P) . R,la) .  
11-5. Prove that thr qwternion rotdtion matrix Eq. 11-40 reduces to the mdtrix representd- 

tion in  E q  11 - 5  when the rotation axis is the coordmare z axis. 

1 1  - 6 .  Prove that Eq. 1 I ..11 is equivalent to  the general rotallcm Iranstormahon given In E q .  
11 -30. 

11-7. Write a procrdurv tu implement general rotation transiormations using the rotallon 
matrlx 11 -35. 

11-8. Write a routlna to ~n~plernent quaternion rotations, Eq 1 31, tor any hpecihed axls. 

1 1  -9. Derive the tranhrmation niatrlx for scaling an object by d scaling factor s In d direc- 
tion dei inrd by [he d~rection angles a, 9, and y. 

1 1  -10. Develop an algor~thm for scaling an object defined in an octree representation. 

11-11. Develop a procedure (or animating an object by incrementally rotating it about any 
speciited . >x i \  L J s e  appropriate ~pproximntions to thv trigonometric equations to 
speed up !he ralruldtions, and reset the object to ts initial posit~on after each com- 
plete revolufion ahout !he axis. 

11 -1 2. Devihe d procedurcl ior rotating an object that is represented in  an wtree sttucture. 

11  - 1  3.  Drvelop A routlnc 'o reilec~ an object about an arbitrar~ly selected plane 
11-14, Write a progranr 10 shear an object with respect to anv c ~ i  the Ihree coordinate axes, 

using 111pul values lor the shedr~ng parameters. 

11 - 1  5. Develop ,i proced~re for converting an object definitwr in  one coord~nare reierence 
to anv othrmr rocxtlinate system defined dative to the ur\l system. 

1 1  - 16. Develop a romplvtc algorithm ior implernenting the procecures ior cnnstruclive 
sohd inodc41n~ b\ combining three-dimensional p i l r n t ~ \ e  to ?,enprate n r w  shapes. 
Initially, the prirn~l~ves tan  be combined to form subaswmblie,. then the sul~assem- 
blies cdn be col-~l).ned witb each other and w ~ t h  pr~rnitve shapes to form the ilnal 
assemblv. Interacti\e input of translation and rotation pnramcters Cdn be used to po- 
sition the ohjec:\ Output of the algorithm is to he the sequence of operations 
needcd to product, the final CSG object. 





I n two-dimensional graphics applications, viewing operations transfer posi- tions from the world-coordinate plane to pixel positions in the plane of the 
output device. Using the rectangular boundaries for the world-coordinate win- 
dow and the device viewport, a two-dimensional package maps the world scene 
to device coordinates and clips the scene against the four boundaries of the view- 
port. For three-dimensional graylucs applications, the situation is a bit more in- 
volved, since we now have more choices as to how views are to be generated. 
First of all, we can view an object from any spatial position: from the front, from 
above, or from the back. Or we could generate a view of what we would see if we 
were standing in the middle of a group of objects or inside a single object, such as 
a building. Additionally, three-dimensional descriptions of objects must be pro- 
jected onto the flat viewing surface of the output device. And the clipping 
boundaries now enclose a volume of space, whose shape depends on the type of 
projection we select. In this chapter, we explore the general operations needed to 
produce views of a three-dimensional scene, and we also discuss specific viewing 
procedures provided in packages such as PHIGS and GL. 

12-1 
VIEWING PIPELINE 

The steps for computer generation of a view of a three-dimensional scene are 
somewhat analogous to the processes involved in taking a photograph. To take a 
snapshot, we first need to position the camera at a particular point in space. Then 
we need to decide on the camera orientation (Fig. 12-1): Which way do we point 
the camera and how should we rotate it around the line of sight to set the up  di- 
rection for the picture? Finally, when we snap the shutter, the scene is cropped to 
the size of the "window" (aperture) of the camera, and light from the visible sur- 

selection of a camera poslnun ana 
z orientation 



- Project~on - Wor(cPtation , Dev~ce 
Coordinates Trsngformat~on I Coord~nates 

Figure 12-2 
General three-dimensional transformation pipeline, from modeling coordinates to final 
device coordinates. 

faces is projected onto the camera film. We need to keep in mind, however, that 
the camera analogy can be carried only so far, since we have more flexibility and 
many more options for generating views of a scene with a graphics package than 
we do with a camera. 

Figure 12-2 shows the general processing steps for modeling and convert- 
ing a world-coordinate description of a scene to device coordinates. Once the 
scene has been modeled, world-coordinate positions are converted to viewing co- 
ordinates. The viewing-coordinate system is used in graphics packages as a refer- 
ence for specifying the observer viewing position and the position of the projec- 
tion plane, which we can think of in analogy with the camera film plane. Next, 
projection operations are performed to convert the viewing-coordinate descrip 
tion of the scene to coordinate positions on the projection plane, which will then 
be mapped to the output device. Objects outside the specified viewing limits are 
clipped h m  further consideration, and the remaining objects are processed 
through visible-surface identification and surface-rendering procedures to pro 
duce the display within the device viewport. 

12-2 
VIEWING COORDINATES 

Generating a view of an object in three dimensions is similar to photographing 
the object. We can walk around and take its picture from any angle, at various 
distances, and with varying camera orientations. Whatever appears in the 
viewfinder is projected onto the flat film surface. The type and size of the camera 
lens determines which parts of the scene appear in the final picture. These ideas 
are incorporated into threedimensional graphics packages so that views of a 
scene can be generated, given the spatial position, orientation, and aperture size 
of the "camera". 

Specify~ng the View Plane 

We choose a particular view for a scene by first establishing the viewing-coordi- 
nate system, also called the view reference coordinate system, as shown in 
Fig. 12-3. A view plane, or projection plane, is then set up perpendicular to the 

. - 
F~,qrtrr 12- 3 
A right-handed viewing- 
coordinate system, with axes 
x:,, yo, and z,,. relative to a 
world-coordinate scene. 



View Plane 

Figure '12-4 
Onentations of the view 
plane for specified normal 
vector coordinate5 relative to 
the world origin, Position ( I ,  
0,O) orients the view plane as 
in (a), while ( I ,  G, I )  gives the 
orientation in (h). 

- -- - 

Flgrrrt. 12-6 
Specifying the view-up vector 
with a twist angle H,. 

viewing z ,  axls. We can think of the view plane as the film plane in a camera that 
has becn positioned and or~enled fur a particular shot of the scene. World-coordi- 
nate positions in the scene are transformed to viewing coordinates, then viewing 
coordinates are projected onto the view plane. 

To establish the v~ewing-coordinate reference frame, we first pick a world- 
coordinate positiqn callrd the view reference point. This point is the origin of 
our viewing-coordinate system. The view reference point is often chosen to be 
close to or on the surface of some object in a scene. But we could also choose a 
point that is at the cent4.r of an object, or at the center of a group of objects, or 
somewhere out rn front of the scene to be displayed. If we choose a point that is 
near to or on some object, we tan think of this point as the position where we 
might want to aim a camera to take a picture of the object. Alternatively, if we 
choose a point that is at some distance from a scene, we could think of this as the 
camera position. 

Next, we select thc positive direction for the viewing z, axis, and the orien- 
tation of the view plane, by specifying the view-plane normal vector, N. We 
choose a world-coordinate position, and this point establishes the direction for N 
relative either to the world origm or to the viewing-coordinate origin. Graphics 
packages such as CKS i~nd PHIGS, for example, orient N relative to the world- 
coordinate origin, as shown in Fig. 12-4. The view-plane nc)rmal N is then the di- 
rected line segment from the world origin to the selected coordinate position. In 
other words, N is simply specified as  a world-coordinate vector. Some other 
packages (GL from ~ilicon ~ r a ~ h i c s ,  for instance) establish the direction for N 
using the selected coonilnate position as a look-at poi111 relative to the view refer- 
ence point (,viewing-coordinate origin). Figure 12-5 illustrates this method for 
defining the direction of N, which is from the look-at point to the view reference 
point. Another possibll~ty is to set up a left-handed viewing system and take N 
and the positive xi, axls irom the vie\i.ing origin to the look-at point. Only the di- 
rection of N is needed to establish the z,. direction; the magnitude is irrelevant, 
because N will be normalized to a unit vector by the viewing calculations. 

F~nally we choosc thc up dirertlon for the view by specifving a vector V, 
called the vierv-up vector This wctor is used to establish the positive direction 
for the !I:. axi5. kc tor  V also ran be defined as a world-roord~nate vector, or in 
some packages, rt is specified with a t71lrst R I I X ~ ~ ~  H, about thc z:,  axis, as shown in 
Fig. 12-6. For a general vrientation of the normal vector, ~t can be difficult (or at 
least time consuming) to d e t e r n ~ l n ~  the directlon for V that is precisely perpen- 
dicular to N Therefore, \ irwing procedurc3 typically adjust the user-defined ori- 
entation of vector V, as shown in Fig. 12-7, so that V is projected into a plane that 
is perpendicular to the normal vector. We can choose the view.up vector V to be 
in any convenient dirvcti~)n, as long a5 i t  is not parallel to IV, AS an example, con. 



sider an interactwe specification of viewing reference coordinates using PHIGS, 
where the view reference point is often set at the center of an object to be viewed. 
If we then want to view the object at the angled direction shown in Fig. 12-8, we 
can simply choose V as the world vector (0.1, O), and this vector will be projected 
into the plane perpendicular to N to establish the y, axis. This is much easier than 
trying to input a v~ctor  that is exactly perpendicular to N. 

Using vectors N and V, the graphics package can compute a third vector U, 
perpendicular to both N and V, to define the direction for the xu axis. Then the di- 
rection of V can be adjusted so that it is perpendicular to both N and U to estab- 
lish the viewing y, direction. .4s we will see in the next section (Transformation 
from World to Viewing Coordinates), these computations are conveniently car- 
ried out with unit axis vectors, which are also used to obtain the elements of the 
world-to-viewing-coordinate transformation matrix. The viewing system is then 
often described as a uvn system (Fig. 12-9). 

Generally, graphics packages allow users to choose the position of the view 
plane (with some restrictions) along the z,  axis by specifying the vieu-plane dis- 
tance from the viewing origin. The view plane is always parallel to the x,y,, plane, 
and the projection of objects to the view plane correspond to the view of the 
scene that will be displayed on the output device. Figure 12-10 gives examples of 
view-plane position~ng. If we set the view-plane distance to the value 0, the x,y,, 
plane (or ur! plane) of viewing coordinates becomes the view plane for the projec- 
tion transformation. Occasionally, the term "uv plane" is used in reference to the 
viewing pIane, no matter where it is positioned in relation to the x,y, plane. But 
we will only use the term "uv plane" to mean the x,y,, plane, which is not neces- 
sarily the view plane. 

Left-handed viewing coordinates are sometimes used in graphics packages 
so that the viewing direction is in the positive i,, direction. But right-handed 
viewing systems are more common, because they have the same orientation as 
the world-reference frame. This allows graphics systems to deal with only one 
coordinate orientation for both world and viewing references. We will follow thc 
convention of PHIGS and GL and use a right-handed viewing system for all al- 
gorithm development. 

Figure 12-7 
Adjusting the input position 
of the view-up vector V to a 
position perpendicular to the 
normal vector N. 

Figure 12-8 
Clmoc.sing V along they,, axis 
sets the up orientation for the 
\.lew plane in the desired 
direction. 

To obtain a serles of views of a scene, we can k w p  the view reference point 
fixed and change the direction of N, as shown in Fig. 12-11. This corresponds to 
generating views as we move around the viewing-coordinate origin. In interac- 

F~,qurr 12-9 
A right-handed viewing 
system defined with unit 
vectors u, v, and n. 

- - - - - . - - . - - 
F i p r r  12-10 
Viewplane positioning along the z, 
axis. 



Chapter I2 
Three-Dimenrional Wewing 

Figure 12-11 
Viewing a scene from different directions with a fixed view-reference 
point. 

tive applications, the normal vector N is the viewing parameter that is most often 
changed. By changing only the direction of N, we can view a scene fmm any di- 
rection except along the line of V. To obtain either of the two possible views 
along the line of V, we would need to change the direction of V. If we want to 
simulate camera motion through a scene, we can keep N fixed and move the 
view reference point around (Fig. 12-12). 

Figure 12-12 
Moving around inr scene by 
:hanging the position of the view 
reference point. 



Transforrnstion from World lo Viewing Coordinates Section 12-2 
\ ' i e w q  Coordinaltr 

Before object descriptions can be projected to the view plane, they must be trans- 
ferred to viewing coordinates. Conversion of object descriptions from world to 
viewing coordinates is equivalent to a transformation that superimposes the 
viewing reference frame onto the world frame using the basic geometric trans- 
late-rotate operations discussed in Section 11-7. This transformation sequence is 

I .  Translate the view reference point to the origin of the world-coordinate sys- 
tem. 

2. Apply rotations to align the x,, y,, and z,. axes with the world xu,, y,., and z,, 
axes, respectively. 

If the view reference point is specified at world position (xo yo, zo), this point is 
translated to the world origin with the matrix transformation 

The rotation sequence can require up  to three coordinate-axis rotations, de- 
pending on the direction we choose for N. In general, if N is not aligned with an): 
work-coordinate axis, we can superimpose the viewing and world systems with 
the transformation sequence R, . R, . R,. That is, we first rotate around the world 
x,, axis to bring z,. into the x , ~ ,  plane. Then, we rotate around the world y,, axis to 
align the z, and z,, axes. The final rotation is about the z ,  axis to align the y,, and 
y,. axes. FUI-ther, i f  the view reference system is left-handed, a reflection of one of 
the viewing axes (for example, the z,, axis) is also necessary. F~gure 12-13 illus- 
trates the general sequence of translate-rotate traniformations The composite 
transformation matrix is then applied to world-coordinate descriptions to trans- 
fer them to viewing coordinates. 

Another method for generating the rotation-transformation matrix is to cal- 
culate unit uvn vectors and form the composite rotation matrix directly, as dis- 

- - - - - . - - - - - - - -. . - . - . . - - - - - - 
Figure 12-13 
Aligning a viewmg system with the worldcoordinate axes using a 
sequence of translate-rotate transformations. 



Chapter 12 cussed in Section 11-7. Civen vectors N and V, these unit vectors are calculated 
Thr~e-Dimens~onal Viewmg as 

This method also autoniat~cally adjusts the direction for V so that v is perpendic- 
ular to n. The composite rotation matrix for the viewing transformation is then 

which transforms u onto the world x,,, axis, v onto the !I:,. axis, and n onto the z .  
axis. In addition, this matrix automatically performs the reflection necessary to 
transform a left-handed viewing svstem onto the right-handed world system. 

The complete urorld-to-viewing coordinate tran!;formation matrix is oh- 
tained as the matrix product 

This transformation is then applied to coordinate descriptions of objects in the 
scene to transfer them to the viewing reference frame. 

12-3 
PROJECTIONS 

Once world-coordinate descriptions of the objects in a scene are converted to 
viewing coordinates, we can project the three-dimensional objects onto the trio- 
dimensional view plane. There are two basic projection melhods. In a parallel 
projection, coordinate positions are transformed to the v ied  plane along parallel 
lines, as  shown in the ,example of Fig. 12-14. For a perspective projection (Fig. 
12-15), object positions are transformed to the view plane a!ong lines that con- 
verge to a point called the projection reference point (or center of projection). 
The projected view oi ,In objcct is determined calcrllatin~ the interjection of 
the projection lines with the vie\z. plane. 

View 
Plane 

. . - . - . . . . . . . . . . 
f ' ipir is  12-14 
Parallel project~on d ,in obiect to 
thv \.let\' planc 



Plane 

.. . 
Fi,yrrrc. 12-15 
I'zrspective projection of an  object 
:(' the view plane. 

A parallel projection preserves relat~ve proportions of objects, and this is 
the method used in drafting to produce scale drawings of thrce-dimensional ob- 
jects. Accurate views of the various sides of an object are obtained with a parallel 
projection, but this does not give us a realistic representation of the appearance of 
a three-dimensional object. A perspective projection, on theother hand, produces 
realistic views but does not preserve relative proportions. Projections of distant 
objects are smaller than the projections of objects of the same size that are closer 
to the projection plane (Fig. 12-16). 

Parallel Projections 

We can specify a parallel projection with a projection vector that defines the dl- 
rection for the projection lines. When the projection is perpendicular to the view 
plane, we have a n  orthographic parallel projection. Otherwise, we have an 
oblique parallel projection. Figure 12-17 illustrates the two types of parallel pro- 
jections. Some graphics packages, such as GL on Silicon Graphics workstations, 
do not provide for oblique projections. In this pa(-kage, for example, a parallel 
projection is spec~fied by s ~ n ~ p l y  giving the boundary edges of a rectangular par- 
allelepiped. 

Ili.. 

Projection 
Reference 

Section 12-3 

Projecllons 



Orthographic Pro]-.ctton 
la )  

Oblique Projection 
(h) 

Orthographic proleitions are most often used to produce the front, side, and 
top wews of an object, a s  shown in Fig. 12-18 Front, side, and rear orthographic 
projections of an object are called elezatiorrs; and a top orthographic projection is 
callcd a \hri :Gi13. Engineering and architectr~ral drawings commonly employ 
these orthographic projritions, because lengths and angles are accurately de- 
yictc,d and can be rne , i s~~rd  from the drawings. 

We can also form orthographic projections that dlsplav more than one face 
of an object. Such vier\.> are called axonometric orthographic projections. The 
most commonlv used axcmometric projection is the isometric projection. We gen- 
erate an isometric projei-t-on by aligning the projection plane so that it intersects 
each coordinate axis in  W ~ I I C ~  the object is defined (called the pr i t l c ip l  nxes) at the 
same d~st~lncc iron1 the origin. Figure 12-19 shows an isometric projection for a 

Plan VIOL,, - 
-----A 

Front Elevation View 

Side Elevation View 

/ / 



Section 12-3 
Projections 

Figure 12-19 
Isometric projection for a cube. 

cube. The isometric projection is obtained by aligning the projechon vector with 
the cube diagonal. There are eight positions, one in each Want, for obtaining an 
isometric view. All three principal axes are foreshortened equally in an isometric 
projection so that relative proportions are maintained. This is not the case in a 
general axonometric projection, where scaling factors may be different for the 
three principal directions. 

Transformation equations for an orthographic parallel projection are 
straightfonvard. If the view plane is placed at position z ,  along the z ,  axis (Fig. 
12-20), then any point (x ,  y, z) in viewing coordinates is transformed to projection 
coordinates as 

where the original z-coordinate value is preserved for the depth information 
needed in depth cueing and visible-surface determination procedures. 

An oblique projection is obtained by projechng points along parallel lines 
that are not perpendicular to the projection plane. In some applications packages, 
an oblique projection vector is specified with two angles, a and 4, as shown in 
Fig. 12-21. Point (x ,  y, z )  is projected to position (x,, y,) on the view plane. Ortho- 
graphic projection coordinates on the plane are ( x ,  y ) .  The oblique projection line 
from (x ,  y ,  z )  to (x,, y,) makes an angle a with the line on the projection plane that 
joins (x,, y,,) and ( x ,  y ) .  This line, of length L, is at an angle I$ with the horizontal 
direction in the projection plane. We can express the projechon coordinates in 
terms of x, y, L, and 6 as 

Figure 12-20 
Orthographic projection of a point 
onto a viewing plane. 



Three-Dimensronal Viewing 

- ~ . . 

Figure 1 2 ~ 2  1 
Oblique projection of coordinate 
position (x y, z) l o  position (I,, y,,) 
on the view plane. 

Length L dependsort the angle a and the z coordinate of the point to be pro- 
jected: 

tana = - 
L 

Thus, 

where L, is the inverse of tana, which is also the value of L when z = 1. We can 
then write the oblique prcjection equations 12-6 as 

The transformation matrix for producing any parallel projection onto the 
xu% plane can be written 21s 

An orthographic projt*c.t~cln is ohtamed when L ,  = 0 (wh~ch occurs at a projection 
angle a of 90"). Oblique projections are generated with nonzero values for L , .  
Projection matrix 12-10 has a structure similar to that of ,I z-axis shear matrix. In 
fact, the effect of this projection matrix is to shear planes ot constant 2 and project 
them onto the view plane. The x- and y-coordinate valut..; within each plane of 
constant z are shifted by an amount proportional to the : value of tlic plane so 
that angles, distances, anJ parallel lines in the plane arc projected acc-urately. This 



effect is shown in Fig. 12-22, where the back plane of the box is sheared and over- 12-3 
lapped with the hont plane in the propction to the viewing surface. An edge of Proiections 
the box connecting the front and back planes is projected into a line of length L, 
that makes an angle 4 with a horizontal line in the projection plane. 

Common choices for angle I$ are 30" and 4S0, which display a combination 
view of the front, side, and top (or front, side, and bottom) of an obpct. Two com- 
monly used values for a are those for which tana = 1 and tana = 2. For the first 
case, a = 45" and the views obtained are called cavalier propctions. All lines per- 
pendicular to the projection plane are projected with no change in length. Exam- 
ples of cavalier projections for a cube are given in Fig. 12-23. 

When the propction angle a is chosen so that tan a = 2, the resulting view is 
called a cabinet projection. For this angle (=63.4"), lines perpepdicular to the 
viewing surface are projected at one-half their length. Cabinet projections appear 
more realistic than cavalier projections because of this reduction in the length of 
perpendiculars. Figure 12-24 shows examples of cabinet projections for a cube. 

Perspective Projections 

To obtain a perspective projection of a three-dimensional ob*, we transform 
points along projection lines that meet at the projection reference point. Suppose 
we set the propchon reference point at position z ,  along the z, axis, and we 

F i p w  12-23 
Cavalier pro)ections of a cube onto a view plane for two values of 
angle 4. 
Note: Depth of the cube is projected equal to the width and height 



Chaoter 1 2  

Three-Dimensional Viewing 

Figunz 12-24 
Cabinet projections of a cube onto a view phne for two values of 
angle I$. Depth is projected as one-half that of the width and 
height. 

place the view plane at  z,, as shown in Fig. 12-25. We can write equations de- 
scribing coordinate positions along this perspective projection line in parametric 
form as 

Parameter u takes values from 0 to 1, and coordinate position (x', y', z') repre- 
sents any point along the projection line. When u = 0, we are at  position P = (x ,  
y, z). At the other end of the line, u = 1 and we have the projection reference 
point coordinates (0, 0, z,,). On the view plane, z' = z ,  and we  can solve the z' 
equation for parameter u at this position along the projection line: 

Substituting this value of u into the equations for x' and y', we obtain the per- 
spective transformation equations 

Figure 12-25 
Perspective projedion of a point 
P with coordinates (1, y, z) to 

View, 
Plane 

position (x,, yr 2,) on the view 
plane. 



Section 12-3 

Projections 

(12-13) 

whered, = z,, - z,is thedistance of the view plane from the projection refer- 
ence point. 

Using a three-dimensional homogeneous-coordinate representation, we can 
write the perspectiveprojection transformation 12-13 in matrix form as 

In this representation, the homogeneous factor is 

and the projection coordinates on the view plane are calculated from the homo- 
geneous coordinates as  

where the original z-coordinate value would be retamed in projection coordinates 
for visible-surface and other depth processing. 

In general, the projection reference point does not have to be along the i,, 
axis. We can select any coordinate position (x,,,,, yP,, z,) on either side of the 
view plane for the projection reference point, and we discuss this generalization 
in the next section. 

There are a number of special cases for the perspective transformation 
equations 12-13. If the view plane is taken to be the uv plane, then 2, = 0 and the 
projection coordinates are 

And, in some graphics packages, the projection reference point is always taken to 
be at the viewing-coordinate origin. In this case, z,,,, = 0 and the projection coor- 
dinates on the viewing plane are 



Chapter 12 When a three-dimensional obpct is projected onto a view plane using per- 
Three-Dimensional Viewing spective transformation equations, any set of parallel lines in the object that are 

not parallel to the plane are projected into converging lines. Parallel Lines that are 
parallel to the view plane will be projected as parallel lines. The point at whch a 
set of projected parallel lines appears to converge is called a vanishing point. 
Each such set of projected parallel lines will have a separate vdnishing point; and 
in general, a scene can have any number of vanishing points, depending on how 
many sets of parallel lines there are in the scene. 

The vanishmg point for any set of lines that are parallel to one of the princi- 
pal axes of an object is referred to as a principal vanishing point. We control the 
number of principal vanishing points (one, two, or three) with the orientation of 
the projection plane, and perspective projections are accordingly classified as 
one-point, two-point, or three-point projections. The number of principal vanish- 
ing points in a projection is determined by the number of principal axes intersect- 
ing the view plane. Figure 12-26 illustrates the appearance of one-point and two- 
point pespective projections for a cube. In. Fig. 12-26(b), the view plane is 
aligned parallel to the xy object plane so that only the object z axis is intersected. 

(a1 
Coordinate 
Descr~pt~on 

L I 
lt, 

One PWVI 
Perspecwe 
Prolecr~on 

. - -  
Vanishing 

Van~sh~ng 
- .  

Poinl 
Po~nt 

Two-Point 
Perspective 
Projectcrion 

Perspective views and principal vanishing points ot a cube for 
various orientations of the view plane relative to the principal 
axes of the object 



This orientation produces a one-point perspective projection with a z-ax~s vanish- 
ing point. For the view shown i n ~ i ~ .  12-26(c), the projection plane intersects both 
the x and z axes but not the y axis. The resulting two-point perspective projection 
contains both x-axis and z-axis vanishing points. 

12-4 
VIEW VOLUMES A N D  GENERAL PROIECTI(')N 
TRANSFORMATIONS 

In the camera analogy, the type of lens used on the camera is one factor that de- 
termines how much of the scene 1s caught on film. A wideangle lens takes in 
more of the scene than a regular lens. In three-dimensional viewing, a rectangu- 
lar view window, or projection window, in the vlerv plane is used to the same 
effect. Edges of the view window are parallel to the x,y,, axes, and the window 
b o u n d a i p s i t i o n s  are specified in viehing coordinates, as shown in Fig. 12-27. 
The view window can be placed anywhere on the view plane. 

Given the specification of the view window, we can set up  a view volume 
usine the window boundaries. Onlv those obiects within the view volume will " 
appear in the generated display on an output dev~ce; all others are clipped from 
the display. The size of the view volume depends on the size of the window, 
while the shape of the view volume depends on the type of projection to be used 
to generate the display. In any case, four sides of the volume are planes that pass 
through the edges of the window. For a parallel projection, these four sides of the 
view volume form an infinite parallelepiped, as in Fig. 12-28. For a perspective 
projection, the view volume is-a with apex at  the projection reference 
point (Rg. 12-29). 

A finite view vdume is obtained by limiting the extent of the volume in the 
2, .  direction. This is done by specifying positions for one or two additional 
boundary planes. These z,-boundary planes are referred to as the front plane and 
back plane, o r  the near plane and the far plane, of the viewing volume. The 
front and back planes are parallel to the view plane at specified-positions z,,, 
and z,,,,. Both planes must be on the same side of the projection reference point, 
and the back plane must be farther from the projectmn point than the front plane. 
Including the front and back planes produces a view volume bounded by six 
planes, as shown in Fig. 12-30 With an orthographic parallel projection, the six 
planes form a rectangular parallelepiped, while an oblique parallel projection 
produces an oblique parallelepiped view volume. With a perspective projection, 
thefront and back clipping planes truncate the infinite pyramidal view volume 
to form a frustum. 

Front and back clipping planes allow us to eliminate parts of the scene from 
the viewing operations based on depth. We can then p ~ c k  oul parts of a scene that 
we would like to view and exclude objects that are 111 fronl of or behind the par1 
that we want to look at. Also, in a perspective projection, we can use the front 
clipping plane to take out large objects close to the view plane that can project 
into unrecogn~zable sections within the view window. Similarly, the back clip- 
ping plane can he used to cut out objects far from the projection reference point 
that can project to small blotson the output devict*. 

Relative placement of the view plane and the front and hack clipping planes 
depends on thr  type of view we want to generate and the limitations of a particu- 
lar graphics package. With PHICS, the view plane can be positioned anywhere 
along the z, axis except that i t  cannot contain the prolection reference point. And 

View 
Plane 

t i q ~ r r c ,  12-27 
Window specification on the 
view plane, with minimum 
and maxlmum coordinates 
given in the viewing 
reference system. 



Chapter 12 

Three-Dimensional Viewing 

Orthogrephic. 
Projeclion 

View Volume 
(a) 

Oblique 
Proiection 

View Volume 
(C) 

Orthographic- 
Projection 

View Volume 
(b) 

window 'Arane 
Oblique- 

Projection 
View Volume 

(d) 

- .  . .. - .- 

Figrrre 12-28 
View volume for a parallel propction. In (a) and (b), the side and top 
views of the view volume for an orthographic projection are shown; and 
in (c) and (d), the side and top views of an oblique view volume are 
shown. 

Projection 
Reference 

Point 

Projection 
Reference 

Poirn 
Ib) 

.- -- - - . - - -. - . - - - 
Fiprt. 12-29 
Examples of a perspective-projection view volume for various positions of the projection 
reference point. 



Parallelpiped 

/" 
Back 
Plane 

Plane 

Palallel Projection 
(a) 

kction 12-4 

View Valumes and General 
Projection Transformalions 

f 
Back 
Plane 

Front Point 
Plane 

Perspective Projeftion 
(b) 

- -. . 
Figlire 12-30 
View volumes bounded by front and back planes, and by top, bottom, and side planes. Front 
and back planes are parallel to the view plane at positions zh, and zkk along the zi, axis. 

the front and back planes can be in any position relative to the view plane as long 
as  the projection reference point is not between the front and back planes. Figure 
12-31 illustrates possible arrangements of the front and back planes in relation to 
the view plane. The default view volume in lJHIGS is formed as a unit cube 
using a parallel projection with z,,,, = 1, zbaCk = 0, the view plane coincident with 
the back plane. and the projection reference point at position (0.5,0.5,1.0) on the 
front plane. 

Orthographic parallel projections are not affected by view-plane position- 
ing, because the pryection lines are perpendicular to the view plane regardless of 

1 
- Back Plane V i e w  Plane - Back Plane = View Plane 
- View Plane I B a c k  Plane - Front Plane 

(cl 
2" 2" - Front Plane - Front Plane 

(al (b) 

- A - - - - -. 
Figorr' 12-37 
Possible arrangements of the front and back clipping planes relative to the view plane. 



Chapter 12 

Three-Dimensional Wewing 
window 

View Plane 

Projection 
Reference 

Point 

View Plane window 

-75Y 
Projection 
Reference 

Point 
la) (bi 

Figure 12-32 
Changing the shape of the oblique-projection view volume by 
moving the wmdow posihon, when the projection vector V, is 
determined by the projection reference point and the window 
position. 

its location. Oblique projections may be affected by view-plane positioning, de- 
pending on how the projection direction is to be specified. In PHIGS, the oblique 
projection direction is parallel to the line from the projection reference point to 
the center of the window. Therefore, moving the position of the view plane with- 
out moving the projection reference point changes the skewness of the sides of 
the view volume, as shown in Fig. 12-32. Often, the view plane is positioned at 
the view reference point or on the front clipping plane when generating a parallel 
projection. 

Perspective effects depend on the positioning of the projection reference 
point relative to the view plane, a s  shown in Figure 12-3.3 If-we place the p r o p -  

View 
', Window ,' 
r 

\ I 

View 
Window 

Projection 
Reference 

Point 

(81 

Projection 
Reference 

Point 

(b) 

View 
I Windm I 
I I 
I I 
I I 
I I 
I I 
I I 
I I 
I I 
I I 
I I 
I I 
I I 

I , I 
I 
I 

I 
< I 
I I 
I I 
I I 
I I 

I 

Projection 
Reference 
Very Far 

from window 

(ci 

~ ---- ~~~ -~ ~ 

Fi*prc 12-33 
Changing perspective effects by moving the proiect~on reference point amray from !he 
view plane 



Projection 
Reference 

Point 

(a1 

Projection 
Reference 

Point 
(bl 

Figure 12-34 
Projected object size depends on whether the view plane is positioned in 
front of the o b i t  or behind it, relative to the position of the projection 
reference point. 

tion reference point close to the view plane, perspective effects are emphasized; 
that is, closer objects will appear much larger than more distant obpcts of the 
same size. Similarly, as we move the projection reference point farther from the 
view plane, the difference in the size of near and. far objects decreases. In the 
limit, as we move the projection reference point infinitely far from the view 
plane, a perspective projection approaches a parallel projection. 

The projected size of an object in a perspective view is also affected by the 
relative position of the object and the view plane (Fig. 12-34). If the view plane is 
in front of the object (nearer the projection reference point), the projected size is 
smaller. Conversely, object size is increased when we project onto a view plane in 
back of the object. 

View-plane positioning for a perspective projection also depends on 
whether we want to generate a static view or an animation sequence. For a static 
view of a scene, the view plane is usually placed at the viewing-coordinate ori- 
gin, which is at some convenient point in the scene. Then it is easy to adjust the 
size of the window to include all parts of the scene that we want to view. The 
projection reference point is positioned to obtain the amount of perspective d e  
sired. In an animation sequence, we can place the projection reference point at 
the vicwing-coordinate origin and put the view plane in front of the scene (Fig. 
12-35). This placement simulates a camera reference frame. We set the field of 
view (lens angle) by adjusting the size of the window relative to the distance of 
the view plane from the projection reference point. We move through the scene 
by moving the viewing reference frame, and the projection reference point will 
move with the view reference point. 

k t i o n  12-4 

View Volumes and General 
Projection Transformations 



Thrre-Dimensional Wewing 

View 
Volume 

. . ~- - 
f'i:411in I?-<;  
Regular parallelepiped view 
volume obtained by shearing 
the view volume in Fig. 12-36. 

View 
Plane 

-- -- - - - - . - - -- -. . - - 
Fig lrr~  12-35 
View-plane positioning to simulate a camera reference frame for an 
animation sequence. 

General Parallel-Projection Transformations 

In PHIGS, the direction of a parallel projection is specified with a projection vec- 
tor from the projection reference point to the center of the view window. Figure 
12-36 shows the general shape of a finite view volume for a given projection vec- 
tor and projection window in the view plane. We obtain the obliqueprojection 
transformation with a shear operation that converts the view volume in Fig. 
12-36 to the regular parallelepiped shown in Fig. 12-37. 

The elements of the shearing transformation needed to generate the view 
volume shown in Fig. 12-37 are obtained by considering the shear transformation 
of the projection vector. If the projetion vector is specified in world coordinates, 
it must first be transformed to viewing coordinates using the rotation matrix dis- 
cussed in %ion 12-2. (The p r o w o n  vector is unaffected by the translation, 
since it is simply a direction with no fixed position.) For graphics packages that 
allow specification of the p r o m o n  vector in viewing coordinates, we apply the 
shear directly to the input elements of the projection vector. 

Suppose the elements of the projection vector in viewing coordinates are 

We need to determine the elements of a shear matrix that will align the projection 
vector Vp w ~ t h  the,view plane normal vector N (Fig. 12-37). This transformation 
can be expressed as 

View 
Volume 

N 
window 

F i p r e  12-36 
Oblique projection vector and 
associated view volume. 

-'" 

I 



V'p = Mp."el . Vp k c ( b n  12-4 
View Volumes and General 
Projection Transformations 

(12-20) 

where Gel is equivalent to the parallel projechon matrix 12-10 and represents 
a z-axis shear of the form 

1 0 0 0  
O l b O  

--= [. 0 1 
0 0 0 1  

The explicit transformation equations from 12-20 in terms of shear parameters a 
and b are 

so that the values for the shear parameters are 

Thus, we have the general parallel-propchon matrix in terms of the e l e  
ments of the projection vector as 

This matrix is then concatenated with transformation Re T, from Section 12-2, to 
produce the transformation from world coordinates to parallel-projection coordi- 
nates. For an orthographic parallel projection, p, = p,. = 0, and is the iden- 
tity matrix. From Fig. 12-38, we can relate the components of the projection vec- 
tor to parameters L, a, and $(Section 12-3). By similar triangles, we see that 

which iliustrates the equivalence of the elements of transformation inatrices 12- 
10 and 12-24. In Eqs. 12-25, z and p, are of opposite signs, and for the positions il- 
lustrated in Fig. 12-38, z < 0. 



Cha@er 12 
Three-Dimensional Wewing 

Figrtre 12-38 
Relationship between the parallel- 
projection vector V, and parameters 
L, N, and 4. 

General Perspective-Projection Transformations 

With the PHIGS programming standard, the projection reference point can be lo- 
cated at any position in the viewing system, except on the view plane or between 
the front and back clipping planes. Figure 12-39 shows the shape of a fmite view 
volume for an arbitrary position of the propchon reference point. We can obtain 
the general perspective-projection transformation with the following two opera- 
tions: 

1. Shear the view volume so that the centerliie of the frustum is perpendicu- 
lar to the view plane. 

2. Scale the view volume with a scaling factor that depends on 1 / z .  

The second step (scaling the view volume) is equivalent to the perspective trans- 
formation discussed in Section 12-3. 

A shear operation to align a general perspective view volume with the pro- 

/- Frustum Centerline 

-- .- .- 
Figurp 12-39 
General shape for the perspective view volume with a 
projetion reference point that is not on the z .  axir; 



- Frustum 
Centerline 

jection window is shown in Fig. 12-40. This transformation has the effect of shift- 
ing all positions that lie along the frustum centerline, including the window cen- 
ter, to a line perpendicular to the view plane. With the projection reference point 
at a general position ( x ~  yprp, z ~ ) ,  the transformation involves a combination 2- 
axis shear and a translat~on: 

where the shear parameters are 

Mi 12-4 

Vlew Volumes and General 
Projection Transformations 

Figure 12-40 
Shearing a general perspective view 
volume to center it on the projection 
window. 

Points within the view volume are transformed by this operation as 

When the projection reference point is on the z,, axis, xprp = yprp = 0. 
Once we have converted a position ( x ,  y, z )  in the ongmal view volume to 

position (x', y', 2') in the sheared h s t u m ,  we then apply a scaling transformation 
to produce a regular parallelepiped (Fig. 13-40). The transformation for this con- 
version is 



Chapter 1 2 

Three-Dimensional Wewing 

and the homogeneous matrix representation is 

Therefore, the general perspective-projection transformation can be ex- 
pressed in matrix form as 

Mpnpcbvc = K a l e  . Mshcar (22-31) 
The complete transformation from world coordinates to perspective-proption 
coordinates is obtained by right concatenating Mpenpedlve with the composite 
viewing transformation R T from Section 12-2. 

12-5 
CLIPPING 

In this section, we first explore the general ideas involved in three-dimensional 
clipping by considering how clipping could be performed using the view-vol- 
ume clipping planes directly. Then we discuss more efficient methods using nor- 
malized view volumes and homogeneous coordinates. 

An algorithm for three-dimensional clipping identifies and saves all surface 
segments within the view volume for display on the output device. All parts of 
objects that are outside the view volume are discarded. Clipping in three dimen- 
sions can be accomplished using extensions of two-dimensional clipping meth- 
ods. Instead of clipping against straight-line window boundaries, we now clip 
objects against the boundary planes of the view volume. 

To clip a line segment against the view volume, we would need to test the 
relative position of the line using the view volume's boundary plane equations. 
By substituting the line endpoint coordinates into the plane equation of each 
boundary in turn, we couM determine whether the endpoint is inside or outside 
that boundary. An endpoint (x, y, z )  of a line segment is outside a boundary plane 
if Ax + By + Cz + D > 0, where A, 8 ,  C, and D are the plane parameters for 
that boundary. Similarly, the point is inside the boundary if Ax + By + Cz + 
D < 0. Lines with both endpoints outside a boundary plane are discarded, and 
those with both endpoints inside all boundary planes are saved. The intersection 
of a line with a boundary is found using the line equations along with the plane 
equation. intersection coordinates (xl,  y,, z,) are values that are on the line and 
that satisfy the plane equation Ax, + Byl + Cz, + D = 0. 

To clip a polygon surface, we can clip the individual polygon edges. First, 
we could test the coordinate extents against each boundary of the view volume 
to determine whether the object is completely inside or completely outs~de that 



boundary. If the coordinate extents of the object are inside all boundaries, we -12-5 
save it. If the coordinate extents are outside all boundaries, we discard it. Other- Clipping 
wise, we need to apply the intersection calculations. We could do this by deter- 
mining the polygon edge-intersection positions with the boundary planes of the 
view volume, as described in the previous paragraph. 

As in two-dimensional viewing, the projection operations can take place be 
fore the view-volume clipping or after clipping. All obpcts within the view vol- 
ume map to the interior of the specified projection window. The last step is to 
transform the window contents to a two-dimensional viewport, which specifies 
the location of the display on the output device. 

Clipping in two dimensions is generally performed against an upright rec- 
tangle; that is, the dip window is aligned with the x and y axes. This greatly sirn- 
plifies the clipping calculations, because .each window boundary is defined by 
one coordinate value. For example, the intersections of all lines crossing the left 
boundary of the window have an x coordinate equal to the left boundary. 

View-volume clipping boundaries are planes whose orientations depend on 
the type of projection, the propchon window, and the position of the projection 
reference point. Since the front and back clipping planes are parallel to the view 
plane, each has a constant z-coordinate value. The z coordinate of the inters- 
tions of lines with these planes is simply the z coordinate of the corresponding 
plane. But the other four sides of the view volume can have arbitrary spatial ori- 
entations. To find the intersection of a line with one of the view volume bound- 
aries means that we must obtain the equation for the plane containing that 
boundary polygon. This process is simplified if we convert the view volume be  
fore clipping to a rectangular parallelepiped. In other words, we first perform the 
projection transformation, which converts coordinate values in the view volume 
to orthographic parallel coordinates, then we carry out the clipping calculations. 

Clipping against a regular parallelepiped is much simpler because each sur- 
face is now perpendicular to one of the coordinate axes. As seen in Fig. 12111, the 
top and bottom of the view volume are now planes of constant y, the sides are 
planes of constant x,  and the front and back are planes of constant z. A line cut- 
ting through the top plane of the parallelepiped, for example, has an intersection 
point whose y-coordinate value is that of the top plane. 

In the case of an orthographic parallel projection, the view volume is al- 
ready a rectangular parallelepiped. As we have seen in Section 12-3, obliquepro- 
jechon view volumes are converted to a rectangular parallelepiped by the shear- 
ing operation, and perspective view volumes are converted, in general, with a 
combination shear-scale transformation. 

v 

Figure 12-41 
An obpct intersecting a rectangular 
parallelepiped view volume. 



Chapter 1 2  Normalized View Volumes 
Three-Dimensional Mewing 

Figure 12-42 shows the expanded PHIGS transformation pipeline. At the first 
step, a scene is constructed by transforming obpct descriptions from modeling 
coordinates to world coordinates. Next, a view mapping convert: the world de- 
scriptions to viewing coordinates. At the projection stage, the viewing coordj- 
nates are transformed to projection coordinates, which effectively converts the 
view volume into a rectangular parallelepiped. Then, the parallelepiped is 
mapped into the unit cube, a normalized view volume called the normalized 
projection coordinate system. The mapping to normalized projection coordi- 
nates is accomplished by transforming points within the rectangular paral- 
lelepiped into a position within a specified three-dimensional viewport, which 
occupies part or all of the unit cube. Finally, at the workstation stage, normalized 
projection coordinates are converted to device coordinates for display. 

The normalized view volume is a region defined by the planes 

A similar transformation sequence is used in other graphics packages, with indi- 
vidual variations depending on the system. The, GL package, for example, maps 
the rectangular parallelepiped into the interior of a cube with boundary planes at 
positions 2 I in each coordinate direction. 

There are several advantages to clipping against the unit cube instead of the 
original view volume or even the rectangular parallelepiped in projection coordi- 
nates. First, the normalrzed view volume provides a standard shape for repre- 
senting any sized view volume. This separates the viewing transformations from 
any workstation considerations, and the unit cube then can be mapped to a 
workstation of any size. Second, clipping procedures are simplified and stan- 
dardized with unit clipping planes or the viewport planes, and additional c l p  
ping planes can be specified within the normalized space before transforming to 

1- World - Vbwing 1 - Viewing , Modeling - Modeling 
Coordinates ~ransfonnmion Coordinates Transformation Coordinates 

Normalized 

1 

. . . - . . .- - - -- . . - - - - - . . . . . - . - - 
FIS I I IL '  12-42 
Expanded PHIGS transformation pipeline 



device coordinates. Third, depth cueing and visiblesurface determination are W i n  12-5 
simplified, since the z axis always points toward the viewer (the projection refer- Clipping 
ence point has now been transformed to the z axis). Front faces of objects are 
those with normal vedors having a component along the positive z direction; 
and back surfaces are facing in the negative z direction. 

Mapping positions within a rectangular view volume to a three-dimen- 
sional rectangular viewport is accomplished with a combination of scaling and 
translation, similar to the operations needed for a two-dimensional window-to- 
viewport mapping. We can express the three-dimensional hansformation matrix 
for these operations in the form 

Factors D,, D ,  and D, am the ratios of the dimensions of the viewport and regu- 
lar parallelepiped view volume in the x,  y, and z directions (Fig. 12-43): 

Parallelepiped 
View Volume 

(a) 
Unil Cube 

(bl 

-- - - - - -- -. - -- - .. - - - 
Figure 12-43 
Dimensions of the view vo111me and three-dimensional viewport. 



C h a w  12 

Three-Dimensional Wewing 
where the view-volume boundaries are established by the window limits (mu,,,, 
nu,, ywmin, yw,,) and the positions zht and zb.& of the front and back planes. 
Viewport boundaries are set with the coordinate values xumin, XU-, yumin, yumnx, 
zv,,, and zv,,. The additive translation factors K,, Ky, and K, in the transforma- 
tion are 

Viewport Clipping 

Lines and polygon surfaces in a scene can be clipped against the viewport 
boundaries with procedures similar to those used for two dimensions, except 
that objects are now processed against clipping planes instead of clipping edges. 
Curved surfaces are processed using the defining equations for the surface 
boundary and locating the intersection lines with the parallelepiped planes. 

The two-dimensional concept of region codes can be extended t9 three di- 
mensions by considering positions in front and in back of the three-dimensional 
viewport, as well as positions that are left, right, below, or above the volume. For 
twwdimensional clipping, we used a fourdigit binary region code to identify the 
position of a line endpoint relative to the viewport boundaries. For threedimen- 
sional points, we need to expand the region â‚¬ode to six bits. Each point in the d e  
scription of a scene is then assigned a six-bit region code that identifies the rela- 
tive position of the point with respect to the viewport. For a line endpoint 
at position (x ,  y, z), we assign the bit positions in the region code from right to 
left as 

hit 1 = 1, if x < xvmi,(left) 

bit 2 = 1, if x > xv,,(right) 

bit 3 = 1, if y < yv,,,,(below) 

bit 4 = 1, if y > yv,,,(above) 

bit 5 = 1, if z <zv,,,(front) 

bit 6 = 1 ,  if z > zv,,,(back) 

For example, a region code of 101000 identifies a point as above and behind the 
viewport, and the region code 000000 indicates a point within the volume. 

A line segment can immediately identified as completely within the 
viewport if both endpoints have a region code of 000000. If either endpoint of a 
line segment does not have a regon code of 000000, we perform the logical and 
operation on the two endpoint codes. The result of this and operation will be 
nonzero for any line segment that has both endpoints in one of the six outside re- 
gions. For example, a nonzero value will be generated if both endpoints are be- 
hind the viewport, or both endpoints are above the viewport. If we cannot iden- 
tify a line segment as completely inside or completely outside the volume, we 
test for intersections with the bounding planes of the volume. 

As in two-dimensional line clipping, we use the calculated intersection of a 
line with a viewport plane to determine how much of the line can be thrown 



away. The remaining part of the line is checked against the other planes, and we Seaion 12-5 
continue until either the line is totally discarded or a section is found inside the Clipping 
volume. 

Equations for three-dimensional line segments are conveniently expressed 
in parametric form. The two-dimensional parametric clipping methods of 
Cyrus-Beck or Liang-Barsky can be extended to three-dimensional scenes. For a 
line segment with endpoints PI = (x,, yl, z,) and P2 = (x2, y2, z2), we can write the 
parametric line equations as 

Coordinates (x,  y, z )  represent any point on the line between the two endpoints. 
At u = 0, we have the point PI, and u = 1 puts us at P2. 

To find the intersection of a line with a plane of the viewport, we substitute 
the coordinate value for that plane into the appropriate param*c expression of 
Eq. 12-36 and solve for u. For instance, suppose we are testing a line against the 
zv,, plane of the viewport. Then 

When the calculated value for u is not in the range from 0 to 1, the line segment 
does not intersect the plane under consideration at any point between endpoints 
PI and P2 (line A in Fig. 12-44). If the calculated value for u in Eq. 12-37 is in the 
interval from 0 to 1, we calculate the intersection's x and y coordinates as 

If either xl or yl is not in the range of the boundaries of the viewport, then this 
line intersects the front plane beyond the boundaries of the volume (line B in Fig. 
12-44). 

Clipping in Homogeneous Coordinates 

Although we have discussed the clipping procedures in terms of three-dimen- 
sional coordinates, PHIGS and other packages actually represent coordinate posi- 
tions in homogeneous coordinates. This allows the various transformations to be 
represented as 4 by 4 matrices, which can be concatenated for efficiency. After all 
viewing and other transformations are complete, the homogeneouscoordirtate 
positions are converted back to three-dimensional points. 

As each coordinate position enters the transfonnation pipeline, it is con- 
verted to a homogeneous-coordinate representation: 



Figure 1244 
Side view of two line segments that are to be dipped against the zv,, 
plane of the viewport. For line A, Eq. 12-37 produces a value of u 
that is outside the range from 0 m I. For line'B, Eqs. 12-38 produce 
intersection coordinates that are outside the range from yv,,, to 

The various transformations are applied and we obtain the final homogeneous 
point: 

whgre the homogeneous parameter h may not be 1. In fact, h can have any real 
value. Clipping is then performed in homogeneous coordinates, and clipped ho- 
mogeneous positions are converted to nonhomogeneous coordinates in three- 
dimensional normalized-proption coordinates: 

We will, of course, have a problem if the magnitude of parameter h is very small 
or has the value 0; but normally this will not occur, if the transformations are car- 
ried out properly. At the final stage in the transformation pipeline, the normal- 
ized point is transformed to a.thrre-dimensiona1 device coordinate point. The xy 
position is plotted on the device, and the z component is used for depth-informa- 
tion processing. 

Setting up clipping procedures in homogeneous coordinates allows hard- 
ware viewing implementations to use a single procedure for both parallel and 
perspective projection transformations. Objects viewed with a parallel projection 
could be corredly clipped in threedimensional normalized coordinates, pro- 



vided the value h = 1 has not been altered by other operations. But perspective ktion'24 
projections, in general, produce a homogeneous parameter that no longer has the Hardware I ~ l e ~ n t a t i o n s  
value 1. Converting the sheared frustum to a rectangular parallelepiped can 
change the value of the homogeneous parameter. So we must clip in homogp 
neous coordinates to be sure that the clipping is carried out comctly. Also, ratiw .- - 
nal spline representations are set up in homogeneous coordinates &th arbitrary 
values for the homogeneous parameter, including h < 1. Negative values for the 
homogeneous can-also be generated in perspective projechon. when 
coordinate po&ions are behind the p&qection reference point. This can occur in 
applications where we might want to move inside of a building or other object to 
view its interior. 

To determine homogeneous viewport clipping boundaries, we note that 
any homogeneous-coord&ate position (&, n, zk:h) ginside the viewport if it sat- 
isfies the inequalities 

Thus, the homogeneous clipping limits are 

And clipping is carried out with procedures similar to those discussed in the pre- 
vious section. To avoid applying both sets of inequalities in 12-42, we can simply 
negate the coordinates for any point with h < 0 and use the clipping inequalities 
for h > 0. 

12-6 
HARDWARE IMPLEMENTATIONS 

Most graphics processes are now implemented in hardware. Typically, the view- 
ing, visible-surface identification, and shading algorithms are available as graph- 
in chip sets, employing VLSl (very largescale integration) circuitry techniques. 
Hardware systems are now designed to transform, clip, and project objects to the 
output device for either three-dimensional or two-dimensional applications. 

Figure 12-45 illustrates an arrangement of components in a graphics chip 
set to implement the viewing operations we have discussed in this chapter. The 
chips are organized into a pipeline for accomplishing geomehic transformations, 
coordinatesystem transformations, projections, and clipping. Four initial chips 
are provided for rnahix operations involving scaling, translation, rotation, and 
the transformations needed for converting world coordinates to projechon coor- 
dinates. Each of the next six chips performs clipping against one of the viewport 
boundaries. Four of these chips are used in two-dimensional applications, and 
the other two are needed for clipping against the front a.nd back planes of the 
three-dimensional viewport. The last two chips in the pipeline convert viewport 
coordinates to output device coordinates. Components for implementation of vis- 
ible-surface identification and surface-shading algorithms can be added to this 
set to provide a complete three-dimensional graphics system. 



Transformarion Operations .I 
World-Cwrdinale 

I 

Clipping Operaions 

I Conversion to Device Coordinates 

~hardwam implementation of three-dimensional viewing operations using 12 chips for 
the coordinate transformations and clipping operations. 

Other spxialized hardware implementations have been developed. These 
include hardware systems for pracessing octree representations and for display- 
ing three-dimensional scenes using ray-tracing algorithms (Chapter 14). 

12-7 
THREE-DIMENSIONAL VIEWING FUNCTIONS 

Several procedures are usually provided in a three-dimensional graphics library 
to enable an application program to set the parameters for viewing transfonna- 
tions. The= are, of course, a number of different methods for structuring these 
procedures. Hem, Ge d k c i ~ ~ ~  the PHlGS functions for three-dimensional view- 
ing. 

With parameters spenfied in world coordinates, elements of the matrix for 
transforming worldcoordinate descriptions to the viewing reference frame are 
calculated using the function 

evaluateViewOrientationHatrix3 (xO, yo,' 20, xN, yN, zN, 
xv, yv, zV, error. viewllatrix) 

This function creates the v i  ewMa t r i x  from input coordinates defining the view- 
ing system, as discussed in Section 12-2. Parameters xo, yo, and z0 specify the 



origin (view reference point) of the viewing system. World-coordinate vector (xN, Section 12-7 
y ~ ,  ZN) defines the normal to the view plane and the direction of the positive z,, Three-Dimensional Viewing 
viewing axis. And world-coordinate vector (xV, yv, zv) gives the elements of the FVnC"OnS 

view-up vector. The projection of this vector perpendicular to (xN, y ~ ,  zN) estab 
lishes the direction for the positive y, axis of the viewing system. An integer error 
code is generated in parameter error if input values are not specified correctly. 
For example, an error will be generated if we set (XV, YV, ZV) parallel to (xN, 
YN, zN). 

To specify a second viewing-coordinate system, we can redefine some or all 
of the coordinate parameters and invoke evaluatevieworientationMa- 
trix3 with a new matrix designation. In this way, we can set up any number of 
world-to-viewingcoordinate matrix transformations. 

The matrix pro jMatrix for transforming viewing coordinates to normal- 
ized projection coordinates is created with the function 

evaluate~iewMappingHatrix3 lxwmin, m x .   in, p a % .  
xvmin, xvmax, yvmin, yvmax. zvmin. zvmax. 
projlype, xprojRef, yprojRef, zprojRef, zview, 

zback, zfront. error, projMatrix) 

Window limits on the view plane are given in viewing coordinates with parame- 
ters xwmin, xwmax, w i n ,  and ywmax. Limits of the three-dimensional viewport 
within the unit cube are set with normalized coordinates xvmin, xvmax, yvmin, 
yvmax, zvmin, and zvrnax. Parameter pro jrype is used to choose the projec- 
tion type as either prallel or perspective. Coordinate position (xpro jRef, ypro j - 
Ref, zpro jRef) sets the projection reference point. This point is used as the ten- 
ter of projection if projType is set to perspective; otherwise, this point and the 
center of the view-plane window define the parallel-projection vector. The posi- 
tion of the view plane along the viewing z, axis is set with parameter zview. Po- 
sitions along the viewing z,, axis for the front and back planes of the view volume 
are given with parameters zfront and zback. And the error parameter re 
turns an integer error code indicating erroneous input data. Any number of pro- 
jection matrix transformations can be created with this function to obtain various 
three-dimensional views and projections. 

A particular combination of viewing and projection matnces is selected on 
a specified workstation with 

setViewRepresentation3 (ws,  viewIndex, viewMatrix, proj~atrix. 
xcl ipmin, xclipmax, yclipmin. yclipmax, zclipmin, 
zclipmax, clipxy, clipback, clipfront) 

Parameter ws is ased to select the workstation, and parameters viewMatrix and 
projMatrix select the combination of viewing and projection matrices to be 
used. The concatenation of these matrices is then placed in the workstation view 
table and referenced with an integer value assigned to Farameter viewIndex. 
Limits, given in normalized projection coordinates, for clipping a scene are set 
with parameters xclipmin, xclipmax, yclipmin, yclipmax, zclipmin, and 
zc 1 ipmax. These limits can be set to any values, but they are usually set to the 
limits of the viewport. Values of clip or noclip are assigned to parameters clipxy, 
clipfront, and clipback to turn the clipping routines on or off for the ry 
planes or for the front or back planes of the view volume (or the defined clipping 
limits). 



C h a w  12 There are sevefal times when it is convenient to bypass the dipping rou- 
Three-Dimcmional u&iw tines. For initial constructions of a scene, we can disable clipping so that trial 

placements of objects can be displayed quiddy. Also, we can eliminate one or 
mom of the clipping planes if we know that all objects are inside those planes. 

Once the view tables have been set up, we select a particular view represen- 
tation on each workstation with the function 

The view index number identifies the set of viewing-transformation parameters 
that are to be applied to subsequently speafied output primitives, for each of the 
adive workstations. 

Finally, we can use the workstation transformation functions to select sec- 
tions of the propaion window for display on different workstations. These oper- 
ations are similar to those discussed for two-dimensional viewing, except now 
our window and viewport regions air three&mensional regions. The window 
fundion selects a region of the unit cube, and the viewport function selects a dis- 
play region for the output device. Limits, in normalized projection coordinates, 
for the window are set with 

and limits, in device coordinates, for the viewport are set with 

Figure 1 2 4  shows an example of interactive selection of viewing parameters in 
the PHIGS viewing pipeline, using the PHIGS Toolkit software. This software 
was developed at the University of Manchester to provide an interface to P H I S  
with a viewing editor, windows, menus, and other interface tools. 

For some applications, composite methods are used to create a display con- 
sisting of multiple views using different camera orientations. Figure 12-47 shows 

Figure 12-46 
Using the PHlGS Toolkit, 
developed at the University of 
Manchester, to interactively control 
parameters in the viewing pipeline. 
(Courfcsy of T. L. 1. Houwrd, I .  G. Willinms, 
and W. T. Hmilt, D e p r t m n t  o\Compuk 
Science, Uniwrsily ~ M n n c k f c r ,  U~rifrd 
Kingdom ) 



w e n  sections, each from a slightly 
different viewing direction. (Courtesy 
3f t k  N E W  Cmtnfbr Suprrompuling 
.4pplications, Unirmity of Illinois at 
YrbP~-Chmlwign.) 

a wide-angle perspective display produced for a virtual-reality environment. The 
wide viewing angle is attained by generating seven views of the scene from the 
same viewing position, but with slight shifts in the viewing direction. 

SUMMARY 

Viewing procedures for three-dimensional scenes follow the general approach 
used in two-dimensional viewing. That is, we first create a world-coordinate 
scene from the definitions of objects in modeling coordinates. Then we set up a 
viewingcoordinate refemce frame and transfer object descriptions from world 
coordinates to viewing coordinates. Finally, viewing-coordinate descriphons are 
transformed to deviecoordir@es. 

Unlike two-dimensional viewing, however, three-dimensional viewing re- 
quires projechon routines to transform object descriptions to a viewing plane be- 
fore the t~ansformation to device coordinates. Also, t h m d i i i o n a l  viewing 
operations involve more spatial parameters. We can use the camera analogy to 
describe tluee-dimensional viewing parameters, which include camera position 
and orientation. A viewingcoordinate reference frame is established with a view 
reference point, a view-plane normal vector N, and a view-up vector V. View- 
plane position is then established along the viewing z axis, and object descrip- 
tions are propded to this plane. Either perspectxve-projection or parallel-pro+- 
tion methods can be used to transfer object descriptions to the view plane. 

Parallel propchons are either orthographic or oblique and can be specified 
with a propchon vector. Orthographic parallel projections that display more than 
one face of an object are called axonometric projections. An isometric view of an 
object is obtained with an axonometric projection that foreshortens each principal 
axis by the same amount. Commonly used oblique proje&ons are the cavalier 
projection and the cabinet projechon. Perspective projections of objects are o b  
tained with projection lines that meet at the projxtion reference point. 

O b j s  in three-dimensional scenes are clipped against a view volume. The 
top, bottom, and sides of the view volume are formed with planes that are paral- 
lel to the projection lines and that pass through the view-plane window edges. 
Front and back planes are used to create a closed view volume. For a parallel pro- 
jection, the view volume is a parallelepiped, and for a perspective projection, the 
view volume is a hustum. Objects are clipped in three-dimensional viewing by 
testing object coordinates against the bounding planes of the view volume. Clip- 
ping is generally carried out in graph& packages in homogeneous coordinates 



Chapter 1 2  after all viewing and other transformations are complete. Then, homogeneous co- 
Three-Dimensional Mewing ordinates are converted to three-dimensionalCartesian coordinates. 

REFERENCES 

For additional information on threedimensional viewing and clipping operations in PHlGS 
and PHIGS+, see Howard et dl. (1991). Gaskins (1992). and Blake (1993). Discussions of 
three-dimensional clipping and viewing algorithms can be found in Blinn and Newell 
(1978). Cyrus and Beck (1978), Riesenfeld (1981). Liang and Barsky (1984), ANO (1991). 
and Blinn (1993). 

EXERCISES 

12-1. Write a procedure to implement the evaluatev iewor ientat  i o M a t r i x 3  func- 
tion using Eqs. 12-2 through 12-4. 

12.2. Write routines to implement the setViewRepresentacion3 and setViewIndex 
functions. 

12-3. Write a procedure to transform the vertices of a polyhedron to projection coordinates 
using a parallel projection with a specified projection vector. 

12-4. Write a procedure to obtain different parallel-projection vlews of a polyhedron by 
first applying a specified rotation. 

12-5. Write a procedure to perform a one-point perspective projection of an object. 
12-6. Write a procedure to perform a two-point perspective projection of an object. 
12-7. Develop a routine to perform a three-point perspective projection of an object, 

12-8. Write a routine to convert a perspective projection frustum to a regular paral- 
lelepiped. 

12-9. Extend the Sutherland-Hodgman polygon clipping algorithm to clip threedimen- 
sional planes against a regular parallelepiped. 

12-10. Devise an algorithm to clip objects in a scene against a defined frustum. Compare 
the operations needed in this algorithm to those needed In an algorithm that clips 
against a regular parallelepiped. 

12-11. Modify the two-dimensional Liang-Barsky linetlipping algorithm to clip three-di- 
mensional lines against a specified regular parallelepiped. 

12-12. Modify the two-dimensional Liang-Barsky line-clipping algorithm to clip a given 
polyhedron against a specified regular parallelepiped. 

12-13. Set up an algorithm for clipping a polyhedron against a parallelepiped. 
12-14. Write a routine to perform clipping in homogeneous coordinates. 

12-15. Using any clipping procedure and orthographic parallel projections, write a program 
to perform a complete viewing transformation from world coordinates to device co- 
ordinates. 

12-16. Using any clipping pocedure, wr'ite a program to perform a complete viewing trans- 
formation from world coordinates to device coordinates for any specified parallel- 
projection vector. 

12-17. Write a program to perform all steps in the viewing p~peline for a perspective trans- 
formation. 





A mapr consideration in the generation of realistic graphics displays is identifying those parts of a scene that are visible from a chosen viewing 
position. There are many approaches we can take to solve this problem, and nu- 
merous algorithms have been devised for efficient identification of visible objects 
for different types of applications. Some methods require more memory, some in- 
volve more processing time, and some apply only to special types of objects. De- 
ciding upon a method for a particular application can depend on such factors as 
the complexity of the scene, type of objects to be displayed, available equipment, 
and whether static or animated displays are to be generated. The various algo- 
rithms are referred to as visible-surface detection methods. Sometimes these 
methods are also referred to as hidden-surface elimination methods, although 
there can be subtle differences between identifying visible surfaces and eliminat- 
ing hidden surfaces. For wireframe displays, for example, we may not want to 
actually eliminate the hidden surfaces, but rather to display them with dashed 
boundaries or in some other way to retain information about their shape. In this 
chapter, we explore some of the most commonly used methods for detecting visi- 
ble surfaces in a three-dimensional scene. 

13-1 
CLASSIFICATION OF VISIBLE-SURFACE DETECTION 
ALGORITHMS 

Visible-surface detection algorithms are broadly classified according to whether 
they deal with object definitions directly or with their projected images. These 
two approaches are called object-space methods and image-space methods, re- 
spectively. An object-space method compam objects and parts of objects to each 
other within the scene definition to determine which surfaces, as a whole, we 
should label as visible. In an irnage-space algorithm, visibility is decided point by 
point at each pixel position on the projection plane. Most visible-surface algm 
rithms use image-space methods, although objectspace methods can be used ef- 
fectively to locate visible surfaces in some cases. Linedisplay algorithms, on the 
other hand, generally use objjt-space methods to identify visible lines in wire- 
frame displays, but many image-space visible-surface algorithms can be adapted 
easily to visible-line detection. 

Although there are major differences in the basic approach taken by the var- 
ious visible-surface detection algorithms, most use sorting and coherence meth- 
ods to improve performance. Sorting is used to facilitate depth cornparisms by 
ordering the individual surfaces in a scene according to their distance from the 



view plane. Coherence methods are used to take advantage of regularities in a kction 13-2 
scene. An individual scan line can be expected to contain intervals (runs) of con- Back-Face Detection 
stant pixel intensities, and scan-line patterns often change little from one line to 
the next. Animation frames contain changes only in the vicinity of moving ob- 
jects. And constant relationships often can be established between objects and 
surfaces in a scene. 

13-2 
BACK-FACE DETECTION 

A fast and simple object-space method for identifying the back faces of a polyhe 
dron is based on the "inside-outside" tests discussed in Chapter 10. A point (x ,  y, 
z) is "inside" a polygon surface with plane parameters A, B, C, and D if 

When an inside point is along the line of sight to the surface, the polygon must 
be a back face (we are inside that face and cannot see the front of it from our 
viewing position). 

We can simplify this test by considering the normal vector N to a polygon 
surface, which has Cartesian components (A, B, C) .  In general, if V is a vector in 
the viewing direction from the eye (or "camera") position, as shown in Fig. 13-1, 
then this polygon is a back face if  

Furthermore, i f  object descriptions have been converted to projection coordinates 
and our viewing direction is parallel to the viewing z,. axis, then V = (0, 0, V;) 
and 

so that we only need to consider the sign of C, the ; component of the normal 
vector N 

In a right-handed viewing system with viewing direction along the nega- 
tive z,, axis (Fig. 13-21, the polygon is a back face if C < 0. AIso, we cannot see any 
face whose normal has z component C ..- 0, since our viewing direction is grazing 
that polygon. Thus, in general, we can label any polygon as a back face if its nor- 
mal vector has a ztomponent value: 

N = ( A . 8  C) -. . -- -- - . - . 
F i p r . c .  1 3 - 1  
Vector V in t h e  wewing direction 
and a back-face normal vector N of 
a polyhedron 



rigure 13-2 
N = I A .  6 . C )  fl "'t A polygon surface with plane parameter C < 0 in a right-handed viewing coordinate system is 

identified as a back face when the - viewing d~rection 1s along the 
I ,  negahve z ,  axis. 

Similar methods can be used in packages that employ a left-handed view- 
ing system. In these packages, plane parameters A, B, C: and D can be calculated 
from polygon vertex coordinates specified in a clockwise direction (instead of the 
counterclockwise direction used in a right-handed system). Inequality 13-1 then 
remains a valid test for inside points. Also, back faces have normal vectors that 
point away from the viewing position and are identified by C 2 0 when the 
viewing direction is along the positive z, axis. 

By examining parameter C for the different planes defining an object, we 
can immed~ately identify all the back faces. For a single convex polyhedron, such 
as the pyramid in Fig. 13-2, this test identifies all the hidden surfaces on the ob- 
ject, since each surface is either completely visible or completely hidden. Also, if 
a scene contains only nonoverlapping convex polyhedra, then again all hidden 
surfaces are identified with the back-face method. 

For other objects, such as the concave polyhedron in Fig. 13-3, more tests 
need to be carried out to determine whether there are additional faces that are to- 

Figure 13-3 tally or partly obscured by other faces. And a general scene can be expected to 
View of a concave contain overlapping objects along the line of sight. We then need to determine 
polyhedron with one face where the obscured objects are partially or comp1etel.y hidden by other objects. In 
partially hidden by other general, back-face removal can be expected to eliminate about half of the polygon 
faces. surfaces in a scene from further visibility tests. 

DEPTH-BUFFER METHOD 

A commonly used image-space approach to detecting visible surfaces is the 
depth-buffer method, which compares surface depths at each pixel position on 
the projection plane. This procedure is also referred to as the z-buffer method, 
since object depth is usually measured from the view plane along the z axis of a 
viewing system. Each surface of a scene is processed separately, one point at a 
time across the surface. The method is usually applied to scenes containing only 
polygon surfaces, because depth values can be computed very quickly and the 
method is easy to implement. But the mcthod can be applied to nonplanar sur- 
faces. 

With object descriptions converted to projection coordinates, each ( x ,  y, 2 )  
position on a polygon surface corresponds to the orthographic projection point 
( x ,  y) on the view plane. Therefore, for each pixel pos~tion (x,  y) on the view 
plane, object depths can be compared by comparing z values. Figure 13-4 shows 
three surfaces at varying distances along the orthographic projection line from 
position (1, y) in a view plane taken as the x ~ , ,  plane. Surface 5, is closest at this 
position, so its surface intensity value at (x, y) is saved. 

We can implement the depth-buffer algorithm in normalized coordinates, 
so that z values range from 0 at the back clipping plane tn 7,,,, at the front c l~p-  



Depth-Buffer Method 

Figure 13-4 
At view-plane position ( x ,  y), 
surface S ,  has the smallest depth 
from the view plane and so is 
visible at that position. 

ping plane. The value of z,, can be set either to 1 (for a unit cube) or to the 
largest value that can be stored on the system. 

As implied by the name of this method, two buffer areas are required. A 
depth buffer is used to store depth values for each (x, y) position as surfaces are 
processed, and the refresh buffer stores the intensity values for each position. Ini- 
tially, all positions in the depth buffer are set to 0 (minimum depth), and the re- 
fresh buffer is initialized to the background intensity. Each surface listed in the 
polygon tables is then processed, one scan line at a time, calculating the depth (z 
value) at each (x, y) pixel position. The calculated depth is compared to the value 
previously stored in the depth buffer at that position. If the calculated depth is 
p a t e r  than the value stored in the depth buffer, the new depth value is stored, 
and the surface intensity at that position is determined and in the same xy 
location in the refresh buffer. 

We summarize the steps of a depth-buffer algorithm as follows: 

-- -- -- -- - 

1. Initialize the depth buffer and refresh buffer so that for all buffer posi- 
tions ( x ,  y), 

2. For each position on each polygon surface, compare depth values to 
previously stored values in the depth buffer to determine visibility. 

Calculate the depth t for each ( x ,  y) position on the polygon. 
If z > depth(x, y), then set 

where Ikkgnd is the value for the background intensity, and I,,,,,(x,y) is 
the projected intensity value for the surface at pixel position (x,y). 
After all surfaces have been processed, the depth buffer contains 
depth values for the visible surfaces and the r e k h  buffer contains 
the corresponding intensity values for those surfaces. 

Depth values for a surface position (x ,  y) are calculated from the plane 
equation for each surface: 



For any scan line (Fig. 13-5), adjacent horizontal positions across the line differ by 
1, and a vertical y value on an adjacent scan line differs by 1. If the depth of posi- 
tion ( x ,  y) has been determined to be z, then the depth z' of the next position (x + 
1, y) along the scan line is obtained from Eq. 13-4 as 

Y - 1  

Figure 13-3 
From position (x, y) on a scan 
line, the next position across 
the line has coordinates 
( X  + 1, y), and the position 
immediately below on the 
next line has coordinates 
(1, y - 1). 

The ratio - A / C  is constant for each surface, so succeeding depth values across a 
scan line are obtained from precrd~ng values with a single addition. 

On each scan line, we start by calculating the depth on a left edge of the 
polygon that intersects that scan line (Fig. 13-6). Depth values at each successive 
position across the scan line are then calculated by Eq. 13-6. 

We first determine the y-coordinate extents of each polygon, and process 
the surface from the topmost scan line to the bottom scan line, as shown in Fig. 
13-6. Starting at a top vertex, we can recursively calculate x positions down a left 
edge of the polygon as x' = x - l/m, where rn is the slope of the edge (Fig. 13-7). 
Depth values down the edge are then obtained recursively as 

If we are processing down a vertical edge, the slope is infinite and the recursive 
calculations reduce to 

An alternate approach is to use a midpoint method or Bresenham-type al- 
gorithm for determining x values on left edges for each scan line. Also the 
method can be applied to curved surfaces by determining depth and intensity 
values at each surface projection point. 

For polygon surfaces, the depth-buffer method is very easy to impjement, 
and it requires no sorting of the surfaces in a scene. But it does require the avail- 
ability of a second buffer in addition to the refresh buffer. A system with a resolu- 

y scan line 
left edge 

intersecoon 

bonom scan line 

- 

F i p w  13-6 
Scan lines intersecting a polygon surface 



Figure 13-7 
Intersection positions on successive scan Lines along a left 
p~lygon edge. 

tion of 1024 by 1024, for example, would require over a million positions in the 
depth buffer, with each position containing enough bits to represent the number 
of depth increments needed. One way to reduce storage requirements is to 
process one section of the scene at a time, using a smaller depth buffer. After each 
view section is processed, the buffer is reused for the next section. 

13-4 
A-BUFFER METHOD 

An extension of the ideas in the depth-buffer method is the A-buffer method (at 
the other end of the alphabet from "z-buffer", where z represents depth). The A- 
buffer method represents an antialiased, area-averaged, acc&dation-buffer method 
developed by Lucasfilm for implementation in the surface-rendering system 
called REYES (an acronym for "Renders Everything You Ever Saw"). 

A drawback of the depth-buffer method is that it can only find one visible 
surface at  each pixel position. In other words, it deals only with opaque surfaces 
and cannot accumulate intensity values for more than one surface, as is necessary 
if transparent surfaces are to be displayed (Fig. 13-81. The A-buffer method ex- 
pands the depth buffer so that each position in the buffer can reference a linked 
list of surfaces. Thus, more than one surface intensity can be taken into consider- 
ation at each pixel position, and object edges can be antialiased. 

Each position in the A-buffer has two fields: 

depth field - stores a positive or negative real number 
intensity field - stores surface-intensity information or a pointer value. 

background 
opaque 
surface ' 

foreground Fipw 13-$ 
transparent 

Viewing an opaque surface through 
a transparent surface requires 
multiple surface-intensity 
contributions for pixel positions. 

Section 13-4 

A-Buffer Method 



Chapter 13 

Visible-Surface Detection Methods 
depth intensiry depth intensity 
field f~eld field field 

(a) (b) 

Flgure 13-9 
Organization of an A-buffer pixel position: (a) single-surface overlap of 
the corresponding pixel area, and (b) multiplesurface overlap 

If the depth field is positive, the number stored at that position is the depth of a 
single surface overlapping the corresponding pixel area. The intensity field then 
stores the RCB components of the surface color at that point and the percent of 
pixel coverage, as illustrated in Fig. 13-9(a). 

If  the depth field is negative, this indicates multiple-surface contributions to 
the pixel intensity. The intensity field then stores a p i n t e r  to a linked Iist of sur- 
face data, as in Fig. 13-9(b). Data for each surface in the linked list includes 

RGB intensity components 
opacity parameter (percent of transparency) 
depth 
percent of area cm7erage 
surface identifier 
other surface-rendering parameters 
pointer to next surface 

The A-buffer can be constructed using methods similar to those in the - 
depth-buffer algorithm. Scan lines are processed to determine surface overlaps of 
pixels across the individual scanlines. Surfaces are subdivided into a polygon 
mesh and clipped against the pixel boundaries. Using the opacity factors and 
percent of surface overlaps, we can calculate the intensity of each pixel as an av- 
erage of the contributions from the overlappmg surfaces. 

13-5 
SCAN-LINE METHOD 

This imagespace method for removing hidden surface5 is an extension of the 
scan-linealg&ithni for tilling polygon interiors. Instead of filling just one surface, 
we now deal with multiple surfaces. As each scan line is processed, all polygon 
surfaces intersecting that line are examined to determine which are visible. 
Across each scan line, d ~ p t h  calculations are made for each overlapping surface 
to determine which is nearest to the view plane. When the visible surface has 
been determined, the mtensity value for that position is entered into the refresh 
buffer. 

We assume that tables are-set up for the various surfaces, as discussed in 
Chapter 10, which include both an edge table and a polygon table. The edge table 
contains coordinate endpoints for each line in-the scene, the inverse slope of each 
line, and pointers into the polygon table to identify the surfaces bounded by each 



line. The polygon table contains coefficients of the plane equation for each sur- Section 13-5 
face, intensity information for the surfaces, and possibly pointers into the edge Scan-Line Melhod 
table. To facilitate the search for surfaces crossinga @ven scan line, we can set u p  
an active list of edges from information in the edge table. This active list will con- 
tain only edges that cross the current scan line, sorted in order of increasing x. In 
addition, we define a flag for each surface that is set on or off to indicate whether 
a position along a scan line is inside or outside of the surface. Scan lines are 
processed from left to right. At the leftmost boundary of a surface, the surface 
flag is turned on; and at the rightmost boundary, it is turned off. 

Figure 13-10 illustrates the scan-line method for locating visible portions of 
surfaces for pixel positions along the line. The active list for &an line 1 contains 
information from the edge table for edges AB, BC, EH, and FG. For positions 
along this scan line between edges AB and BC, only the flag for surface Sl is on. 
Therefo~,  no depth calculations are necessary, and intensity information for sur- 
face S, is entered from the polygon table into the refresh buffer. Similarly, be- 
tween edges EH and FG, only the flag for surface S2 is on. NO other positions 
along scan line 1 intersect surfaces, so the intensity values in the other areas are 
set to the background intensity. The background intensity can be loaded through- 
out the buffer in an initialization routine. 

For scan lines 2 and 3 in Fig. 13-10, the active edge l ~ s t  contains edges AD, 
EH, BC, and FG. Along scan line 2 from edge A D  to edge EH,  only the flag for 
surface S, is on. But between edges EH and BC, the flags for both surfaces are on. 
In this interval, depth calculations must be made using the plane coefficients for 
the two surfaces. For this example, the depth of surface SI is assumed to be less 
than that of S,, so intensities for surface S, are loaded into the refresh buffer until 
boundary BC is encountered. Then the flag for surface SI goes off, and intensities 
for surface S2 are stored until edge FG is passed. 

We can take advantage of-coherence along the scan lines as we pass from 
one scan line to the next. In Fig. 13-10, scan line 3 has the same active list of edges 
as  scan line 2. Since no changes have occurred in line intersections, it is unneces- 
sary again to make depth calculations between edges EH and BC. The two sur- 

Scan L m e  2 
Scan L m e  3 

x. 

F i p r r  13-10 
Scan lir.es crossing the projection of two surfaces, 5 ,  and Sr in the 
view plane. Dashed lines indicate the boundaries of hidden surfaces. 



Subdiv~ding 
Line 

>,. .' 

- - - -- 
Figrtrc 13-21 
Intersecting and cyclically overlapping surfaces that alternately obscure one another. 

faces must be in the same orientation as determined on scan line 2, so the intensi- 
ties for surface S, can be entered without further calculations. 

Any number of overlapping polygon surfaces can be processed with this 
scan-line method. Flags for the surfaces are set to indicate whether a position is 
inside or outside, and depth calculations are performed when surfaces overlap. 
When these coherence methods are used, we need to be careful to keep track of 
which surface section is visible on each scan line. This works only if surfaces do 
not cut through or otherwise cyclically overlap each other (Fig. 13-11). If any kind 
of cyclic overlap is present in a scene, we can divide the surfaces to eliminate the 
overlaps. The dashed lines in this figure indicate where planes could be subdi- 
vided to form two distinct surfaces, so that the cyclic overlaps are eliminated. 

13-6 
DEPTH-SORTING METHOD 

Using both ~mage-space and object-space operations, the depth-sorting method 
performs the following basic functions: 

1. Surfaces are sorted in order of decreasing depth. 
2. Surfaces are scan converted in order, starting with the surface of greatest 

depth. 

Sorting operations are carried out in both image and object space, and the scan 
conversion of the polygon surfaces is performed in image space. 

This method for solving the hidden-surface problem is often referred to as 
the painter's algorithm. In creating an oil painting, an artist first paints the back- 
ground colors. Next, the most distant objects are added, then the nearer objects, 
and so forth. At the final step, the foreground objects are painted on the canvas 
over the background and other obpcts that have been painted on the canvas. 



Each layer of paint covers up the previous layer. Using a similar technique, we M i o n  13-6 
first sort surfaces according to their distance from the view plane. The intensity Depth-Sorting Method 
values for the farthest surface are then entered into the refresh buffer. Taking 
each succeeding surface in hxrn (in decreasing depth order), we "paint" the sur- 
face intensities onto the frame buffer over the intensities of the previously 
processed surfaces. 

Painting polygon surfaces onto the frame buffer according to depth is 
carried out in several steps. Assuming we  are wewing along the-z direction, 
surfaces are ordered on the first pass according to the smallest z value on each 
surface. Surface 5 with the greatest depth is then compared to the other sur- 
faces in the list to determine whether there are any overlaps in depth. If no 
depth overlaps occur, 5 is scan converted. Figure 13-12 shows two surfaces 
that overlap in the xy plane but have no depth overlap. This process is then re- 
peated for the next surface in the list. As long as no overlaps occur, each sur- 
face is processed in depth order until all have been scan converted. If  a depth 
overlap is detected at any point in the list, we need to make some additional 
comparisons to determine whether any of the surfaces should be reordered. 

We make the following tests for each surface that overlaps with 5. If any 
one of these tests is true, no reordering is necessary for that surface. The tests are 
listed in order of increasing difficulty. 

1. The bounding rectangles in the xy plane for the two surfaces do not over- 
l ap  

2. Surface 5 is completely behind the overlapping surface relative to the view- 
ing position. 

3. The overlapping surface is completelv in front of 5 relative to the viewing 
position. 

4. The projections of the two surfaces onto the view plane d o  not overlap. 

We perform these tests in the order listed and proceed to the next overlapping 
surface as soon as we find one of the tests is true. If all the overlapping surfaces 
pass at least one of these tests, none of them is behind 5. No reordering is then 
necessary and S is scan converted. 

Test 1 is performed in two parts. We first chwk for overlap in the x direc- 
tion, then we check for overlap in the y direction. I f  either of these directions 
show no overlap, the two planes cannot obscure one other. A n  example of two 



Chapter 13 surfaces that overlap in the z direction but not in the x direction is shown in Fig. 
Visible-Surface Detection Methods 13-13. 

We can perform tests 2 and 3 with an "inside-outside" polygon test. That is, 
we substitute the coordinates for all vertices of S into the plane equation for the 
overlapping surface and check the sign of the result. If the plane equations are set 
up so that the outside of the surface is toward the viewing position, then S is be- 
hind S' if all vertices of S are "inside" S' (fig. 13-14). Similarly, S' is completely in 
front of S if all vertices of S are "outside" of S'. Figure 13-15 shows an overlap 
ping surface S' that is completely in front of S, but surface S is not completely 
"inside" S' (test 2 is not true). 

If tests 1 through 3 have all failed, we try test 4 by checking for intersections 
between the bounding edges of the two surfaces using line equations in the xy 
plane. As demonstrated in Fig. 13-16, two surfaces may or may not intersect even 
though their coordinate extents overlap in the x, y, and z directions. 

Should all four tests fail with a particular overlapping surface S', we inter- 
change surfaces S and S' in the sorted l i t .  An example of two surfaces that 

- 
F l p r r  13-13 
Two surfaces with depth overlap 
but no overlap in the x direction. 

-- 

Surface S is completely behind 
("inside") the overlapping surface 

- -- - - -. . - 

-- F I , ~ ~ I ~ I >  7.3- I j 
x, Overlapping surface S' IS 

completely in front ("outside") of 
surface S, but S IS not conipletelv 
hehind 5' 



? - - _ _ - - - - - - - -  J I I I I 
la) I I 

I I '. - --. 
. ,. - -. - - . - - - . - . - - -. - - - 

I'ipre 13-16 
Two surfaces with overlappmg bounding rectangles In 
the xy plane. 

would be reordered with this procedure is given in Fig. 13-17. At this point, we 
still d o  not know for certain that we have found the farthest surface from the 
view plane. Figure 13-18 illustrates a situation in which we would first inter- 
change S and s''. But since S" obscures part of S', we need to interchange s'' 
and S' to get the three surfaces into the correct depth order. Therefore, we need 
to repeat the testing process for each surface that is reordered in the list. 

It is possible for the algorithm just outlined to get into an infinite loop if 
two or more surfaces alternately obscure each other, as in Fig. 13-11. In such sit- 
uations, the algorithm would continually reshuffle the positions of the overlap- 
ping surfaces. To avoid such loops, we can flag any surface that has been re- 
ordcrcd to a farther depth position so that it cannot be moved again. If an 
attempt is made to switch the surface a second time, we divide it into two parts 
to eliminate the cyclic overlap. The original surface is then replaced by the two 
new surfaces, and we continue processing as before. 

13-7 
BSP-TREE METHOD 

A binary space-partitioning (BSP) tree is an efficient method for determining 
object visibility by painting surfaces onto the screen from back to front, as in the 
painter's algorithm. The BSP tree is particularly useful when the view reference 
point changes, but the objects in a scene are at fwed positions. 

Applying a BSP tree to visibility testing involves identifying surfaces that 
are "inside" and "outside" the partitioning plane at each step of the space sub- 
divrsion, relative to the viewing direction. Figure 13-19 illustrates the basic con. 
cept in this algorithm. With plane PI, we firstpartition the space into two sets of 
objects. One set of objects is behind, or in back of, plane P, relative to the view- 
ing direction, and the other set is in front of PI. Since one object is intersected by 
plane PI, we divide that object into two separate objects, labeled A and B. Ob- 
jects A and C are in front of P,, and objects B and Dare behind PI. We next parti- 
tion the space again with plane P2 and construct the binary tree representation 
shown in Fig. 13-19(b). In this tree, the objec?s are represented as terminal 
nodes, with front objects as left branches and back objects as right branches. 

Figrrre 13-17 
Surface S has greater depth 
but obscures surface 5'. 

Fi,qun' 1.3-18 
Three surfaces entered into 
the sortcd surface list in the 
order 5, 5', 5" should be 
reordered S', Sf, 5. 



(:hapter 13 

Vwhle Surface DP~PC~IOII Methods 

A C 8 with two planes l', and P2 to form 
t i , )  the BSP tree representation in (b). 

For objects dtascribed with polygon facets, we chose the partitioning planes 
to coincide with tne polygon planes. The polygon equations are then used to 
identify "inside" and "outside" polygons, and the tree is constructed with one 
partitioning plane for each polygon face. Any polygon intersected by a partition- 
ing plane is split into two parts. When the BSP tree is complete, we process the 
tree by selecting :he surfaces for display in the order back to front, so that fore- 
ground objects are painted over the background objects. Fast hardware imple- 
mentations for c.onstructing and processing DSP trees are used in some systems. 

13-8 
AREA-SUBDIVISION b1ETHOD 

This te~hnique for hidden-surface removal is essentially an image-space method, 
but object-space operations can be used to accomplish depth ordering of surfaces. 
The area-subdivision method takes advantage of area coherence in a scene by lo- 
cating those view areas that represent part of a single surface. We apply this 
method by successively dividing the total viewing area into smaller and smaller 
rectangles until each small area is the projection of part of n single visible surface 
or no surface at all. 

To implement this method, we need to establish tests tnat can quickly iden- 
tify the area as part of a single surface or tell us that the area is too complex to an- 
alyze easily. Starting with the total view, we apply the tests to determine whether 
we should subdivide the total area into smaller rectangles. If the tests indicate 
that the view is sufficiently complex, we subdivide it. Next. we apply the tests to 



each of the smaller areas, subdividing these if the tests indicate that visibility of a s d o n  13-13 
single surface is still uncertain. We continue this process until the subdivisions Area-Subdivision Method 
are easily analyzed as belonging to a single surface or until they are reduced to 
the size of a single pixel. An easy way to do this is to successively divide the area 
into four equal parts at each step, as shown in Fig. 13-20. This approach is similar 
to that used in constructing a quadtree. A viewing area with a resolution of 1024 
by 1024 could be subdivided ten times in this way before a.subarea is reduced to 
a p in t .  

Tests to determine the visibility of a single surface within a specified area rn 
are made by comparing surfaces to the boundary of the area. There am four p s -  
sible relationships that a surface can have with a specified area boundary. We can 
describe these relative surface characteristics in the following way (Fig. 13-21): 

Surrounding surface-One that completely encloses the area. 
Overlapping surface-One that is partly inside and partly outside the area. 
Inside surface-One that is completely inside the area. Figure 13-20 

Dividing a square area into Outside surface-One that is completely outside the area. equal-sized quadrants at each 
step. 

The tests for determining surface visibility within an area can be stated in 
terms of these four classifications. No further subdivisions of a specified area are 
needed if one of the following conditions is true: 

1. All surfaces are outside surfaces with respect to the area. 
2. Only one inside, overlapping, or surrounding surface is in the area. 
3. A surrounding surface obscures a l l  other surfaces within the area bound- 

aries. 

Test 1 can be camed out by checking the bounding rectangles of all surfaces 
against the area boundaries. Test 2 can also use the bounding rectangles in the xy 
plane to iden* an inside surface. For other types of surfaces, the bounding m- 
tangles can be used as an initial check. If a single bounding rectangle intersects 
the area in some way, additional checks are wd to determine whether the sur- 
face is surrounding, overlapping, or outside. Once a single inside, overlapping, 
or surrounding surface has been identified, its pixel intensities are t r a n s f e d  to 
the appropriate area within the frame buffer. 

One method for implementing test 3 is to order surfaces according to their 
minimum depth from the view plane. For each surrounding surface, we then 
compute the maximum depth within the area under consideration. If the maxi- 

Surrounding Charlawing Inside 
Sudsee Surfurn Surface 

Figurc 13-21 
Possible relationships between polygon surfaces and a rectangular area. 



Figure 13-22 
Within a specified area, a 

I (Surrounding surrounding surface with a 
max~mum depth of z,,, obscures all - 

Y" surfaces that have a min~rnurn 
2, 

Area depth beyond z,,,. 

mum depth of one of these surrounding surfaces is closer to the view plane than 
the minimum depth of all other surfaces within the area, test 3 is satisfied. Figure 
13-22 shows an example of the conditions for this method. 

Another method for carrying out test 3 that does not require depth sorting 
is to use plane equations to calculate depth values at the four vertices of the area 
for all surrounding, overlapping, and inside surfaces, If  the calculated depths for 
one of the surrounding surfaces is less than the calculated depths for all other 
surfaces, test 3 is true. Then the area can be filled with the intensity values of the 
surrounding surface. 

For some situations, both methods of implementing test 3 will fail to iden- 
tify correctly a surrounding surface that obscures all the other surfaces. Further 
testing could be carried out to identify the single surface that covers the area, but 
it is faster to subdivide the area than to continue with more complex testing. 
Once outside and surrounding surfaces have been identified for an area, they 
will remain outside and surrounding suriaces for all subdivisions of the area. 
Furthermore, some ins~de and overlapping surfaces can be expected to be elimi- 
nated as the subdivision process continues, so that the areas become easier to an- 
alyze. In the limiting case, when a subdivision the size of a pixel is produced, we 
simply calculate the depth of each relevant surface at that point and transfer the 
in&-nsity of the nearest surface to the frame buffer. 

- .. . - - ..---,-L...- -~ 

f i p w  1 3 - 2  l 
Area A is mbdiwded into 4 ,  and .?, using the boundary of 
surface S 011 the view plane. 



As a variation on the basic subdivision pnress, we could subdivide areas 13-9 
along surface boundaries instead of dividing them in half. If the surfaces have &tree Methods 
been sorted according to minimum depth, we can use the surface with the small- 
est depth value to subdivide a given area. Figure 13-23 illustrates this method for 
subdividing areas. The propdion of the boundary of surface S is used to parti- 
tion the original area into the subdivisions A, and A2. Surface S is then a sur- 
rounding surface for A, and visibility tests 2 and 3 can be applied to determine 
whetha further subdividing is necessary. In general, fewer subdivisions are r e  
qumd using this approach, but more processing is needed to subdivide areas 
and to analyze the relation of surfaces to the subdivision boundaries. 

13-9 
OCTREE METHODS 

When an odree representation is used for the viewing volume, hidden-surface 
elimination is accomplished by projecting octree nodes onto the viewing surface 
in a front-to-back order. In Fig. 13-24, the front face of a region of space (the side 
toward the viewer) is formed with odants 0, 1, 2, and 3. Surfaces in the front of 
these octants are visible to the viewer. Any surfaces toward the rear of the front 
octants or in the back octants (4,5,6, and 7) may be hidden by the front surfaces. 

Back surfaces are eliminated, for the viewing direction given in Fig. 13-24, 
by pnxesing data elements in the octree nodes in the order 0, 1, 2,3,4, 5, 6, 7. 
This results in a depth-first traversal of the ochpe, so that nodes representing oc- 
tants 0, 1.2, and 3 for the entire region are visited before the nodes representing 
octants 4,5,6, and 7. Similarly, the nodes for the front four suboctants of octant 0 
are visited before the nodes for the four back suboctants. The traversal of the oc- 
tree continues in this order for each octant subdivision. 

When a color value is encountered in an octree node, the pixel area in the 
frame buffer corresponding to this node is assigned that color value only if no 
values have previously been stored in this area. In this way, only the bont colors 
are loaded into the buffer. Nothing is loaded if an area is void. Any node that is 
found to be completely obscured is eliminated from further processing, so that its 
subtrees are not accessed. 

Different views of objects ripresented as octrees can be obtained by apply- 
ing transformations to the octree representation that reorient the object according 

7 

Octents 
of a Region 

viewing 
Direction 

Figrrrv 13-24 
Obj ts  in octants 0, 1,2, and 3 
obscure objects in the back octants 
(4,5,6,7) when the viewing 
direction is as shown. 



Chapter 13  to the view selected. We assume that the octree representation is always set up so 
~sible-Surface Detection Methods that octants O,1,2, and 3 of a region form the front face, as in Fig. 13-24. 

A method for displaying an octree is first to map the octree onto a quadtree 
6 of visible areas by traversing oat~e nodes from front to back in a recursive p m e -  

dure. Then the quadtree representation for the visible surfaces is loaded into the 
frame buffer. Figure 13-25 depicts the octants in a region of space and the corre- 
sponding quadrants on the view plane. Contributions to quadrant 0 come from 
odants 0 and 4. Color values in quadrant 1 are obtained from surfaces in octants 
1 and 5, and values in each of the other two quadrants are generated from the 
pair of octants aligned with each of these quadrants. 

Recursive processing of octree nodes is demonstrated in the following proce- 
dure, which accepts an octree description and creates the quadtree representation 
for visible surfaces in the regon. In most cases, both a front and a back octant 
must be considered in determining the correct color values for a quadrant. But if 
the front octant is homogeneously filled with some color, we do not process the 

Octants in Space 

Qundrsntr for 
the View Plane 

-- 

Figure 13-25 
Octant divisions for a 
region of space and the 
corresponding quadrant 
plane. 

back octant. For heterog&eous regions, the is recursively c&ed, pass- 
ing as new arguments the child of the heterogeneous octant and a newly created 
quadtree node. If the front is emutv, the rear octant is processed. Otherwise, two . ,. 
recursive calls are made, one for the rear octant and one for the front octant. 

typedef enum ( SOLID, MIXED 1 Stqtus; 

bdefine EMPTY -1 

typedef struct tOctree ( 
int id; 
Status status; 
union ( 

int color; 
struct tOctree ' children[8]: 

) data; 
1 Octree: 

typedef struct tQuadtree i 
int id: 
Status status; 
union [ 

int color; 
struct tQuadtree chiLdren(41; 

) data; 
) Quadtree; 

int nQuadtree = 0. 

void octreeToQuadtrse (Octree ' olree. Quadtree qTree) 
( 
Octree + front. ' bdck: 
Quadtree ' ne-uadtree; 
int i, j ;  

if (oTree->status == SOLID) ( 
qTree->status = SOLID: 
qTree->data.color = oTree->data color: 
return: 

) 
qTree->status = MIXED: 
/ '  Fill in each quad of the quadtree * I  
for ( i = O ;  i<4; i++) ( 
front = oTree->data.children[il; 



back = oTree->data.~hildrenli+41; 
newpuadtree - (Quadtree * )  malloc (sizeof (Quadtree)): 
nemuadtree->id = nQuadtree++; 
nmuadtree->status = SOLID; 
qTree->data.childrenIil = newQuodtree; 

if (front->status == SOLID) 
if (front->data.color != EMPTY) 
qTree->data.children[i]->data.color = front->data.color; 

else 
if (back->status == SOLID) 
if (back->data.color ! =  EMPTY) 
qTree->data.children[i]->data.color = back->data.color; 

else 
qTree->data.children[il->data.color = MPTY;  

else ( / *  back node is mixed * /  
newQuadtree->status = MIXED; 
octreeToQuadtree (back, newguadtree): 

I 
else ( I *  front node is mixed * /  

newquadtree->status = MIXED; 
octreeToQuadrree (back, newQuadtree); 
octreeToQuadtree (front, nemuadtree): 

) 

krtion 13-10 
Raycasting Method 

13-1 0 
RAY-CASTING METHOD 

If we consider the line of sight from a pixel position on the view plane through a 
scene, as in Fig. 13-26, we can determine which objects in the scene (if any) inter- 
sect this line. After calculating all raysurface intersections, we identify the visi- 
ble surface as the one whose intersection point is closest to the pixel. This visibil- 
itydetection scheme uses ray-casting procedures that were introduced in Section 
10-15. Ray casting, as a visibilitydetection tool, is based on geometric optics 
methods, which trace the paths of light rays. Since there are an infinite number of 
light rays in a scene and we are interested only in those rays that pass through 

Figure 13-26 
A ray along the line of sight from a 
pixel position through a scene. 



Chapter 13 pixel positions, we can trace the light-ray paths backward from the pixels 
visible-Surface Detection Merhcds through the scene. The ray-casting approach is an effective visibility-detection 

method for scenes with curved surfaces, particularly spheres. 
We can think of ray casting as a variation on the depth-buffer method (Sec- 

tion 13-3). In the depth-buffer algorithm, we process surfaces one at a time and 
calculate depth values for all projection points over the surface. The calculated 
surface depths are then compared to previously stored depths to determine visi- 
ble surfaces at each pixel. In rayxasting, we  process pixels one at a time and cal- 
culate depths for all surfaces along the projection path to that pixel. 

Ray casting is a special case of ray-tracing algorithms (Section 14-6) that trace 
multiple ray paths to pick up  global reflection and refraction contributions from 
multiple objects in a scene. With ray casting, we only follow a ray out from each 
pixel to the nearest object. Efficient ray-surface intersection calculations have 
been developed for common objects, particularly spheres, and we discuss these 
intersection methods in detail in Chapter 14. 

13-1 1 
CURVED SURFACES 

Effective methods for determining visibilit$ for objects with curved surfaces in- 
clude ray-casting and octree methods. With ray casting, we calculate raysurface 
intersections and locate the smallest intersection distance along the pixel ray. 
With octreff, once the representation has been established from the input defini- 
tion of the objects, all visible surfaces are identified with the same processing pro- 
cedures. No special considerations need be given to different kinds of curved 
surfaces. 

We can also approximate a curved surface as a set of plane, polygon sur- 
faces. In the list of surfaces, we then replace each curved surface with a polygon 
mesh and use one of the other hidden-surface methods previously discussed. 
With some objects, such as spheres, it can be more efficient as well as more accu- 
rate to use ray casting and the curved-surface equation. 

Curved-Surface Representations 

We can represent a surface with an implicit equation of the form f(x, y, z)  = 0 or 
with a parametric representation (Appendix A). Spline surfaces, for instance, are 
normally described with parametric equations. In some cases, it is useful to ob- 
tain an explicit surface equation, as, for example, a height function over an .ry 
ground plane: 

Many objects of interest, such as spheres, ellipsoids, cylinders, and cones, have 
quadratic representations. These surfaces are cnmmonly used to model molecu- 
lar structures, roller bearings, rings, and shafts. 

Scan-line and ray-casting algorithms often in\.olve numerical approxima- 
tion techniques to solve the surface equation at the intersection point with a scan 
line or with a pixel ray. Various techniques, including parallel calculations and 
fast hardware implementations, have been developed for solving the curved-sur- 
face equations for commonly used objects. 



Surface Contour Plots 13-11 
Cunnd Surface5 

For many applications in mathematics, physical sciences, engineering and other 
fields, it is useful to display a surface function with a set of contour lines that 
show the surface shape. The surface may be described with an equation or with 
data tables, such as topographic data on elevations or population density.. With 
an explicit functional representation, we can plot the visiblesurface contour lines 
and eliminate those contour sections that are hidden by the visible parts of the 
surface. 

To obtain an xy plot of a functional surface, we write the surface representa- 
tion in the form 

y = fix, 2)  (13-8) 

A curve in the xy plane can then be plotted for values of z within some selected 
range, using a s-ed interval Az. Starting with the largest value of z, we plot 
the curves from "front" to "back" and eliminate hidden sections. We draw the 
curve sections on the screen by mapping an xy range for the function into an sy 
pixel screen range. Then, unit steps are taken in x and the corresponding y value 
for each x value is determined from Eq. 13-8 for a given value of z. 

One way to idenhfy the visible curve sections on the surface is to maintain a 
list of y, and y, values pnwiously calculated for the pixel xcoordinates on the 
screen. As we step from one pixel x position to the next, we check the calculated 
y value againat the stored range, y, and y-, for the next pixel. If y-5 y s 
y- that point on the surface is not visible and we do not plot it. But if the mlcu- 
law y value is outside the stored y bounds for that pixel, the point is visible. We 
then plot the point and reset the bounds for that pixel. Similar pTocedures can be 
used to project the contour plot onto the xz or the yz plane. Figure 13-27 shows an 
example of a surface contour plot with color-coded contour lines. 

Similar methds can be used with a discrete set of data points by detennin- 
ing isosurface lines. For example, if we have a discrete set of z values for an n, by 
5 grid of xy values, we can determine the path of a line of constant z over the 
surface using the contour methods discussed in Section 10-21. Each selected con- 
tour line can then be projected onto a view plane and displayed with straight-line 

- 
Fipw 13-27 
A colorcoded surface contour plot. (Cmntqof Los 
Ahmos N a l i o ~ I  Lnb0rafory.y.) 



Chapter 13 

Visible-Sudace Deleclion h M h d s  

F i p n .  1.3-2s 
Hidden-line sections (dashed) 
for a linethat (a) passesbehind 
a surface and (b) penetrates a 
surface. 

segments. Again, lines can be drawn on the display device in a front-to-back 
depth order, and we eliminate contour sections that pass behind previously 
drawn (visible) contour lines. 

13-1 2 
WIREFRAME METHODS 

When only the outline of an object is to be displayed, visibility tests are applied 
to surface edges. Visible edge sections are displayed, and hidden edge sections 
can either be eliminated or displayed differently from the visible edges. For ex- 
ample, hidden edges could be drawn as  dashed lines, or we could use depth cue- 
ing to decrease the intensity of the lines as a linear function of distance from the 
view plane. Procedures for determining visibility of object edges are referred to 
as wireframe-visibility methods. They are also called vis ible-he detection 
methods or hidden-line detection methods. Special wireframe-visibility proce- 
dures have been developed, but some of the visiblesurface methods discussed in 
preceding sections can also be used to test for edge visibility. 

A direct approach to identifying the visible lines in a scene is to compare 
each line to each surface. The process involved here is sihilar to clipping lines 
against arbitrary window shapes, except that we now want to determine which 
sections of the lines are hidden by surfaces. For each line, depth values are com- 
pared to the surfaces to determine which line sections are not visible. We can use 
coherence methods to identify hidden line segments without actually testing 
each coordinate position. If both line intersections with the projection of a surface 
boundary have greater depth than the surface at those points, the line segment 
between the intersections is completely hidden, as in Fig. 13-28(a). This is the 
usual situation in a scene, but it is also possible to have lines and surfaces inter- 
secting each other. When a line has greater depth at one boundary intersection 
and less depth than the surface at the other boundary intersection, the line must 
penetrate the surface interior, as in Fig. 13-28(b). In this case, we calculate the in- 
tersection point of the line with the surface using the plane equation and display 
only the visible sections. 

Some visible-surface methods are readily adapted to wireframe visibility 
testing. Using a back-face method, we could identify all the back surfaces of an 
object and display only the boundaries for the visible surfaces. With depth sort- 
ing, surfaces can be painted into the refresh buffer so that surface interiors are in 
the background color, while boundaries are in the foreground color. By process- 
ing the surfaces from back to front, hidden lines are erased by the nearer sur- 
faces. An area-subdivision method can be adapted to hidden-line removal by dis- 
playing only the boundaries of visible surfaces. Scan-line methods can be used to 
display visible lines by setting points along the scan line that coincide with 
boundaries of visible surfaces. Any visible-surface method that uses scan conver- 
sion can be modified to an edge-visibility detection method in a similar way. 

13-13 
VISIBILITY-DETECTION FUNCTIONS 

Often, three-dimensional graphics packages accommodate several visible-surface 
detection procedures, particularly the back-face and depth-buffer methods. A 
particular function can then be invoked with the procedure name, such as back- 
Face or depthBuf f er. 



In general programming standards, such as GKS and PHIGS, visibility 
methods are implementation-dependent. A table of available methods is listed at Summary 
each installation, and a particular visibility-detection method is selected with the 
hidden-linehickden-surface-removal (HLHSR) function: 

Parameter v i s  i b i  li tyFunc t ionIndex is assigned an integer code to identify 
the visibility method that is to be applied to subsequently specified output primi- 
tives. 

SUMMARY 

Here, we gve a summary of the visibility-detection methods discussed in this 
chapter and a compariwn of their effectiveness. Back-face detection is fast and ef- 
fective as an initial screening to eliminate many polygons from further visibility 
tests. For a single convex polyhedron, back-face detection eliminates all hidden 
surfaces, but in general, back-face detection cannot cqmpletely identify all hid- 
den surfaces. Other, more involved, visibility-detection schemes will comectly 
produce a list of visible surfaces. 

A fast and simple technique for identifying visible surfaces is the depth- 
buffer (or z-buffer) method. This procedure requires two buffers, one for the pixel 
intensities and one for the depth of the visible surface for each pixel in the view 
plane. Fast incremental methods are used to scan each surface in a scene to calcu- 
late surfae depths. As each surface is processed, the two buffers are updated. An 
improvement on the depth-buffer approach is the A-buffer, which provides addi- 
tional information for displaying antialiased and transparent surfaces. Other visi- 
blesurface detection schemes include the scan-line method, the depth-sorting 
method (painter's algorithm), the BSP-tree method, area subdivision, octree 
methods, and ray casting. 

Visibility-detection methods are also used in displaying three-dimensional 
line drawings. With,cuwed surfaces, we can display contour plots. For wireframe 
displays of polyhedrons, we search for the various edge sections of the surfaces 
in a scene that are visible from the view plane. 

The effectiveness of a visiblesurface detection method depends on the 
characteristics of a particular application. If the surfaces in a scene are spread out 
in the z direction so that there is very little depth overlap, a depth-sorting or BSP- 
h e  method is often the best choice. For scenes with surfaces fairly well sepa- 
rated horizontally, a scan-line or area-subdivision method can be used efficiently 
to locate visible surfaces. 

As a general rule, the depth-sorting or BSP-tree method is a highly effective 
approach for scenes with only a few surfaces. This is because these scenes usually 
have few surfaces that overlap in depth. The scan-line method also performs well 
when a scene contains a small number of surfaces. Either the scan-line, depth- 
sorting, or BSP-tree method can be used effectively for scenes with up to several 
thousand polygon surfaces. With scenes that contain more than a few thousand 
surfaces, the depth-buffer method or octree approach performs best. 'the depth- 
buffer method has a nearly constant processing time, independent of the number 
of surfaces in a scene. This is because the size of the surface areas decreases as the 
number of surfaces in the scene increases. Therefore, the depth-buffer method ex- 
hibits relatively low performance with simple scenes and lelatively high perfor- 



ChapCr 13 rnance with complex scenes. BSP trees are useful when multiple views are to be 
Msible-Surface Detection Methods generated using different view reference points. 

When o&ve representations are used in a system, the hidden-surface elimi- 
nation process is fast and simple. Only integer additions and subtractions are 
used in the process, and there is no need to perform sorting or intersection calcu- 
lations. Another advantage of octrees is that they store more than surfaces. The 
entire solid region of an object is available for display, which makes the octree 
representation useful for obtaining cross-sectional slices of solids. 

If a scene contains curved-surface representations, we use ochee or ray- 
casting methods to identify visible parts of the scene. Ray-casting methodsare an 
integral part of ray-tracing algorithms, which allow scenes to be displayed with 
global-illumination effects. 

It is possible to combine and implement the different visible-surface detec- 
tion methods in various ways. In addition, visibilitydetection algorithms are 
often implemented in hardware, and special systeins utilizing parallel processing 
are employed to in&ase the efficiency of these methods. Special hardware sys- 
tems are used when processing speed is an especially important consideration, as 
in the generation of animated views for flight simulators. 

REFERENCES 

Additional xxlrces of information on visibility algorithms include Elber and Cohen (1 990). 
Franklin and Kankanhalli (19901. Glassner (1990). Naylor, Amanatides, and Thibault 
(19901, and Segal (1990). 

EXERCISES 

13-1. Develop a procedure, based on a back-face detection technique, for identifying all 
the visible faces of a convex polyhedron that has different-colored surfaces. Assume 
that the object i s  defined in a right-handed viewing system with the xy-plane as the 
viewing surface. 

13-2. Implement a back-face detection p r ~ e d u r e  using an orthographic parallel projection 
to view visible faces of a convex polyhedron. Assume that all parts of the object are 
in front of the view plane, and provide a mapping onto a screen viewport for display. 

13-3. Implement a back-face detection procedure using a perspective projection to view 
visible faces of a convex polyhedron. Assume that all parts of the object are in front 
of the view plane, and provide a mapping onto a screen viewprt for display. 

13-4. Write a program to produce an animation of a convex polyhedron. The object is to 
be rotated incrementally about an axis that passes through the object and is parallel 
to the view plane. Assume that the object lies completely in front of the view plane. 
Use an orthographic parallel projection to map the views successively onto the view 
plane. 

13-5. Implement the depth-buffer method to display the visible surfaces of a given polyhe- 
dron. How can the storage requirements for the depth buffer bedetermined from the 
definition of the objects to be displayed? 

13-6. Implement the depth-buffer method to display the visible surfaces in a scene contain- 
ing any number of polyhedrons. Set up efficient methods for storing and processing 
the various objects in the scene. 

13-7. Implement the A-buffer algorithm to display a scene containing both opaque and 
transparent surfaces. As an optional feature, you[ algorithm may be extended to in- 
clude antialiasing. 



13-8. Develop a program to implement the scan-line algorithm for displaying the visible 
surfaces of a given polyhedron. Use polygon and edge tables to store the definition Exercises 
of the object, and use coherence techniques to evaluate points along and between 
x a n  lines. 

13-9. Write a program to implement the scan-line algorithm for a scene containing several 
polyhedrons. Use polygon and edge tables to store the definition of the object, and 
use coherence techniques to evaluate points along and between scan lines. 

13-10. Set up a program to display the visible surfaces of a convex polyhedron using the 
painter's algorithm. That is, surfaces are to be sorted on depth and painted on the 
screen from back to front. 

13-11. Write a program that uses the depth-sorting method to display the visible surfaces of 
any given obi& with plane faces. 

13-1 2. Develop a depth-sorting program to display the visible surfaces in a scene contain~ng 
several polyhedrons. 

13-13. Write a program to display the visible surfaces of a convex polyhedron using the 
BSP-tree method. 

13-14. Give examples of situations where the two methods discussed for test 3 in the area- 
subdivision algorithm will fail to identify correctly a surrounding surbce that ob- 
scures all other surfaces. 

13-15. Develop an algorithm that would test a given plane surface against a rectangular 
area to decide whether it is a surrounding, overlapping, inside, or outside surface. 

13-1 6. Develop an algorithm for generating a quadtree representation for the visible sur- 
faces of an object by applying the area-subdivision tests to determine the values of 
the quadtree elements. 

13-1 7. Set up an algorithm to load a given quadtree representation of an object into a frame 
buffer for display. 

13-1 8. Write a program on your system to display an octree representation for an object so 
that hidden-surfaces are removed. 

13-1 9. Devise an algorithm for viewing a single sphere using the ray-casting method. 
13-20. Discuss how antialiasing methods can be incorporated into the various hidden-sur- 

face elimination algorithms. 
13-21. Write a routine to produce a surface contour plot for a given surface function f ( x ,  y). 
13-22. Develop an algorithm for detecting visible line sections in a xene by comparing 

each line in the xene to each surface. 
13-23. Digcuss how wireframe displays might be generated with the various visible-surface 

detection methods discussed in this chapter. 
13-24. Set up a procedure for generating a wireframe display of a polyhedron with the hid- 

den edges of the object drawn with dashed lines. 



C H A P T E R  

14 ll lumination Models and Surface-Rendering Methods 



R ealistic displays of a scene are obtained by generating perspective projec- tions of objects and by applying natural lighting effects to the visible sur- 
faces. An illumination model, also called a lighting model and sometimes re- 
ferred to as a shading model, is used to calcurate the intensity of light that we 
should see at a given point on the surface of an object. A surface-rendering algo- 
rithm uses the intensity calculations from an illumination model to determine the 
light intensity for all projected pixel positions for the various surfaces in a scene. 
Surface rendering can be performed by applying the illumination model to every 
visible surface point, or the rendering can be accomplished by interpolating in- 
tensities across the surfaces from a small set of illumination-model calculations. 
Scan-line, image-space algorithms typically use interpolation schemes, while ray- 
tracing algorithms invoke the illumination model at each pixel position. Some- 
times, surface-rendering procedures are termed surjace-shading methods. To avoid 
confusion, we will refer to the model for calculating light intensity at a single sur- 
face point as  an illumination model or a lighting model, and we will use the term 
surface rendering to mean a procedure for applying a lighting model to obtain 
pixel intensities lor all the projected surface positions in a scene. 

Photorealism in computer graphics involves two elements: accurate graphi- 
cal representations of objects and good physical descriptions of the lighting ef- 
fects in a scene. Lighting effects include light reflections, transparency, surface 
texture, and shadows. 

Modeling the colors and lighting effects that we see on an object is a corn- 
plex process, involving principles of both physics and psychology. Fundarnen- 
tally, lighting effects arc described with models that consider the interaction of 
electromagnetic energy with object surfaces. Once light reaches our eyes, it trig- 
gers perception processes that determine what we actually "see" in a scene. Phys- 
ical illumination models involve a number of factors, such as object type, object 
position relative to light sources and other objects, and the light-source condi- 
tions that we set for a scene. Objects can be constructed of opaque materials, or 
they can be more or less transparent. In addition, they can have shiny or dull sur- 
faces, and they can have a variety ol surface-texture patterns. Light sources, of 
varying shapes, colors, and positions, can be used to provide the illumination ef- 
fects for a scene. Given the paramctcrs for the optical properties of surfaces, the 
relative positions of the surfaces in a scene, the color and positions of the light 
sources, and the position and orientation of the viewing plane, illumination mod- 
els calculate the intensity projected from a particular surface point in a specified 
viewing direction. 

Illumination models in computer graphics are often loosely derived from 
the physical laws that describe surface light intensities. To minimize intensity cal- 



Chapter 14 

llluminalion Models and 
SurfaceRendering Methods 

Reflecting 

Figure 14-1 
Light viewed from an opaque 
nonluminous surface is in 
general a combination of 
reflected light from a light 
source and reflections of light 
reflections from other 
surfaces. 

Figure 14-2 
Diverging ray paths from a 
point light source. 

culations, most packages use empirical models based on simplified photometric 
calculations. More accurate models, such as the radiosity algorithm, calculate 
light intensities by considering the propagation of radiant energy between the 
surfaces and light sources in a scene. In the following sections, we first take a 
look at the basic illumination models often used in graphics packages; then we 
discuss more accurate, but more time-consuming, methods for calculating sur- 
face intensities. And we explore the various surface-rendering algorithms for a p  
plying the lighting models to obtain the appropriate shading over visible sur- 
faces in a scene. 

LIGHT SOURCES 

When we view an opaque nonlum~nous object, we see reflected light from the 
surfaces of the object. The total reflected light is the sum of the contributions 
from light sources and other reflecting surfaces in the scene (Fig. 14-11. Thus, a 
surface that is not directly exposed to a hght source may still be visible if nearby 
objects are illuminated. Sometimes, light s o m e s  are referred to as  light-emitting 
sources; and reflecting surfaces, such a s  the walls of a room, are termed light-re- 
Pecting sources. We will use the term lighf source to mean an object that is emitting 
radiant energy, such as a Light bulb or the sun. 

A luminous object, in general, can be both a light source and a light reflec- 
tor. For example, a plastic globe with a light bulb insidc both emits and reflects 
light from the surface of the globe. Emitted light from the globe may then illumi- 
nate other objects in the vicinity. 

The simplest model for a light emitter is a point source. Rays from the 
source then follow radially diverging paths from the source position, as  shown in 
Fig. 14-2. This light-source model is a reasonable approximation for sources 
whose dimensions are small compared to the size of objects in the scene. Sources, 
such as the sun, that are sufficiently far from the scene can be accurately modeled 
as point sources. A nearby source, such as  the long fluorescent light in Fig. 14-3, 
is more accurately modeled as a distributed light source. In this case, the illumi- 
nation effects cannot be approximated realistically with a point source, because 
the area of the source is not small compared to the surfaces in the scene. An accu- 
rate model for the distributed source is one that considers the accumulated illu- 
mination effects of the points over the surface of the source. 

When light is incident on  an opaque surface, part of it is reflected and part 
is absorbed. The amount of incident light reflected by a surface dependi on the 
type of material. Shiny materials reflect more of the incident light, and dull sur- 
faces absorb more of the incident light. Similarly, for an illuminated transparent 

i: , 
. . . . Figvrr 14-; 

& #I .  t 
, . a  An o b ~ t  illuminated with a 
4/' distributed light source. 



surface,,some of the incident light will be reflected and some will be transmitted 
through the material. 

Surfaces that are rough, or grainy, tend to scatter the reflected light in all di- 
rections. This scattered light is called diffuse reflection. A very rough matte sur- 
face produces primarily diffuse reflections, so that the surface appears equally 
bright from all viewing directions. Figure 14-4 illustrates diffuse light scattering 
from a surface. What we call the color of an object is the color of the diffuse re- 
flection of the incident light. A blue object illuminated by a white light source, for 
example, reflects the blue component of the white light and totally absorbs all 
other components. If the blue object is viewed under a red light, it appears black 
since allof the incident light is absorbed. 

In addition to diffuse reflection, light sources create highlights, or bright 
spots, called specular reflection. This highlighting effect is more pronounced on 
shiny surfaces than on dull surfaces. An illustration of specular reflection is 
shown in Fig. 14-5. 

14-2 
BASIC ILLUMINATION MODELS 

Section 14-2 

Bas~c lllurnination hlodels 

Figure 14-4 
Diffuse reflections from a 
surface. 

Here we discuss simplified methods for calculating light intensities. The empiri- 
cal models described in this section provide simple and fast methods for calculat- 
ing surface intensity at a given point, and they produce reasonably good results 
for most scenes. Lighting calculations are based on the optical properties of sur- 
faces, the background lighting conditions, and the light-source specifications. 
Optical parameters are used to set surface properties, such as glossy, matte, 
opaque, and transparent. This controls the amount of reflection and absorption of 
incident light. All light sources are considered to be point sources, specified wlth 
a coordinate position and an intensity value (color). 

Figure 14-5 
Specular reflection 
superimposed on diffuse 
reflection vectors. 

Ambient Light 

A surface that is not exposed directly to a light source still will be visible it 
nearby objects are illuminated. In our basic illumination model, we can set a gen- 
eral level of brightness for a scene. This is a simple way to model the combina- 
tion of light reflections from various surfaces to produce a uniform illumination 
called the ambient light, or background light. Ambient light has no spatial or di- 
rectional characteristics. The amount of ambient light incident on each object is a 
constant for all surfaces and over all directions. 

We can set the level for the ambient light in a scene with parameter I,, and 
each surface is then illuminated with this constant value. The resulting reflected 
light is a constant for each surface, independent of the viewing direction and the 
spatial orientation of the surface. But the intensity of the reflected light for each 
surface depends on the optical properties of the surface; that is, how much of the 
incident energy is to be reflected and how much absorbed. 

Diftuse Reflection 

Ambient-light reflection is an approximation of global diffuse lighting effects. 
Diffuse reflections are constant over each surface in a scene, independent of the 
viewing direction. The fract~onal amount of the incident light that is diffusely re- 



Chapta 14 

Illumination Models and Surface- 
Rendering Methods 

Fiprrc 14- 7 
A surface perpndicular to 
the direction of the incident 
light (a) is more illuminated 
than an equal-sized surface at 
an oblique angle (b) to the 
incoming light direction. 

Figure 14-6 
Radiant energy from a surface area dA in direction &J relative 
to the surface normal direction. 

flected can be set for each surface with parameter kd, the diffuse-reflection coeffi- 
cient, o r  diffuse reflectivity. Parameter kd is assigned a constant value in the in- 
terval 0 to 1, according to the reflecting properties we want the surface to have. If 
we want a highly reflective surface, we set the value of kd near 1. This produces a 
bright surface with the intensity of the refiected light near that of the incident 
light. To simulate a surface that absorbs most of the incident light, we  set the re- 
flectivity to a value near 0. Actually, parameter kd is a function of surface color, 
but for the time being we will assume kd is a constant. 

If a surface is exposed only to ambient light, we can express the intensity of 
the diffuse reflection at any point on the surface as 

Since ambient light produces a flat uninteresting shading for each surface (Fig. 
14-19(b)), scenes are rarely rendered with ambient light alone. At least one light 
source is included in a scene, often as a point source at the viewing position. 

We can model the diffuse reflections of illumination from a point source in a 
similar way. That is, we assume that the diffuse reflections from the surface are 
s c a t t e d  with equal intensity in all directions, independent of the viewing d i m -  
tion. Such surfaces are sometimes referred to as  ideal diffuse reflectors. They are 
also called Lnmbertian reflectors, since radiated light energy from any point on the 
surface is governed by Imrnberl's cosine law. This law states that the radiant energy 
from any small surface area dA in any direction & relative to the surface normal 
is proportional to cash (Fig. 14-6). The light intensity, though, depends on the 
radiant energy per projected area perpendicular to direction &, which is dA 
cos&. Thus, for Lambertian reflection, the intensity of light is the same over all 
viewing directions. We discuss photometry concepts and terms, such as radiant 
energy, in greater detail in Section 14-7. 

Even though there is equal light scattering in all directions from a perfect 
diffuse reflector, the brightness of the surface does depend on the orientation of 
the surface relative to the light source. A surface that is oriented perpendicular to 
the direction of the incident light appears brighter than if the surface were tilted 
at an oblique angle to the direction of the incoming light. This is easily seen by 
holding a white sheet of paper or smooth cardboard parallel to a nearby window 
and slowly rotating the sheet away from the window direction. As the angle be- 
tween the surface normal and the incoming light direction increases, less of the 
incident light falls on the surface, as  shown in Fig. 14-7. This figure shows a beam 
of light rays incident on two equal-area plane surface patches with different spa- 
tial orientations relative to the incident light direction from a distant source (par- 



Fipw 14-8 
An illuminated area projected 
perpendicular to the path of the 

I incoming light rays. 

allel incoming rays). If we denote the angle of incidence between the incoming 
light direction and the surface normal as 0 (Fig. 14-8), then the projected area of a 
surface patch perpendicular to the light direction is proportional to cos0. Thus, 
the amount of illumination (or the "number of incident light rays" cutting across 
the projected surface patch) depends on cos0. If the incoming light from the 
source is perpendicular to the surface at a particular point, that point is fully illu- 
minated. As the angle of illumination moves away h m  the surface normal, the 
brightness of the point drops off. If I, is the intensity of the point light source, 
then the diffuse reflection equation for a point on the surface can be written as  

A surface is illuminated by a point source only if the angle of incidence is in the 
range 0" to 90' (cos 0 is in the interval from 0 to 1). When cos 0 is negative, the 
light source is "behind" the surface. 

If N is the unit normal vector to a surface and L is the unit direction vector TO Light 
to the point light source from a position on the surface (Fig. 14-9), then cos 0 = source L 
N L and the diffuse reflection equation for single point-source illumination is 

I,,&, = k d w  L) (143)  

Reflections for point-source illumination are calculated in world coordinates or ~ i ~ , , , , ~  14-9 
viewing coordinates before shearing and perspective transformations are ap- Angle of incidence @between 
plied. These transformations may transform the orientation of normal vectors so the unit light-source di~ction 
that they are no longer perpendicular to the surfaces they represent. Transforma- vector L and the unit surface 
tion procedures for maintaining the proper orientation of surface normals are normal N. 
discussed in Chapter 11. 

Figure 14-10 illustrates the application of Eq. 14-3 to positions over the sur- 
face of a sphere, using various values of parameter kd between 0 and 1. Each pro- 
jected pixel position for the surface was assigned an intensity as calculated by the 
diffuse reflection equation for a point light source. The renderings in this figure 
illustrate single point-source lighting with no other lighting effects. This is what 
we might expect to see if we shined a small light on the object in a completely 
darkened room. For general scenes, however, we expect some background light- 
ing effects in addition to the illumination effects produced by a direct light 
source. 

We can combine the ambient and pointsource intensity calculations to ob- 
tain an expression for the total diffuse reflection. In addition, many graphics 
packages introduce an ambient-reflection coefficient k, to modify the ambient- 
light intensity I, for each surface. This simply provides us with an additional pa- 
rameter to adjust the light conditions in a scene. Using parameter k,, we can write 
the total diffuse reflection equation as 



Chapter 14 

llluminrtian MocJels and Surface- 
Rendning Methods 

kd. wl th k r  = 0 .0  

Figure 14-10 
Diffuse reflections from a spherical surface illuminated by a point light 
source for values of the dlfhw reflectivity coeffiaent in the interval 
O : s k , , s l .  

- - -- - - .- -- 
Figure ICZI 
Diffuse mfledions hum a spherical surface illuminated with 
ambient light and a single point source for values of k, and 
k, in the interval (0,l). 

where both k, and k, depend on surface material properties and are assigned val- 
ues in the range from 0 to 1. Figulp 14-11 shows a sphere displayed with surface 
intensitities calculated from Eq. 14-4 for values of parameters k, and k,, between 0 
and 1. 

Specular Reflection and the Phong Mudel 

When we look at an illuminated shiny surface, such a s  pnlished metal, an  apple, 
or a person's forehead, we see a highlight, or bright spot, at certain viewing di- 



rections. This phenomenon, called specular ref7ecticv1, is the result of total, or near 
total, reflection of the incident light in a concentrated region around the specular- 
reflection angle. Figure 14-12 shows the specular reflection direction at a point 
on the illuminated surface. The specular-reflection angle equals the angle of the 
incident light, with the two angles measured on opposite sides of the unit normal 
surface vector N. In this figure, we use R to represent the unit vector in the direc- 
tion of ideal specular reflection; L to represent the unit vector directed toward the fig,,w 14-12 
point light source; and V as the unit vector pointing to the viewer from the sur- ~ ~ ~ ~ ~ l ~ ~ - r e f l ~ ~ t i ~ ~  angle 
face position. Angle 4 is the viewing angle relativc to the specular-reflection di- equals angle of incidence 0. 
rection R. For an ideal reflector (perfect mirror), ~nc.ident light is reflected only in 
the specular-reflection direction. In this case, we would only see reflected light 
when vectors V and R coincide ( 4  = 0). 

Objects other than ideal reflectors exhibit spocular reflections over a finite 
range of viewing positions around vector R. shiny surfaces have a narrow specu- 
lar-reflect~on range, and dull surfaces have a wider reflection range. An empirical 
model for calculating the specular-reflection range, developed by Phong Bui 
Tuong and called the Phong specular-reflection model, or simply the Phong 
model, sets the intensity of specular reflection proportional to cosn%$. Angle 4 
can be assigned values in the range 0" to 90â€¢â€¹, so that cos4 varies from 0 to 1. The 
value assigned to specular-reflection parameter n, is determined by the type of sur- 
face that we want to display. A very shiny surface is modeled with a large value 
for n, (say, 100 or more), and smaller values (down to 1) are used for duller sur- 
faces. For a perfect reflector, n, is infinite. For a rough surface, such as chalk or 
cinderblock, n, would be assigned a value near 1. Figures 14-13 and 14-14 show 
the effect of n, on the angular range for which we can expect to see specular re- 
flections. 

The intensity of specular reflection depends on the material properties of 
the surface and the angle of incidence, as well as other factors such as the polar- 
ization and color of the incident light. We can approximately model monochro- 
matic specular intensity variations using a specular-reflection coefficient, W(0), 
for each surface. Figure 14-15 shows the general variation of W(8) over the range 
8 = 0" to 0 = 90â€¢â€¹ for a few materials. In general, W(0) tends to increase as  the 
angle of incidence increases. At 8 = 90â€¢â€¹, W(0) = 1 and all of the incident light is 
reflected. The variation of specular intensity with angle of incidence is described 
by Fresnel's h w s  of Reflection. Using the spectral-reflection function W(B), we can 
write the Phong specular-reflection model as 

where 1, is the intensity of the light source, and 4 is the viewing angle relative to 
the specular-reflection direction R. 

Shiny Surface 
(Large n,) 

Dull Surface 
(Small n,) 

- . - - - - - - - - - . .. , - - .- . - - . - - - - - -. 
r i s~ t r~~  14-7.3 
Modeling spvular reflections (shaded area) with parameter 11,. 



cos- 0 

Plots of cosn~t$ for several values of specular parameter 11, 

As seen in Fig. 14-15, transparent materials, such as glass, only exhibit ap- 
preciable specular reflections as B approaches 90". At 8 = O", about 4 percent of 
the incident light on a glass surface is reflected. And for most of the range of 8. 
the reflected intensity is less than 10 percent of the incident intensity. But for 
many opaque materials, specular reflection is nearly constant for all incidence an- 
gles. In this case, we can reasonably model the reflected light effects by replacing 
W(0) with a constant specular-reflection coefficient k,. We then simply set k, equal 
to some value in the range 0 to 1 for each surface. 

Since V and R are unit vectors in the viewing and specular-reflection direc- 
tions, we can calculate the value of cos4 with the dot product V R. Assuming 
the specular-reflection coefficient is a constant, we can determine the intensity of 
the specular reflection at a surface point with the calculation 



Section 14-2 

Basic llluminalion Models 

Figure 14-15 
Approximate variation of the 

dielectric (glass) L-f!!L- spefular-reflection function of angle of coefficient incidence as for a 
0 90" 8 different materials. 

1 -- 

0.5 

Vector R in this expression can be calculated in terms of vectors L and N. As seen 
in Fig. 14-16, the projection of L onto the direction of the normal vector is ob- 
tained with the dot product N . L. Therefore, from the diagram, we have 

R + L = (2N. L)N 

silver 

and the specular-reflection vector is obtained as 

R = (2N L)N - L (]4-7) Figure 19-16 Calculation of vector R by 
considering projections onto 

Figure 14-17 illustrates specular reflections for various values of k, and n, on a the direction of the 
sphere illuminated with a single point light source. vector N. 

A somewhat simplified Phong model is obtained by using the halfway vector 
H between Land V to>alculate thevrange of specular reflections. If we replace V - 
R in the Phong model with the dot product N . H, this simply replaces the empir- 
ical cos 4 calculation with the empirical cos cu calculation (Fig. 14-18). The 
halfway vector is obtained as 

F i p r c  14-17 
Specular reflections from a Fiprc  14- 1,s 
spherical surface for varying Halfway vector H along the 
specular parameter values and a bisector of the angle between 
single hght source. Land V. 

503 



Chapter 14 I f  both the viewer and the Light source are sufficiently far from the surface, both 
Illumination Models and Surface- V and L are constant over the surface, and thus H is also constant for all surface 

Methods points. For nonplanar surfaces, N - H then requires less computation than V R 
since the calculation of R at each surface point involves the variable vector N. 

For given light-source and viewer positions, vector H is the orientation di- 
rection for the surface that would produce maximum specular reflection in the 
viewing direction. For this reason, H is sometimes referred to as  the surface ori- 
entation direction for maximum highlights. Also, if vector V is coplanar with 
vectors L and R (and thus N), angle cu has the value &/2. When V, L, and N are 
not coplanar, a > 4/2, depending on the spatial relationship of the three vectors. 

Combined Diffuse and Specular Reflectdons 
with Multiple Light Sources 

For a single point light source, we can model the combined diffuse and specular 
reflections from a point on an illuminated surface as  

Figure 14-19 illustrates surface lighting effect. rioLluced by the various terms in 
Eq. 14-9. I f  we place more than one point soun I . I I  ~r scene, we obtain the light re- 
flection at any surface point by bumming t h ~ .  ttjntributions from the individual 
sources: 

To ensure that any pixel intensity does not exceed the maximum allowable 
value, we can apply some type of normalization procedure. A simple approach is 
to set a maximum magnitude for each term in the intensity equation. If  any cal- 
culated term exceeds the maximum, we simply set it to the maximum value. An- 
other way to compensate for intensity overflow is to normalize the individual 
terms by dividing each by the magnitude of the largest term. A more compli- 
cated procedure is first t o  calculate all pixel intensities for the scene, then the cal- 
culated intensities are scaled onto the allowable intensity range. 

Warn Model 

So far we have considerc-d only point light sources. The Warn model provides a 
method for simulating studio lighting effects by controlling light intensity in dif- 
ferent directions. 

Light sources are modeled a s  points on a reflecting surface, using the Phong 
model for the surface points. Then the intensity in different directions is con- 
trolled by selecting values for the Phong exponent In addition, light controls, 
such as "barn doors" and spotlighting, used by studio photographers can be sim- 
ulated in the Warn model. Flaps are used to control the amount of light emitted 
by a source In various directions. Two flaps are provided for each of the x, y, and 
z directions. Spotlights are used to control the amount of light emitted within a 
cone with apex at a point-source position. The Warn model 1s implemented in 



. .- 
F i g u r ~  14-19 
A wireframe scene (a) is displayed only with ambient lighting in (b), and the surface of 
each object is assigned a different color. Using ambient light and d i  reflections due to 
a single source with k, = 0 for all surfaces, we obtain the lighting effects shown in (c). . 
Using ambient light and both diffuse and s p e d a r  reflections due to a single light source. 
we obtain the lighting effects shown in (d l  

PHIGS+, and Fig. 14-20 illustrates lighting effects that can be produced with this 
model. 

Intensity Attenuation 

As radiant energy from a point light source travels through space, its amplitude 
is attenuated by the factor l/fl, where d is the distance that the light has traveled. 
This means that a surface close to the light source (small d )  receives a higher inci- 
dent intensity horn the source than a distant surface (large d) .  Therefore, to pro- 
duce realistic lighting effects, our illumination model should take this intensity 
attenuation into account. Otherwise, we are illuminating all surfaces with the 
same intensity, no matter how far they might be from the light source. If two par- 
allel surfaces with the same optical parameters overlap, they would be indistin- 
guishable from each other. The two surfaces would be displayed as one surface. 



Chapter 14 

Illumination Madels and Surface- 

Figtcre 14-20 
Studio lighting effects produced with the Warn model, using 
five ligh&ur& to illknhate a Chevrolet Carnaru. (~ourtes;of 
Dooid R .  Warn. Cmeml Motors Rrsnrrch lnhomioricc.) 

Our simple point-source illumination model, however, does not always 
produce realistic pictures, if we use the factor l /d2  to attenuate intensities. The 
factor l l d 2  produces too much intensity variations when d is small, and it pro- 
duces very little variation when d is large. This is because real scenes are usually 
not illuminated with point Hght sources, and o w  illumination model is too sim- 
ple to accurately describe red lighting effects. 

Graphics packages have compensated for these problems by using inverse 
linear or quadratic functions of d to attenuate intensities. For example, a general 
inverse quadratic attenuation function can be set up as 

A user can then fiddle with the coefficients a,, a,, and a, to obtain a variety of 
lighting effects for a scene. The value of the constant term a, can be adjusted to 
prevent f(d) from becoming t w  large when d is very small. Also, the values for 
the coefficients in the attenuation function, and the optical surface parameters for 
a scene, can be adjusted to prevent calculations of reflected intensities from ex- 
ceeding the maximum allowable value. This is an effective method for limiting 
intensity values when a single light source is used to illuminate a scene. For mul- 
tiple light-source illumination, the methods described in the preceding section 
are more effective for limiting the intensity range. 

With a given set of attenuation coefficients, we can limit the magnitude of 
the attenuation function to 1 with the calculation 

f (d) = min ( 1, a. + a,d + nd2 ) 
Using this function, we can then write our basic illumination model as 

506 where di is the distance light has traveled from light source i. 



- kah 14-2 
Fipn 14-22 
Light reflections from the surface of 

Basic llluminaion M o d d s  

a black nylon d o n ,  modeled as 
woven cloth patterns and rendered 
using Monte Carlo ray-tracing 
methods. (Collrrayof Strphm H. Westin, 
RDgrenr of CDnpvtn Gnrphia, cmdI 
lhmrdty..) 

Color Considerations 

Most graphics displays of realistic scenes are in color. But the illumination model 
we have described so far considers only monochromatic lighting effects. To incor- 
porate color, we need to write the intensity equation as a function of the color 
properties of the light sources and object s&a&. 

For an RGB desaiption, each color in a scene is expressed in terms of red, 
green, and blue components. We then specify the RGB components of light- 
source intensities and surface colors, and the illumination model calculates the 
RGB components of the reflected light. One way to set surface colors is by speci- 
fylng the reflectivity coefficients as three-element vectors. The diffuse reflation- 
coefficient vector, for example, would then have RGBcomponents (kdR, kdC, kdB). If 
we want an object to have a blue surface, we select a nonzero value in the range 
from 0 to 1 for the blue reflectivity component, kdD while the red and green reflec- 
tivity components are set to zero (kdR = kdC = 0). Any nonzero red or green com- 
ponents in the incident light are absorbed, and only the blue component is re- 
fl&ed. The intensity calculation for this example reduces to the single e x p d o n  

Surfaces typically are illuminated with white l@t sources, and in general we can 
set surface color so that the reflected light has nonzero values for all three RGB 
components. Calculated intensity levels for each color component can be used to 
a d p t  the corresponding electron gun in an RGB monitor. 

In his original specular-reflection model, Phong set parameter k, to a con- 
stant value independent of the surface color. This p d u c e s  specular reflections 
that are the same color as the incident light (usually white), which gives the sur- 
face a plastic appearance. For a nonplastic material, the color of the specular re- 
flection is a function of the surface properties and may be different from both the 
color of the incident light and the color of the diffuse dections.  We can approxi- 
mate specular effects on such surfaces by making the specular-mfledion coeffi- 
cient colordependent, as in Eq. 1414. Figure 14-21 illustrates color reflections 
from a matte surface, and Figs. 14-22 and 14-23 show color reflections from metal 

Fipn 14-22 
Wt dections from a teapot with 
reeeaancepaametassetto 
simulate brushed aluminum 
surfaces and rendered using Monte 
Carlo ray-tracing methods. (Courtesy 
ofsfcphm H. Hkrtbc. RqFrm ofCmnpltln 
Grclph~cs, CorncU Umimsity.) 



Chaptec 14 

Illumination Models and Surface- 
Rendering Methods 1 I 

, 
Figurn 14-23 

1 Light reflections from trombones 
41 ' k  - '  with reflectance parameters set to 

. , 
simulate shiny brass surfaces. 
(Courtesy of SOITIMAGE, Inc.) 

surfaces. Light mflections from object surfaces due to multiple colored light 
sources is shown in Fig. 14-24. 

.Another method for setting surface color is to specify the components of 
diffuse and specular color vecton for each surface, while retaining the reflectivity 
coefficients as single-valued constants. For an RGB color representation, for in- 
stance, the components of these two surfacecolor vectors can be denoted as (SdR, 
SdC, SdB) and (SIR, SrC, SIB). The blue component of the reflected light is then calcu- 
lated as 

This approach provides somewhat greater flexibility, since surface-color parame- 
ters can be set indewndentlv from the reflectivitv values. 

Other color representations besides RGB can be used to d.escribe colors in a 
scene. And sometimes it is convenient to use a color model with more than three 
components for a color specification. We discuss color models in detail in the 
next chapter. For now, we can simply represent any component of a color spec& 
cation with its spectral wavelength A, lntensity calculations can then be ex- 
pressed= 

Transparency 

A hawparent surface, in general, p d u c e s  both reflected and transmitted light. 
The dat ive  contribution of the transmitted light depends on the degree of trans- 

1 - 
Figure 14-24 
Light retlections due to multiple 
light sources of various colon. 
(Courtesy of S u i ~  Micmsystcms.) 



parency of the surface and whether any li8ht sources or illuminated surfaces are 
behind the transparent surface. F i m  14-25 illushates the intensitv contributions 
to the surface lighting for a transp&nt object. 

When a transparent surface is to be modeled, the intensity equations must 
be modified to indude conhibutions from light passing through the surface. In 
most cases, the transmitted light is generated from reflecting objects in back of 
the surface, as in Fig. 14-26. Reflected light from these objects passes through the 
transparent surface and contributes to the total surface intensity. 

Both diffuse and specular transmission can take place at the surfaces of a 
transparent ob*. Diffuse effects are important when a partially transparent sur- 
face, such as frosted glass, is to be modeled. Light passing through such materials 
is scattered so that a blurred image of background objects is obtained. m s e  re- 
fractions can be generated by decreasing the intensity of the refracted light and 
spreading intensity contributions at each point on the refracting surface onto a fi- 
nite area. These manipulations are time-comsuming, and most lighting models 
employ only specular effects. 

Realistic transparency effects are modeled by considering light refraction. 
When light is incident upon a transparent surface, part of it is reflected and part 
is refracted (Fig. 14-27). Because the speed of light is different in different materi- 
als, the path of the refracted light is different from that of the incident light. The 
direction of the refracted lightTspecified by the angle of refraction, is a function 
of the index of refraction of each material and the direction of the incident light. 
Index of refraction for a material is defined as the ratio of the speed of light in a 
vacuum to the speed of light in the material. Angle of refraction 8, is calculated 
from the angle of incidence 8, the index of refraction t); of the "incident" material 
(usually air), and the index of refraction t), of the refracting material according to 
S?#eil's law: 

Ti sin 8, = - sin 8, 
7. 

mcidenl 
light 1 

Figarc 14-25 
Light emission from a 
transparent surface is in 
general a combination of 
reflected and transmitted 
light. 

To Light 
Source L 

direction 

direction 

-- -- 
Figrrrr 14-26 
A ray-traced view of a transparent glass surface, 
showing both light transmission from objects behind 
the glass and light reflection from the glass surface 
(Courtesy of Eric Hairrrs. 3DIEYE Inc.) 

- - -  - 

F i p ~ r c  14-27 
Reflection direction R and 
refraction direction T for a 
ray of light incident upon a 
surface with index of 
refraction v.. 



Actually, the index of refraction of a material is a function of the wave- 
length of the incident Light, so that we different color components of a Light ray 

incident 

Figure 14-28 
Refraction of light through a 
glass object. The emerging 
refracted ray travels along a 
path that is parallel to the 
incident light path (dashed 
line). 

I 

Projection Plane 

Figure 14-29 
The intensity of a background 
obpd at point P can be 
combined with the reflected 
intensity off the surface of a 
transparent obpd along a 
perpendicular projection line 
(dashed). 

wil'ibe refracted at diffeknt angles. For most applications, we can use an average 
index of refration for the different materials that are modeled ir. a scene. The 
index of refraction of air is approximately 1, and that of crown glass is about 1.5. 
Using these values in Eq. 14-17 with an angle of incidence of 30" yields an angle 
of refraction of about 19'. Figure 14-28 illustrates the changes in the path direc- 
tion for a light ray refracted through a glass object. The overall effect of the re- 
fraction is to shift the inadent light to a parallel path. Since the calculations of the 
trigonometric functions in Eq. 14-17 are time-consuming, refraction effects could 
be modeled by simply shifting the path of the incident light a small amount. 

From Snell's law and the diagram in Fig. 14-27, we can obtain the unit 
transmission vcxtor T in the refraction direction 8, as 

where N is the unit surface normal, and L is the unit vector in the direction of the 
light source. Transmission vector T can be used to locate intersections of the re- 
fraction path with obpcts behind the transparent surface. Including refraction ef- 
fects in a scene can p;oduce highly realistic displays, but the determination of re- 
fraction paths and obiect intersections muires considerable computation. Most 
scan-line' imagespace methods model light transmission with approximations 
that reduce processing time. We return to the topic of refraction in our discussion 
of ray-tracing algorithms (Section 14-6). 

A simpler procedure for modeling transparent objects is to ignore the path 
shifts altogether. In effect, this approach assumes there is no change in the index 
of refraction from one material to another, so that the angle of refraction is always 
the same as the angle of incidence. This method speeds up the calculation of in- 
tensities and can produce reasonable transparency effects fur thin plygon sur- 
faces. 

We can combine the transmitted intensity I,, through a surface from a 
background object with the reflected intensity Id from the transparent surface 
(Fig. 14-29) using a transparency coefficient k,. We assign parameter k, a value 
between 0 and 1 to specify how much of the background light is to be transmit- 
ted. Total surface intensity is then calculated as 

The term (1  - k,) is the opacity factor. 
For highly transparent objects, we assign k, a value near 1. Nearly opaque 

o b ~ t s  transmit very little light from background o b j j s ,  and we can set k, to a 
value near 0 for these materials (opacity near 1). It is also possible to allow k, to 
be a function of position over the surface, so that different parts of an object can 
transmit more or less background intensity according to the values assigned to k,. 

Transparency effects are often implemented with modified depth-buffer (z- 
buffer) algorithms. A simple way to do this is to process opaque objects first to 
determine depths for the visible opaque surfaces. Then, the depth positions of 
the transparent obkts  are compared to the values previously strored in the 
depth buffer. If an; transparent &ace is visible, its reflected intensity is calcu- 
lated and combined with the opaque-surface intensity previously stored in the 
frame buffer. This method can be modified to produce more accurate displays by 
using additional storage for the depth and oiher parameters of the transparent 



Section 14-3 
/ Incident Light 

from a 

-- -~ 
Fipre  14-30 
Obacts modeled with shadow regions. 

surfaces. This allows depth values for the transparent surfaces to be compared to 
each other, as  well as to the depth values of the opaque surfaces. Visible transpar- 
ent surfaces are then rendered by combining their surface intensities with those 
of the visible and opaque surfaces behind them. 

Accurate displays of transparency and antialiasing can be obtained with the 
A-buffer algorithm. For each pixel position, surface patches for all overlapping 
surfaces are saved and sorted in depth order. Then, intensities for the transparent 
and opaque surface patches that overlap in depth ;ire combined in the proper vis- 
ibility order to produce the final averaged intensity for the pixel, as discussed in 
Chapter 13. 

A depth-sorting visibility algorithm can be modified to handle transparency 
by first sorting surfaces in depth order, then determining whether any visible 
surface is transparent. If we find a visible transparent surface, its reflected surface 
intensity is combined with the surface intensity ot' objects behind it to obtain the 
pixel intensity at each projected surface point. 

Shadows 

Hidden-surface methods can be used to locate areils where light sources produce 
shadows. By applying a hidden-surface method with a light source at the view 
position, we can determine which surface sections cannot be "seen" from the 
light source. These are the shadow areas. Once we have determined the shadow 
areas for all light sources, the shadows could be treated as surfacc patterns and 
stored in pattern arrays. Figure 14-30 illustrates the generation of shading pat- 
terns for two objects on a table and a distant light source. All shadow areas in 
this figure are surfaces that are not visible from the position of the light source. 
The scene in Fig. 14-26 shows shadow effects produced by multiple light sources. 

Shadow patterns generated by a h~dden-surface method are valid for any 
selected viewing position, as long as the light-source positions are not changed. 
Surfaces that are visible from the view position are shaded according to the light- 
ing model, which can be combined with texture patterns. We can display shadow 
areas with ambient-light iniensity only, or we  can combirhe the ambient light with 
specified surface textures. 

14-3 
DISPLAYIN(; LIGHT INTENS11 IES 

Values oi intensity cnlculated by an illumination lnodel must be converted to one 
of thc allowable intemitv levels for the particular graphics system in use. Some 

Displaying Light Intensities 



Chapter 14 systems are capable of displaying several intensity levels, while others are capa- 
Illumination Models and Sudace- ble of only two levels for each pixel (on or off). In the first case, we convert inten- 

Methods sities from the lighting model into one of the available levels for storage in the 
frame buffer. For bilevel systems, we can convert intensities into halftone pat- 
terns, as  discussed in the next section. 

Assigning Intensity Levels 

We first consider how grayscale values on a video monitor can be distributed 
over the range between 0 and 1 so that the distribution corresponds to our per- 
ception of equal intensity intervals. We perceive relative light intensities the same 
way that we p e ~ e i v e  relative sound intensities: on a logarithmic scale. This 
means that if the ratio of two intensities is the same as the ratio of two other in- 
tensities, we perceive the difference between each pair of intensities to be the 
same. As an example, we perceive the difference between intensities 0.20 and 
0.22 to be the same as  the difference between 0.80 and 0.88. Therefore, to display 
n + t successive intensity levels with equal perceived brightness, the intensity 
levels on the monitor should be spaced so that the ratio of successive intensities 
is constant: 

Here, we denote the lowest level that can be displayed on the monitor as lo and 
the highest as I,. Any intermediate intensity can then be expressed in terms of I ,  
as 

We can calculate the value of r, given the values of lo and n for a particular sys- 
tem, by substituting k = n in the preceding expression. Since I,, = 1, we have 

Thus, the calculation for Ik in Eq. 14-21 can be rewritten as 

As an example, if 1, = 1/8 for a system with n = 3, we have r. = 2, and the four 
intensity values are 1 /8,1/4,1/2, and 1. 

The lowest intensity value I,  depends on the characteristics of the monitor 
and is typically in the range from 0.005 to  around 0.025. As we saw in Chapter 2, 
a "black" region displayed on a monitor will always have some intensity value 
above 0 due to reflected light from the screen phosphors. For a black-and-white 
monitor with 8 bits per pixel (n = 255) and I, = 0.01, the ratio of successive inten- 
sities is approximately r = 1.0182. The approximate values for the 256 intensities 
on this system are 0.0100, 0.0102, 0.0104, 0.0106, 0.0107, 0.0109, . . . , 0.9821, and 
1.0000. 

With a color system, we  set up intensity levels for each component of the 
color model: Using the RGB model, for example, we can relate the blue compo- 
nent of intensity at level k to the lowest attainable blue value as in Eq. 14-21: 



Section 14-3 
Displaying Light Intensities 

normalized electrongun voltage 

-- 
Frgure 14-31 
A typical monitor response curve, 
showing the displayed screen 
intensity as a function of 
normalized electron-gun voltage. 

and n is the liumber of intensity levels. Similar expressions hold for the other 
color components. 

Gamma Correction and Video Lookup Tables 

Another problem associated with the display of calculated intensities is the non- 
linearity of display devices. illumination models produce a linear range of inten- 
sities. The RGB color (0.25,0.25, 0.25) obtained from a lighting model represents 
one-half the intensity of the ~oloi(0.5,0.5,0.5). Usually, these calculated intensi- 
ties am then stored in an image file as integer values, with one byte for each of 
the three RGB components. This intensity file is also linear, so that a pixel with 
the value (64, 64,M) has onehalf the intensity of a pixel with the value (128,128, 
128). A video monitor, however, is a nonlinear device. If we set the voltages for 
the electron gun proportional to the linear pixel values, the displayed intensities 
will be shifted according to the monitor response curve shown in Fig. 14-31. 

To correct for monitor nonlinearities, graphics systems use a video lookup 
table that adjusts the linear pixel values. The monitor response curve is described 
by the exponential function 

Parameter 1 is the displayed intensity, and parameter V is the input voltage. Val- 
ues for parameters a and y depend on the characteristics of the monitor used in 
the graphics system. Thus, if we want to display a particular intensity value 1, the 
correct voltage value to prbduce this intensity is 



I 
0.5 1 .O 

pixel-inrensity value 

Figure 14-32 
A video lookupcorrection curve for mapping pixel 
intensities to electron-gun voltages uslng gamma 
correction with y = 2.2. Values for both pixel 
intensity and monitor voltages are normalized on 
the interval 0 to 1. 

This calculation is referred to as gamma correction of intensity. Monitor gamma 
values are typically between 2.0 and 3.0. The National Television System Com- 
mittee (NTSC) signal standard is y = 2.2. Figure 14-32 shows a gammacorrection 
curve using the NTSC gamma value. Equation 14-27 is used to set u p  the video 
lookup table that converts integer pixel values In the image file to values that 
control the electron-gun voltages. 

We can combine gamma correction with logarithmic intensity mapping to 
produce a lookup table that contams both conversions. If I is an input intensity 
value from an illumination model, we first locate the nearest intensity 4 from a 
table of values created with Eq. 14-20 or Eq. 14-23. Alternatively, we could deter- 
mine the level number for this intensity value with the calculation 

then we compute the intensity value at this level using Eq. 14-23. Once we have 
the intensity value Ik, we can calculate the electron-gun voltage: 

Values Vk can then be placed in the lookup tables, and values for k would .be 
stored in the frame-buffer pixel positions. If a particular system has no lookup 
table, computed values for V k  can be stored directly in the frame buffer. The com- 
bined conversion to a logarithmic intensity scale followed b~ calculation of the V ,  
using Eq.14-29 is also sometimes~referred to a s  gamma rnrrrctinn. 



( . '  

Fiprn. I4 4 3  
A continuous-tune photograph (a)  printed with ibl two intensiv levels, 
(c)  four intensity levels, and (d) eight intensity levels. 

If the video amplifiers of a monitor are designed to convert the linear input 
pixel values to electron-gun voltages, we cannot combine the two intensity-con- 
version processes. In this case, gamma correction is built into the hardware, and 
the logarithmic values 1, must be precomputed and stored in the frame buffer (or 
the color table). 

Displaying Continuous-Tone Images 

Highquality computer graphics systems generally provide 256 intensity levels 
for each color component, but acceptable displays can be obtained for many ap- 
plications with fewer levels. A four-level system provides minimum shading ca- 
pability for continuous-tone images, while photorealistic images can be gener- 
ated on systems that are capable of from 32 to 256 intensity levels per pixel. 

Figure 14-33 shows a continuous-tone photograph displayed with various 
intensity levels. When a small number of intensity levels are used to reproduce a 
continuous-tone image, the borders between the different intensity regions 
(called contours) are clearly visible. In the two-level reproduction, the features of 
the photograph are just barely identifiable. Using four intensity levels, we begin 
to identify the original shading patterns, but the contouring effects are glaring. 
With eight intensity levels, contouring effects are still obvious, but we begin to 
have a better indication of the original shading. At 16 or more intensity levels, 
contouring effects diminish and the reproductions are very close to the original. 
Reproductions of continuous-tone images using more than 32 intensity levels 
show only very subtle differences from the original. 



14-4 
HALFTONE PATTERNS AND DITHERING TECHNIQUES 

When an output device has a limited intensity range, we can create an apparent 
increase in the number of available intensities by incorporating multiple pixel po- 
sitions into the display of each intensity value. When we view a small region con- 
sisting of several pixel positions, our eyes tend to integrate or average the fine 
detail into an overall intensity. Bilevel monitors and printers, in particuhr, can 
take advantage of this visual effect to produce pictures that appear to be dis- 
played with multiple intensity values. 

Continuous-tone photographs are reproduced for publication in newspa- 
pers, magazines, and books with a printing process called halftoning, and the re- 
produced pi- are called halftones. For a black-and-white photograph, each 
intensity area is reproduced as a series of black c i ~ ~ l e s  on a white background. 
The diameter of each circle is proportional to the darkness required for that in- 
tensity region. Darker regions ire printed with larger circles, a i d  lighter regions 

Figlrrc 14-34 are printed with smaller circles (mom white area). Figure 14-34 shows an en- 
An enlarged section of a larged section of a gray-scale halftone reproduction. Color halftones are printed 
~ho to~a~hre~roduc*  with using dots of various sizes and colors, as shown in Fig. 14-35. Book and maga- 
a halftoning zine halftones are printed on highquality paper using approximately 60 to 80 cir- 

toneSam Tresented cles of varying diameter per centimeter. Newspapers use lower-quality paper 
with varying size dots. and lower resolution (about 25 to 30 dots per centimeter). 

Halftone Approximations 

In computer graphics, halftone reproductions are approximated using rectangu- 
lar pixel regions, called halftone patterns or pixel patterns. The number of intensity 

( 1 1  

- -- 

Figwe 74-35 
Color halftone dot patterns. The top half of theclock in the color halftone (a) is enlarged 
by a factor of YO in (b) and by a factor of 50 in (c). 



Section 14-4 

Halkone Patlerns and Dithermg m- a Techniques 
- 

figvre 14-36 
A 2 by 2 pixel grid used to display five intensity levels on a bilevel 
system. The intensity values that would be mapped to each grid are 
listed below each pixel pattern. 

- - . - - - - 
figure 14-37 
A 3 by 3 pixel grid can be used to display 10 intens~ties on a bilevel 
system. The intensity values that would be mapped to each grid are 
listed below each pixel pattern. 

levels that we can display with this method depends on how many pixels we in- 
clude in the rectangular grids and how many levels a system can display. With 11 
by n pixels for each grid on a bilevel system, we can represent r 1 2  + 1 intensitv 
levels. Figure 14-36 shows one way to set u p  pixel to represent five in- 
tensity levels that could be used with a bilevel system. In pattern 0, all pixels are 
turned off; in pattern 1, one pixel is turned on; and in pattern 4, all four pixels are 
turned on. An intensity value I in a scene is mapped to a particular pattern ac- 
cording to the range listed below each grid shown in the figure. Pattern 0 is used 
for 0 5 I < 0.2, pattern 1 for 0.2 5 I < 0.1, and pattern 4 is used for 0.8 5 I 5 1.0. 

With 3 by 3 pixel grids on a bilevel system, we can display 10 intensity lev- 
els. One way to set up the 10 pixel patterns for these levels is shown in Fig. 14-37. 
Pixel positions are chosen at each level so that the patterns approximate the in- 
creasing circle sizes used in halftone reproductions. That is, the "on" pixel posi- 
tions are near the center of the grid for lower intensity levels and expand out- 
ward as the intensity level increases. 

For any pixel-grid size, we can represent the pxel patterns for the various 
possible intensities with a "mask of pixel position numbers. As an example, the 
following mask can be used to generate thinine 3 by 3 grid patterns for intensity 
levels above 0 shown in Fig. 14-37. 



Chapter 14 

Illumination Models and Surface- 
Rendering Methods 

To display a particular intensity with level number k, we turn on each pixel 
whose position number is less than or equal to k. 

Although the use of n by n pixel patterns increases thenumber of intensities 
that can be displayed, they reduce the resolution of the displayed picture by a 
fador of l / n  along each of the x and y axes. A 512 by 512 screen area, for in- 
stance, is reduced to an area containing 256 by 256 intensity points with 2 by 2 
grid patterns. And with 3 by 3 patterns, we would reduce the 512 by 512 area to 
128 intensity positions along each side. 

Another problem with pixel grids is that subgrid patterns become apparent 
as the grid size increases. The grid size that can be used without distorting the in- 
tensity variations depends on the size of a displayed pixel. Therefore, for systems 
with lower resolution (fewer pixels per centimeter), we must be satisfied with 
fewer intensity levels. On the other hand, high-quality displays require at least 61 
intensity levels. This means that we need 8 by 8 pixel grids. And to achieve a res- 
olution equivalent to that of halftones in books and magazines, we must display 
60 dots per centimeter. Thus, we need to be able to display 60 X 8 = 480 dots per 
centimeter. Some devices, for example high-quality film recorders, are able to dis- 
play this resolution. 

Pixel-grid patterns for halftone approximations must also be constructed to 
minimize contouring and other visual effects not present in the original scene. 
Contouring can be minimized by evolving each successive grid pattern from the 
previous pattern. That is, we form the pattern at level k by adding an "on" posi- 
tion to the grid pattern at level k - 1. Thus, if a pixel position is on for one grid 
level, it is on for all higher levels (Figs. 14-36 and 14-37). We can minimize the in- 
troduction of other visual effects by avoiding symmetrical patterns. With a 3 by 3 
pixel grid, for instance, the third intensity level above zero would be better repre- 
sented by the pattern in Fig. 14-%(a) than by any of the symmetrical arrange- 
ments in Fig. 14-38(b). The symmetrical patterns in this figure would produce 
vertical, horizontal, or diagonal streaks in any large area shaded with intensity 
level 3. For hard-copy output on devices such as film recorders and some print- 
ers, isolated pixels are not eHectly reproduced. Therefore, a grid pattern with a 
single "on" pixel or one with isolated "on" pixels, as in Fig. 14-39, should be 
avoided. 

- - - - . . -. . . 
Figwe 14-38 
For a 3 by 3 pixel grid, pattern (a) is to be preferred to the patterns in (b) for representing 
the third intensity level above 0. 



Figure 14-39 
Halftone grid patterns with isolated pixels that cannot be effectively 
reproduced on some hard-mpy devices. 

Figure 14-40 
Intensity levels 0 through 12 obtained with halftone approximations 
using 2 by 2 pixel grids on a four-level system. 

Halftone approximations also can be used to increase the number of inten- 
sity options on systems that are capable of displaying more than two intensities 
per pixel. For example, on a system that can display four intensity levels per 
pixel, we can use 2 by 2 pixel grids to extend the available intensity levels from 4 
to 13. In Fig. 14-36, the four grid patterns above zero now represent several levels 
each, since each pixel position can display three intensity values above zero. Fig- 
ure 14-40 shows one way to assign the pixel intensities to obtain the 13 distinct 
levels. Intensity levels for individual pixels are labeled 0 through 3, and the over- 
all levels for the system are labeled 0 through 12. 

Similarly, we can use pixel-grid patterns to increase the number of intensi- 
ties that can be displayed on a color system. As an example, suppose we have a 
three-bit per pixel RGB system. This gives one bit per color gun in the monitor, 
providing eight colors (including black and white). Using 2 by 2 pixd-grid pat- 
terns, we now have 12 phosphor doh that can be used to represent a particular 

dub 
color value, as shown in Fig. 14-41. Each of the three RGB colors has four phos- FiSlrrr 14-41 
phor dots in the pattern, which allows five possible settings per color. This gives An RGB 2 by 2 pixel-grid 
us a total of 125 different color combinations. pattern. 

Dithering Techniques 

The term dithering is used in various contexts. Primarily, it refers to techniques 
for approximating halftones without reducing resolution, as pixel-grid patterns 
do. But the term is also applied to halftone-approximation methods using pixel 
grids, and sometimes it is used to refer to color halftone approximations only. 

Random values added to pixel intensities to break up  contours are often re- 
ferred to as dither noise. Various algorithms have been used to generate the ran- 



Chapter 14 dom distributions. The effect is to add noise over an entire picture, which tends 
illumination Models and Surface- to soften intensity boundaries. 

Rendering ~ h o d r  Ordeddither methods generate intexisity variations with a one-to-one map- 
ping of points in a scene to the display pixels. To obtain n2 intensity levels, we set 
up an n by n dither matrh D,, whose elements are distinct positive integers in 
the range 0 to n2 - 1. For example, we can generate four intensity levels with 

and we can generate nine intensity levels with 

The matrix elements for D2 and D, are in the same order as the pixel mask for set- 
ting up 2 by 2 and 3 by 3 pixel grids, respectively. For a bilevel system, we then 
determine display intensity values by comparing input intensities to the matrix 
elements. Each input intensity is first scaled to the range 0 5 1 5  n2. If the inten- 
sity I is to be a plied to screen position ( x ,  y), we calculate row and column num- 
bers for the dit ! er matrix as 

If I > D,(i,~7, we turn on the pixel at position ( x ,  y). Otherwise, the pixel is not 
turned on. 

Elements of the dither matrix are assigned in accordance with the guide- 
lines discussed for pixel grids. That is, we want to minimize added visual effect 
in a displayed scene. Order dither produces constant-intensity areas identical to 
those generated with pixel-grid patterns when the values of the matrix elements 
correspond to the grid mask. Variations from the pixel-grid displays occur at 
boundaries of the intensity levels. 

Typically, the number of intensity levels is taken to be a multiple of 2. 
Higher-order dither matrices are then obtained from lower-order matrices with 
the m r r e n c e  relation: 

assuming n r 4. Parameter UnI2 is the "unity" matrix (all elements are 1). As an 
example, if D2 is specified as in Eq. 14-31, then recurrence relation 14-34 yields 

Another method for mapping a picture with rn by n points to a display area 
with rn by n pixels is error diffusion. Here, the rrror between an input intensity 



value and the displayed pixel intensity level at a given position is dispersed, or M i o n  1 U  
diffused, to pixel positions to the right and below the current pixel position. tialfiorte Panernrand Dithering 
Starting with a matrix M of intensity values obtained by scanning a photograph, 
we want to construct an array I of pixel intensity values for an area of the screen. 
We do this by first scanning across the rows of M, from left to right, top to bot- 
tom, and determining the nearest available pixel-intensity level for each element 
of M. Then the error between thevalue s t o d  in matrix M and the displayed in- 
tensity level at each pixel position is distributed to neighboring elements in M, 
using the following simplified algorithm; 

for (k0; ian;  i++) 
for (j=0; j<n; I++) l 

/* Determine the available intensity level It '/ 
/* that is dosest to the value M,* */ 
I  .= 1 ~  2" := M . . - I . . .  , L,' 
M,,,, := k; ., + a. err; 
M,,~,-, := hi+,,-, + p . err; 
Mi+,, := + y . err; 
Y+Iit~ := + 8.err; 

I 

Once the elements of matrix I have been assigned intensity-level values, we then 
map the matrix to some area of a display device, such as a printer or video moni- 
tor. Of course, we cannot disperse the error past the last matrix column ( j  = n) or 
below the last matrix row (i = m). For a bilevel system, the available intensity 
levels are 0 and 1. Parameters for distributing the error can be chosen to satisfy 
the following relationship 

One choice for the errordiffusion parameters that produces fairly good re- 
sults is (a, p, y, 8) = (7/16, 3/16, 5/16, 1/16). Figure 14-42 illustrates the error 
distribution using these parameter values. Error diffusion sometimes pmduces 
"ghosts" in a picture by repeating, or echoing, certain parts of the picture, partic- 
ularly with facial features such as hairlines and nose outlines. Ghosting can be re- 

F ipre  14-42 
Fraction of intensity error that can be distributed to neighboring pixel 
positions using an error-diffusion scheme. 

column j 

, 

/ 
3 
i?, 

- 
I \, 
1 
5 
i?, 

7 -- G 
\ 

1 - 
16 

row i 

r o w i + l  



Chapter 14 

Illumination Models and Surface- 
Rendering Methods 

50 62 45 13 18 

38 46 54 37 25 17 26 

Figure 11-43 
One possible distribution scheme 
for dividing the intensity array into 
64 dot-diffusion classes, numbered 
from 0 through 63. 

duced by choosing values for the error-diffusion parameters that sum to a value 
less than 1 and by rescaling the matrix values after the dispersion of errors. One 
way to rescale is to multiply all elements of M by 0.8 and then add 0.1. Another 
method for improving picture quality is to alternate the scanning of matrix rows 
from right to left and left to right. 

A variation on the error-diffusion method is dot diffusion. In this method, 
the rn by n array of intensity values is divided into 64 classes numbered from 0 to 
63, as shown in Fig. 14-43. The error between a matrix value and the displayed 
intensity is then distributed only to those neighboring matrix elements that have 
a larger class number. Distribution of the 64 class numbers is based on minimiz- 
ing the number of elements that are completely surrounded by elements with a 
lower class number, since this would tend to direct all errors of the surrounding 
elements to that one position. 

14-5 
POLYGON-RENDERING METHODS 

In this section, we consider the application of an illumination model to the ren- 
dering of standard graphics objects: those formed with polygon surfaces. The ob- 
jects are usually polygon-mesh approximations of curved-surface objects, but 
they may also be polyhedra that are not curved-surface approximations. Scan- 
line algorithms typically apply a lighting model to obtain polygon surface ren- 
dering in one of two ways. Each polygon can be rendered with a single intensity, 
or the intensity can be obtained at each point of the surface using an interpola- 
tion scheme. 

C o n s t a n t - I n t e n s i t y  Shading 

A fast and simple method for rendering an object with polygon surfaces is con- 
stant-intensity shading, also caned flat shading. In this method, a single inten- 
sity is calculated for each polygon. All points over the surface of the polygon are 
then displayed with the same intensity value. Constant shading can bc useful for 
quickly displaying thegeneral appearance of a curved surface, as in Fig. 14-47. 

In general, flat shading of polygon facets provides an accurate rendering for 
an object if all of the following assumptions are valid 

The object is a polyhedron and is not an approximation of an object with a 
curved surface. 



kction 14-5 
Polygon-Rendering Memods 

Figure 14-44 
The normal vector at vertex V is 
calculated as the average of the 
surface normals for each polygon . 
sharing that vertex. 

All light sources illuminating the obpzt are suficiently far from the surface 
so that N . L and the attenuation function are constant over the surface. 

*. The viewing position is sufficiently far from the surface so that V R is con- 
stant over the surface. 

Even if all of these conditions are not true, we can still reasonably approximate 
surface-lighting effects using small polygon facets with flat shading and calculate 
the intensity for each facet, say, at the center of the polygon. 

Gouraud Shading 

This inteneity-interpolation scheme, developed by Gouraud and generally re- 
ferred to as Gouraud shading, renders a polygon surface by linearly interpolat- 
ing intensity values across the surface. Iqtensity values for each polygon are 
matched with the values of adjacent polygons along the common edges, thus 
eliminating the intensity discontinuities that can occur in flat shading. 

Each polygon surface is rendered with Gouraud shading by performing the 
following calculations: 

Determine the average unit normal vector at each polygon vertex. 
Apply an illumination model to each vertex to calculate the vertex intensity. 
Linearly interpolate the vertex intensities over the surface of the polygon. 

At each polygon vertex, we obtain a normal vector by averaging the surface 
normals of all polygons sharing that vertex, as illustrated in Fig. 14-44. Thus, for 
any vertex position V, we obtain the unit vertex normal with the calculation 

Once we have the. vertex nonnals, we can determine the intensity at the vertices 
from a lighting model. 

~ i & e  1445 demonstrates the next step: interpolating intensities along the 
polygon edges. For each scan line, the intensity at the intersection of the scan line 
with a polygon edge is linearly interpolated from the intensities at the edge end- 
points. For the example in Fig. 14-45, the polygon edge with endpoint vertices at 
positions 1 and 2 is intersected by the scan line at point 4. A fast method for ob- 
taining the intensity at point 4 is to interpolate bekeen intensities I, and I, using 
only the vertical displacement of the scan line: 



Figure 14-45 
For Gouraud shading, the intensity 
at point 4 is linearly interpolated 
from the intensities at vertices 1 and 
2. The intensity at point 5 is linearly 
interpolated from intensities at 
vertices 2 and 3. An interior point p 
is then assigned an intensity value 
that is linearly interpolated from 
intensities at positions 4 and 5. 

Similarly, intensity at the right intersection of this scan line (point 5) is interpo- 
lated from intensity values at vertices 2 and 3. Once these bounding intensities 
are established for a scan line, an interior point (such as point p in Fig. 14-45) is 
interpolated from the bounding intensities at points 4 and 5 as 

Incremental caldations are used to obtain successive edge intensity values 
between scan lines and to obtain successive intensities along a scan line. As 
shown in Fig. 14-46, if the intensity at edge position (x, y) is interpolated as 

then we can obtain the intensity abng this edge for the next scan line, y - I, as 

Incremental interpolation of intensity values along a 
polygon edge for su-ive r a n  lines. 



1.1 1 I h 1 
-. --- - - - - 
Fcgrrrr 14-47 
A polygon mesh approwmahon of an object (a) is rendered wlth flat 
shading (b) and with Couraud shading (c). 

Similar calculations are used to obtain intensities at successive horizontal pixel 
positions along each scan line. 

When surfaces are to be rendered in color, the intensity of each color com- 
ponent is calculated at the vertices. Gouraud shading can be combined with a 
hidden-surface algorithm to fill in the visible polygons along each scan line. An 
example of an object shaded with the Gouraud method appears in Fig. 14-47. 

Gouraud shading removes the intensity discontinuities associated with the 
constant-shading model, but it has some other deficiencies. Highlights on the 
surface are sometimes displayed with anomalous shapes, and the linear intensity 
interpolation can cause bright or dark intensity streaks, called Mach bands, to ap- 
pear on the surface. These effects can be reduced by dividing the surface into a 
greater number of polygon faces or by using other methods, such as Phong shad- 
ing, that require more calculations. 

Phong Shading 

A more accurate method for rendering a polygon surface is to interpolate normal 
vectors, and then apply the illumination model to each surface point. This 
method, developed by Phong Bui Tuong, is called Phong shading, or nonnal- 
vector interpolation shading. It displays more realistic highlights on a surface 
and greatly reduces the Mach-band effect. 

A polygon surface is rendered using Phong shading by carrying out the fol- 
lowing steps: 

Determine the average unit normal vector at each polygon vertex. 
Linearly &erpolate the vertex normals over the surface of the polygon. 
Apply an illumination model along each scan line to calculate projected 
pixel intensities for the surface points. 

Interpolation of surface normals along a polygon edge between two vertices 
is illustrated in Fig. 1448. The normal vector N for the scan-line intersection 
point along the edge between vertices 1 and 2 can be obtained by vertically inter- 
polating between edge endpoint normals: 



Chapter 14 N, 

illumination Models and Surface- 
Rendering Methods 

scan line 

Figutp 14-48 
Interpolation of surface normals 
alonga polygon edge 

Incremental methods are used to evaluate normals between scan lines and along 
each individual scan line. At each pixel position along a scan line, the illumina- 
tion model is applied to determine the surface intensity at that point. 

Intensity calculations using an approximated n o ~ a l  vector at each point 
along the scan line produce more accurate results than the direct interpolation of 
intensities, as in Gouraud shading. The trade-off, however, is that Phong shading 
requires considerably more calculations. 

Fast Phong Sh~ding 

Surface rendering with Phong shading can be speeded up by using approxima- 
tions in the illumination-model calculations of normal vectors. Fast Phong shad- 
ing approximates the intensity calculations using a Taylor-series expansion and 
hiangular surface patches. 

Since Phong shading interpolates normal vectors from vertex normals, we 
can express the surface normal N at any point (x ,  y) over a triangle as 

where vectors A, B, and C are determined from the three vertex equations: 

with (g, yk) denoting a vertex position. 
Omitting the reflectivity and attenuation parameters, we can write the cal- 

culation for light-source diffuse reflection from a surface point (x,  y) as 

- - L . (Ax + By + C) 
I L I  I A X  + B~ + C I  



We can rewrite this expression in the form Section 14-6 
Ray-Tracirig Methods 

where parameters such as a, b, c, and d are used to represent the various dot 
products. For example, 

Finally, we can expre5s the denominator in Eq. 14-46 as a Taylor-series expansion 
and retain terms up to second degree in x and y. This yields 

where each T, is a function of parameters a, b, c, and so forth. 
Using forward differences, we can evaluate Eq. 14-48 with only two addi- 

tions for each pixel position (x,  y )  once the initial forward-difference parameters 
have been evaluated. Although fast Phong shading reduces the Phong-shading 
calculations, it still takes approximately twice as long to render a surface with 
fast Phong shading as it does with Gouraud shading. Normal Phong shading 
using forward differences takes about six to seven times longer than Gouraud 
shading. 

Fast Phong shading for d i h s e  reflection can be extended to include specu- 
lar reflections. Calculations similar to those for diffuse reflections are used to 
evaluate specular terms such as (N . H)"s in the basic illumination model. In ad- 
dition, we can generalize the algorithm to include polygons other than triangles 
and finite viewing positions. 

14-6 
RAY-TRACING METHODS 

In Section 10-15, we introduced the notion of ray cnsting, where a ray is sent out 
from each pixel position to locate surface intersections for object modeling using 
constructive solid geometry methods. We also discussed the use of ray casting as 
a method for determining visible surfaces in a scene (Section 13-10). kay tracing 
is an extension of this basic idea. Instead of merely looking for the vislble surface 
for each pixel, we continue to bounce the ray around the scene, as illustrated in 
Fig. 14-49, collecting intensity contributions. This provides a simple and power- 
ful rendering technique for obtaining global reflection and transmission effects. 
The basic ray-tracing algorithm also provides for visible-surface detection, 
shadow effects, transparency, and multiple light-source illumination Many ex- 
tensions to the basic algorithm have been developed to produce photorealistic 
displays. Ray-traced displays can be highly realistic, particularly for shiny ob- 
jects, but they require considerable computation time to generate. An example of 
the global reflection and transmission effects possible with ray tracing is shown 
i n  Fig. 14-50. 



Chapter 14 
lllumination Models and Surface- 

point 

Figure 14-49 
Tracing a ray from the projection reference point through a pixel 
position with multiple reflections and transmissions. 

Basic Ray-Tracing Algorithm 

We first set up  a coordinate system with the pixel positions designated in the r!/ 
plane. The scene description is given in this reference frame (Fig. 14-51]. From the 
center of projection, we then determine a ray path that passes through the center 
of each screen-pixel position. lllumination effects accumulated along this ray 
path are then assigned to the pixel. This rendering approach is based on the prin- 
ciples of geometric optics. Light rays from the surfaces in a scene emanate in all 
directions, and some will pass through the pixel positions in the projection plane. 
Since there are an infinite number of ray paths, we determine the contributions to 
a particular pixel by tracing a light path backward from the pixel to the scene. We 
first consider the basic ray-tracing algorithm with one ray per pixel, which is 
equivalent to viewing the scene through a pinhole camera. 

A iay-traced scene, showing global 
reflection and transmission 
illumination effects from object 

- <- surfaces. (Cuurtrsy of Ei,ans 6 
Strlhcrln~td ) 



pixel screen area 
centered on vlswtng 
coordinate o r i g ~ n  

-- ----- -. . - prolecuon 
reference polnl ' 

For each p~xcl  ray, w e  test each surface in the hcene to determine i f  it is ill- 
tcrsected by the ray. I f  a surface is intersected, we calculate the distance from the 
pixel to the surtace-intersection point. The smallest calculated intersection dis- 
tance identifies the visible surface for that pixel. We then reflect the ray off the 
\.~siblc surfact- along a spccular path (angle of refierticm equals angle of inci- 
dence) I f  the surface is transparent, we also send a r.ay through the surface in the 
retraction dircctmn. Reflection and refraction rays arc referred to as x~lnrdny 
r,11/2. 

Thib proc,eJurr. is repeated for each secondary :a!-: Objects are tested for in- 
tcmection, and the nearest surface along a vxond ,~ ry  ray path is used to recur- 
3n.ely produce the next generation of rdlec\lon and reiractiun paths. As the rays 
from a p1xc.l ricochet through the scene, each succ~wively intersected surface is 
' ~dded  to a blnary my-lmcing tree, a s  shown in Fig 14-52. We use left branches in 
the trec to represent reflection paths, and right branches represent transmission 
paths, Max~rnuni d tyth  of the ray-tracing trees can be set as a user option, or it 
i,ui be determined by the amount of storagc availal~lc. A path in the tree is then 
terniina ted i f  it reaches the preset maximum or if the ray strikes a light source. 

The intensity assigned to a pixel is then determined by accumulating the in- 
tensity contributions, starting at the bottoni (terminal nodes) of its ray-tracing 
tree. Surface ~ntensity from each node in the tree is attenuated by the distance 
from the "parent" surface (next node up  the tree) and added to the intensity of 
the parent surface. Pixel intensity is then the sum of the attenuated intensities at 
the root node of Ihe ray tree. If  no surfaces are inter~ccted by a pixel ray the ray- 
tracing tree is eniptv and the pixel is assigned the intensity value of the back- 
ground If a pxc l  my intersects a nonreflecting light source. the pixel can be as- 
s p e d  the mtensity of the source, although light sources are usuallv placed 
hevond the path of the initial rays. 

Figure 14-53 shows a surface intersected by a ray and the unit vectors 
needed for the reflected light-intensity calculations. Unit vector u is in the direc- 
t ~ o n  of the ray p.ith, N is the unit surface normal, R i \  the unit reflection \vctor, L 
i t .  thc unit \ ~ ~ t n r .  pointing to the light source, and H 15 the unit \.ector halfway br- 
t \ \ w n  V toppusite lo u) and L. The path along L ib rvterred to as the shadow ray. 
I t  , inv object ~ntrrsects the shadow ray between thts surface and the point light 

Section 14-6 

E.lv-Trac~nfi Methods 



Chapter I4 

Illumination Models and Surface. 
Rendering Methods 

pro)ection 
reference point 

-- - - - - - - - - A 

Figure 14-52 
(a) Reflection and refraction ray paths through a scene lor a 
screen pixel. (b) Binary ray-tracing tree for the paths shown 
in (a). 

source, the surface is in shadow with respect to that source. Ambient light at the 
surface is calculated as kJ,; diffuse reflection due to the source is proportional to 
kd(N . L); and the specular-reflection component is proportional to k&H. NP. As 
discussed in Section 14-2, the specular-reflection direction for the secondary ray 
path R depends on the surface normal and the incorning ray direction: 

For a transparent surface, we also need to obtain intensity contributions 
from light transmitted through the material. We can locate the source of this con- 
tribution by tracing a xcondary ray along the transmission direction T, as shown 
in Fig. 14-54. The unit transmission vector can be obtained from vectors u and N 
as 



, light \ ,  , - ., \.A -- -- - source 
,! $ \'\ 

Figur~. 14-53 
Unit vectors at the surface of an o b j t  intersected by an 
incoming ray along direction u. 

Figure 14-54 
Refracted ray pathT through a transparent material. 

Parameters qi and T, are the indices of rehadion in the incident material and the 
reha,cting material, respectively. Angle of refraction 0, can be calculated from 
Snell's law: 

Ray-Surface Intersection Calculations 

A ray can be described with an initial position Po and unit direction vector u, as 
illustrated in Fig. 14-55. The coordinates of any point P along the ray at a distance 
s from Po is computed from the ray equation: 

Initailly, Po can be set to the position of the pixel on the projection plane, or it 
could be chosen to be the projection reference point. Unit vector u is initially ob- 

Section 14-6 

Ray-Tracing Methods 



Chapter 14 

Illumination Models and Surface- 
Rendering Melhods 

-- 
Figure 14-55 
Describing a ray with an initial- 

x position vector Po and unit direction 
vector u. 

tained from the position of the pixel through which the ray passes and the projec- 
tion reference point: 

At each intersected surface, vectors Po and u are updated for the secondary rays 
at the ray-surface intersection point. For the secondary rays, reflection direction 
for u is R and the transmission direction is T. To locate surface intersections, we 
simultaneously solve the ray equation and the surface equation for the individ- 
ual objects in the scene. 

The simplest objects to ray trace are spheres. If we have a sphere of radius r 
and center position P, (Fig. 14-56), then any point P on the surface must satisfy 
the sphere equat~on: 

Substituting the ray equatlon 14-53, we have 

I f  we let AP = P, - P,, m d  expand the dot product, we obtain the quadrat~c equa- 
tion 



kcbion 14-6 
Ray-Tracing Mcthodr 

Figun 14-57 
A "spherefhke" rendered with ray 
tracing using 7381 spheres and 3 
light sources. (CouHcq of Eric Hains, 
3DIEYE Inc.) 

whose solution is 

If the d i i n a n t  is negative, the ray does not intersect the sphere. Otherwise, 
the surfa~intersection coordinates are obtained from the ray equation 14-52 
using the smaller of the two values from Eq. 14-57. 

For small spheres that are far from the initial ray position, Eq. 14-57 is sus- 
ceptible to roundoff emrs. That is, if 

we could lose the 9 term in the precision error of I AP 1 '. We can avoid this for 
most cases by rearranging the calculation for distance s as 

Figure 14-57 shows a snowflake pattern of shiny spheres rendered with ray trac- 
ing to display global surface reflections. 

Polyhedra require moxe processing than spheres to locate surface intersec- 
tions. For that reason, it is often better to do an initial i n t d o n  test on a 
bounding volume. For example, Fig. 14-58 shows a polyhedron bounded by a 
sphere. If a ray does not intersect the sphexe, we do not need to do any further 
testing on the polyhedron. But if the ray does intersect the sphere, we first locate 
"front" faces with the test 

w h  N is a surface normal. For each face of the polyhedron that satisfies in- 
equality 14-59, we solve the plane equation 

for surface position P that also satisfies the ray equation 14-52. Here, N = (A, B, 8 



Chapter 14 

Illumination Models and Surface- 
Rendering Methods 

- 

Figure 11-58 
Polyhedron enclosed by a boundmg sphere. 

and D is the fourth plane parameter. Position P is both on the plane and on the 
ray path if 

And the distance from the initial ray position to the plane is 

This gives us a position on the infinite plane that contains the polygon face, but 
this position may not be inside the polygon boundaries (Fig. 14-59). So we need 
to perform an "inside-outside" test (Chapter 3) to determine whether the ray in- 
tersected this face of the polyhedron. We perform this test for each face satisfying 
inequality 14-59. The smallest distance s to an inside point identifies the inter- 
sected face of the polyhedron. If no intersection positions from Eq. 14-62 are in- 
side points, the ray does not intersect the ob j j t .  

Similar procedures are used to calculate ray-surface intersection positions 
for other objects, such as quadric or spline surfaces. We combine the ray equation 
with the surface definition and solve for parameter s. In many cases, numerical 
root-finding methods and incremental calculations are used to locate intersection 

intersection 

polygon 

- -- - - - . - 
Fipr13 I + i Y  
Ray intersection wi th  the plane of n polvgon. 



Section 14-6 

Ray-Tracmg Methods 

I ' i p r e  14-60 
A ray-haced scene showing global reflection of surfacetexture 
patterns. (Courtesy of Sun Micms.ystms.) 

points over a surface. Figure 14-60 shows a ray-traced scene containing multiple 
objects and texture patterns. 

Reducing Object-Intersection Calculations 

Raysurface intersection calculations can account for as much as 95 percent of the 
processing time in a ray tracer. For a scene with many objects, most of the pro- 
cessing time for each ray is spent checking objects that are not visible along the 
ray path. Therefore, several methods have been developed for reducing the pro- 
cessing time spent on these intersection calculations. 

One method for reducing the intersection calculations is to enclose groups 
of adjacent objects within a bounding volume, such as a sphere or a box (Fig. 14- 
61). We can then test for ray intersections with the bounding volume. If the ray 
does not intersect the bounding object, we can eliminate the intersection tests 
with the enclosed surfaces. This approach can be extendcd to include a hierarchy 
of bounding volumes. That is, we enclose several bounding volumes within a 
larger volume and carry out the intersection tests hierarchically. First, we test the 
outer bounding volume; then, if necessary, we test the smaller inner bounding 
volumes; and so on. 

Space-Subdivision Methods 

Another way to reduce intersection calculations, is to use space-subdivision meth- 
ods. We can enclose a scene within a cube, then we successively subdivide the 
cube until each subregion (cell) contains no more than a preset maximum num- 
ber of surfaces. For example, we could require that each cell contain no more 
than one surface. If parallel- a11d vector-processing capabilities are available, the 
maximum number of surfaces per cell can be determined by the size of the vector 

bounding 
sphere 

...- 
Fqpn 14-hl 
A group of objects enclosed within 
a bound~ng sphere. 



Chapter 14 

lllum~nat~on Models and Sutiace- 
Render~ng Methods 

pixel 
ray 

Figrcre 14-62 
Ray intersertion with a cube 
enclosing all objects in a scene. 

registers and the number of processors. Space subdivision of the cube can be 
s t o d  in an octree or in a binary-partition tree. In addition, we can perform a 
uniform subdivision by dividing the cube into eight equal-size octants at each step, 
or we can perform an adaptive subdivision and subdivide only those regions of the 
cube containing objects. 

We then trace rays through the individual cells of the cube, performing in- 
tersection tests only within those cells containing surfaces. The first object surface 
intersected by a ray is the visible surface for that ray. There is a trade-off between 
the cell size and the number of surfaces per cell. If we set the maximum number 
of surfaces per cell too low, cell size can become so small that much of the sav- 
ings in reduced intersection tests goes into cell-traversal processing. 

Figure 14-62 illustrates the intersection of a pixel ray with the front face of 
the cube enclosing a scene. Once we calculate the intersection point on the front 
face of the cube, we determine the initial cell intersection by checking the inter- 
section coordinates against the cell boundary positions. We then need to process 
the ray through the cells by determining the entry and exit points (Fig. 14-63) for 
each cell traversed by the ray until we intersect an object surface or exit the cube 
enclosing the scene. 

Given a ray direction u and a ray entrv position Pi, for a cell, the potential 
exit faces are those for which 

If the normal vectors for the cell faces in Fig. 14-63 are aligned with the coordi- 
nates axes, then 

Fixrrrc 14-63 
Ray traversal through a subregion 
(cell) of a cube enclosing a scene. 



and we only need to check the sign of each component of u to determine the s d i  14-6 , 
three candidate exit planes. The exit position on each candidate plane is obtained Ray-Trac~ng Methods 
h m  the ray equation: 

where st is the distance along the ray from Pi, to P,*. Substituting the ray equa- 
tion into the plane equation for each cell face: 

we can solve for the ray distance to each candidate exit face as 

and then select smallest s,. This caiculation can be simplified if the normal vec- 
tors N, are aligned with the coordinate axes. For example, if a candidate normal 
vector is (1, 0, O), then for that plane we have 

where u = (u,, u,, u,), and xk is the value of the right boundary face for the cell. 
Various modifications can be made to the cell-traversal procedures to speed 

up the processing. One possibility is to take a trial exit plane k as the one perpen- 
dicular to the direction of the largest component of u. The sector on the hial 
plane (Rg. 14-61) containing P,t,kdetermines the true exit plane. If the intersec- 
tion point is in sector 0, the trial plane is the true exit plane and we are 
done. If the intersection point is sector 1, the true exit plane is the top plane and r$ 4 
we simply need to calculate the exit point on the top boundary of the cell. Simi- 
larly, sector 3 identifies the bottom plane as the true exit plane; and sectors 4 and 
2 identify the true exit plane as the left and right cell planes, respectively. When 8 
the trial exit point falls in sector 5,6,7, or 8, we need to cany out two additional 
intersection calculations to identify the true exit plane. Implementation of these Fi,v,llr 
methods on parallel vector machines provides further improvements in perfor- sectors of the trial exit plane, 
mance. 

The scene in Fig. 14-65 was ray traced using spacesubdivision methods. 
Without space subdivision, the ray-tracing calculations took 10 times longer. 
Eliminating the polygons also speeded up the processing. For a scene containing 
2048 spheres and no polygbns, the same algorithm executed 46 times faster than 
the basic ray tracer. 

Figure 14-66 illustrates another ray-traced scene using spatial subdivision 
and parallel-processing methods. This image of Rodin's Thinker was ray traced 
with over 1.5 million rays in 24 seconds. 

The scene shown in Fig. 14-67 was rendered with a light-buffer technique, a 
form of spatial partitioning. Here, a cube is centered on each point light source, 
and each side of the cube is partitioned with a grid of squares, A sorted list of ob- 
jects that are visible to the light through each square is then maintained by the 
ray tracer to speed up processing of shadow rays. To determine surface-illumina- 
tion effects, the square for each shadow ray is computed and the shadow ray is 
then processed against the list of objects for that square. 



Chaptu 14 Intersection testa in ray-tracing programs can also be reduced with direc- 
illumination ~odels and Surface- tional subdivision procedures, by considering sectors that contain a bundle of 

Rendering M*hods rays. W~thin each sector, we can sort surfaces in depth order, as in Fig. 14-68. 
Each ray then only needs to test objects within the sector that contains that ray. 

Antialiased Ray Tracing 

Two basic tdmiques for antialiasing in ray-tracing algorithms are supersmtrpling 
and adpptive sampling. Sampling in ray tracing is an extension of the sampling 
methods we discussed in Chapter 4. In supersampling and adaptive sampling, 

Figure 14-65 
A parallel ray-traced scene containing 37 spheres and 
720 polygon surfaces. The ray-tracing algorithm 
used 9 rays per pixel and a tree depth of 5. Spatial 
subdivision methods pxocsd  the scene 10 times 
faster than the basic ray-tracing algorithm on an 
AUiant FW8. (Courtmy of La-Hian Quek, Information 
Tdnology Imtihrtc, Republic of Singapon.) 

- 
Figirrr : 

1 This ra; 
14-66 
y-traced scene took 24 

seconds to render on a Kepdall 
Square Research KSRl parallel 
computer with 32 ~TOC~SSOIS. 
Rodin's Thinker was modeled with 
3036 primitives. Two light sources 
and one primary ray per pixel 
were used to obtain the giobal 
illumination effects from the 
1,675,776 rays processed. (Courtesy of 
M. 1. Kealps and R.  1. Hubbold, Dcpllrtmrnt 
olCmnpuln Scirnu, Univmrfy of 
Manchheslcr.) 



Figure 14-67 
A room scene illuminated with 5 light sources (a) was rendered using 
the ray-tracing light-buffer technique to process shadow rays. A closeup 
(b) of part of the room shown in (a) fflustrates the global illumination 
effects. The mom is modeled with 1298 polygons, 4 spheres, 76 
cylinders, and 35 quadrics. Rendering time was 246 minutes on a VAX 
11 /780, compared to 602 minutes without using light buffers. (Courtesy of 
Erlc Heines end Donald I? Grembng, Ptvparn of Computn Graphics, Cornell 
Univnsify.) 

Bundle of Rays 

Figrcrc 14-68 
Directional subdivision of space. All rays in this sector 
only need to test the surfaces within the &or in depth 
order. 

the pixel is treated as a finite square area instead of a single point. Supersampling 
uses multiple, evenly spaced rays (samples) over each pixel area. Adaptive sam- 
pling uses unevenly spaced rays in some regions of the pixel area. For example, 
more rays can be used near object edges to obtain a better estimate of the pixel in- 
tensities. Another method for sampling is to randomly distribute the rays over 
the pixel area. We discuss this approach in the next section. When multiple rays 



Illumination Modcls and Surface- 
Rendering Mehods 

Figure 14-70 
Subdividing a pixel into nine 
subpixels with one ray at 
each subpixel corner. 

F i p n  14-71 
Ray positions centered on 
subpixel areas. 

Pixel Pmitiona 
on Pmjec(ion Plane 

Reference Point 

-- 

Figure 14-69 
Supersampling with four rays per pixel, one at each pixel corner. 

per pixel are used, the intensities of the pixel rays are averaged to produce the 
overall pixel intensity. 

Figure 14-69 illustrates a simple supersampling procedure. Here, one ray is 
generated through each comer of the pixel. If the intensities for the four rays are 
not approximately equal, or if some sman object lies between the four rays, we 
divide the pixel area into subpixels and repeat the process. As an example, the 
pixel in Fig. 14-70 is divided into nine subpixels using 16 rays, one at each sub- 
pixel corner. Adaptive sampling is then used to further subdivide those subpixels 
that do not have nearly equal-intensity rays or that subtend some small o b j .  
This subdivision process can be continued until each subpixel has approximately 
equal-intensity rays or an upper bound, say, 256, has been reached for the num- 
ber of rays per pixel. 

The cover picture for this book was rendered with adaptive-subdivision ray 
tracing, using Rayshade version 3 on a Macintosh 11. An extended light source 
was used to provide realistic soft shadows. Nearly 26 million primary rays were 
generated, with 33.5 million shadow rays and 67.3 million reflection rays. Wood 
grain and marble surface patterns were generated using solid texturing methods 
with a noise function. Total rendering time with the extended llght source was 
213 hours. Each image of the stereo pair shown in Fig. 2-20 was generated in 45 
hours using a point light source. 

Instead of passing rays through pixel corners, we can generate rays through 
subpixel centers, as in Fig. 14-71. With this approach, we can weight the rays ac- 
cording to one of the sampling schemes discussed in Chapter 4. 

Another method for antialiasing displayed scenes is to treat a pixel ray as a 
cone, as shown in Fig. 14-72. Only one ray is generated per pixel, but the ray now 
has a finite cross section. To determine the percent of pixel-area coverage with 
obpcts, we calculate the intersection of the pixel cone with the object surface. For 
a sphere, this q u i r e s  finding the intersection of two circles. For a polyhedron, 
we must find the intersection of a circle with a polygon. 

Distributed Ray Tracing 

This is a stochastic sampling method that randomly distributes rays according to 
the various parameters in an illumination model. Illumination parameters in- 



Section 14-6 

Ray-Tracing Methods 

clude pixel area, reflection and refraction directions, camera lens area, and time. 
Aliasing efferts are thus replaced with low-level "noise", which improves picture 
quality and allows more accurate modeling of surface gloss and translucency, fi- 
nite camera apertures, finite light sourres, and motion-blur displays of moving 
objects. ~istributcd ray tracing (also referred to as distribution ray tracing) essen- 
tially provides a Monte Carlo evaluation of the multiple integrals that occur in an 
accurate description of surface lighting. 

Pixel sampling is accomplished by randomly distributing a number of rays 
over the pixel surface. Choosing ray positions completely at random, however, 
can result in the rays clusteringtog&her in a small -%ion of the pixel area, and 

angle, time, etc.), as explained in the following discussion. Each subpixel ray is 
then processed through the scene to determine the intensity contribution for that 
ray. The 16 ray intensities are then averaged to produce the overall pixel inten- pi,e, using 16 
sity. If the subpixel intensities vary too much, the pixel is further subdivided. subpixel areas and a jittered 

To model camera-lens effects, we set a lens of assigned focal length f in front position from !he center 
of the projection plane ,and distribute the subpixel rays over the lens area. As- coordinates for each subarea. 
suming we have 16 rays per pixel, we can subdivide the lens area into 16 zones. 
Each ray is then sent to the zone corresponding to its assigned code. The ray po- 
sition within the zone is set to a jittered position from the zone center. Then the 
ray is projected into the scene from the jittered zone position through the focal 
point of the lens. We locate the focal point for a ray at a distance f from the lens 
along the line from the center of the subpixel through the lens center, as shown in 
Fig. 14-74. Objects near the focal plane are projected as sharp images. Objects in 
front or in back of the focal plane are blurred. To obtain better displays of out-of- 
focus objects, we increase the number of subpixel rays. 

Ray reflections at surfaceintersection points are distributed about the spec- 
ular reflection direction R according to the assigned ray codes (Fig. 14-75). The 

leaving other parts of the pixel unsampled. A better approximation of the light 
distribution over a pixel area is obtained by using a technique called jittering on a 
regular subpixel grid. This is usually done by initially dividing the pixel area (a 
unit square) into the 16 subareas shown in Fig. 14-73 and generating a random 
jitter position in each subarea. The random ray positions are obtained by jittering 
the center coordinates of each subarea by small amounts, 6, and Gy, where both 6, 
and 6, are assigned values in the interval (-0.5,0.5). We then choose the ray po- 
sition in a cell with center coordinates (x, y) as the jitter position ( x  + S,, y + SY). 

Integer codes 1 through 16 are randomly assigned to  each of the 16 rays, 
and a table Imkup is used to obtain values for the other parameters (reflection 

- 

a 

e " *  



, -  Focal 

Ray 
Direction 

Figure 14-74 
Distributing subpixel rays over a 
camera lens of focal length/. 

incoming maximum spread about R is divided into 16 angular zones, and each ray is re- + fleeted in a jittered position from the zone center corresponding to its integer code. We can use the Phong model, cosn%$, to determine the maximum reflection spread. If the material is transparent, refracted rays are distributed about the 
transmission direction T in a similar manner. 

Extended light sources are handled by distributing a number of shadow 
11'  rays over the area of the light source, as demonstrated in Fig. 14-76. The light 

source is divided into zones, and shadow rays are assigned jitter directions to the 
various zones. Additionally, zones can be weighted according to the intensity of 

Figure 14-75 the light source within that zone and the size of the projected zone area onto the 
Dishibutingsub~ivelraYs object surface. More sFdow rays are then sent to zones with higher weights. If 
about themfledion direction some shadow rays are blocked by opaque o b w s  between the surface and the 
R and the transmission light source, a penumbra is generated at that surface point. Figure 14-77 illus- diredion T. 

trates the regions for the umbra and penumbra on a surface partially shielded 
from a light source. 

We create motion blur by distributing rays over time. A total frame time 
and the frame-time subdivisions are'determined according to the motion dynam- 
ics required for the scene. Time intervals are labeled with integer codes, and each 
ray is assigned to a jittered time within the interval corresponding to the ray 
code. 0 b G t s  are then moved to their positions at that time, and the ray is traced 

Figure 14- 76 
Distributing shadow rays over a 
finitesized light source. 

Sun Earth 

-. 
Penumbra 

Figure 14-77 
Umbra and penumbra regions created by a solar eclipse on the surface 
of the earth. 



f 
Scc(ion 11-6 

Ray-Tracing M o d s  

Figurr 24-78 
A scene, entitled 1984, mdered withdisbibuted ray bating, 
illustrating motion-blur and penumbra e m .  (Courtesy of Pimr. Q 1984 
Pirnr. All rights d.) 

through the scene. Additional rays are u s 4  for highly blurred objects. To reduce 
calculations, we can use bounding boxes or spheres for initial ray-intersection 
tests. That is, we move the bounding object according to the motion requirements 
and test for intersection. If the ray does not intersect the bounding obpct. we do 
not need to process the individual surfaces within the bowding volume. F i p  
14-78 shows a scene displayed with motion blur. This image was rendered using 
distributed ray hacing with 40% by 3550 pixels and 16 rays per pixel. In addition 
to the motion-blurred reflections, the shadows are displayed with penumbra 
areas resulting from the extended light sources around the room that are illumi- 
nating the pool table. 

Additional examples of objects rendered with distributed ray-tracing meth- 
ods are given in Figs. 14-79 and 14-80. Figure 14-81 illushates focusing, drat- 
tion, and antialiasing effects with distributed ray tracing. 

Fiprrc 14-79 
A brushed aluminum wheel 
showing reflectance and shadow 
effects generated with dishibuted 
ray-tracing techniques. (Courtesy of 
Stephen H. Wcsfin, Pmgram of Compvtn 
Graphics, Carnell Uniwsity ) 



Figurn 14-80 
A mom scene d a P d  with 
distributed ray-tracing methods. 
~ ~ r t c s y  of jdrn Snyder, jd Lm& 
Dmoldm Kalrn, and U Bwr, Computer 
Gmphks Lab, C11if.Mlr Imtihrte of 
Tachndogy. Cqyright O 1988 Gltcrh.) 

Figurn 14-81 
A scene showing the fodig ,  
antialias'i and illumination 
effects possible with a combination 
of ray-tracing and radiosity 
methods. Realistic physical models 
of light illumination were used to 
generate the refraction effects, 
including the caustic in the shadow 
of the glass. (Gurtrsy of Pctn Shirley, 
Department of Cmnputer Science, lndicrna 
U n h i t y . )  

14-7 
RADlOSlTY LIGHTING MODEL 

We can accurately model diffuse reflections from a surface by considering the ra- 
diant energy transfers between surfaces, subject to conservation of energy laws. 
This method for describing diffuse reflections is generally refermi to as the ra- 
diosity model. 

Basic Radiosity Model 

In this method, we need to consider the radiant-energy interactions between all 
surfaces in a scene. We do this by determining the differential amount of radiant 
energy dB leaving each surface point in the scene and summing the energy con- 
hibutions over all surfaces to obtain the amount of energy transfer between sur- 
faces. With mference to Fig. 14-82, dB is the visible radiant energy emanating 
from the surface point in the direction given by angles 8 and 4 within differential 
solid angle do per unit time per unit surface area. Thus, dB has units of joules/(sec- 
ond . metd) ,  or watts/metd. 

Intensity 1, or luminance, of the diffuse radiation in direction (8, 4) is the ra- 
diant energy per unit time per unit projected area per unit solid angle with units 
mtts/(mete$ . steradians): 



/' Direction of 

Figure 14-82 
Visible radiant energy emitted from 
a surface point in direction ( O , + )  
within solid angle dw. 

Figure 14-83 
For a unit surface element, the 
projected area perpendicular t'o the 
direction of energy transfer is equal 
to cos +. 

Radiosity Lighting Model 

Assuming the surface is a n  ideal diffuse reflector, we can set intensity I to a con- 
stant for all viewing directions. Thus, d B / d o  is proportional to the projected sur- 
face area (Fig. 14-83). To obtain the total rate of energy radiation from the surface 
point, we need to sum the radiation for all directions. That is, we want the to- 
tal energy emanating from a hemisphere centered on the surface point, as  in 
Fig. 14-84: 

For a perfect diffuse reflector, I is a constant, so we can express radiant energy B 
as  

Also, the differential element of solid angle d o  can be expressed as (Appendix A) 

Figure 14-84 
Total radiant energy from a surface 
point is the sum of the 
contributions in all directions over a 
hemisphere cented on the surface 
point 



Chapter 14 

Illumination Models and Surface- 
Rendering Methods 

Figure 14-85 
An enclosure of surfaces for the radiosity model. 

so that 

A model for the light reflections from the various surfaces is formed by set- 
ting up  an "enclosure" of surfaces (Fig. 14-85). Each surface in the enclosure is ei- 
ther a reflector, an emitter (light source), or a combination reflector-emitter. We 
designate radiosity parameter Bk as the total rate of energy leaving surface k per 
un i txea .  Incident-energy parameter Hk is the sum of the energy contributions 
from all surfaces in the enclosure arriving at  surface k per unit time per unit area. 
That is, 

where parameter Flk is the form factor for surfaces j and k. Form factor Flk is the 
fractional amount of radiant energy from surface j that reaches surface k. 

For a scene with n surfaces in the enclosure, the radiant energy from surface 
k is described with the radiosity equation: 

If surface k is not a light source, E,: = 0. Otherwise, E,  is the rate of energy em~tted 
from surface k per unit area (watts/meter?. Parameter p, is the reflectivity factor 
for surface k (percent of incident light that is reflected in all directions). This re- 
flectivity factor is related to the diffuse reflection coefficient used in empirical il- 
lumination models. Plane and convex surfaces cannot "see" themselves, so that 
no self-incidence takes place and the form factor F ,  for these surfaces is 0. 



To obtain the illumination effects over the various surfaces in the enclosure, section 14-7 
we need to solve the simultaneous radiosity equations for the n surfaces given Radiosity Lightmg Model 
the array values for Ek, pl, and Fjk That is, we must solve 

We then convert to intensity values I! by dividing the radiosity values Bk by T. 
For color scenes, we can calculate the mdwidual RGB components of the rad~os- 
ity (Bw,  B,, BkB) from the color components of pl and E,. 

Before we can solve Eq. 14-74, we need to determine the values for form 
factors Fjk We do this by considering the energy transfer from surface j to surface 
k (Fig. 1486). The rate of radiant energy falling on a small surface element dAk 
from area element dA,  is 

dB, dA, = (1, cos 4j do)dA,  (14-76) 

But solid angle d o  can be written in terms of the projection of area element dAk 
perpendicular to the direction dB,: 

Figun 14-86 
Rate of energy transfer dB, from a surface element with area dAj to 
surface element dA,. 



Chapter 14 so we can express Eq. 14-76 as 
lllurninat~on Models and Surface- 

Rendering Methods 

The form factor between the two surfaces is the percent of energy emanating 
from area dA, that is incident on dAk: 

energy incident on dAk 
F'IA,.IA~ = total energy leaving dA, 

- - I, cos 4j cos 4 dA, dAk . - 1 
rZ B, dA, 

Also B, = rrl,, so that 

The fraction of emitted energy from area dA, incident on the entire surface k is 
then 

where Ak is the area of surface k.  We now can define the form factor between the 
two surfaces as the area average of the previous expression: 

C O S ~ ,  COS 4 
dAk dA, 

(14-82) 

Integrals 14-82 are evaluated using numerical integration techniques and stipu- 
lating the following conditions: 

1;- ,F,, = 1, for all k (conservation of energy) 
Af,, = AAFk, (uniform light reflection) 
F,! = 0, for all i (assuming only plane or convex surface patches) 

Each surface in the scene can be subdivided into many small polygons, and 
the smaller the polygon areas, the more realistic the display appears. We can 
speed up  the calculation of the form factors by using a hemicube to approximate 
the hemisphere. This replaces the spherical surface with a set of linear (plane) 
surfaces. Once the form factors are evaluated, we can solve the simultaneous lin- 



ear q u a  tions 14-74 using, say, Gaussian elimination or LU decomposition rneth- %tion 14-7 
ods (Append~x A).  Alternatively, we can start with approximate values for the B, Radiosity Lighting Model 
and solve the set of linear equat~ons iteratively using the Gauss-Seidel method. 
At each iteration, we calculate an estimate of the radiosity for surface patch k 
using the previously obtained radiosity values in the radiosity equation: 

We can then display the scene at each step, and an improved surface rendering is 
viewed at each iteration until there is little change in the c a l ~ l a t e d  radiosity val- 
ues. 

Progressive Refinement Radiosity Method 

Although the radiosity method produces highly realistic surface rendings, there 
are tremendous storage requirements, and considerable processing time is 
needed to calculate the form [actors. Using progressive refinement, we can reshuc- 
ture the iterative radiosity algorithm to speed up the calculations and reduce 
storage requirements at each iteration. 

From the radiosity equation, the radiosity contribution between two surface 
patches is calculated as 

B, due to B, = (14-83) 

Reciprocally, 

B, due to Bk = p,B,F,,, for all j (14-64)  

which we can rewrite as 

A 
B, due to B,  = pjBkFJk ;i:, tor all j (14-851 

This relationship is the basis for the progressive rrfinement approach to the ra- 
diosity calculations. Using a single surface patch k, we can calculate all form fac- 
tors F,, and "shoot" light from that patch to all other surfaces in the environment 
Thus, we need only to compute and store one hemicube and the associated form 
factors at a time. We then discard these values and choose another patch for the 
next iteration. At each step, we display the approximation to the rendering of the 
scene. 

Initially, we set Bk = El: for all surface patches. We then select the patch with 
the highest radiosity value, which will be the brightest light emitter, and calcu- 
late the next approximation to the radiosity for all other patches. This process is 
repeated at each step, so that light sources are chosen first in order of highest ra- 
diant energy, and then other patches are selected based on the amount of light re- 
ceived from the light sources. The steps in a simple progressive refinement ap- 
proach are given In the following algorithm. 



-- - - - .  
Chapter 1 4 

llluminarion Models and Surface 
Rendering Methods 

- 
Figure 14-87 
Nave of Chams Cathedral 
rendered with a progressive- 
refinement radiosity model by John 
Wallace and John Lin, using the 
Hewlett-Packard Starbase Radiosity 
and Ray Tracing software. Radiosity 
form factors were computed with 
. ray-tracing methods. (Courtesy of Eric 
Haines, 3D/EYE Inc. O 1989. Hewklt- 
Packrrrd Co.) 

For each patch k 
/'set up hemicube, calculate form factors F,, '/ 

for each patch j I 
Arad := p,B,FJkA,/At; 
AB, := AB, + Arad; 
B, := B, + Arad: 

1 

At each step. the surface patch with the highest value for A B d k  is selected as the 
shooting patch, since radiosity is a measure of radiant energy per unit area. And 
we  choose the initial values as ABk = Bk = Ek for all surface patches. This progres- 
sive refinement algorithm approximates the actual propagation of light through a 
scene. 

Displaying the rendered surfaces at each step produces a sequence of views 
that proceeds from a dark scene to a fully illuminated one. After the first step, the 
only surfaces illuminated are the light sources and those nonemitting patches 
that are visible to the chosen emitter. To produce more useful initial views of the 
scene, we can set an ambient light level so that all patches have some illumina- 
tion. At each stage of the iteration, we then reduce the ambient light according to 
the amount of radiant energy shot into the scene. 

Figure 14-87 shows a scene rendered with the progressiverefinement ra- 
diosity model. Radiosity renderings of scenes with various lighting conditions 
are illustrated in Figs. 14-88 to 14-90. Ray-tracing methods are often combined 
with the radiosity model to produce highiy realistic diffuse and specular surface 
shadings, as in  Fig. 14-81. 



Rad~osily Lighting Model 

Figure 14-88 
lmage of a constructivist museum 
rendered with a progressive- 
refinement radiosity method. 
(Courtesy of Shmchmg Eric Chm,  Sfuart I .  
Feldman, and Inlic Dorrty, Program of 
Computer Grapltics. Corndl U n i m i t y .  
O 1988, Corndl U n t m i f y ,  Program of 
G m p u f c r  G r a p h i d  

Figure 14-89 
Simulation of the stair tower of 
the Engineering Theory Center 
Building at Cornell University 
rendend with a progressive- 
refinement radiosity method. 
(Courtesy of Keith Howie and Ben 
h r m b a ,  Pmgrnm o f h p u t e r  Gnphics. 
Cmnrll Uniarsity. 0 1990, Cornell 
Unicmsity, Program of Computer 
Graphin.) 

Figrrrr 14-90 
Simulation of two lighting schemes for the Parisian garret from the Metropolitan Opera's 
production of La Boheme: (a) day view and (b) night view. (Courtesy of Jltlie Dorsq nnd Mnrk 
Sltqard, Program of  Compufrr Gmphics, Conrdl Ll~rirrrsity. 0 1991, Cornell Uniiursiry, Progrntn of 
Comptrlrr Graphics.) 



Sphertcel 

- . - - . - - . - - 
F?qrirc 14-91 
A spherical enclosing universe 
contaming the environment map 

An alternate proicdun, tilr nwdel~ng global reflections IS to define an  array of in- 
tenyity valurs t h ~ t  dew r.~h~as the environment around a single object or a set of 
object3. lnstsad of mtcw hlcct rail tracing or radiosip calculations to pick u p  thc 
global specular and J ~ t t . ~ s t .  ~llumination effects, w e  simply map the envrronrntwt 
urrny unto an o b l ~ t  117 ~~ ln t~on . ; l i i p  to the bwwing direction. This procedure is re- 
ferred to as environment mapping, also called reflection mapping although 
transpnrvncy ~t'fcct, ( x u l  d 'also bc  nodel led with the en\.lronment map. Environ- 
ment mapping IS somtJ t~mes reterred tci as the "pocr person's ray-tracing" 
nwthod, slnce ~t is a t ~ - t  approx~md tion of the more accurate global-illumination 
r t d e r i n g  t e c h ~ r ~ c l u r ~  \ \ c  dixussed in the previous two scxtions. 

The environmenr map is defined over the surfacc cif an enclosing univerw. 
Intinmation In the cwT. ~ r ~ ~ n n i e n t  map includes intensity values for light sources, 
the skv, and other hackg-ound objects. Figure 14-91 s h o ~ s  the enclosing universe 
as a sphere, hut a cubc (11- a cylinder is often used as the enclosing universe. 

To rmder tlir surt'lce of an object, we project pixel areas onto the surfacc 
and then reflect tht- p~,.~lt,cted pixel area onto the e n \ h n r n e n t  map to pick u p  the 
surface-shading attributvs for each pixel. I f  the object is mnsparent,  we can also 
refract the projected pixt?l area to the environment map. The environment-map- 
ping process for reflection of a projected pixel area is ]!lustrated in Fig. 14-92. 
Pixel intensity is determined by averaging the intensit!, \~alues within the inter- 
sected region of the en1.i-on~nent map. 

onto Envtronmenr 



14-9 Section 1 4 9  
ADDING SURFACE DETAIL Adding Surface Detail 

So far we have discussed rendering techniques for displaying smooth surfaces, 
typically polygons or splines. However, most objects do not have smooth, even 
surfaces. We need surface texture to model accurately such objects as brick walls, 
gravel roads, and shag carpets. In addition, some surfaces contain patterns that 
must be taken into account in the rendering procedures. The surface of a vase 
could contain a painted design; a water glass might have the family crest en- 
graved into the surface; a tennis court contains markings for the alleys, service 
areas, and base line; and a four-lane highway has dividing lines and other mark- 
ings, such as oil spills and tire skids. Figure 14-93 illustrates objects displayed 
with various surface detail. 

Modeling Surface Detail with Polygons 

A simple method for adding surface detail is to model structure and patterns 
with polygon facets. For large-scale detail, polygon modeling can give good re- 
sults. Some examples of such large-xaIe detail are squares on a checkerboard, di- 
viding lines on a highway, tile patterns on a linoleum floor, floral designs in a 
smooth low-pile rug, panels in a door, and Iettering on the side of a panel truck. 
Also, we could model an irregular surface with small, randomly oriented poly- 
gon facets, provided the facets were not too small. 

- - -- - - . . --- 
F i p w  14-93 
Scenes illustrating corncter graphics generation of surface detail. 
((a)  0 1992 Deborah R.  Fow , Przemyslav Pmsinkitwicz, and \ohanrlrs Battjes; 
(b )  0 1992 Deboruh R. Fowler, Hins Meinherdt, and PrznnysImu Pnrsinkinu~cz, 
Uni tmi fy  of Culgury; (cJ and (d )  Courtesy of SORIMACE, Inc.) 



Chdpter 14 

Illurn~nal~on Models and Surtacr- 
Rendering Methods Space: 

(s, tl Array iu, v )  S d a c e  ix, y )  Pixel I Parameters 1 1 Coordinates I -- - 
Texture-Scrface Viewing and 
Transformation Project~on 

Transformation 

- . - - - . - - -- - . - - - . . - 
Fiprc  14-94 
Coordinate reference sy-items for texture space, object space, and image 
space. 

Surface-pattern polygons are generally overlaid on ,a larger surface polygon 
and are processed with the parent surface. Only the parent polygon is processed 
by the visible-surface algorithms, but the illumination parameters for the surface- 
detail polygons take precedence over the parent polygon. When intricate or fine 
surface detail is to be modeled, polygon methods are not practical. For example, 
it would be difficult to .accurately model the surface structure of a raisin with 
polygon facets. 

Texrure Mdpl~ing 

A common method for adding surface detail is to map tcxture patterns onto the 
surfaces of objects. The texture pattern may either be defined in a rectangular 
array or as a procedure that modifies surface intensity values. This approach is 
referred to as texture mapping or pattern mapping. 

Usually, the texture pattern is defined with a rectangular grid of intensity 
values in a texture space referenced with (s, I )  coordinate values, as shown in Fig. 
14-94. Surface positions In the scene are referenced with u v  object-space coordi- 
nates, and pixel positions on the proyction plane are referenced in x y  Cartesian 
coordinates.Texture mapping can be accomplished in one of two ways. Either we 
can map the texture pattern to object surfaces, then to the projection plane; or we 
can map pixel areas onto object surfaces, then to texture space. Mapping a texture 
pattern to pixel roordmates is sometimes called fcrture scunnitig, while the map- 
ping from plxel coordinates to texture space is referred tn as pixel-order sm~irling 
or imerse  scannrrig or irnrigc-c~rder scanlrrrlg. 

To simplify calculations, the mapping from texture space to object space is 
often specified with parametric linear functions 

The object-to-image space mapping is accomplished with the concatenation of 
the viewing and projection transformations. A disadvantage of mapping from 
texturc space to pixel space is ~ h a l  a selected texturr patch usually does not 
match up with the pixel boundar~es, thus requiring calculation of the fract~onal 
area of pixel coverage Therefore, mapping from pixel space to texture space (Fig. 
14-95) is the most commmly used texture-mapping method. This avoids pixel- 
subdivision  calculation^. and allows antialiasing (filtering) procedures to be eas- 



Projected 
Pixel Area 

Surface Area 

Figure 14-95 
Texture mapping by projecting pixel areas to texture space. 

Ektended 
, Pixel Area 

Figure 14-96 
!%tended area for a pixel that 
includes centers of adjacent pixels. 

ily applied. An effective antialiasing procedure is to project a slightly larger pixel 
area that includes the centers of neighboring pixels, as shown in Fig. 14-96, and 
applying a pyramid function to weight the intensity values in the texture pattern. 
But the mapping from image space to texture space does require calculation of 
the inverse viewing-projection transformation M;b and the inverse texture-map 
transformation M i '  In the following example, we illustrate this approach by 
mapping a defined pattern onto a cylindrical surface. 

Example 14-1 Texture Mapping 

To illustrate the steps in texture mapping, we consider the transfer of the pattern 
shown in Fig. 14-97 to a cylindrical surface. The surface parameter? are 

with 



I d '  Ibl 

. - -. . -. - . . - - - - - - - -- - - - -- - - - - 
F ~ X I I ~ E  14-97 
Mapping a texturt pattern def~ned or1  a unit square (a) to a cylindrical 
surface (b). 

And the parametric rqlresentation for the surface in the Cartesian reference 
frame is 

We can map the array pattern to the surface with the following linear transforma- 
tion, which maps the pattern origin to the lower left corner of the surface. 

Next, we select a ~'iewing position and pertorm the inverse viewing transforma- 
Lion from pixel coordina:es to the Cartesian reference ior the cylindrical surface. 
Cartesian coordinates 3n3 then mapped to the surface parameters with the trans- 
formation 

and projected pixel poslt~ons are mapped to texture spact* with the inverse trans- 
formation 

Intensity values in thepi~ttern array covered by each proj(acted pixel area are then 
averaged to obtain the p w l  intensity. 

Another method for adding surface texture is to use proct,dural definitions of the 
color variations that art! to be applied to the object5 in a scene. This approach 
avoids the transformation calculations invol\red in transferring two-dimensional 
texture patterns to object surfaces. 

When values art. awigned throughour a region of three-d~mensional space, 
the obiert color varlaturc are referred to as solid textures. Values from fcrturr 



Adding Surface Detail 

! Figure 14-98 
A scene with surface characteristics 
generated using solid-texture 
methods. (Coudrsy of Peter Shirley, 
Cornpurer Scimu Dcptirnrnl, Indiana 
Universify.) 

spce  are transferred to object surfaces using procedural methods, since it is usu- 
ally impossible to store texture values for all points throughout a region of space. 
Other procedural methods can be used to set up texture values over two-dirnen- 
sional surfaces. Solid texturing allows cross-sectional views of three-dimensional 
objects, such as bricks, to be rendered with the same texturing as the outside sur- 
faces. 

As examples of procedural texhuing, wood grains or marble patterns can 
be mated using harmonic functions (sine curves) defined in three-dimensional 
space. Random variations in the wood or marble texturing can be attained by su- 
perimposing a noise function on the harmonic variations. Figure 14-98 shows a 
scene displayed using solid textures to obtain wood-grain and other surface pat- 
terns. The scene in Fig. 14-99 was m n d e d  using pmcedural descriptions of ma- 
terials such as stone masonry, polished gold, and banana leaves. 

Figur~ 24-99 
A scene tendered with VG Shaders 
and modeled with RenderMan 
using polygonal facets for the gem 

I faces, quadric surfaces, and bicubic 
patches. In addition to surface 

' texhuing, procedural methods were 
, u s 4  to create the steamy jungle 

ahnosphem and the forest canopy 
I dap led lighting effect. (court& if 
1 t k  G m p .  Rqrintnl from Graphics 

Crms MI, editei by Dpvid Kirk. Cqyighl 
Q 1992. Academic Rcss, lnc.) 



Chapter 14 Bump Mapping 
lllurninat~on Models and Surface- 

~ ~ ~ d ~ ~ i ~ ~  ~ ~ ~ h ~ d ~  Although texture mapping can be used to add fine surface detail, it is not a good 
method for modeling the surface roughness that appears on objects such as or- 
anges, strawbemes, and raisins. The illumination detail in the texture pattern 
usually d w s  not correspond to the illumination direction in the scene. A better 
method for creating surface bumpiness is to apply a perturbation function to the 
surface normal and then use the perturbed normal in the illumination-model cal- 
culations. This techniques is call& bump mapping. 

If P(u, V )  represents a position on a parametric surface, we can obtain the 
surface normal at that point with the calculation 

N = PU X P,. (14-87) 

where P, and PI, are the partial derivatives of P with respect to parameters u and 
v.  To obtain a perturbed normal, we modify the surface-position vector by 
adding a small perturbation function, called a burnpfunction: 

This adds bumps to the surface in the direction of the 1.lnit surface normal n = 
N/ ( N I .  The perturbed surface normal is then obtained as 

We calculate the partial derivative with respect to 11 ol the perturbed position 
vector as 

a 
P: = -(P + bn) all 

= P,, + b,,n t hn,, 

Assuming the bump function b is small, we can neglect the last term and write: 

Similarly, 

And the perturbed surface normal is 

N' = P, x P,, + b p , ,  x n) + b,,(n x PJ + b,h,,(n x n)  

But n X n = 0. so that 

The final step is to nom~,ilize N' for use in the illumination-model calculations 



Section 14-9 

Adding Surface Detail 

Figure 1 4 - 1 0  
Surface roughness characteristics rendered with bump mapping. 
(Courtesy of (a) Peter Shirk ,  Computer Science DPpPrtmmr, Indiana Unrucrsifyand 
(b) SOJTlMAGE, Inc.) 

Figure 14-101 
. The stained-glass knight from the 
motion picture Young Sherlork 
Holmes. A combination of bump 
mapping, environment mapping, 
and texture mapping was used to 
render the armor surface. (Courtesy of 
lnduslrul Light &Magic. CoWrighr 0 
1985 Paramount PicturpslAmblin.) 

There are several ways in which we can specify the bump function b(u, v) .  
We can actually define an analytic expression, but bump values are usually ob- 
tained with table lookups. With a bump table, values for b can be obtained 
quickly with linear interpolation and incremental calculations. Partial derivatives 
b, and b, are approximated with finite differences. The bump table can be set up  
with random patterns, regular grid patterns, or character shapes. Random pat- 
terns are useful for modeling irregular surfaces, such as a raisin, while a repeat- 
ing pattern could be used to model the surface of an orange, for example. To an- 
tialiase, we subdivide pixel areas and average the computed subpixel intensities. 

Figure 14-100 shows examples of surfaces rendered with bump mapping. 
An example of combined surface-rendering methods is given in Fig. 14-101. The 
armor for the stained-glass knight in the film Your~g Sherlock Holmes was rendered 
with a combination of bump mapping, environment mapping, and texture m a p  
ping. An environment map of the surroundings was combined dith a bump map 
to produce background illumination reflections and surface roughness. Then ad- 
ditional color and surface illumination, bumps, spots of dirt, and stains for the 
seams and rivets were added to produce the overall effect shown in Fig. 14-101. 

Frame Mapping 

This technique is an extension of bump mapping. In frame mapping, we perturb 
both the surface normal N and a local coordinate system (Fig. 14-102) attached to 



N The local ccwrdin,ltt~~ arc' defined 1~1th '1 surtnce-tangent vector T and a binor- 
mal veclor B - T k N 

Frame. mdpping 15 used lo model anisotropic surixes. We orient T along 
the "grain" ot the surt.~ct' ~ n d  apply directional perturbations, in addition to 
bump perturbation\ i ~ .  :he direction of N In t h ~ s  wag i\.t, can model wood-grain 
patterns, cross-thread ;-,attuns 111 cloth, and streaks 111 marble or similar materi- 
als. Both bump anti d~r r  it~c,nal perturbations can be obtained with table lookups. 

In general, an object I,  ;llumin,~ted with radiant energy from light-emitting 
sources and f ro~n  the ~rdlective surfaces of other objects in the scene. Light 
sources can be nlodel~ad a s  point sources or as distributcd (extended) sources. Ob- 
jects can be either crpaqut' or transparent. And lighting eflects can be described in 
terms of diffuse and specular components for both reflections and refractions. 

An empiric'll, p o ~ ~ i t  light-source, illumination model can be used to de- 
icribe diffuse retlec.tion\ w ~ t h  Lmlbert's cosine law anJ to describe specular re- 
flections with thc I ' h o n ~  model General background ('lrnbirnt) lighting can be 
modeled with a tixed 111ttwiity level and a coefficient ot reflection for each sur- 
face. In this basic model, we can approximate transparency effects by combining 
surface intensities using , I  transparency coefticient. Accurate geometric modeling 
of light paths through transparent materials is obtained by calculating refraction 
angles using Snell's Id\\ Cdor  1s ~ncorporated into the model by assigning a 
triple of RGB values tu ~ntensities and suriace reflection coefficients. We can also 
extend the bas~c nlodel to ~ncorporate distributed light sources, studio lighting 
effects, and intensity attc>nu,ltion. 

Intensity values calculated .&ith an illumination model must be mapped to 
the intensity levels ava~lablc on the display system in we. A logarithmic intensity 
scale is used to provide .l set of intensity levels with equ.11 perceived brightness. 
In addition, gamma correction is applied to intens;!? \..llues to correct for the 
nonlinearity of diaplay dev~ces. With bilevel monitors, h e  can use halftone pat- 
terns and dithering techriiques to s~mulate a range of intensity values. Halftone 
approximations can also he used to increase the number cf intensity options on 
systems thar are c-apable of displaying more than two ~rtensi t~es per pixel. Or- 
dered-dither, error-ciiffuwn, and dot-diffusion methods nre used to simulate a 
range of intensities when the number of points to be plotttd in a scene is equal to 
the number of pixt4s on !IIL, display device. 

Surface rendering <an be accomplished by applymg a basic illumination 
~ncldel to theobjects in a scene. We apply an illuminnt:[~i~ model using either con- 



stant-intensity shading, Gouraud shading, or Phong shading. Constant shading 
is accurate for polyhedrons or for curved-surface polygon meshes when the References 
viewing and light-source positions are far from the objects in a scene. Gouraud 
shading approximates light reflections from curved surfaces by calculating inten- 
sity values at  polygon vertices and interpolating these intensity values across the 
polygon facets. A more accurate, but slower, surface-rendering procedure is 
Phong shading, which interpolates the average normal vectors for polygon ver- 
tices over the polygon facets. Then, surface intensities are calculated using the in- 
terpolated normal vectors. Fast Phong shading can be used to speed up the calm- 
lations using Taylor series approximations. 

Ray tracing provides an accurate wethod for obtaining global, specular w- 
flection and transmission effects. Pixel rays are traced through a scene, bouncing 
from object to o b p t  while accumulating intensity contributions. A ray-tracing 
tree is constructed for each pixel, and intensity values are combined from the ter- 
minal nodes of the tree back up  to the root. object-intersection calculations in ray 
tracing can be reduced with space-subdivision methods that test for ray-object in- 
tersections only within subregions of the total space. Distributed (or distribution) 
ray tracing traces multiple rays per pixel and distributes the rays randomly over 
the various ray parameters, such as direction and time. This provides an accurate 
method for modeling surfam gloss and translucency, finite camera apertures, dis- 
tributed light sources, shadow effects, and motion blur. 

Radiosity methods provide accurate modeling of diffuse-reflection effects 
by calculating radiant energy transfer between the various surface patches in a 
scene. Progressive refinement is used to speed up the radiosity calculations by 
considering energy transfer from one surface patch at a time. Highly photorealls- 
tic scenes are generated using a combination of ray tracing and radiosity. 

A fast method for approximating global illumination effects is environment 
mapping. An environment array is used to store background intensity informa- 
tion for a scene. This array is then mapped to the objects in a scene based on the 
specified viewing direction. 

Surface detail can be added to objects using polygon facets, texture map- 
ping, bump mapping, or frame mapping. Small polygon facets can be overlaid 
on laf-ger surfaces to provide various kinds of designs. Alternatively, texture pat- 
terns can be defined in a two-dimensional array and mapped to object surfaces. 
Bump mapping is a means for modeling surface irregularities by applying a 
bump function to perturb surface normals. Frame mapping is an extension of 
bump mapping that allows for horizontal surface variations, as well as vertical 
variations. 

REFERENCES 

A general discussion of energy propagation, transfer equations, rendering processes, and our 
perception of light and color is given in  Glassner (1994). Algorithms for various surface- 
rendermg techniques are presented in Classner (1990). ANO (1991), and Kirk (1992). For 
further discussion of ordered dither, error diffusion, and dot diffusion see Knuth (1987). 
Additional information on ray-tracing methods can be iound in Q u e k  and Hearn (1988). 
Classner 11989). Shirley (1990). and Koh and Hearn (1992). Radiosity methods are dis- 
cussed in Coral et al. (1984), Cohen and Creenberg (19851, Cohen et al. (1988), Wallace, 
Elmquist, and Haines (1989). Chen et al. (1991). Dorsey, Sillion, and Creenberg (1991). 
He et al. (1  992). S~llion et al. ( 1  991 ), Schoeneman et al (1  993). and Lischinski, Tampieri, 
and Greenberg (19931. 



Chapter 14 ---- 
lllurnirialiori Model, and Surface- 
- EXERCISES 

Rendering Melhods 
14.1 Write a routine to implement Eq. 14-4 of the basic illurn~nation model using a single 

point light source and constant surface shading for the faces of a specified polyhe- 
dron. The object description is to be given as a set of pohgon tables, including sur- 
face normals for each of the polygon faces. Additiondl Input parameters include the 
ambient intensity, Iigtit-source intensity, and the surlate reflection coefficients. All 
coordinate information can be spec~fied drrectly in the viewing reierence frame 

14-2. Modify [he routlne !n Exercise 14-1 to render a polygon wrfare mesh ubing Gouraud 
shading 

14-3. Modify the routine in Exercise 14-1 to render a polyqon iurtace mesh using I'hong 
shading 

14.4. Write a routine to ~mplenient Eq. 14-9 of the basrr illunilii,~t~on model using a single 
point light source and Louraud suriace shading for the ta.es of a specified polygon 
mesh. The object debc ription is to be given as a set oi pol.,gon tables, including sur- 
face normals for each ol the polygon faces. Additional input includes values ior the 
ambient intensity, liglil-source ~ntensity, suriace reilection toeffic~ents, and the spec- 
ular-reflection parameter All coordinate inrormation can t ~ e  specified directlv in the 
viewing reference frdnie. 

14-5. Modify the routine In Ikxercise i4-4 to render the polygon :urfacrr, using Phong shad- 
ing. 

14-6. Modify the routrne rn I.xercise 14-4 to Include a linear inten\ity attenuation function. 

14-7. Modify the routine In I.xerc ise :4-4 to renaer the polygon ?urfacer using Phong shdd~ 
ing and a linear inten51t) attenuation function 

14-8. Modify the routine in ixercise 14-4 to mplement E q  13 1 i w t h  any specified num- 
ber of polyhedrons ~ 1 x 1  light sources in the scene. 

14-9. Modify the routine in .:xerc-~sc 14-4 to implement E q  I d  1 - 1  iwth any sperificd nuni- 
her of polyhedrons a11d light sources in the scene. 

14-10, Modify the routlnc 111 Iwrc ise 14-4 lo implcmcnt E q  I I- I i with any 5pc~ific.d nu111 
ber of polyhedrons ~ n c l  light scurcts in I ~ P  scene. 

14-11. Modify :he iuutinc I , ]  t~e lc isc  14-4 to l ~ r ~ p l e i l ~ ~ i i t  Lq< 1 4 - 1 5  d~ ld  14. I 9  w t l i  <?II\. 
specified number oi i14ht wurcrs m d  polvhedrom (e t k r  ol)aquz or kran\p,lrc,ntm 111 
the scene. 

14-12. Discuss the diiferen~v~. \o i l  m~ght cxpec I IO w c  in thc~al~pt~,ir.~~icc. (11 specula. rc4rc 
tions modeled w ~ t h  ih' . HI". (.ompared to .pc'tular rrilt'ctli-iv rnodcled with (V . R:". 

14-13. Verify that 2a = 6 in FIG I ?  18 \vhen all \?<tors ale ~~)p Id i i '~ r ,  ~ U I  that in genw.ll, ? < Y  
+ @- 

14-16 Set up an algori~hni t i iwd  on one o i  the vishlr-wrbcc ( i t  tee Iiori rnc.rhcd5,  ha^ \\,I1 
identify shadow arcw r i  ,I xen? illum~nattd hy a di5tmt ~~o111t wcrc e 

14-17 How many inrensit) lewrl. cdn be d~spldyed w ~ t h  halftow ,~pproxirnd~or,s uslnp ;i Iiv 
npixel grids where e.ic h pixel can be disp1ayc.d w i ~ h  111 tlitr:*rcwt i i i t c v w l i c ~ s !  



14-21. Write a procedure to display a given array of intensity values using the ordered- 
dither method. Exercises 

14-22. Write a procedure to implement the error-diffusion algorithm for a given m by n 
array of intensity values. 

14-23. Write a program to implement the basic ray-tracing algorithm for a scene containing 
a single sphere hovering over a checkerboard ground square. The scene is to be illu- 
minated with a s~ngle point light source at the viewing position. 

14-24. Write a program to implement the basic ray-tracing algorithm for a scene containing 
any specified arrangement of spheres and polygon surfaces illuminated by a given 
set of point light sources. 

14-25. Write a program to implement the basic ray-tracing algorithm using space-subdivl- 
sion methods for any specified arrangement of spheres and polygon surfaces illumi- 
nated by a given set of point light sources. 

14-26. Write a program to implement the following features of distributed ray tracing: pixel 
sampling with 16 jittered rays per pixel, distributed reflection directions, distributed 
refraction directions, and extended light sources. 

14-27. Set up an algorithm for modeling the motion blur of a moving object using distrib- 
uted ray tracing. 

14-28. Implement the basic radiosity algorithm for rendering the inside surfaces of a cube 
when one inside face of the cube is a light source. 

14-29. Devise an algorithm for implementing the progressive refinement radiosity method. 
14-30. Write a routine to transfotm an environment map to the surface of a sphere. 

14-31. Write a program to implement texture mapping tor (a) spherical surfaces and (b) 
polyhedrons. 

14-32. Given a spherical surface, write a bump-mapping procedure to generate the bumpy 
surface of an orange. 

14-33. Write a bump-mapping routine to produce surface-normal variations for anv speci- 
fied bump function. 



C H A P T E R  - Color Models and Color 
I 7 Applications 

4 Lwr 



0 ur discussions of color up to this point have concentrated on the mecha- nisms for generating color displays with combinations of red, green, and 
blue light. This model is helpful in understanding how color is represented on a 
video monitor, but several other color models are useful as well in graphics ap- 
plications. Some models are used to describe color output on printers and plot- 
ters, and other models provide a more intuitive color-parameter interface for the 
user. 

A color model is a method for explaining the properties or behavior of 
color within some particular context. No single color model can explain all as- 
pects of color, so we make use of different models to k l p  describe the different 
perceived characteristics of color. 

15-1 
PROPERTIES OF LIGHT 

What we perceive as 'light", or different colors, is a narrow frequency band 
within the electromagnetic spectrum. A few of the other frequency bands within 
this spectrum are called radio waves, microwaves, infrared waves, and X-rays. 
 ips 15-1 shows the approximate frequency ranges for some of the electromag- 
netic bands. 

Each frequency value within the visible band corresponds to a distinct 
color. Atthe low-f~equency end is a red color (4.3 X 10" hertz), and the highest 
frequency we can see is a violet color (7.5 X 10" hertz). Spectral colon range 
from the reds through orange and yellow at the low-frequency end to greens, 
blues, and violet at the high end. 

I I 1 I I I I I 1 Frequency 
I r I I 

1 0 106 10" 10'0 1012 101' 1 0  10" 10" (hertz) 

~ . - -. . - 
Figrrc 15-7 
Electromagnetic spectrum. 



Chapter IS 

Color Models and Color 
Applications 

F~gure 15-2 
T i e  variations for oneelectric frequency component of a plane- 
polarized electromagnetic wave. 

Since light is an electromagnetic wave, we can describe the various colors in 
terms of either the frequency f o r  the wavelength A of the wave In Rg. 15-2, we 
illustrate the oscillations present in a monochromatic electromagnetic wave, po- 
larized so  that the electric oscillations are in one plane. The wavelength and fre- 
quency of the monochromatic wave are inversely proport~onal to each other, with 
the proportionality constant as the speed of light c: 

Frequency is constant for aH materials, but the speed of light and the wavelength 
are material-dependent. In a vacuum, c = 3 x 101â€¢â€¹ cm/sec. Light wavelengths are 
very small, so length units for designating spectral colors are usually either 
angstroms (1A = lo -@ cm) or nanometers (1 nm = lo-' cm) An equivalent term 
for nanometer is millimicron. Light at the red end of thv specln~m has a wave- 
length of approximately 700 nanometers (nm), and the wavelength of the violet 
light at the other end of the spectrum is about 400 nm. Since wavelength units are 
somewhat more convenient to deal with than frequencv units, spectral colors are 
typically specified in terms of wavelength. 

A light source such as the sun or a light bulb emits all frequencies within 
the visible range to produce white light. When white light is incident upon an ob- 
ject, some frequencies are reflected and some are absorbed by the object. The 
combination of frequencies present in the reflected light determines what we per- 
ceive as the color of the object. If low frequencies are predominant in the reflected 
light, the object is described as red. In this case, we say the perceived light has a 
dominant frequency (or dominant wavelength) at the red end of the spectrum. 
The dominant frequency is also called the hue, or simply the color, of the light. 

Other properties besides frequency are needed to describe the various char- 
acteristics of light. When we view a source of light, our eves respond to the color 
(or dominant frequency) and two other basic sensations. One of these we call the 
brightness, which is the perceived intensity of the light. Intensity is the radiant 
energy emitted per unit time, per unit solid angle, and p:r unit projected area of 
the source. Radiant energy is related to the luminance of the source. The second 



%ion 15-1 

Propenies of Light 

F~gure 15-3 
Energy distribution of a whitelight 
source. 

perceived characteristic is the purity, or saturation, of the light. Purity describes 
how washed out or how "pure" the color of the light appears. Pastels and pale 
colors are described as less pure. These three characteristics, dominant frequency, 
brightness, and purity, are commonly used to describe the different properties we 
perceive in a source of light. The term chromaticity is used to refer collectively to 
the two properties describing color characteristics: purity and dominant fre- 
quency. 

Energy emitted by a white-light source has a distribution over the visible 
frequencies a s  shown in Fig. 15-3. Each frequency component within the range 
from red to violet contributes more or less equally to the total energy, and the 
color of the source is described as white. When a dominant frequency is present, 
the energy distribution for the source takes a form such as that in Fig. 15-4. M1e 
would now describe the light as  having the color corresponding to the dominant 
frequency. The energy density of the dominant light component is labeled as ED 
in this figure, and the contributions from the other frequencies produce white 
light of energy density Ew We can calculate the brightness of the source as the 
area under the curve, which gives the total energv density emitted. Purity de- 
pends on the difference between ED and E w .  The larger the energy ED of the dom- 
inant frequency compared to the white-light component EN, the more pure the 
light. We have a purity of 100 percent when E N  = 0 and a purity of 0 percent 
when E, = ED. 

When we view light that has been formed by i\ combination of two or more 
sources, we see a resultant light with characteristics determined by the original 
sources. Two different-color light sources with suitably chosen intensities can be 
used to produce a range of other colors. If the two color sources combine to pro- 

I Dorninanl Frequency 
p~ -- 

Fi,yiiw 75-4 
Energy distribution of a light source 
w ~ t h  a dominant frequency near the 
red end of the frequency range. 



Chapter IS 

Color Models and Color 

rigure 15-5 
Amounts of RGB primaries needed to display 
spectral colors. 

duce white light, they are referred to as'complementary colors. Examples of 
complementary color pairs are red and cyan, green and magenta, and blue and 
yellow. With a judicious choice of two or more starting colors, we can form a 
wide range of other colors. Typically, color models that are used to describe com- 
binations of light in terms of dominant frequency (hue) use three colors to obtain 
a reasonably wide range of colors, called the color gamut for that model. The two 
or three colors used to produce other colors in such a color model are referred to 
as primary colors. 

No finite set of real primary colors can be comb~ned to produce all possible 
visible colors. Nevertheless, three primaries are sufficient for most purposes, and 
colors not in the color gamut for a specified set of primaries can still be described 
by extended methods. I f  a certain color cannot be produced by combining the 
three primaries, we can mix one or two of the primaries with that color to obtain 
a match with the combination of remaining primaries. In this extended sense, a 
set of primary colors can be considered to describe all colors. Figure 15-5 shows 
the amounts of red, green, and blue needed to produce any spectral color. The 
curves plotted in Fig. 15-5, called color-matching functior~s, were obtained by aver- 
aging the judgments of a large number of observers. Colors in the vicinity of 500 
nm can only be matched by "subtracting" an amount of red light from a combi- 
nation of blue and green I~ghts. This means that a color around 500 nm is de- 
scribed only by combining that color with an amount of red light to produce the 
blue-green combination specified in the diagram. Thus, an RGB color monitor 
cannot display colors in the neighborhood of 500 nm. 

15-2 
STANDARD PRIMARIES AND THE CHROMATICITY 
DIAGRAM 

Since no finite set of color light sources can be combined to display all possible 
colors, three standard primaries were defined in 1931 by the International Com- 
mjssion on Illumination, referred to as the CIE (commission lnternationale de  
I'Eclairage). The three standard primaries are imaginarv colors. They are defined 
mathematically with positive color-matching functions (Fig. 15-61 that specity the 



Section 15-2 

1.5 -- 

m 
.C 0 
C c g 2 1.0-- 
7 5 
O W  - - 
6 0.5 - -  

Figwe 15-6 
0 ,Ao &, ,iO A(nml Amounts of CIE primaries needed 

to display spectral colors. 

amount of each primary needed to describe any spectral color. This provides an 
international standard definition for all colors, and the CIE primaries eliminate 
negative-value color matching and other problems associated with selecting a set 
of real primaries. 

XYZ Color Model 

The set of CIE primaries is generally referred to as the XYZ, or (X, Y, Z), color 
model, where X, Y, and Z represent vectors in a three-dimensional, additive color 
space. Any color C, is then expressed as 

where X, Y, and Z designate the amounts of the standard primaries needed to 
match C,. , .. 

In discussing color properties, it is convenient to normalize the amounts in 
Eq. 15-2 against luminance (X + Y + Z). Normalized amounts are thus calculated 

with x + y + z = 1. Thus, any color can be represented with just the x and y 
amounts. Since we have normalized against luminance, parameters x and y are 
called the chromaticity wlues because they depend only on hue and purity. Also, if 
we specify colors only with x and y values, we cannot obtain the amounts X, Y, 
and Z. Therefore, a complete description of a color is typically given with the 
three values x, y, and Y. The remaining CIE amounts are then calculated as 

where z - 1 - x - y. Using chromaticity coordinates (x, y), we can represent all 
colors on a two-dimensional diagram. 

CIE Chromaticity D~agranl 

When we plot the normalized amounts x and y for colors in the visible spectrum, 
we obtain the tongue-shaped curve shown in Fig. 15-7. This curve is called the 
CIE chromaticity diagram. Points along the curve are the "pure" colors in the 

Standard Primaries and the 
Chromaticity Diagram 



Color Models and Color 
Applications 

(Red) 
- . . - - - - 

Figure 13-7 
CIE chromaticity diagram. Spectral 
color positions along the curve are 
labeled in wavelength units (nm). 

electromagnetic spectruni, labeled according to wavelength in nanometers from 
the red end to the violet end of the spectrum. The line joining the red and violet 
spectral points, called the purple line, is not part of the spectrum. Interior points 
represent all possible visible color combinations. Point C in the diagram corre- 
sponds to the white-light position. Actually, this point is plotted for a white-light 
sburce known as i ~ ~ u i i n H n t  C, which is k e d  as a standard approximation for 
"average" daylight. 

Luminance values are not available in the chromaticity d~agram because of 
normalization. Colors with different luminance but the same chromaticity map to 
the same point. The chromaticity diagram is useful for the following: 

Comparing color gamuts for different sets of primaries. 
Identifying complementary colors. 
Determining dominant wavelength and purity of a given color. 

Color gamuts are represented on the chromaticity diagram as straight line 
segments or as polygons. All colors along the line joining points C, and C2 in Fig. 
15-8 can be obtained by mixing appropriate amounts of the colors C, and CI. If  a 
greater proportion of C, is used, the resultant color is closer to C, than to Cz.-The 
color gamut for three points, such as  C, C,, and C, in Fig. 15-8, is a triangle with 
vertices at the three color positions. Three primaries can only generate colors in- 
side or on the bounding edges of the triangle. Thus, the chromaticity diagram 
helps us understand why no set of three primaries can be additively combined to 
generate all colors, since no triangle within the diagram can encompass all colors. 
Color gamuts for video monitors and hard-copy devices are conveniently coni- 
pared on the chromaticity diagram. 

Since the color gamut for two points is a straight line, complementary col- 
ors must be represented on the chromaticity diagram as two points situated on 
opposite sides of C and connected with a straight line. When we mix proper 
amounts of the two colors C, and Cz in Fig. 15-9, we can obtain white light. 

We can also use the interpretation of color gamut for two primaries to de- 
termine the dominant wavelength of a color. For color point C, in Fig. 1510, we 
can draw a straight line from C through C, to intersect the spectral curve at point 



Figure 15-8 
Color gamuts defined on the 
chromaticity diagram for a 
two-color and a three-color 
system of primaries. 

Figure 15-9 
Representing complementary 
colors on the chromaticity 
diagram. 

Figure 15-10 
Determining dominant 
wavelength and purity with 
the chromaticity diagram. 

C,. Color C, can then be represented as a combination of white light C and the 
spectral color C,. Thus, the dominant wavelength of C, is C,. This method for de- 
termining dominant wavelength will not work for color points that are between 
C and the purple line. Drawing a line from C through point C2 in Fig. 15-10 takes 
us to point Cp on the purple line, which is not in the visible spectrum. Point C2 is 
referred to as a nonspectral color, and its dominant wavelength is taken as the 
compliment of C ,  that lies on the spectral curve (point C,). Nonspectral colors 
are in the purple-magenta range and have spectral distributions with subtractive 
dominant wavelengths. They are generated by subtracting the spectral dominant 
wavelength (such as C,,) from white light. 

For any color point, such as C, in Fig. 15-10, we determine the purity as the 
relative distance of C, from C along the straight line joining C to C,. If d,, denotes 
the distance from C to C, and d, is the distance from C to C,, we can calculate pu- 
rity as the ratio d,, /d, , .  Color C ,  in this figure is about 25 percent pure, since it is 
situated at about one-fourth the total distance from C to C,. At position C,, the 
color point would be 100 percent pure. ' 

15-3 
INTUITIVE COLOR CONCEPTS 

An artist creates a color painting by mixing color p~gments with white and black 
pigments to form the various shades, tints, and tones in the scene. Starting with 
the pigment for a "pure color" (or "pure hue"), the artist adds a black pigment to 
produce different shades of that color. The more black pigment, the darker the 
shade. Similarly, different tints of the color are obtmned by adding a white pig- 
ment to the original color, making it lighter as more white is added. Tones of the 
color are produced by adding both black and white pigments. 

To many, these color concepts are more intuiti\,e than describing a color as a 
set of three numbers that give the relative proport~ons of the primary colors. It is 
generally much easier to think of making n color lighter by adding white and 
making a color darker by adding black. Therefore. graphics packages providing 



Chap- 15 culor palettes to a user often employ two or more color models. One model pro- 
Color Models and Color vides an intuitive color interface for the user, and others describe the color com- 

Applications ponents for the output devices. 

15-4 
RGB COLOR MODEL 

Based on the tristimulus theoy of vision, our eyes perceive color through the stim- 
ulation of three visual pigments in the cones of the wtina. These visual pigments 
have a peak sensitiv~ty at wavelengths of about 630 run (red), 530 nm (green), 
and 450 nm (blue). By comparing intensities in a light source, we perceive the 
color of the light. This theory of vision is the basis for displaying color output on 
a video monitor using the three color primaries, red, green, and blue, referred to 
as the RGB color model. 

We can represent this model with the unit cube defined on R, C, and B axes, 
as shown in Fig. 15-11. The origin represents black, and the vertex with coordi- 
nates (1; 1 , l )  is white. Vertices of the cube on the axes represent the primary col- 
ors, and the remaining vertices represent the complementary color for each of the 
primary colors. 

As with {he XYZ color system, the RGB colot scheme is an additive model. 
Intensities of the primary colors are added to produce other colors. Each color 
p i n t  within the bounds of the cube can be represented as the triple ( R ,  G, B), 
where values for R, G, and B are assigned in the range from 0 to 1. Thus, a color 
C, is expressed in RGB components as 

The magenta vertex is obtained by adding red and blue to produce the triple (1, 
0,l). and white at (1,l. 1) is thesum of the red, green, and blue vertices. Shades 
of gray are represented along the main diagonal of the cube from the origin 
(black) to the white vertex. Each point along this diagonal has an equal contribu- 
tion from each primary color, so that a gray shade halfway between black and 

I Black I ,I 

Blue Magenta 
c0,o. 1 )  11.0, 11 

/' 8 

- - . -- - - - - --- 
I i p r , ,  15-11 
The RCB color model, defining 
colors with an qdditive process 
wthin the unit cube 



Sccliom 1 5 4  

RCB Color Model 

Figuw 15-12 
Two views of the RGB mlor dbe: (a) along the grayscale 
diagonal from white to black and (b) along the grayscale diagonal 
from.black'to white. 

TABLE 15-1 
RGB (X ,  Y) CHROMACITY COORDINATES 

- - - - -- 

NTSC Standard ClE ~ o d e /  Appro~. ~ o l o r ~ o n i t o r  Values 
R (0.670.0.330) (0.735, 0.265) (0.628.0.346) 
G (0.210, 0.710) (0.274, 0.71 7) (0.268.0.586) 
B (0.140, 0.080) (0.167, 0.009) (0.1 50, 0.070) 

F i p n  15-13 
RGB color gamut. 

white is represented as (0.5, 0.5, 0.5). The color graduations along the front and 
top planes of the RGB cube are illustrated in Fig. 15-12. 

Chromaticity coordinates for an NlSC standard RGB phosphor are listed in 
Table 15-1. Also listed are the RGB chromaticity ~ r d i n a t e s  for the CIE RGB 
color model and the approximate values used for phosphors in color monitors. 
Figure 15-13 shows the color gamut for the NTSC standard RGB primaries. 



Chapter I5 15-5 
Color Models and Color ylQ COLOR MODEL Applicalions 

Whereas an RGB monitor requires separate signals for the red, green, and blue 
components of an image, a television monitor uses a single composite signal. The 
National Television System Committee (NTSC) color model for forming the com- 
posite video signal is the YIQ model, which is based on concepts in the CIE XYZ 
model. 

In the MQ color model, parameter Y is the same as in the XYZ model. Lu- 
minance (brightness) information is contained in the Y parameter, while chro- 
maticity information (hue and purity) is incorporated into the 1 and Q parame- 
ters. A combination of red, green, and blue intensities are chosen for the Y 
parameter to yield the standard luminosity curve. Since Y contains the luminance 
information, black-and-white television monitors use only the Y signal. The 
largest bandwidth in the NTSC video signal (about 4 MHz) is assigned to the Y 
information. Parameter I contains orange-cyan hue information that provides the 
flesh-tone shading, and occupies a bandwidth of approximately 1.5 MHz. Para- 
meter Q carries green-magenta hue information in a bandwidth of about 0.6 
MHz. 

An RGB signal can be converted to a television signal using an NTSC en- 
coder, which converts RGB values to YIQ values, then modulates and superim- 
poses the I and Q information on the Y signal. The conversion from RGB values 
to YIQ values is accomplished with the transformation 

This transformation is based on the NTSC standard RGB phosphor, whose chro- 
maticity coordinates were given in the preceding section. The larger proportions 
of red and green assigned to parameter Y indicate the relative importance of 
these hues in determining brightness, compared to blue. 

An NTSC video signal can be converted to an RGB s~gnal using an NTSC 
decoder, which separates the video signal into the YlQ components,'then con- 
verts to RGB values. We convert frum YIQ space to RGB space with the inverse 
matrix transformation from Eq. 15-6: 

15-6 
CMY COLOR MODEL 

A color model defined with the primary colors cyan, magenta, and yellow (CMY) 
is useful for describing color output to hard-copy devices. Unlike video monitors, 
which produce a color pattern by combining light from the screen phosphors, 



hard-copy devices such as plotters produce a color picture by coating a paper M i o n  15-7 
with color pigments. We see the colors by reflected light, a subtractive process. HSV Color Model 

As we have noted, cyan can be formed by adding green and blue light. 
Therefore, when white light is reflected from cyan-colored ink, the reflected light MI 
must have n o  red component. That is, red light is absorbed, or subtracted, by the ~~~~~~~l~ 
ink. Similarly, magenta ink subtracts the green component from incident light, Magenta 

and yellow subtracts the blue component. A unit cube representation for the 
CMY model is illustrated in Fig. 15-14. 

In the CMY model, point (1, 1, 1) represents black, because all components 
of the incident light are subtracted. The origln represents white light. Equal 
amounts of each of the primary colors produce grays, along the main diagonal of 
the cube. A combination of cyan and magenta ink produces blue light, because 
the red and green components of the mcident hght are absorbed. Other color 
combinations are obtained by a similar subtractive process 

The printing process often used with the CMY model generates a color -- 
point with a collection of four ink dots, somewhat as an RGB monitor uses a col- F i W r  15-14 
lection of three phosphor dots. One dot is used for each of the primary colors ne 

(cyan, magenta, and yellow), and one dot is black. A black dot is included be- definingco10rs w'th a 

cause the combination of cyan, magenta, and yellow inks typically produce dark process a 
unit cube. gray instead of black. Some plotters produce different color combinations by 

spraying the ink for the three primary colors over each other and allowing them 
to mix before they dry. 

We can express the conversion from an RGB representahon to a CMY repre- 
sentation with the matrix transformation 

where the white is represented in the RGB system iis the unit column vector. Sim- 
ilarly, we convert from a CMY color representation to an RGB representation 
with the matrix transformation 

where black is represented In the CMY system as the unit column vector. 

15-7 
HSV COLOR MODEL 

lnstead of a set of color primaries, the HSV model uses color descriptions that 
have a niore intuitive appeal to a user. To give a color specification, a user selects 
a spectral color and the amounts of white and black that are to be added to oh- 
tain d~fferent shades, tints, and tones. Color parameters in this model are hrrr ( H ) ,  
snt~rrntioir (9, and r~alue (V) .  



Chapter IS 

Color Models and Color 
Applications 

Grwn 

Color Hexaeon 

(bl 

Figure 15-15 
When the RGB color cube (a) is viewed along the diagonal horn 
white to black, the color-cubeoutline is a hexagon 6). 

The three-dimensional representation of the HSV model is derived from the 
RGB cube. If we imagine viewing the cube along the diagonal from the white 
vertex to the origin (black), we see an outline of the cube that has the hexagon 
shape shown in Fig. 15-15. The boundary of the hexagon represents the various 
hues, and it is used as the top of the HSV Jlexcone (Fig. 15-16). In the hexcone, 
saturation is measured along a horizontal ads, and value is along a vertical axis 
through the center of the hexcone. 

Hue is represented as an angle about the vertical axis, ranging from 0" at 
red through 360". Vertices of the hexagon are separated by 60" intervals. Yellow is 
at 60â€¢â€¹, green at 120â€¢â€¹, and cyan opposite red at H = 180'. Complementary colors 
are 180" apart. 

Cvan 

~ i , p w  15-16 
The HSV hexcone. 



Scction 15-7 

HSV Color Model 

- 
Figlire 15-17 
Cross section of the HSV hexcone, 
showing regions for shades, tints, 
and tones. 

Saturation S varies from 0 to 1 .  It is represented in this model as the ratio of 
the purity of a selected hue to its maximum purity at S = 1. A selected hue is said 
to be one-quarter pure at the value S = 0.25. At S = 0, we have the gray scale. 

Value V varies from 0 at the apex of the hexcone to I at the top. The apex 
represents black. At the top of the hexcone, colors have their maximum intensity. 
When V = 1 and S = 1,  we have the "pure" hues. White is the point at V = 1 and 
S = o .  

This is a more intuitive model for most users. Starting with a selection for a 
pure hue, which specifies the hue angle H and sets V = 5 = 1, we describe the 
color we want in terms of adding either white or black to the pure hue. Adding 
black decreases the setting for V while S is held constant. To get a dark blue, V 
could be  set to  0.4 with S = 1 and H = 240". Similarly, when white is to be added 
to the hue selected, parameter S is decreased while keeping V constant. A light 
blue could be designated with S = 0.3 while V = 1 and H = 240". By adding 
some black and some white, we decrease both V and S. An interface for this 
model typically presents the HSV parameter choices in a color palette. 

Color concepts associated with the terms shades, tints, and tones are repre- 
sented in a cross-sectional plane of the HSV hexcone (Fig. 15-17). Adding black to 
a pure hue decreases V down the side of the hexcone. Thus, various shades are 
represented with values S - 1 and 0 ~ V 5 1 .  Adding white to a pure tone pro- 
duces different tints across the top plane of the hexcone, where parameter values 
are V = 1 and O s S s l .  Various tones are specified by adding both black and 
white, producing color points within the triangular cross-sectional area of the 
hexcone. 

h he human eye can distinguish about 128 different hues and about 130 dif- 
ferent tints (saturation levels). For each of these, a number of shades (value set- 
tings) can be detected, depending on the hue selected. About 23 shades are dis- 
cernible with yellow colors, and about 16 different shades can be seen at the blue 
end of the spectrum. This means that we can distinguish about 128 x 130 x 23 = 
82,720 different colors. For most graphics applications, 128 hues, 8 saturation lev- 
els, and 15 value settings are sufficient. With this range of parameters in the HSV 
color model, 16,384 colors would be available to a user, and the system would 
need 14 bits of color storage per pixel. Color lookup tables could be used to re- 
duce the storage requirements per pixel and to increase the number of available 
c0:olors. 



Chapter I 5  15-8 
color ModelsandCo'or CONVERSION BETWEEN HSV AND RGB MODELS Apphcat~ons 

I f  HSV color parameters are made available to a user of a graphics package, these 
parameters are transformed to the RGB settings needed for the color monitor. To 
determine the operations needed in this transformation, we first consider how 
the HSV hexcone can be derived from the RGB cube. The diagonal of this cube 
trom black (the origin) to white corresponds to the V axis of the hexcone. Also, 
each subcube of the RGB cube corresponds to a hexagonal cross-sectional area of 
the hexcone. At any cross section, all sides of the hexagon and all radial lines 
from the V axis to any vertex have the value V. For any set of RGB values, V is 
equal to the maximum value in this set. The HSV point corresponding to the set 
of RGB values lies on the hexagonal cross section at value V. Parameter S is then 
determined as the relative distance of this point from the V axis. Parameter H is 
determined by calculating the relative position of the point within each sextant of 
the hexagon. An algorithm for mapping any set of RGB values into the corre- 
sponding HSV values is given in the following procedure: 

I* Input: h,  s ,  v In range [O. .11 
Outputs: r ,  g ,  t In range [0..11 ' /  

void hsvToRgbIfloat h .  float s, float v ,  float r ,  float ' g ,  float ' b )  
I 

i n t  i :  
float aa,  bb, c c .  t .  

~f ( S  = =  0 )  /. Grayscale ./ 
'r  = 'g = 'b = v :  

else ( 
i f  (h  = =  1.0) h = 0: 
h '= 6.0: 
i = ifloor (h); 
f = h - i :  
aa = v '  (1 - s ) :  
bb = v (1 - ( s  ' f ) ) :  
c c  = v ' ( 1  - ( s  ' (1 - f ) ) ) ;  
switch ( i )  I 
case  0: 'r = v; 'g = cc;  * b  = aa: break; 
case 1 :  'r = bb; ' g  - v; *b = dd; break; 
case  2 :  'r = aa; .g = v; * b  = cc; break; 
case  3 :  'r = aa; 'g = bb; 'b = v; break: 
c a s e  4 :  'r = cc;  *g = aa; ^b = v; break; 
c a s e  5 :  'r = v: ' g  = aa: 'b = bb: break: 
1 

1 

We obtain the transformation from HSV parameters to RGB parameters by 
determining the inverse 
verse operations are can 
transformation equations 

of the equations in rgbToHsv p r o c e d u r e .  These in- 
ied out for each sextant of the hexcone. The resulting 
are summarized in the following algorithm: 



I #defme NO-HUE - 1  
/ ' I n p u t :  r ,  g ,  b i n r a n g e  L0..11 

outputs: h ,  s ,  v i n  range ( 0 .  .11 
' /  
void  rgbToHsv (float r ,  float g ,  float b, float ' h, float ' 6 ,  float v )  
( 

floatmax=MAX ( r ,  MAX (g, b ) ) ,  m i n = M I N  ( r ,  MIN ( g ,  b ) ) ;  
float d e l t a  = max - min; 
* V  = max; 
i f  (max ! =  0 . 0 )  

' s  = d e l t a  / max; 
else 

' S  = 0 . 0 ;  
i f  ('s = =  0 . 0 )  *h = NO-HUE; 
else ( 

i f  (r == max) 
*h = (g - b) / d e l t a ;  

e l s e  if (g == max) 
'h = 2 + ( b  - r )  i d e l t a ;  

else i f  (b == max) 
* h  = 4 + ( r  - g )  i d e l t a ;  

't. * =  6 0 . 0 ;  
i f  ( 'h  < 0) 'h += 3.60.0; 
*k / =  360 .0;  

1 
1 

15-9 
HLS COLOR MODEL 

Another model based on intuitive color parameters is the HLS system used by 
Tektronix. This model has the double-cone representation shown in Fig. 15-18. 
The three color parameters in this model are called hue (H), lightness (L), and 
saturation 6). 

Hue has the same meaning as in the HSV model. It  specifies an angle about 
the vertical axis that locates a chosen hue. In this model, H = 0' corresponds to 
blue. The remaining colors are specified around the perimeter of the cone in the 
same order as in the HSV model. Magenta is at 60â€¢â€¹. red is at 120â€¢â€¹, and cyan is lo- 
cated at H = 180". Again, complementary colors are 180" apart on the double 
cone. 

The vertical axis in this model is called lightness, L. At L = 0, we have 
black, and whitc is at L = 1. Gray scale is along the L axis, and the "pure hues" lie 
on the L = 0.5 plane. 

Saturation parameter S again specifies relative purity of a color. This para- 
meter varies from 0 to 1, and pure hues are those for which S = 1 and L = 0.5. As 
S decreases, the hues are said to be less pure. At S = 0, we have the gray scale. 

As in the HSV model, the HLS system allows a user to think in terms of 
making a selected hue darker or lighter. A hue is selected with hue angle H, and 
the desired shade, tint, or tone is obtained by adjusting L and S. Colors are made 
lighter by increasing L and made darker by decreasing L. When S is decreased, 
the colors move toward gray. 

Section 15-9 

H L S  Cdor Model 



Chapter 15 

Color Mcdels and Color 
Applications 

-- -. -- 
F'igitrr 1.5-18 
The HLS double cone. 

15-1 0 
COLOR SELECTION A N D  APPLICATIONS 

A graphics package can provide color capabilities in a way that aids us in making 
color selections. Various combinations of colors can be selected using sliders and 
color wheels, and the system can also be designed to aid in the selection of har- 
monizing colors. In addition, the designer of a package can follow some basic 
color rules when designing the color displavs that are to be presented to a user. 

One method for obtaining a set of coordinating colors is to genelate the set 
from some subspace of a color model. If colors are selected at regular intervals 
along any straight line within the RGB or CMY cube, for example, we can expect 
to obtain a set of well-matched colors. Rahdomly selected hues can be expected 
to produce harsh and clashing color combinations. Another consideration in the 
selection of color combinations is that diflerent colors are perceived at different 
depths. This occurs because our eyes focus on colors according to their frequency. 
Blues, in particular, tend to recede. Displaying a blue pattern next to a red pattern 
can cause eyedatigue, because we continually need to refocus when our attention 



1s switched from one area to the other T h ~ s  problem can be reduced by separat- 
ing these colors or by using colors from one-half cr less of the color hexagon In References 
the HSV model. With this techn~que, a d~splay contains either blues and greens 
or reds and yellows. 

As a general rule, the use of a smaller number of colors produces a more 
pleasing display than a large number of colors, and tints and shades blend better 
than pure hues. For a background, gray or the complement of one of the fore- 
ground colors 1s usually best. 

SUMMARY 

In this chapter, we have discussed the basic properties of light and the concept of 
a color model. Visible light can be characterized as a narrow frequency distribu- 
tion within the electromagnetic spectrum. Light sources are described in terms of 
their dominant frequency (or hue), luminance (or bnghtness), and purity (or sat- 
uration). Complementary color sources are those that combine to produce white 
light. 

One method for defining a color model is to specify a set of two or more 
primary colors that are combined to produce various other colors. Common color 
models defined with three primary colors are the RGB and CMY models. Video 
monitor displays use the RGB model, while hardcopy devices produce color out- 
put using the CMY model. Other color models, bdsed on specification of lumi- 
nance and purity values, include the YIQ, HSV, and HLS color models. Intuitive 
color models, such as the HSV and HLS models, allow colors to be specified by 
selecting a value for hue and the amounts of white and black to be added to the 
selected hue. 

Since no nlodel specified with a hnite set of color parameters is capable of 
describing all possible colors, a set of three hypothetical colors, called the CIE 
primaries, has been adopted as the standard for defining all color combinations. 
S e  set of CIE primaries is commonly referred to as the XYZ color model. Plot- 
ting normalized values for the X and Y standards produces the CIE chromaticity 
diagram, which gives a representation for any colo~ in terms of hue and purity. 
We can use this diagram to compare color gamuts for different color models, to 
identify complementary colors, and to determine dominant frequency and purity 
for a given color. 

An important consideration in the generation of a color display is the selec- 
tion of harmonious color combinations. We can d o  this by following a few simple 
rules. Coordinating colors usually can be selected from within a small subspace 
of a color model. Also, we should avoid displaying adjacent colors that differ 
widely in dominant frequency. And we should limit displays to a small number 
of color combinations formed with tints and shades, rather than with pure hues. 

REFERENCES 

A comprehensive discussion of the sclrnce of color is givw In Wyszecki and S~iles (1 982). 
Color models and color display techniques are discussed in Durrett (19871, Hall (1989). 
and Traris (1991). Algorithms for varlous color applications are presented In Classner 
11990), Arvo (1991), and Kirk (1992). For additional information on the human visual sys- 
tern and our perception of light and color, see Classner I 1q94). 



Chap:er 15 5p 

Color Models and Color 
EXERCISES 

Apphcahons 
15.1 Derive expressions ior converting RCB color parameters to HSV values. 

15.2.  Deriveexpressions for converting HSV color values to RCB values. 

15-3.  Write an interactive procedure that allows selection of HSV color pa:arneters from a 
displayed menu, ther~ the HSV values are to be converted to RCB values for storage 
in a frame buffer. 

15-4. Derive expressions (or converting RCB color values to HLS color parameters. 

15-5.  Deriveexpress~ons for converting HLS color values to RCH values. 

15-6.  Write a program that allows interactive selection of HLS values from a color menu 
then converts these values to corresponding RCB values. 

15.7. Write a program that will produce a set of colors that are linearly interpolated be- 
tween any two specified positions in RCB space. 

15-8.  Write an interactive routine for selecting color values from with~n a specified sub- 
space of RCB space. 

15-9. Write a program that will produce a set of colors that are linearly interpolated be- 
tween any two specified positions in HSV space. 

15.10. Write a program that will produce a set of colors that are linearly interpolated be- 
tween any two specified positions in HLS space. 

15-1  1 .  Display two RCB color grids, side by side on a video monitor. Fill one grid with a set 
of randomly selecred RCB colors, and fill the other grid with a set of colors that are 
selected from a small RCB subspace. Experiment with different random selections 
and different RCB subspaces and compare the two color ~r ids.  

15-12 ,  Display the two color grids in Exercise 15-11 using color selections from either the 
HSV or the HLS color space. 





S ome typical applications of computer-generated animation are entertain- ment (motion pictures and cartoons), advertismg, scientific and engineering 
studies, and training and education. Although we tend to think of animation as 
implying obpct mot~ons, the term computer animation generally refers to any 
time sequence of visual changes in a scene. In addition to changing object posi- 
tion with translations or rotations, a computer-generated animation could dis- 
play time variations in object size, color, hansparency, or surface textule. Adver- 
tising animations often transition one object shape into another: for example, 
transforming a can of motor oil into an automobile engine. Computer animations 
can also be generated by changing camera parameters, such as position, orienta- 
tion, and focal length. And we can produce computer animations by chariging 
lighting effects or other parameters and procedures associated with illumination 
and rendering. 

Many applications of computer animation require realistic displays. An ac- 
curate representation of the shape of a thunderstorm or other natural phenomena 
described with a numerical model is important for evaluating the reliability of 
the model A h ,  simulators for training aircraft pilots and heavy-equipment oper- 
ators must produce reasonably accurate representations of the environment. En- 
tertainment and advertising applications, on the other hand, are sometimes more 
interested in visual effects. Thus, scenes may be displayed with exaggerated 
shapes and unrealistic motions and transformations. There are many entertain- 
ment and advertising applications that do require accurate representations for 
computer-generated scenes. And in some scientific and engineering studies, real- 
ism is not a goal. For example, physical quantities are often displayed with 
pseudo-colors or abstract shapes that change over time to httlp the reseamher un- 
derstand the nature of the physical process. 

16-1 
DESIGN OF ANIMATION SEQUENCES 

In general, an animation sequence is designed with the tollowing steps: 

Storyboard layout 
Object definitions 
Key-frame specititatlons 
Generation o f  in-between frame5 



This standard approach for animated cartoons is applied to other animation a p  16-1 
plications as well, although there are many special ap$licationa that d o  not fol- Desiw of Animation Seqwnces 
low this sequence. Real-time computer animations produced by Bight simulators, 
for instance, display motion sequences in r e s p  to seftiqp on the aLnaft con- 
trols. And visualization applications are generated by the solutions of the numer- 
ical models. For frame-by-jrume animation, each frame of the scene is separately 
generated and stored. Later, the frames can be recoded on film or they can be 
consecutively displayed in "real-time playback" mode. 

The s t q b w r d  is an outline of the action. It defines the motion sequence as a 
set of basic events that are to take place. Depending on the type of animation to 
be produced, the storyboard could consist of a set of rough sketches or it could be 
a list of the basic ideas for the motion. 

An object definition is given for each partidpant in the action. Objects can be 
defined in terms of basic shapes, such as polygons or splines. In addition, the as- 
sociated movements for each object are s p e d e d  along with the shape. 

A keyframe is a detailed drawing of the scene at a certain time in the anima- 
tion sequence. Within each key frame, each object is positioned according to the 
time for that frame. Some key frames are chosen at extreme positions in the ac- 
tion; others are spaced so that the time interval between key frames is not tuo 
great. More key frames are specified for intricate motions than for simple, slowly 
varing motions. 

In-behueols are the intermediate frames between the key frames. The nurn- 
ber of in-betweens needed is determined by the media to be used to display the 
animation. Film requires 24 frames per second, and graphics terminals are re- 
freshed at the rate of 30 to 60 frames per second. Typically, time intervals for the 
motion are set up so that there arr from three to five in-betweens for each pair of 
key frames. Depending on the speed specified for the motion, some key frames 
can be duplicated. For a I-minute film sequence with no duplication, we would 
need 1440 frames. With five in-betweens for each pair of key frames, we would 
need 288 key frames. If the motion is not too complicated, we could space the key 
frames a little farther apart. 

There are several other tasks that may be required, depending on the appli- 
cation. They include motion verification, editing, and production and synchro- 
nization of a soundtrack. Many of the functions needed to produce general ani- 
mations are now computer-generated. Figures 16-1 and 16-2 show examples of 
computer-generated frames for animation sequences. 

I Figl ire  16-1 
One frame from the award-winning 
computer-animated short film Luxo 
11. The film was designed using a 
key-frame animation system and' 
cartoon animation techniques to 
provide lifelike adions of the 
lamps. Final images were rendered 
with multiple light sourres and 
procedural texturing techniques. 
(Courfesy o(Pixnr. 6 1986 Pixar.) 



Figure 16-2 
one frame from the short film Tin 
Toy, the first computer-animated 
film to win an Oscar. Designed 
using a key-frame animation 
system, the film also required 
extensive facial expression 
modeling. Final images were 
rendered using procedural shading, 
self-shadowing techniques, motion 
blur, and texture mapping. (CourkSsv 
of Pixar. 0 1988 Pixor.) 

16-2 
GENERAL COMPUTER-ANIMATION FUNCTIONS 

Some steps in the development of an animation sequence are well-suited to com- 
puter solution. These include object manipulations and rendering, camera mo- 
tions, and the generation of in-betweens. Animation packages, such a s  Wave- 
front, for example, provide special functions for designing the animation and 
processing individual objects. 

One function available in animation packages is provided to store and man- 
age the object database. Object shapes and associated parameters are stored and 
updated in the database. Other object functions include those for motion genera- 
tion and those for object rendering. Motions can be generated according to speci- 
fied constraints using two-dimensional or three-dimensional transformations. 
Standard functions can then be applied to identify visible surfaces and apply the 
rendering algorithms. 

Another typical function simulates camera movements. Standard motions 
are zooming, panning, and tilting. Finally, given the specification for the key 
frames, the in-betweem can be automatically generated. 

16-3 
RASTER ANIMATIONS 

On raster systems, we can generate real-time animation in limited applications 
using raster operations. As we have seen in Section 5-8, a simple method for trans- 
lation in the xy plane is to transfer a rectangluar block of pixel values from one 
location to another. Twodimcnsional rotations in multiples of 90" are also simple 
to perform, although we  can rotate rectangular blocks of pixels through arbitrary 
angles using antialiasing procedures. To rotate a block of pixels, we need to de- 
termine the percent of area coverage for those pixels that overlap the rotated 
block. Sequences of raster operations can be executed to produce real-time ani- 
mation of either two-dimensional or three-dimensional objects, as long a s  we re- 
strict the animation to motions in the projection plane. Then no viewing or visi- 
ble-surface algorithms need be invoked. 

We can also animate objects along two-dimensional motion paths using the 
color-table transformlions. Here we predefine the object at successive positions 
along the motion path, and set the successive blocks of pixel values to color-table 



- Section 16-4 
-- 

Figure 16-3 
Real-time raster color-tahle 
an~mation. 

entries. We set the pixels at the first position of the object to "on" values, and we 
set the pixels at the other object positions to the background color. The animation 
is then accomplished by changing the color-table values so that the object is "on" 
at successively positions along the ariimation path as the preceding posltion is set 
to the background intensity (Fig. 16-3). 

16-4 
COMPUTER-ANIMATION LANGUAGES 

Design and control of animation sequences are handled with a set of animation 
routines. A general-purpose language, such as C, Lisp, Pascal, or FORTRAN, is 
often used to program the animation functions, but several specialized animation 
languages have been developed. Animation functions include a graphics editor, a 
key-frame generator, an in-between generator, and standard graphics routines. 
The graphics editor allows us to design and modify object shapes, using spline 
surfaces, constructive solid-geometry methods, or other representation schemes. 

A typical task in an animation specilkation is scene description. This includes 
the positioning of objects and light sources, defining the photometric parameters 
(light-source intensities and surface-illumination properties), and setting the 
camera parameters (position, orientation, and lens characteristics). Another stan- 
dard function is action specifimtion. This involves the layout of motion paths for 
the objects and camera. And we need the usual graphics routines: viewing and 
perspective transformations, geometric transformations to generate object move- 
ments as a function of accelerations or kinematic path specif~cations, visible-sur- 
face identification, and the surface-rendering operations. 

Key-frame systems are specialized animation languages designed simply 
to generate the in-betweens from the user-specified key frames. Usually, each ob- 
ject in the scene is defined as a set of rigid bodies connected at the joints and with 
a limited number of degrees of freedom. As an example, the single-arr.1 robot in 
Fig. 16-4 has six degrees of freedom, which are called arm sweep, shoulder 
swivel, elbow extension, pitch, yaw, and roll. We can extend the number of de- 
grees of freedom for this robot arm to nine by allowing three-dimensional trans- 
lations for the base (Fig. 16-51. If we also allow base rotations, the robot arm can 
have a total of 12 degrees of freedom. The human body, in comparison, has over 
200 degrees of freedom. 

Parameterized systems allow object-motion characteristics to be specified 
as part of the object definitions. The adjustable parameters control such object 
characteristics as degrees of freedom, motion limitations, and allowable shape 
changes. 

Computer-Anmalion Ldngudges 



Elbow 
Computer Arrimation 

Swivel 

aw 

Roll 

. - - - - - - - - - 
ri ,q~we 16-5 
Translational and rotational 
degrees ot freedom for the 
base of the robot arm. 

- . - - - - - ?. - -- 
Figure 16-4 
Degrees of freedom for a stationary, 
single-arm robot 

Scripting systems allow object specifications and aninlation sequences to 
be defined with a user-input script. From the script, a library of various objects 
and motions can be constructed. 

16-5 
KEY-FRAME SYSTEMS 

We generate each set of in-betweens from the speciiicatlon of two (or more) key 
frames. Mot~on paths can be given with a kinematic dm-ripti011 as a set of spline 
curves, or the motions can be physicdly bnscd by specifying the forCec acting on 
the objects to be animated. 

For complex scenes, we can separate the frames into individual components 
or objects called cels  celluloid transparencies), an acronym froni cartoon anima- 
tion. Given the animation paths, we can interpolate the positions of ~ndividual 
objects between any two tlmes. 

With complex object transformations, the shapes of objects may change 
over time. Examples are clothes, facial features, magnified detail, evolving 
shapes, exploding or disintegrating objects, and transforming one object into an- 
other objrct. If all surfaces are described with polygon meshes, then the number 
of edges per polygon can change from one frame to the next. Thus, the total num- 
ber of line segments can be different in different frames. 

Transformation of object shapes from one form to another is called morphing, 
which is a shortened iorm of metamorphosis. Morphing methods can he applied 
to any motion or transition involving a change in shape. 

Givcn two key frames for an object transformation, we first adjust the objrct 
specification in one of the frames so that the number of polygon edges (or the 
number of vertices) is the same for the two frames. This preprocessing step is 11- 
lustrated in Fig. 16-6. A straight-line segment in key franie k 15 transformed into 
two line segments in kev frame k t 1. Since key frame k + 1 has an extra vertex, 
n'e add n veytex bctr\.rtw \wtices 1 and 2 in kcv frame k to balance the number of 
vertices (and edges) In the two key frames. Using linear interpolation to generate 
the in-betweens. wc trmsition the added vertex in key frclme k into vertex 3' 
along the straight-linv path shown in Fis. 16-7. An eianlple ol a triangle linearly 
cxp"11ding into ,I quad~.~lateral is given In Fig. 16-8. Figures 16-9 and 16-10 show 
examples uf morphing 111 television advertising 



Key 
Frame k 

Key 
Frame k + 1 

Figuw 16-6 
An edge with vertex positions 1 and 2 in key frame k 
evolves into two connected edges in key frame k + 1 

added 
point 

- 
Fipm 16-7 
Linear interpolation for transforming a line segment in 
key frame k into two connected line segments in key 
frame k + 1. 

---  - - 
- - - - - - _ _  

Frame 
k Frame Frame 

Figure 16-8 
Linear interpolation for transforming a triangle into a quadrilateral. 

We can state general prepmessing rules for equalizing key frames in terms 
of either the number of edges or the number of vertices to be added b a key 
frame. Suppose we equalize the edge count, and parameters Lk and L,,, denote 
the number of line segments in two consecutive frames. We then define 





Then the preprocessing is accomplished by 

1. dividing N, edges of keyframe,, into N, + 1 sections 
2. dividing the remaining lines of keyframe,, into N, sections 

As an example, if Lk = 15 and Lk+, = 11, we would divide 4 lines of Jzyframek+, 
into 2 sections each. The remaining lines of keyframe,, , are left intact. 

If we equalize the vertex count, we can use parameters V k  and V k + ]  to de- 
note the number of vertices in the two consecutive frames. In this case, we define 

and 

Preprocessing using vertex count is performed by 

1. adding N, points to N,, line sections of keyframe,, 
2. adding N, - 1 points to the remaining edges of keyframe,, 

For the triangle-toquadrilateral example, V k  = 3 and Vk+,  = 4. Both N,, and N, 
are 1, so we would add one point to one edge of keyframek No points would be 
added to the remaining lines of keyframe,, , . 

Simulating Accelerations 

Curve-fitting techniques are often used to specify the animation paths between 
key frames. Given the vertex positions at the key frames, we can fit the positions 
with linear or nonlinear paths. Figure 16-11 illustrates a nonlinear fit of key-frame 
positions. This determines the trajectories for the in-betweens. To simulate accel- 
erations, we can adjust the time spacing for the in-betweens. 

For constant speed (zero acceleration), we use equal-interval time spacing 
for the in-betweens. Suppose we want n in-betweens for key frames at times t ,  
and t2 (Fig. 16-12). The time interval between key frames is then divided into n + 
1 subintervals, yielding an in-between spacing of 

We can calculate the time for any in-between as 

and determine the values for coordinate positions, color, and other physical para- 
meters. 

Nonzero accelerations are used to produce realistic displays of speed 
changes, particularly at the beginning and end of a motion sequence. We can 
model the start-up and slowdown portions of an animation path with spline or 

%ion 16-5 
Key-Frame Systems 



Chapter 16 

Computer Animation 

Figure 16-11 
Fitting key-frame vertex positions with nonlinear splines. 

trignometric functions. Parabolic and cubic time functions haw been applied to 
acceleration modeling, but trignometric functions are more commonly used in 
animation packages. 

To model increasing speed (positive acceleration), we want the time spacing 
between frames to increase so that greater changes in position occur as the o b j j  
moves faster. We can obtain an increasing interval size with the function 

1-case, O < B < m / 2  

For n in-betweens, the time for the jth in-between would then be calculated as 

where 6Eis the time difference between the two key frames. Figure 16-13 gives a 
plot of the higonometric acceleration function and the in-between spacing for n 
= 5. 

We can model decreasing speed (deceleration) with sine in the range 0  < 8 
< d 2 .  The time position of an in-between is now defined as 

tB, = t, + At sin j = 1 , 2 , . . , n  
2(n+l)' 

Figuw 16-12 
In-between positions for motion at constant speed. 



Figure 16-13 
A trigonometric acceleration function and the corresponding in-between spacing for n = 5 
and 0 = jr/ 12 in Eq. 16-7, producing increased coordinate changes as the object move 
through each time interval. 

A plot of this function and the decreasing size of the time intervals is shown in 
Fig. 16-14 for five in-betweens. 

Often, motions contain both speed-ups and slow-downs. We can model a 
combination of increasing-decreasing speed by first increasing the in-between 
time spacing, then we decrease this sparing. A function to accomplish these time 
changes is 

Figwe 16-14 
A trigonometric deceleration function and the corresponding in-between spacing for n = 5 
and 0 = jr/ 12 in Eq. 16-8, pducing decreased coordinate changes as the obpct moves 
through each time interval. 



Figuw 16-15 
A trigonometric accelerate-decelerate function and the corresponding in-between spacing 
for n = 5 in Eq. 16-9. 

The time for the jth in-between is now calculated as 

with At denoting the time difference for the two key frames. l ime  intervals for 
the moving obpd  first in~wase, then the'time intervals decrease, as shown in Fig. 
16-15. 

kocessing the in-betweens is simplified by initially modeling "skeleton" 
(wiref~ame) objects. This allows interactive adjustment of motion sequences. 
After the animation sequence is completely defined, objects can be fully ren- 
dered. 

16-6 
MOTION SPECIFICATIONS 

There are several ways in which the motions of objects can be specified in an ani- 
mation system. We can define motions in very explicit terms, or we can use more 
abstract or more general approaches. 

Direct Motion Specification 

The most straightforward method for defining a motion sequence is direct specifi- 
cation of the motion parameters. Here, we explicitly give the rotation angles and 
translation vectors. Then the geomehic transformation matrices are applied to 
transform coordinate positions. Alternatively, we  could use an approximating 



Figure 16-16 
Approximating the motion of a bouncing ball with a damped sine function (Eq. 16-10). 

equation to specify certain kinds of motions. We can approximate the path of a 
bouncing ball, for instance, with a damped, rectified, stne curve (Fig. 16-16): 

where A is the initial amplitude, w is the angular frequence, 0, is the phase angle, 
and k is the damping constant. These methods can be used for simple user-pro- 
grammed animation sequences. 

Goal-Directed Systems 

At the opposite extreme, we can specify the motions that are to take place in gen- 
eral terms that abstractly describe the actions. These systems are referred to as 
goal dir~cted because they determine specific motion parameters given the goals 
of the animation. For example, we could specify that we want an object to "walk" 
or to "run" to a particular destination. Or we could state that we want an object 
to "pick up" some other specified object. The input directives are then inter- 
preted in terms of component motions that will accomplish the selected task. 
Human motions, for instance, can be defined as a hierarchical structure of sub- 
motions for the torso, limbs, and so forth. 

Kinematics and Dynamics 

We can also construct animation sequences using kinematic or dyrramic descrip- 
tions. With a kinematic description, we specify the animation by giving motion 
parameters (position, velocity, and acceleration) without reference to the forces 
that cause the motion. For constant velocity (zero acceleration), we des~gnate the 
motions of rigid bodies in a scene by giving an  initial position and velocity vector 



Chapter l 6  for each object. As an example, if a velocity is specified as (3,0, - 4 )  km/sec, then 
CompuferAnimalion this vector gives the direction for the straight-line motion path and the speed 

(magnitude of velocity) is 5 kmlsec. If we also specify accelerations (rate of 
change of velocity), we can generate speed-ups, slowdowns, and C U W ~  motion 
paths. Kinematic specification of a motion can also be given by simply describing 
the motion path. This is often done using spline curves. 

An alternate approach is to use inverse kinemfirs. Here, we specify the ini- 
tial and final positions of objects at specified times and the motion parameters are 
computed by the system. For example, assuming zero accelerations, we can de- 
termine the constant velocity that will accomplish the movement of an object 
from the initial position to the final position. This method is often used with com- 
plex objects by giving the positions and orientations of an end node of an object, 
such as a hand or a foot. The system then determines the motion parameters of 
other nodes to accomplish the desired motion. 

Dynamic descriptions on the other hand, require the specification of the 
forces that produce the velocities and accelerations. Descriptions of object behav- 
ior under the irifluence of forces are generally referred to as a physically based 
modeling (Chapter 10). Examples of forces affecting object motion include electro- 
magnetic, gravitational, friction, and other mechanical forces. 

Object motions are obtained from the force equations describing physical 
laws, such as Newton's laws of motion for gravitational and friction processes, 
Euler or Navier-Stokes equations describing fluid flow, and Maxwell's equations 
for electromagnetic forces. For example, the general form of Newton's second 
law for a particle pf mass m is 

with F as the force vector, and v as the velocity vector If  mass is constant, we 
solve the equation F = ma, where a is the acceleration vector. Otherwise, mass is 
a function of time, as in relativistic motions or the motions of space vehicles that 
consume measurable amounts of fuel per unit time. We can also use inverse dy- 
nntnics to obtain the forces, given the initial and final positions of objects and the 
type of motion. 

Applications of physically based modeling include complex rigid-body sys- 
tems and such nonrigid systems as cloth and plastic materials. Typically, numeri- 
cal methods are used to obtain the motion parameters incrementally from the dy- 
namical equations using initial conditionsor boundary values. 

SUMMARY 

A computer-animation sequence can be set up by specifying the storyboard, the 
object definitions, and the key frames. The storyboard is an outline of the action, 
and the key frames define the details of the object motions for selected positions 
in the animation. Once the key frames have been established, a sequence of in-be- 
tweens can be generated to construct a smooth motion from one key frame to the 
next. A computer animation can involve motion specifications for the objects in a 
scene as well as motion paths for a camera that moves through the scene. Com- 
puter-animation systems include key-frame systems, parameterized systems, and 
scripting systems. For motion in two-dimensions, we can use the raster-anima- 
tion techniques discussed in Chapter 5. 



For some applications, key frames are used to define the steps in a rnorph- 
ing sequence that changes one object shape in to  another. Other in-between meth- Exerciser 
ods include generation o f  variable time intervals t o  simulate accelerations and 
decelerations in the motion. 

Mot ion  specifications can be given in terms of translation and rotation para- 
meters, or mot ions can be described w i th  equations o r  w i t h  kinematic o r  dy- 
namic parameters. Kinematic mot ion  descriptions specify positions, velocities, 
and accelerations. Dynamic mot ion  descriptions are given in terms of the forces 
acting on  the objects in a scene. 

REFERENCES 

For additional information on computer animation systems and techniques, see Magnenat- 
Thalrnann and Thalrnann (1985), Barzel (1992). and Watt and Wan (1992). Algorithms for 
anlrnation applications are presented in Glassner (1990). Arvo (19911, Kirk (1992). Cas- 
cue1 (1993), Ngo and Marks (1993). van de Panne and Flume (1993), and in Snyder et al. 
(1993). Morphing techniques are discussed In Beier and Neely (1992). Hughes (1992). 
Kent, Carlson, and Parent (1992). and in Sederberg and Greenwood (1992). A discussion 
o i  animation techniques in PHlGS is given in Gaskins ( 1  992). 

EXERCISES 

16-1. Design a storyboard layout and accompanving ke) (rames for an animation of a sin- 
gle polyhedron. 

16-2 Write a program to generate the in-betweens for the key frames specified in Exercise 
16-1 using linear interpolation. 

16-3. Expand the animation sequence in Exercise I b.1 lo Include two or more moving ob- 
jects. 

16-4. Write a program to generate the in-betweens for the key trames in Exercise 16-3 
using linear interpolation. 

16.5. Write a morphing program to transform a sphere into a specified polyhedron. 

16-6. Set up an anmation speciiication involving accelerations and implement Eq. 16-7. 

16-7 Set up an animation specification involving both accelerations and deceleiations and 
implement the ~n-between spacing calculations given in Eqs. 16-7 and 16-8. 

16-8. Set up an animalion specification implementing the acceleration-deceleration calcu- 
lat~ons of Eq. 16-9. 

16-9. Wrlte a program to simulate !he linear, two-dimensional motion of a filled circle 
inside a given rectangular area. The circle is to be given an initial velocity, and the 
circle is to rebound from the walls with the angle of reflection equal to the angle of 
incidence. 

16-10. Convert the program of Exerclse 16-9 into a ball and paddle game by replacing one 
s~de of the rectangle with a short line segment that can be moved back and forth to 
intercept the clrcle path. The game is over when the circle escapes from the interior 
of the rectangle. Initial input parameters include circle position, direction, and speed 
The game score can include the number of times the circle is intercepted by the pad- 
dle. 

16-11. Expand the program of Exercise 16-9 to simulate the three-d~mensional motion of a 
sphere moving inside a parallelepiped. Interactive viewing parameters can be set to 
view the motion from different directions. 

16-1 2. Write a program to implement the simulation of a bouncing ball using Eq. 16-10. 
16-1 3. Write a program to implement the motion of a bouncing ball using a downward 



Chapter 16 gravitational force and a ground-plane friction force. In~tially, the ball is lo  be pro- 
Computer Animation jected into space with a given velocity vector. 

16-1 4. Write a program to implement the two-player pillbox game. The game can be imple- 
mented on a flat plane with fixed pillbox positions, or random terrain features and 
pillbox placements can be generated at the start of the game. 

16-15. Write a program to implement dynamic motion spec~fications. Specify a scene with 
two or more objects, initial motion parameters, and specified forces. Then generate 
the animation from the solution of the force equations. (For example, the objects 
could be the earth, moon, and sun with attractive gravitational forces that are propor- 
tional ro mass and inversely proportional to distance squared.) 



A P P E N D I X  

A Mathematics for Computer Graphics 



- 

C omputer graphics algorithms make use of many mathematical concepts and techniques. Here, we  provide a brief reference for the topics from ana- 
lytic geometry, linear algebra, vector analysis, tensor analysis, complex numbers, 
numerical analysis, and other areas that are referred to in the graphics algorithms 
discussed throughout this book. 

A- 1 
COORDINATE REFERENCE FRAMES 

Graphics packages typically require that coordinate parameters be specified with 
respect to Cartesian reference frames. But in many applications, non-Cartesian 
coordinate systems are useful. Spherical, cylindrical, or other symmetries often 
can be exploited to simplify expressions involving object descriptions or manipu- 
lations. Unless a specialized graphics system is available, however, we must first 
convert any nonCartesian descriptions to Cartesian coordinates. In this section, 
we first review standard Cartesian coordinate systems, then we consider a few 
common non-Cartesian systems. 

Two-Dimensional Cartewn Reference Frames 

Figure A-1 shows two possible orientations for a Cartesian screen reference sys- 
tem. The standard coordinate orientation shown in Fig. A-l(a), with the coordi- 
nate origin in the lower-left comer of the screen, is a commonly used reference 

. 

Figure A - 1 
Scmn Cartesian reference systems: (a) coordinate origin at the lower- 
left screen corner and (b) coordinate origin in the upper-left corner. 



Section A-1 
Coordinate Reference Frames 

Figure A -2 
A polar coordinate reference frame, 
formed with concentric circles and 
radial lines. 

KT-- - Figure A-3 Relationship between polar and ""' Cartesian coordinates. 
frame. Some systems, particularly personal computers, orient the Cartesian refer- 
ence frame as in Fig. A-l(b), with the origin at the upper left comer. In addition, 
it is possible in some graphics packages to select a position, such as the center of 
the screen, for the coordinate origin. 

Polar Coordinates in the xy Plane 

A fquent ly  used non-Cartesian system is a polar-coordinate reference frame 
(Fig. A-2), where a coordinate position is specified with a radial distance r from 
the coordinate origin, and an angular displacement I3 from the horizontal. Posi- 
tive angular displacements are counterclockwise, and negative angular displace- 
ments are clockwise. Angle I3 can be measured in degrees, with one complete o 
counterclockwise revolution about the o r i p  as 360". The relation between Carte- 
sian and polar coordinates is shown in Fig. A-3. Considering the right triangle in FigureA-4 
Fig. A-4. and using the definition of the trigonometric functions, we transform Right triangle with 
from polar coordinates to Cartesian coordinates with the expressions hypotenuse rand sides x and 

Y. 
x = rcose, y = rsine (A-1) 

The inverse transformation from Cartesian to polar coordinates is 

Other conics, besides circles, can be used to specify coordinate positions. 
For example, using concenhic ellipses instead of circles, we can give coordinate 
positions in elliptical coordinates. Similarly, other types of symmetries can be ex- 
ploited with hyperbolic or parabolic plane coordinates. 



Appendix A Angular values can be specified in degrees or they can be given in dimen- 
sionless units (radians). Figure A-5 shows two intersecting lines in a plane and a 
circle centered on the intersection point P. The value of angle 0 in radians is then 

/ 
given by 

5 e =  - (A-3 )  
r 

where s is the length of the circular arc subtending 0, and r is the radius of the cir- 

r ,  cle. Total angular distance around point P is the length of the circle perimeter 
I (2m) divided by r, or 2 w  radians. 

' . 
Three-Dimensional Cartes~an Reference Frames 

-- Figure A-6(a) shows the conventional orientation for the coordinate axes in a 

Figure A-.5 three-dimensional Cartesian reference system. This is called a right-handed sys- 
An angle esubtended by a tem because the right-hand thumb points in the positive z direction when we  
circular arc of lengths and imagine grasping the z axis with the fingers curling from the positive x axis to the 
radius r. positive y axis (through 90â€¢â€¹), as illustrated in Fig. A-6(b). Most computer graph- 

ics packages require object descriptions and manipulations to be specified in 
right-handed Cartesian coordinates. For discussions t h u g h o u t  this book (in- 
cluding the appendix), we assume that all Cartesian reference frames are right- 
handed. 

Another possible arrangement of Cartesian axes is the left-handed system 
shown in Fig. A-7. For t h s  system, the left-hand thumb points in the positive z 
direction when we imagne grasping the z axis so that the fingers of the left hand 
curl from the positive x axis to the positive y axis through 90". This orientation of 
axes is sometimes conven~ent for describing depth of obpcts relative to a display 
screen. If screen locations are described in the xy plane of a left-handed system 
with the coordinate origin in the lower-left screen corner, positive z values indi- 
cate positions behind the screen, as in Fig. A-7(ah Larger values along the posi- 
tive z axis are then interpreted as being farther from the viewer. 

Three-Dimensional Curbilinear Coordinate Systems 

Any non-Cartesian reference frame is referred to as a curvilinear coordinate sys-p 
tern. The choice of coordinate system for a particular graphics application de- 
pends on a number of factors, such as symmetry, ease of computation, and visu- 

- - - . . . . 
F i p m  A-6 
Coordinate representation of a point P at position ( x ,  y, 
z )  in a right-handed Cartesian reference system. 



Figure A-7 
Left-handed Cartesian coordinate system superimposed on the surface 
of a video monitor. 

figure A -8 

* . ,' x, axis A general c u ~ l i n e a r  coordinate 
'*ye- reference frame. 

alization advantages. Figure A-8 shows a general curvilinear coordinate reference 
frame formed with three coordinate surfaces, where each surface has one coordl- 
nate held constant. For instance, the x,x2 surface is defined with x, held constant. 
Coordinate axes in any reference frame are the intersection curves of the coordl- 
nate surfaces. If the coordinate surfaces intersect at right angles, we have an or- 
thogonaI curvilinear coordinate system. Nonorthogonal reference frames are 
usefd for specialized spaces, such as visualizations of motions governed by the 
laws of general relativity, but in general, they are used less frequently in graphics 
applications than orthogonal systems. 

A cylindrical-coordinate specification of a spatial position is shown in Fig. A- 
9 in relation to a Cartesian reference frame. The surface of constant p i sa  vertical 

r axis 

I -. 

-- 

Figure A-9 
Cylindrical coordinates: p, 8, z 

Section A-1 

Coordinate Reference Framer 



Appendix A 

Figure A-10 
Spherical coordinates: r, 6.9. 

cylinder; the surface of constant 9 is a vertical plane containing the z axis; and the 
surface of constant z is a horizontal plane parallel to the Cartesian xy plane. We 
transform from a hlindrical coordinate specification to a Cartesian reference 
frame with the calculations 

x = pcost? y = psin8, z = z (A-4) 

Figure A-10 shows a spherical-coordinate specification of a spatial position in 
reference to a Cartesian reference frame. Spherical coordinates are sometimes re- 
ferred to as polar coordlmfes in space. The surface of constant r is a sphere; the sur- 
face of constant t9 is a vertical plane containing the z axis (same 8 surface as in 
cylindrical coordinates); and the surface of constant $J is a cone with apex at the 
coordinate origin. If 4 < 90â€¢â€¹, the cone is above the xy plane. If 4 > 90â€¢â€¹, the cone 
is below the xy plane. We transfrom from a sphericalcoordinate specification to a 
Cartesian reference frame with the calculations 

Solid Angle 

We define a solid angle in analogy with that for a two-dimensional angle 8 be- 
tween two intersecting lines (Eq. A-3). Instead of a circle, we consider any sphere 
with center position P. The solid angle o within a cone-shaped region with apex 
at P is defined as 

where A is the area of the spherical surface intersected by the cone (Fig. A-ll), 
and r is the radius of the sphere. 

Also, in analogy with two-dimensional polar coordinates, the dimension- 
less unit for solid angles is called the steradian. The total solid angle about a 
point is the total area of the spherical surface (4d) divided by #, or 4a 
steradians. 



-. 

Figure A-11 
A solid angle w subtended by a 
spherical surface patch of area A 
with radius r.  

A-2 
POINTS AND VECTORS 

There is a fundamental difference between the concept of a point and that of a 
vector. A point is a position specified with coordinate values in some reference 
frame, so that the distance from the origin depends on the choice of refer- 
ence frame. Figure A-12 illustrates coordinate specification in two reference 
frames. In frame A, point coordinates are given by the values of the ordered pair 
( x ,  y). In frame B, the same p i n t  has coordinates (0.0) and the distance to the ori- 
gin of frame B is 0. 

A vector, on the other hand, is defined as the difference between two point 
positions. Thus, for a two-dimensional vector (Fig. A-13), we have 

where the Cartesian components (or Cartesian elements) V, and V, are the projec- 
tions of V onto the x and y axes. Given two point positions, we can obtain vector 
components in the same way for any coordinate reference frame. 

We can describe a vector as a directed line segment that has two fundamental 
properties: magnitude and direction. For the two-dimensional vector in Fig. A- 
13, we calculate vector magnitude using the Pythagorean theorem: 

frame A 

Points and Vecton 

.- . . . . .. . -- 
I ; p w  A-72 
Position of pant P with respect to two different 
Cartesian reference frames. 



Figure A-13 
Vector V in the q plane of a Cartesian 
reference frame. 

The direction for this two-dimensional vector can be given in terms of the angu- 
lar displacement from the x axis as 

A vector has the same properties (magnitude and direction) no matter where we 
position the vector within a single coordinate system. And the vector magnitude 
is independent of the coordinate representation. Of course, if we change the coor- 
dinate representation, the values for the vector components change. 

For a three-dimensional Cartesian space, we calculate the vector magnitude 
as 

Figure A-14 
I v ~  = V C + ~ + V ;  (A-10) 

Direction angles a, /3, and y. 
Vector direction is given with the direction angles, a, p, and y, that the vector 
makes with each of the coordinate axes (Fig. A-14). Direction angles are the ps i -  

_--- tive angles that the vector makes with each of the positive coordinate axes. We . - calculate these angles as 

earth ,k V X  v cosa = --, cosp = 3, cosy == 2 (A-11) I v I IvI Ivl 
I 
I 
I 
I 
1 , O V, since ,,, The values cosa, cosp, and cosy are called the direction comes  of the vector. Actu- ally, we only need to specify two of the direction cosines to give the direction of 
i 

Figure A-15 Vectors are used to represent any quantities that have the properties of 
A gravitational force vector F magnitude and direction. Two common examples are force and velocity (Fig. 
and a velocity vector v. A-15). A force can be thought of as a push or a pull of a certain amount in a par- 

606 



Section A-2 
Points and Vectom 

T ~ O  vectors (a) can be added geometrically by positiorung the 
two vectors end to end (b) and drawing the resultant vector from 
the start of the first vector to the tip of the second vector. 

ticular direction. A velocity vector specifies how fast (speed) an object is moving 
in a certain direction. 

Vector Addition and Scalar Multiplication 

By definition, the sum of two vectors is obtained by adding corresponding com- 
ponents: 

Vector addition is illustrated geometrically in Fig. A-16. We obtain the vector sum 
by placing the start position of one vector at the tip of the other vector and draw- 
ing the sumination vector as in Fig. A-16. 

Addition of vectors and scalars is undefined, since a scalar always has only 
one numerical value while a vector has n numerical components in an n-dimen- 
sional space. Scalar multiplication of a thlPeaimensional vector is defined as 

For example, if the scalar parameter a has the value 2, each component of V is 
doubled. 

We can also multiply two'vectors, but there are two possible ways to do 
this. The multiplication can be carried out so that either we obtain another vector 
or we obtain a scalar quantity. 

Scalar Product of Two Vectors 

Vector multiplication for producing a scalar is defined as 

v , * V , =  I v , I  I~,lcos0, O ~ Q S T  
where tc is the angle between the two vectors (Fig. .4-17). This product is called figure A-17 
the scalar product (or dot product) of two vectors. It is also referred to as the ~ h ,  dot Drodua of two 
inner product, particularly in discussing scalar products in tensor analysis. Equa- veaors &tained by 
tion A-15 is valid in any coordinate representation and can be interpreted as the multiplying parallel 
product of parallel components of the two vectors. components. 



Appendix A In addition to the coordinate-independent form of the scalar product, we 
can express this product in speafic coordinate representations. For a Cartesian 
reference frame, the scalar product is calculated as 

The dot product of a vector with itself is simply another statement of the 
Pythagorean theorem. Also, the scalar product of two vectors is zero if and only 
if the two vectors are perpendicular (orthogonal). Dot products are commutative 

because this operation produces a scalar, and dot products are distributive with 
respect to vector addition 

Vector Product of Two Vectors 

Multiplication of two vectors to produce another vector is defined as 

where u is a unit vector (magnitude 1) that is perpendicular to both V, and V2 
(Fig. A-18). The direction for u is determined by the right-hand mle: We grasp an 
axis that is perpendicular to the plane of V, and V2 so that the fingers of the right 
hand curl from V, to V,. Our right thumb then points in the d i d o n  of u. This 
product is called the vector product (or cross product) of two vectors, and Equa- 
tion A-19 ie valid in any coordinate representation. The cross product of two vec- 
tors is a vector that is perpendicular to the plane of the two vectors and with 
magnitude equal to the area of the parallelogram formed by the two vectors. 

We can also express the cross product in terms of vector components in a 
s p d c  reference frame. In a Cartesian coordinate system, we calculate the com- 
ponents of the apss product as 

If we let u, 14, and u, represent unit vectors (magnitude 1) along the x, y, and z 
axes, we can write the cross product in terms of Cartesian components using de- 
terminant notation: 

Figure A-18 
The )le p d u d  of two vectors is 
a vector in a direction 
perpendicular to the two onginal 

v, vectors and with a magnitude equal m the area of the shaded 
parallelogram. 



Basis Vedors and the Metric 
(A -21 ,  ,",, 

The cross product of any two parallel vectors is zero. Therefore, the cross 
product of a vector with itself is zero. Also, the cross product is not commutative; 
it is anticommutative: 

And the cross product is not associative: 

But the cross product is distributive with resped to vector addition; that is, 

A-3 
BASIS VECTORS AND THE METRIC TENSOR 

We can specify the coordinate axes in any reference frame with a set of vectors, 
one for each axis (Fig. A-1-19]. Each coordinate-axis vector gives the direction of 
that axis at any point along the axis. These vectors form a linearly independent 
set of vectors. That is, the axis vectors cannot be written as linear combinations of 
each other. Also, any other vector in that space can be written as a linear combi- 

- < '3 
nation of the axis vectors, and the set of axis vectors is called a basis (or a set of - 
base vectors) for the space, in general, the space is referred to as a vector space Figure A-19 
and the basis contains the minimum number of vectors to represent any other Cumilinearcoordinate-axis 
vector in the space as a linear combination of the base vectors. vectors. 

Orthonormal Basis 

Often, vectors in a basis are normalized so that each vector has a magnitude of 1. 
In this case, the set of unit vectors is called a normal basis. Also, for Cartesian 
reference frames and other commonly used coordinate systems, the coordinate 
axes are mutually perpendicular, and the set of base vectors is referred to as  an 
orthogonal basis. if, in addition, the base vectors are all unit vectors, we have an 
orthonormal basis that satisfies the following conditions: 

u, . u, = 1, for all k 
uj - u, = 0, for all j #  k 

Most commonly used reference frames are orthogonal, but nonorthogonal coor- 
dinate reference frames are useful in some applications including relativity the- 
ory and visualization of certain data sets. 

For a two-dimensional Cartesian system, the orthonormal basis is 



Appendix A 

And the orthonormal basis for a three-dimensional Cartesian reference frame is 

Tensors are generalizat~ons of the notion of a vector. Spec~f~cally, a tensor is a 
quantity having a number of components, depending on the tensor rank and the 
dimension of the space, that satisfy certain transformation properties when con- 
verted from one coordinate representation to another. For orthogonal systems, 
the transformation properties are straightforward. Formally, a vector is a tensor 
of rank one, and a scalar is a tensor of rank zero. Another way to view this classi- 
fication is to note that the components of a vector are specified with one sub- 
script, while a scalar always has a single value and; hence, no subscripts. A ten- 
sor of rank two thus has two subscripts, and in three-dimensional space, a tensor 
of rank two has nine conqxments (three values for each subscript). 

For any general (curvilinear) coordinate system, the elements (or coeffi- 
cients) of the metric tensor for that space are defined as 

Thus, the metric tensor is of rank two and it is symmetric.: g,k = g,,. Metric tensors 
have several useful properties. The elements of a metric tensor can be used to de- 
termine (1) distance between two points in that space, (2) transformation equa- 
tions for conversion to another space, and (3) components of various differential 
vector operators (such as gradient, divergence, and curl) within that space. 

In an orthogona1 space: 

And in a Cartesian coordinate system (assuming unit base vectors): 

The unit base vectors in polar coordinates can bc, expressed in terms of 
Cartesian base vectors as 

Substituting these expressions into Eq. A-23, w~ obtain the elements of the metric 
tensor, which can be written in the matrix form: 

For a cylindrical rcwrdinate reference frame, the b~w: vectors are 



And the matrix representation for the metric tensor in cylindrical coordinates is Section A 4  
Matrices 

(A-34) 

We can write the base vectors in spherical coordinates as 

Then the matrix representation for the metric tensor in spherical coordinates is 

A-4 
MATRICES 

A matrix is a rectangular array of quantities (numbers, functions, or numerical 
expressions), called the elements of the matrix. Some examples of matrices are 

We identify matrices according to the number of rows and number of columns. 
For these examples, the matrices in left-to-right order are 2 by 3, 2 by 2, 1 by 3. 
and 3 by 1 .  When the number of rows is the same as the number of columns, as 
in the second example, the matrix is called a square matrir. 

In general, we can write an m by n matrix as 

where the a,k represent the elements of matrix A. The first subscript of any ele- 
ment gives the row number, and the second subscript gives the column number. 

A matrix with a single row or a single column represents a vector. Thus, the 
last two matrix examples in A-37 are, respectively, a row vector and a column vec- 
tor. In general, a matrix can be viewed as a collection of row vectors or as a col- 
lection of column vectors. 

When various operations are expressed in matrix form, the standard mathe- 
matical convention is to represent a vector with a column matrix. Following this 
convenfion, we write the matrix representation .for a three-dimensional vector in 



Appendix A Cartesian coordinates as 

We will use this matrix representation for both points and vectors, but we must 
keep in mind the distinction between them. It is often convenient to consider a 
pomt as a vector with start position at the coordinate origin within a single coor- 
dinate reference frame, but points do not have the properties of vectors that re- 
main invariant when switching from one coordinate system to another. Also, in 
general, we cannot apply vector operations, such as vector addition, dot product, 
and moss product, to points. 

Scalar Multiplication and Matrix Addition 

To multiply a matrix A by a scalar value s, we multiply each element aIk by the 
scalar. As an example, i f  

then 

Matrix addition is defined only for matrices that have the same number of 
rows m and the same number of columns n. For any two rn by n matrices, the 
sum is obtained by adding corresponding elements. For example, 

Matrix Multiplication 

n e  product of two matrices is defined as a generalization of the vector dot prod- 
uct. We can multiply an rn by n matrix A by a p by q matrix B to form the matrix 
product AB, providing that the number of columns in A is equal to the number 
of rows in B (i.e., n = p). We then obtain the product matrix by forming sums of 
the products of the elements in the row vectors of A with the corresponding ele- 
ments in the column vectors of B. Thus, for the following product 

we obtain an rn by q matrix C whose elements are calculated as 

In the following example, a 3 by 2 matrix is postmultiplied by a 2 by 2 ma- 
trix to produce a 3 by 2 product matrix: 



Vector multiplication in matrix notation produces the same result as the dot 
product, providing the first vector is expressed as a row vector and the second 
vector is expressed as a column vector: 

This vector product results in a matrix with a single element (a I-by-1 matrix). If 
we multiply the vectors in reverse order, we obtain a 3 by 3 matrix: 

As the previous two vector products illustrate, matrix multiplication, in 
general, is not commutative. That is, 

But matrix multiplication is distributive with respect to matrix addition: 

Matrix Transpose 

The transpose AT of a matrix is obtained by interchanging rows and columns. 
For example, 

For a matrix product, the transpose is 

Determinant of a Matrix 

For a square matrix, we can combine the matrix elements to produce a single 
number called the determinant. Determinants are defined wrsively. For a 2 by 
2 matrix. the second-order determinant is defined to be 



Appendix A We then calculate higher-order determinants in terms of lower-order determi- 
nants. To calculate the determinants of order 3 or  greater, we can select any col- 
umn k of an n by n matrix and compute thedeterminant as 

n 

detA = ~ ( - l ) ~ * ~ a ~ d e t ~ ~  
,:I 

where detAik is the (n-1) by (n- 1) determinant of the submatrix obtained from A 
by deleting the jth row and the kth column. Alternatively, we can select any row j 
and calculate the determinant as 

detA = ~ ( - l ) l + k a i k d e t ~ ; l  
k - 1  

Calculating determinants for large matrices (n > 4, say) can be  done more 
efficiently using numerical methods. One way to compute a determinant is to de- 
compose the matrix into two factors: A = LU, where all elements of matrix L that 
are above the diagonal are zero, and all elements of matrix U that are below the 
diagonal are zero. We then compute the product of the diagonals for both L and 
U, and we obtain detA by multiplying these two products together. This method 
is based on the following property of determinants: 

Another method for calculating determinants is based on Gaussian elimination 
procedures (Section A-9). 

Matrix Inverse 

With square matrices, we can obtain an inwrse matrix if and only if the determi- 
nant of the matrix is nonzero. If an inverse exists, the matrix is said to be a non- 
singular matrix. Otherwise, the matrix is called a singular matrix. For most prac- 
tical applications, where a matrix represents a physical operation, we can expect 
the inverse to exist. 

The inverse of an 11 by n square matrix A is denoted 2s A-I and 

where I is the identiy matrix. All diagonal elements of I have the value 1, and all 
other (off diagonal) elements are zero. 

Elements for the inverse matrix A-I can be calculated from the elements of 
A as 

(-1)'+"etAk, 
R,i' = 

det A 

where a;' is the element in the jth row and kth column of A-I, and Akj is the 
(n - 1) by (n -- 1) submalrix obtained by deleting the kth row and jth column of 
matrix A. Again, numerical methods can be used to evaluate the determinant 
and the elements of the inverse matrix for large values of 1 1 .  



A-5 M i o n  A-5 
COMPLEX NUMBERS Complex Numbers 

By definition, a complex number z is an ordered pair of real numbers: 

where x is called the real part of z, and y is called the imaginary part of z. Real 
and imaginary parts of a complex number are designated as 

Geometrically, a complex number is represented in the complex plane, as in Fig. 
A-20. 

Complex numbers arise from solutions of equations such a9 

which have no real-number solutions. Thus, complex numbers and complex 
arithmetic are set up as extensions of real numbers that provide solutions to such 
equations. 

Addition, subtraction, and scalar multiplication of complex numbers are 
carried out using the same rules as for ho-dimensional vectors. Multiplication of 
complex numbers is defined as 

This definition for complex numbers gives the same result as for real-number 
multiplication when the imaginary parts are zero: 

Thus, we can write a real number in complex form as 

Similarly, a pure imaginary number has a real part equal to O: (0, y). 
The complex number (0,l) is called the imaginn y unit, and it is denoted by 

imaginary axis I 

Figure A-20 
Position of a point z in the complex 

real axis plane. C 



m i x A  Eledrical engineers often use the symbol j for the imaginary unit, because the 
symbol i is used to represent elechical current. From the rule for complex multi- 
plication, we have 

Therefore, 12 is the real number - 1, and 

Using the d e  for complex multiplication, we can write any pure imaginary 
number in the form 

Also, by the addition d e ,  we can write any complex number as the sum 

Therefore, another qnexntation for a complex number is 

which is the usual form used in practical applications. 
Another concept assodated with a complex number is the complex conjugate: 

Modulus, or absolute wlue, of a complex number is defined to be 

l z l  =a=- (A-59) 

which gives the length of the "vectof' representing the complex number (i.e., the 
distance from the origin df the complex plane to point z). Real and imaginary 
parts for the division of two complex numbers is obtained as 

A particularly useful representation for complex numbers is to express the 
real and imaginary parts in terms of polar coordinates (Fig. A-21): 



We can also write the polar form of z as 

Figure A-21 
Polar coordinate position of a 
complex number z. 

where e is the base of the natural logarithms (e = 2.118281828. . .), and 
g8 = a s 0  + isin0 (A-63) 

which is Eulds  formula. Complex multiplications and divisions are easily ob- 
tained as 

And the nth roots of a complex number are calculated as 

The n roots Lie on a d e  of radius fi with center at the origin of the complex 
plane and form the vertices of a regular polygon with n sides. 

Complex number concepts are extended to higher dimensions with quaternione, 
which are numbers with one real part and three imaginary parts, written as 

where the coefficients a, b, and c in the imaginary terms are real numbers, and pa- 
rameter s is a real number called the scalar prt .  Parameters i, j, k are defined with 
the properties 

From these properties, it follows that 

jk = -kj = i ki = -ik = j 



Appendix A Scalar multiplication is defined in analogy with the corresponding opera- 
tions for vectors and complex numbers. That is, each of the four components of 
the quaternion is multiplied by the scalar value. Similarly, quaternion addition is 
defined as  

Multiplication of two quaternions is carried out using the operations in Eqs. A-66 
and A-67. 

An ordered-pair notation for a quaternion is also formed in analogy with 
complex-number notation: 

where v is the vector (a, b, c ) .  In this notarion, quaternion addition is expressed as 

Quaternion multiplication can then be expressed in terms of vector dot and cross 
products as  

As an extension of complex operations, the magnitude squared of a quater- 
nion is defined using the vector dot product as 

And the inverse of a quaternion is 

so that 

A-7 
NONPARAMETRIC REPRESENTATIONS 

When we write object descriptions directly in terms of the coordinates of the ref- 
erence frame in use, the respresentation is called nonparametric. For example, 
we can represent a surface with either of the following Cartesian functions: 

fix, y, Z )  = 0, or z = fix,  y) (A-74) 

The first form in A-74 glves an implicit expression for the surface, and the second 
form gives an explicit representation, with x and y as the independent variables, 
and with z as the dependent variable. 



Similarly, we can represent a three-dimensional curved line in nonparamet- M i o n  A-8 
ric form as the intersection of two surface functions, or we could represent the Parametric Repcsentltions 
curve with the pair of functions 

where coordinate x is selected as the independent variable. Values for the depen- 
dent variables y and z are then determined from Eqs. A-75 as we step through 
values for x from one line endpoint to theother endpoint. 

Nonparametric representations are useful in describing objects within a 
given reference frame, but they have some disadvantages when used in graphics 
algorithms. If we want a smooth plot, we must change the independent variable 
whenever the first derivative (slope) of either fix) or g(x) becomes greater than 1. 
This means that we must continually check values of the derivatives, which may 
become infinite at some points. Also, Eqs. A-75 provide an awkward format for 
representing multiple-valued functions. For instance, the implicit equation of a 
circle centered on the origin in the xy plane is 

and the explicit expression for y is the multivalued function 

In general, a more convenient representation for objc~t descriptions in graphics 
algorithms is in terms of parametric equations. 

A-8 
PARAMETRIC REPRESENTATIONS 

Euclidean curves are one-dimensional objects, and positions along the path of a 
three-dimensional curve can be described with a single parameter u. That is, we 
can express each of the three Cartesian coordinates in terms of parameter u, and 
any point on the curve can then be represented with the following vector point 
Function (relative to a particular Cartesian reference frame): 

Often, the coordinate equations can be set up so  that parameter u is defined over 
the unit interval from 0 to 1. For example, a circle in the xy plane with center at 
the coordinatc origin could be defined in parametric form as 

Other parametric fomx are also possible for describing circles and circular arcs. 
Curved (or plane) Euclidean surfaces are two-dimensional objects, and po- 

sitions on a surface can be described with two parameters u and v. A coordinate 
position on the surface is then represented with the parametric vector function 



Appendix A where the Cartesian coordinate values for x,  y, and z are expressed as functions of 
parameters u and v. As with curves, it is often possible to arrange the parametric 
descriptions so that parameters u and v arc defined over the range from 0 to 1. A 
spherical surface with center at the coordinate origin, for example, can be de- 
scribed with the equations 

X(U,D) = r s in(m) cos(2n-v) 

y(u,v) = r s in(m) sin(2m) 

z(u,v) = rcos(m) (A-79)  

where r is the radius of the sphere. Parameter u describes lines of constant lati- 
tude over the surface, and parameter v describes lines of constant longitude. By 
keeping one of these parameters fixed while varying the other over a subinterval 
of the range from 0 td 1, we could plot latitude and longitude lines for any spher- 

Figure A-22 ical section (Fig. A-22). 
Seaion of a spherical surfaw 
described by lines of constant 
u and lines of constant v in 
Eqs. A-79. A-9 

NUMERICAL METHODS 

In computer graphics algorithms, it is often necessary to solve sets of linear equa- 
tions, nonlinear equations, integral equations, and other Eunctional forms. Also, 
to visualize a discrete set of data points, it may be useful to display a continuous 
curve or surface function that approximates the points of the data set. In this sec- 
tion, we  briefly summarize some common algorithms for solving various numer- 
ical problems. 

Solving Sats of Linear Equations 

For variables rb k = 1,2, . . . , n, we can write a system of n linear equations as 

where the values for parameters at and bi are known. This set of equations can be 
exprrssed in the matrix form: 

with A as an n by n square matrix whose elements are the coefficients ajk, X as the 
column matrix of x, values, and B as  the column matrix of b, values. The solution 
for the set of simultaneous linear equation can be expressed in matrix form as 

X = A-'B (A-82) 

which depends on the inverse of the coefficient matrix A. Thus the system of 
equations can be solved if and only if A is a nonsingular matrix; that is, its deter- 
minant is nonzero. 



One method for solving the set ol equahons is c.'ramer's Rule: 

det A, 
X k  = - 

det A 

where A, 1s the matrix A with the kth column replaced with the elements of B. 
This method is adequate for problems with d few variables. For more than three 
or four variables, the method is extremely inefficient due to the large number of 
multiplications needed to evaluate each determinant. Evaluation o f a  single n by 
n determinant reauires more that n! multiolications. 

We can solve the system of equations more efficiently using variations of 
Gaussian rliminat~on. The basic ideas in Gaussian ellmination can be illustrated 
with the following set of two simultaneous equations 

To solve this set of equations, we can multiply the iirst equation by 3 ,  then we 
add the two equations to eliminate the x,  term, yielding the equation 

which has the solution x, = -13/2. This value can then be substituted into either 
of the original equations to obtain the solution for x,, which is 9. Efficient algo- 
rithms have been d e v w d  to carry out the elimination and back-substitution 
steps. 

Gaussian elimination is sometimes susceptable to high roundoff errors, and 
it may not be possible to obtain an accurate solution. In those cases, we may be 
able to obtain a solution using the Gauss-Seidel method. We start with an initial 
"guess" for the values of variables xi, then we repeatedly calculate successive ap- 
proximations until the difference between successive values is "small." At each 
iteration, we calculate the approximate values for the variables as 

If we can rearrange matrix A so that each diagonal element has a magnitude 
greater than the sum of the magnitudes of the other elements across that row, 
than the Gauss-Seidel method is guaranteed to converge to a solution. 

Finding Roots of Nonlinear Equations 

A root of a function f ( x )  is a value for x  that satisfies the equation f ( x )  = 0. One of 
the most popular methods for finding roots of nonlinear equations is the NPZLJ- 
ton-Raphson algorithm. This algorithm is an iterative procedure that approximates 
a function / (XI  with a straight line at each step of the iteration, as shown in Fig. 
A-23. We start wlth an initial "guess" x,, for the value of the root, then we ralru- 

Section A-9 

Numerical Methods 



tangent Figure A-23 
Approximating a curve at an initial 

/ 
I /  ;o I value xo with a straight h e  that is 
/ I tangent to the curve at that point. 

late the next approximation to the root as x ,  by determining where the tangent 
line from xo crosses the x axis. At x,, the slope (first derivative) of the curve is 

Thus, the next approximation to the root is 

We repeat this procedure at each calculated approximation until the difference 
between successive approximations is "small enough". 

If the Newton-Raphson algorithm converges to a root, i t  will converge 
faster than any other kot-finding method. But it may not always converge. For 
example, the method fails i f  the derivative f ' (x)  is 0 at some point in the iteration. 
Also, depending on the oscillations of the curve, successive approximation may 
diverge from the position of a root. The Newton-Raphson algorithm can be ap- 
plied to a function of a complex variable, fir), and to sets of simultaneous nonlin- 
ear functions, real or complex. 

Another method, slower but guaranteed to converge, is the bisectiorl tn~ tho t i .  
Here we need to first determine an x interval that contains a root, then we apply 
a binary search procedure to close in on the root. We first look at the midpoint of 
the interval to determine whether the root is in the lower or upper half of the in- 
terval. This procedure 1s repeated for each successive subinterval until tho differ- 
ence between successive midpoint positions is smaller than some preset value. A 
speedup can be attained by interpolating successive x  positions instead of halv- 
ing each subinterval (fnlsv-position method). 

Evaluating Integrals 

lnkgration is a summation process. For a function of a single variable x, the inte- 
gral of f i x )  is the area "under" the curve, as illustrated in Fig. A-24. 

An integral of f i x )  can be numerically approximated with the following 
summation 

where f k ( x )  is an approx~mation to fix) over the interval Ax,. For example, we can 
approximate the curve w ~ t h  a constant value in each subinterval and add the 
areas of the resultmg rectangles (Fig. A-25). The smaller the subdivisions for 
the interval from R to b,  the better the approximation (up to a point). Actually, if 



Srction A-9 

Numerical Methods 

-- 
Figure A-24 
The ~ntegral of f i x )  is equal to the 
amount of aRa between the 
function and the x axis over the 
interval from a to b. 

Figure A-25 
Approximating an integral as the sum of the areas of small 
rectangles. 

the intervals get too small, the values of successive rectangular areas can get lost 
in the roundoff error. 

Polynomial approximations for the function in each subinterval generally 
give better results than the rectangle approach. Using a linear approximation, we 
obtain subareas that are trapezoids, and the approximation method is then re- 
ferred to as  the trapezoid rule. If we use a quadratic polynomial (parabola) to ap- 
proximate the function in each subinterval, the method is called Simpson's rule 
and the integral approximation is 

where the interval from a to b is divided into n equal-width intervals: 

b - a  
Ax = -- 

where n is a multiple of 2, and with 

For functions with high-frequency oscillations (Fig. A-26), the approxima- 
tion methods previously discussed may not give accurate results. Also, multiple 
integrals (involving several integration variables) are difficult to solve with S i m p  



Figure A-26 
! - A function with high-fmpency 

a 6 x oscillations. 

son's rule or the other approximation methods. In these cases, we  can apply 
Monte Carlo integration techniques. The term Monte Carlo is applied to any 
method that uses random numbers to solve deterministic problems. 

We apply a Monte Carlo method to evaluate the integral of a function such 
as  the one shown in Fig. A-26 by generating n random positions in a rectangular 
area that contains fix) over the interval from a to b (Fig. A-27). An approximation 
for the integral is then calculated as  

where parameter n,,, IS the count of the number of random points that are be- 
tween f i x )  and the x axis. A random position ( x ,  y) in the rectangular region is 
computed by first generating two random numbers, r ,  and r2, and then canying 
out the calculations 

Similar methods can be applied to multiple integrals. 
Random numbers r ;  and r2 are uniformly distributed over the interval (0,l). 

We can obtain random numbers from a random-number function in a high-level 
language, or from a statistical package, or we can use the following algorithm, 
called the linear congruential generator: 

where parameters a, c, m, and i, are integers, and i, is a starting value called the 
seed. Parameter m is  chosen to be as large as possible on a particular machine, 
with values for a and c chosen to make the string of random numbers as  long as  
possible before a value is repeated. For example,on a machine with 32-bit integer 
representations, we can set m = 232, a = 1664525, and c = 1013904223. 

F i g u r ~  A-27 
A rectangular area enclosing a 
function K x )  over the interval (a, b). 



Fitting Curves to Data Sets Section A-9 
Numerical Methods 

A standard method for fittin% a funchon (linear or nonlinear) to a set of data 
points is the least-squares al&ithm. For a two-dimensional set of data points 
(xk, yk), k = 1, 2, ; . . , we first select a functional form fix), which could be a 
straight-line hnction, a polynomial function, or some other curve shape. We then 
determine the differences (deviations) between Jx) and the yk values at each xk 
and compute the sum of deviations squared: 

Parameters in the function fDr)  are determined by minimizing the expression for 
E. For example, for the linear function 

parameters a, and a, are assigned values that minimize E. We determine the val- 
ues for a, and a ,  by solving the two simultaneous linear equations that result 
from the minimization requirements. That is, E will be minimum if the partial de- 
rivative with respect to a, is 0 and the partial derivative with respect to a ,  is 0: 

Similar calculations are carried out for other functions. For the polynomial 

we need to solve a set of n linear equations to determine values for parameters a, 
And we can also apply least-squares fitting to functions of several variables 
f ix , ,  x,, . . . , x.,) that can be linear or nonlinear in each of the variables. 



Bibliography 

AKELEY, K. AND T. JERMOLUK (1988). ''High-Performance 
Polygon Rendering", in proceedings of SIGGRAPH '88, 
Computer Graphics, 22(4), pp. 239-246. 

AKELEY, K. (1993). "RealityEngine Graphics", in proceed- 
ings of SIGGRAPH '93, Computer Graphics Proceedings. 
pp. 109-116. 

AMANATIDES, J (1984). "Ray Tracing with Cones", in pro. 
ceedings of SIGGRAPH '84, Computer Graphics, 18(3). 
pp. 129-135. 

AMBURN, P., E. GRANT AND T. WHITED (1986). "Managing 
Geometric Complexity with Enhanced Procedural Mod- 
els", in pnxeedings of SIGGRAPH '86, Computer Graph- 
I C S ,  20(4), pp. 189-196. 

ANJYO, K., F USAMI AND T. KURIHARA (1992). "A Simple 
Method for Extracting the Natural Beauty of Hair", in 
proceedings of SIGGRAPH '92, Computer Graphics, 
26(2), pp. 111-120 

APPLE COMPUTER, INC. (19850. lnsrde Macintosh, Volume 1, 
Addison-Wesley, Reading, MA. 

APPLE COMPUTER, INC. (1987). Human lnterfacr Guidelines, 
The Apple Desktop Interfnce, Addison-Wesley, Reading. 
MA. 

ARVO, J .  ANDD. KIRK (1987). "Fast Ray Tracing by Ray Clas. 
sification", in proceedings of SICGRAPH '87, Computer 
Graphics, 21(4), pp 55-64 

ARVO, J. AND D. KIRK (1990). "Particle Transport and lmage 
Synthesis", in proceed~ngs of SICGRAPH '90, Computer 
Graphrcs, 24(4), pp. 63-66. 

ARVO, J., ED. (1991). Craphics Gcnls 11, Academic Press, Inc., 
San Diego, CA. 

ATHERTON, I' R. (1Y83). "A kan-Line H~dden  Surface R e  
moval Procedure fur Constructive Solid Geometrv". in 
proceedings of SIGGRAPH '83. Computer ~r&hics  
17(3), pp. 73-82. 

BARAF, D. (1989) "Analytical Methods for Dynamic Simu- 
lation of Non-Penetrating Rigid Bodies", in proceedings 
o f  SIGGRAPH '89, C ~ m p u t r r  Grflphics, 23(3), p p  
223-232. 

BARAFF, D. A N D  A. WITKIN (1992). "Dynamic Simulation of 
Non-Penetrating Flexible Bodies", in proceedings of 
SIGGRAPH '92, Co~njnitrr Graph~ci, 26(2), pp. 303-308. 

BARKANS, A. C. (1990). "High-speed, High-Quality, An- 
tialiwd Vector Generation", in proceedings of SIG- 
GUAPH '90, Computer Graphics, 24(4), pp. 319-326. 

BARNSLEY, M. F., A. JACQUIN, F. MALASSENT, ET AL. (1988). 
"Harnessing Chaos for Image Synthesis", in proceed- 
ings of SIGCRAPH '88, Computer Graphics, 22(4), pp. 
131-140. 

BARNSLEY, M. (1993). Fractals Everywhere, Second Edition, 
Academic Press, hc., San Diego, CA. 

BARR, A. H. (1981). "Superquadrics and Angle-Preserving 
Transformations", IEEE Computer Graphics and Applica- 
tions, 1(1), pp. 11-23. 

BARR, A. H. (1986). "Ray Tracing Deformed Surfaces", in 
proceedings of SIGCRAPH '86, Computer Graphics, 
20(4), pp. 287-296. 

BARSKY, B. A. AND J. C. B E A ~  (1983). "Local Control of Bias 
and Tension in Beta-Splinrs", ACM Transactions on 
Graphics, 2(2). pp. 109-134. 

BARSKY, B. A. (1984). "A Discription and Evaluation of Vari- 
ous 3-D Models", IEEE Computer Graphics nnd Applica- 
tions,4(1), pp. 38-52. 

BARZEL, R. AND A. H. BARR (1988). "A Modeling System 
Based on Dynamic Constraints", in proceedings of SIG- 
GRAPH 'R8, Computer Graphrrs, 22(4), pp. 179-188. 

BARZEL, R. (1992). Physically-Based Modelrng for Cornputer 
Graphics, Academic Press, Inc-., San Diego, CA. 

BAUM, D. R., 5. MANN, K. P. SMTH, ET AL. (1991). "Making 
Radiosity Usable: Automatic Preprocessing and Mesh- 
ing Techniques for the Generation of Accurate Radiosity 
Solutions", in proceedings o i  SIGGRAPH '91, Conlputer 
Graphrcs, 25(4), pp. 51-61. 

BECKER, 5. C., Mr. A. BARRFIT, AND D. R. OLSEN JR. (1991). 
"Interactive Measurement of Three-Dimensional Oh- 
jects Using a Depth Buffer and Linear Probe", ACM 
Transactions on Graphlcs, 10(2), pp. 201-207. 

DECKER, B. G. AND N. L. MAX (1993). "Smooth Transitions 
between Bump-Rendering AIgorithms", in prtreedings 
of SIGGRAPH '93, Complct~r Gral~hics Procecditrgs, pp. 
183-190. 

BEIER, T. A N D  S. NEELY (1992). "Feature-Based lmage Meta- 
morphosis", in proceedings of SIGGRAPH '92, Conr- 
putcr Gmphrcs, 26(2), pp. 35-42. 



BERGMAN, L., H. FUCHS, E. GRM~, FT AL. (1986). "Image 
Rendering by Adaptive Refinement", in proceedings of 
SICLRAPH '86, Computer Graphics, 20(4), pp. 29-38. 

BERCMAN, L. D., J. S. kc-, D. C. RICHARDSON, FT AL. 
(1993). 'mW-an Eploratory Molecular Visualization 
System with User-Definable Interaction Sequences", in 
proceedings of SlGCRAPH '93, Computer Graphics Pro- 
ceedings, pp. 1 17- 126. 

BEZIER, P. (1972). Numzricd Control: Mathemutics and Appli- 
cations, translated by A. R. Forrest and A. F. Pankhurst, 
John Wiley & Sons, London. 

BIER, E. A., S. A. MACKAY, D. A. S m m ,  FT AL. (1986). 
"SnapDragging", in proceedings of SlGGRAPH '86, 
Computer Graphics, 20(4), pp. 241-248. 

BIER, E. A., M. C. STONE, K. PIER, ET AL. (1993). '7001gla5S 
and Magic Lenses: The See-Through Interface", in pro- 
ceedings of SIGGRAPH '93, Computer Graphics Pmceed- 
ings, pp. 73-80. 

BISHOP, G. AND D. M. WIEMER (1986). "Fast Phong Shading", 
in proceedings of SIGGRAPH '86, Computer Graphics, 
20(4), pp. 103-106. 

BLAKE, I. W. (1993). PHIGS and PHIGS Plus, Academic 
Press, London. 

BLESER, T. (1988). "TAE Plus Styleguide User Interface De- 
scription", NASA Goddard Space Flight Center, Green- 
belt, MD. 

BI.INN, J. F AND M,. E. NEWELL (1976). 'Texture and Reflec- 
tion in Computer-Generated Images", CACM, 19(10), 
pp. 542-547. 

BLINN, J. F. (1977). "Models of Light Reflection for Com- 
puter-Synthesized Pictures", Computer Graphm, 11(2), 
p p  192-198. 

BLINN, J. F. AND M. E. NEWELL (1978). "Clipping Using Ho- 
mogeneous Coordinates", Computer Graphics, 12(3), 
pp. 245-251. 

BLINN, I. F. (1978). "Simulation of Wrinkled Surfaces", 
Computer Graphics, 12(3), pp. 286-292. 

BLINN, J. F. (1982). "A Generalization of Algebraic Surface 
Drawing", ACM Transactions on Graphics, 1(3), pp. 
235 -256. 

BLLVN, J. F. (1982). "Light Reflection Functions for Simula- 
tion of Clouds and Dusty Surtaces", in proceedings of 
SlGGRAPH '82, Computer Graphics, 16(3), pp. 21-29. 

BLINN, J. F. (1993). "A Trip Down the Graphics Pipelme: 
The Homogeneous Perspective Transform", IEEE Com- 
puter Graphics and Applrcations, 13(3), pp. 75-80. 

BLOOMEPITHAL, J. (1985). "Modeling the Mighty Maple", in 
proceedings of SIGGRAPH '85, Computer Graphics, 
19(3), pp. 305 -312. 

bNO, P. K., J. L. ENCARNACAO, E R. A. HOPCOOD, ET A L .  
(1982). "GKS. The First Graphics Standard", IEEE Com- 
puler Graphics unrl Applicatiuns, 2(5), pp. 9-23. 

~ ~ 

BCOTH, K. S., M. P. BRYDEN, W. B. COWAN, ET AL. (1987). "On 
the Parameters of Human Visual Performance: An In- 
vestigation of the Benefits of Antialiasing", lEEE Corn- 
puler Graphics and Applications, 7(9), pp. 34-41 

BRESENHAM, J. E. (1965). "Algorithm for Computer Control 
of A Digital Plotter", IBM Systems Journal, MI), pp. 
25-30. 

BRESWHAM, J. E (1977). "A Linear Algorithm for Incremen- 
tal Digital Display of Circular Arcs", CACM, 20(2), pp. 
100-106. 

BROOKS, F, P., ]R. (1986). "Walkthrough: A Dynamic Graph- 
ics System for Simulating Virtual Buildings", Interactive 
3D 1986. 

BROOKS, F. P., JR. (1988). "Grasping Reality Through Illu- 
sion: Interactive Graphics Serving Science", CHI '88, pp. 
1-11. 

BROOKS, J., P. FREDERICK, M. OUH-YOUNG, J. J. BAITER, El' AL. 
(1990). "Projed GROPE - Haptic Display for Scientific 
Visualization", in proceedings of SIGGRAPH '90, Com- 
puter Graphics, 24(4), 24(4), pp. 177-185. 

BROWN, M. H. AND R. SECGEWICK (1984). "A System for Al- 
gorithm Animation", in proceedings of SIGGRAPH '84, 
Computer Graphm, 18(3), pp. 177-186. 

BROWN, J. R. AND S. CUNNINGHAM (1989). Programming the 
User Interface, John Wiley & Sons, New York. 

BRUDERLIN, A. AND T. W. CALVERT (1989). "Goal-Directed, 
Dynamic Animation of Human Walking", in proceed- 
ings of SlGGRAPH '89, Computer Graphics, 23(3), pp. 
233-242. 

BRUNET, P. AND I. NAVAZO (1990). "Solid Representation and 
Operation Using Extended Octrees", ACM Transactions 
on Craphics. 9(2), pp. 170-197. 

BRYSON, S. AND C. LEVIT (1992). 'The Virtual Wind Tunnel", 
IEEE Computer Graph~cs 2nd Applications, 12(4), pp. 
25-34. 

BURT, P. J. AND E. H. ADEWK (19831. "A Multiresolution 
Spline with Application to Image Mosaics", ACM 
Transactions on Graphics, ?(4), pp. 21 7-236. 

BUXTON, W., M. R. LAMB, D. SHERMAN, ET AL. (1983). "TO- 
wards a Comprehensive User Interface Management 
System", in proceedings of SIGGRAPH '83, Computer 
Graphics, 17(3), pp. 35-42. 

BUXTON, W., R. HILL, AND P. ROWLEY (1985). '%sues and 
Techniques in Touch-Sensitive Tablet Input", in pro- 
ceedings of SlGGRAPH '$5, Computer Graphics, 19(3), 
pp. 215-224. 

CALVERT, T., A. BRUDERLIN, J .  DILL, ET AL. (1993) "Desktop 
Animation of Multiple Human Figures", IEEE Computer 
Graphics and Applications, l3(3), pp. 18-26. 

CAMBELL, G., 'r. A. DEFANTI, ]. FREDERIKSEN, FT AL. (1986). 
"Two Bit/Pixel Full-Color Encoding", in proceedings of 
SIGGRAPH '86, Computer Graphics, 20(4), pp. 215-224. 

CAMPBILL, Ill., A. T. AND D. S. FUSSELL (1990). "Adaptive 
Mesh Generation for Global Diffuse Illumination", in 
proceedings of SIGGRAPH '90, Computer G ruphics, 
24(4), pp. 155-164. 

CARD, S. K., I. D. MACKINLAY, 4 N D  G. G. ROBERMN (1991). 
"The Information Visualizer, an lnformat~on Work- 
space", CHI '91, pp. 181-188. 



CARICNAN, .U., Y. YAK, N. M. T H A L M A N ~ ,  ST 4~ (1992) 
"Dressing Animated Synthetic Actors w ~ t h  Co plex 
Deformable Clothes", in proceedmgs of SICCRAPE '42, 
Computer Graphics, 26(2), pp. 99-104. 

C A R L ~ M ,  I., I. C H A K R A V A ~ ,  AND D. VANDEKSCHEL (1985) 
"A Hierarchical Data Stmcture for Representing thc 
Spat~al  Decomposition of 3-D Objects", IEEE Cornpuler 
Graphrcs m d  Applica!ions, 5(4), pp. 24-31. 

CARPENTER, L (1984). "The A-Buffer: An Antialiased Hid- 
den-Surface Method", in proceedings of SIGGRAPH 
'84, Computer Graphus, 18(3), pp. 103-108. 

CARROLL, j. M A N D  C. CARRTTHERS (1984). "Tralnlng Wheels 
In a User Interface", CACM, 27W. p p  800-806 

CASALE M. S. AND E. L. STANTON (1985). "An Overview of 
Analytic Solid Modeling", IEEE Cornpuler Grayihirs nlid 
Applications, 5(2), pp. 45-56. 

CATMULL, E. (1975). "Computer Display of Curved Sur- 
faces", in proceedings of the IEEE Conference on  Com 
puter Graphics, Pallern Recogn~f~on and Datn Sfructur~s 
Also in Freeman (1980), PP. 309-315. 

CATMULL, E. (1984)- "An Analytic Visible Surface Algo. 
rithm for Independent Pixel Processing", in proceed 
ings of SIGGRAPH '84, Computer Gmphirs, IR(3), pp 
109-115. 

CHUELLE, B. AND J.  INCERPI (1984). "Triangulation and 
Shape Complexity", ACM Transurtiorrs on Gmphics, 3(?), 
p p  135-152. 

CHEN, M., S. J. MOUNTFORD, AND A. SELLEN (1988). "A Study 
in Interactive 3D Rotation Usinc: 2D Control Devices". 
in proceedings of SIGGRAPH '88, Conrprtfer Gmphic-s. 
22(4), p p  121'- 130. 

CHEN, S. E., H. E. RUSHMEIER, G. MILLER, ET AI.. (1991). "A 
Progressive Multi-Pass Method for Global Illumina- 
tion". in proceedings of SIGGRAPH '91, Computer 
Graphic-s, 25(4), pp. 165-174. 

CHIN, N. AND S. FEINER (1989). "Near Real-Time S h a d o ~  
Generation Using ESP Trees", in proceedings of S ic -  
GRAPH '89, Computer Graphics, 23(3), pp. 99-106. 

CHVANC, R. AND G. ENTIS (1983). "%I3 Shaded Computer 
Animation-Step by Step", IEEE C o n ~ p f r r  Graphlcs nnd 
Appl~cntinns, 3(3), pp. 18-25. 

CHUNC, J. C ,  ET AL. (1989). "Exploring Virtual Worlds with 
Head-Mounted Visual Displays", Procc~eh'rrgs o j  SPlE 
Meeting on Non-Holographic True 3-Ditncnsional Displau 
Technologies, 1083, January 1989, pp. 15-20. 

CLARK, J. H. (1982). "The Geometry Engine: A VLSl Georn- 
etry System for Graphics", in proceedings of SIG- 
GRAPH '82, Computer Graphics, 16(3), pp. 127-133. 

COHEN, M. AND D. P. GREENBERG (1985). "The Herni~ 
Cube: A Radiosity Solution for Complex Environ- 
ments", in proceedings of SIGGRAPH '85, Computl,~ 
Gmphics. 19(3), pp. 31-40. 

COHEN, M. F ,  S E. CHEN, J .  R. WALLACE, ET AL. (1988). "A 
Progressive Refinement Approach to Fast Radiosity 
Image Generation", in proceedings of SIGGRAPH '88 
Compuler Crnphlcs, 22(4), pp. 75-84. 

C ' O H F ~ ,  M. F. AND 1. R. WALLAC!. (1993'1. Rad~osilynnd Rcalis- 
1 1 ~ -  Image Synthesrs, Academic Press. Boston, MA. 

COOK, R. L. AND K. E. TORRANCF (1982). "A Reflectance 
Model for Computer Graphics", ACM Transncfions on 
Graphics, 1(1),  pp. 7-24. 

CCXIK, R. L., T. PORTER, AND L CARPENTER (1984). "Distrib- 
uted Ray Tracing", in proceedings of SlGGRAPH '84, 
iompufer Gruphics, 18(3), pp 137-145 

COOK, R. L. (1984). "Shade Trees", in proceedings of SIG- 
GRAPH '84, Computer Grnpl'~cs, 180). pp. 223-231. 

C o k ,  R. L. (1986) "Stochasw Sampling in Computer 
Graphics", ACM Tran.wcl~iw~s on Gmphics, 6(1), pp. 
51-72. 

CWK, R. L., L. CARPEMER, A" E. CAT~.IUI.L (1987) "The 
Reyes lmage Rendering Alch~tecture", in proceedings 
of SIGGMPH '87, Comp~rlei Graphics, 21(4), pp. 95-102. 

COOUILLART, S AND P J A ~ C E M  (1991). "Animated  free^ 
Form Deformation. An Interactive Animation Tech- 
niyue", In proceedings of SICGRAPH '91, Computer 
Graphics, 25(4), pp. 23-26. 

CHO\\, F. C. (1977). "The Al~ah~ng  Problem i l l  Computer- 
Synthesized Shaded 1maKe;;', CACM, 20(llj, pp. 
799-805. 

C R O ~ ' ,  F. C. (1977). "Shadow Algorithms for Computer 
Graphics", In proceedings of SIGGRAPH '77, Computer 
Grnphics, 1 1  (2), pp. 242-248. 

C'aorv, F. C. (1978) "The Use of Gravscale for Improved 
Raster Display of Vectors and Characters", in proceed- 
ings of SICGRAPH '78, Crvnpulcr Gmphics, 12(3), pp. 
1-5. 

CRO~Y, F. C. (1381). "A Comparison of Antialiasing Tech- 
niques", IEEE Computer Grrrphirs and Applicalions, 1(1), 
PC. 40-49. 

CROW, F. C. (1982). "A More Fkx~ble Image Generation En- 
vironment". in proceedings of SlGGRAPH '82, Conl- 
prtcr Graphrs. 16(3), pp 9-18, 

CKLZ-NEIRA, C., D. J. S A ~ D I ~ ,  AND T. A. DEFANTI (1993). 
"Surround-Screen Projectio11-Based L'irtual Realltv: The 
Design and Implementatior. of the CAV E ,  in proceed- 
ings of SIGGRAPI1 '93, Co~rrptler Cntpliics Pmcerdings, 
pp. 135-142. 

CUKNINCHAM, 5, N. K. CRAIGHILL, M. W. FONG, ET AL., ED. 
(1992). Computer Grnphic~ Using Objec-1-Orieutcd Pro- 
~rflmming, John Wiley & Sons, New York. 

CULER, E., D. GILLY, AND T O'REILLY, ED (1992). T11t X Win- 
d11i1, Systrm in n Nurshell, Second Edition, OReilly & 
Assoc., Inc.. Sebastopol, CA 

Cwivs, M. AND J. BECK 11976). "Generalized Two- and 
Three-Dimensional Clipping", Computers and Graph- 
ics. 3(1), pp. 23-28. 

DAY, A. M. (1990). "The Implementation of an Algorithm to 
Fmd the Convex Hull of a Set of Three-Dimensional 
Points", ACM Transnctionc or! Graphics, 9(1), pp. 105-132. 

DE REFFYE, P., C. EDELIN, J. FRANCON, ET AL. (1988). "Plant 
Models Faithful to Botanical Structure and Develop 
ment", in proceedings of SIGGRAPH 'M, Cotnputer 
Gmphics, 2214). pp. 151-158. 



DEERING, M. (1992). "High Resolution Virtual Reality", in 
proceedings of SIGGRAPH '92, Computer Graphkj, 
26(2), pp. 195-202. 

DEERINC, M. F. AND S. R. NELSON (1993). "Leo: A System for 
Cost-Effective 3D Shaded Graphics", in proceedings ol 
SIGGRAPH '93, Computer Graphics Proceedings, pp 
101-108. 

DEMKO, S., L. HODGES, AND B. NAYLOR (1985). "Construction 
of Fractal Objects with Iterated Function Systems", in 
proceedings of SICCRAPH '85, Computer Graph~cs, 
19(3), pp. 271 -278. 

DEPP, S. W. AND W. E. HOWARD (1993). "Flat-Panel Dis- 
plays", Scientific American, 266(3), pp. 90-97. 

DEROSE, T. D. (1988). "Geometric Continuity, Shape Para- 
meters, and Geometric Constructions for Catrnull-Rom 
Splines", ACM Transactions on Gmphics, 7(1), pp. 1-41. 

D I G I ~ L  EQU~PMENT COW. (1989). "Digital Equipment Cor- 
poration XU1 Style Guide", Maynard, MA. 

DIPPE, M. AND 1. SWENSEN (1984). "An Adaptive Subdiv~- 
sion Algorithm and Parallel Architecture for Realistic 
Image Synthesis", in procdings  of SIGGRAPH '84. 
Compuler Grnphics, 18(3), pp. 149-158. 

DOBKIN, D., L. GUIBAS, j. HERSHBERCER, !zr AL. (1988). "An 
Efficient Algorithm for Finding the CSG Representation 
of a Simple Polygon", in proceedings of SIGGRAPH '88, 
Computer Grnphics, 22(4), pp. 31-40. 

-R, I.. J .  AND J .  G. TORBERG (i381). "Display Tech- 
niques for Octree-Encoded Obpas", IEEE Computer 
Graphics arid Applications, 1 (3), pp. 29-38. 

DORSEY, 1. O., F. X. SILLION, AND D. E GREENBERG (1991) 
"Design and Simulation of Opera Lighting and Projec.. 
tion Effects", in proceedings of SlGGRAPH '91, Cmr1- 
puler Graphics, 25(4), pp. 41-50. 

DREBIN, R. A., L. CARPENTER, AND P. HANRAHAN (1988) 
"Volume Rendering", in proceedings of SIGGRAPH '8R. 
Computer Graphics, 22(4), pp. 65-74. 

DUFF, T. (1985). "Cornpositing 3D Rendered Images", In 
proceedings of SIGGRAPH '85, Cbmputer Graphiri. 
19(3), pp. 41-44. 

DURRETT, H. I., ED. (1987). Color and the Computer, Academic. 
Press, Boston. 

DWANENKO, V. (1990). "Improved Line Segment Clipping", 
Dr. Dobb's Journal, July 1990. 

DYER, S. AND S. W H ~ A N  (1987). "A Vectorized Scan-Lint! 
Z-Buffer Rendering Algorithm", IEEE Computer Graph. 
ics and Applications, 7(7), pp. 34-45. 

DYER, 5. (1 990). "A Dataflow Toolkit for Visualization", 
IEEE Computer Graphics and Applications, 10(4), pp. 
60-69. 

EARNSHAW, R. A., ED. (1985). Fundamental Algorithms for 
Computer Graphics, Springer-Verlag, Berlin. 

E ~ F ~ S R R ~ N N E R ,  H. (1987). Algorithms in Computatioiial 
Geomehy, Springer-Verlag, Berlin. 

EDEISBRUNNER, H. AND E. P. MUCKE (1990). "Simulation of 
Simplicity: A Technique to Cope with Degenerate Cases 
in Geometric Algorithms", ACM Transactions on Gmpli- 
ics. 9(1), pp. 66-104. 

ELBER, G. AND E. COHEN (1990). "Hidden Curve Removal 
for Free Fonn Surfaces", in proceedings of SICCRAPH 
'90, Compuler Graphics, 24(4), pp. 95-104. 

ENDERLE, G., K. KANSY, AND C.  RAFF (1984). Computer 
Graphics Programming: GKS-The Graphics Standard, 
Springer-Verlag, Berlin. 

FARIN, G. (1988). Curws and Surfoces for Computer Aided Geo- 
metric Design, Academic Press, Boston, U. 

FAROUKI, R. T. AND J. K. HINDS (1985). "A Hierarchy of Ceo- 
metric Forms", IEEE Computer Graphics and Applications, 
5(5), pp. 51-78. 

FEDER, J. (1988). Fractals, Plenum Press, New York. 
FEINER, S., S. NACY, AND A. VAN DAM (1982). "An Experi- 

mental System for Creating and Presenting Interactive 
Graphical Documents", ACM Transacfions on Graphics, 
1 (I), pp. 59-77. 

FERWERDA, 1. A. AND D. P. GREENBERG (1988). "A Psy- 
chophysical Approach to Assessing the Quality of An- 
tialiased Images", IEEE Compute Graphics and Applica- 
tions, 8(5), pp. 85-95. 

FISHKIN, K. P. AND B. A. BARSKY (1984). "A Family of New 
Algorithms for Soft Filling", in proceedings of SlG- 
GRAPH '84, Computer Gmphics, 18(3), pp. 235-244. 

RUME, E. L. (1989). The Mathemnticnl Structure of Raster 
Grnphics, Academic Press, Boston. 

FOLEY, J. D., V. L. WALLACE, AND P. CHAN (1984). 'The 
Human Factors of Computer Graphics interaction Tech- 
niques", IEEE Comprrter Graphics and Applications, 4(11), 
pp. 13-48. 

FOLEY, j. D. (1987). "Interfaces for Advanced Computing", 
Scientific American, 257(4), pp. 126-135. 

FOLEY, J .  D., A. VAN DAM, 5. K. FHNER, ET AL. (1990). Com- 
puter Graphics: Principles a t ~ d  Practice, Addison-Wesley, 
Reading, MA. 

FOURNIER, A., D. FUSSEL, AND L. CARPEN~ER (1982). "Com- 
puter Rendering of Stochastic Models", CACM, 25(6), 
pp. 371-381. 

FOURNIER, A. AND D .  Y. MONTUNO (1984). "Triangulating 
Simple Polygons and Equivalent Problems", ACM 
Transactions on Grnphics, 3(2), pp. 153-174. 

F O U ~ E R ,  A. AND W. T. REEVES (1986). "A Simple ~ o d h l  of 
Ocean Waves", in proceed~ngs of SlGGRAPH '86, Com- 
puter Graphics, 20(4), pp. 75-84. 

- ~ 

FOURNIER, A. AND D. FUSSELL (1988). "On the Power of the 
Frame Buffer", ACM Tra~tsoctions on Graphics, 7(2), pp. 
103-128. 

FOURNIER, A. AND E. FIUME (1988). "Constant-Time Filtering 
with Spacevariant Kernels", in proceedings of SIG- 
GRAPH '88, Computer Gmphics, 22(4), pp. 229-238. 

FOWLER, D. R., H. MEINHARUT, AND I! PRUSLVKlEWlU (1992). 
'Modeling Seashells", in proceedings of SlGGRAPH 
'92, Compuler Graphics, 26(2), pp. 379-387 

Fox, D. AND M. WAITE (1984). Computer Anirrmtion Primer, 
McGraw-Hill, New York. 

FRANCIS, G. K. (1987). A Toprlogical Picturebook, Springer- 
Verlag, New York. 



FRANKLIN, W. R. AND M. S. KANKANHALLI (1990). "Parallel 
Ohm-Space Hidden Surface Removal", in proceedings 
of SIGGRAPH '90, Computer Graph~cs, 24(4), pp. 87-94. 

FREEMAN, H. ED. (1980). Tutorinl and Selected readings in In- 
teractive Computer Graphics,' IEEE Computer Society 
Press, Silver Springs, MD. 

FRENKEL, K. A .  (1989). "Volume Rendering", CACM, 32(4), 
pp. 426-435. 

FNEDER, G., D. GORDON, AND R. A. &WOLD (1985). "Back- 
to-Front Display of Voxel-Based Objects", IEEE Com- 
puter Graphics and Applications, 5(1), pp. 52-60. 

FRIEDHOFF, R. M. AND W BWZON (1989). The Second Com- 
puter Kevolut~on: Visualization, Harry N. Abrams, Inc., 
New York. 

FUCHS, H., S. M. PIZER, E. R. HEINZ, S. H. B ~ M B E R ,  L. TSAI, 
AND D. C. STRICKLAND (1982). "Design of and lmage 
Editing with a Space-Filling Three-Dimensional Display 
Based on a Standard Raster Graphics System", Proceed- 
ings of SPIE, 367, August 1982, pp. 117-127. 

F u c ~ s .  H , j. POULTON, J. EYLES, ET AL. (1989). "Pixel-Planes 
5: A Heterogeneous Multiprocessor Graphics System 
Using Processor-Enhanced Memories", in proceedings 
of SlGGRAPH '89, Computer Graphics, 23(3), pp. 79-88. 

FUJIMOTO, A. AVD Kt IWATA (1983). "Jag-Free Images on 
Raster Displavs", IEEE Computer Graphics and Applica- 
tions, 3(9), pp. 26-34. 

~ k ~ t l o v s a ~ ,  T. A. AND C. H. SEQUIN (1993). "Adaptive Dis- 
play Algorithms for Interactive Frame Rates During Vi- 
sualization Complex Virtual Environments", in prm 
ceedings of SIGGRAPH '93, Computer Gruphics 
Proceedings, pp. 247-254. 

GALYEAN, T. A .  ALL) J. F.  HUGH^ (1991). "Sculpting: An In- 
teractive Volumetric Modeling Technique", in proceed- 
ings of SIGGRAPH '91, Compritrr Graphrcs, 25(4?, p p  
267-274. 

GARDNER, G. Y (1985). "Visual Simulation of Clouds", in 
proceed~ngs of SIGGRAPH '85, Computer Gruphrcl;, 
19(3), pp. 297-334. 

GASCUEL, M.-P (1993). "An Implicit Formulation for PR- 
cise Contact Modeling between Flexlble Sohds", In pro- 
ceedings of SIGGRAPH '93, Computer Gmphics, pp. 
313-320. 

GASKINS, T. (1992). PHlGS IJrogrnmrnhrg Manunl, O'Redly & 
Associates. Sebastopol, CA. 

GHARACHORLOO, hl, S. Ch'TA, R. F. SPROIJLL, ET AL. (1989). 
"A Characterization of Ten Rasterization Algorithms", 
in proceedings of SICGRAPH '89, Compnter Grrrphics, 
23(3), pp. 355-368. 

GIRARD, M. (1987). "Interactrve Des~gn of 3D Computer- 
Anrmafed Legged Animal Motion", IEEE Comprtler 
Graphics und Applications, 7(6), pp. 39-51. 

GLASSNER, A. 5. (1984). "Space Subdivision for Fast Ray 
Tracing", IEFF Coniprricr Grflphrcs 2nd Applicafim7s, 4(10), 
pp. 15-22. 

G L M V ~ K ,  A. S. (19Xh). "Adaptive Precision in Texture 
Mapping", In procrcdlngs of SIGGRAPH '86, Computer 
C,wplrri'.q, 20(43. pp. 297-300. 

GLASSNER, A. S. (1988). "Spacetime Ray Tracing for Anima- 
tion", IEEE Computer Graphics and Applications, 8(2), pp. 
60-70. 

GLASSNER, A. S., ED. (1989). An lntmductron to Ray Tracing, 
Academic Press, San Diego, <:A. 

GLASSNER, A. S., ED. (1990). Graphrcs Gems, Academic Press, 
San Diego, CA. 

GLASSNER, A. S. (1992). "Geometric Substitution: A Tutor- 
ial", IEEE Computer Graphics and Applications, 12(1), pp. 
22-36. 

GLASSNER, A. S. (1994). Principles of Digital Imnge Synthesis, 
M o r g a n - K a h a m ,  Inc., New York 

GLEICHER, M. AND A. W m  (1992). "Through-the-Lens 
Camera Control", in proceedings of SIGGKAPH '92, 
Computer Graphics, 26(2), pp. 331 -340. 

GOLEMITH, J.  AND 1. SALMON (1987). "Automatic Creation 
of Object Hierarchies for Ray Tracing", IEEE Compuler 
Graphics and Applications, 7(5), pp. 14-20. 

GONZALEZ, R. C. AND P. WINTZ (1987). Digital lmnge Process- 
ing, Addison-Wesley, Reading, MA. 

GOOD, D. M., J. A. W H ~ I D E ,  D R. WRON, AND S. I. JONES 
(1%). "Building A User-Derived Interface", CACM, 
27(10), ~ p .  1032-1042. 

GOODMAN, T. AND R. SPENCE (1978). 'The Effect of System 
Response Time on Interactive Computer-Aided Prob- 
lem Solving", in proceedings of SICGRAPH '78, Com- 
puter Graphics, 12(3), pp. 100-104. 

GORAL, C. M., K.  E. TORRANCE. D. P. GREENBERG, ET AL. 
(1984). "Modeling the Interaction of Light Beheen Dif- 
fuse Surfaces", in proceedings of SIGGRAPH '84, Com- 
priter Graphics, 18(3), pp. 213-222. 

GORDON, D. ANDS. CHEN (1991). "Fronl-lo-Back Displdy of 
BSIJ Trees", 1EEE Computcr Graphics and Appl~cntrons, 
lli5). pp. 79-85. 

GORTLER, S. I., P. SCHRODER, M F. COHEN, I3 AL. (1993). 
"Wavelet Radiosity", in proceedings of SIGGRAPH '93, 
Comptrter Graphics Proceedin~s, pp. 221-230. 

GREEN, M. (1985). "The University of Alberta User Interface 
Management System", in prnceedings of SIGGRAPH 
'85, Cornputer Graphics, 19(3), pp. 205-214. 

GREENE, N., M. KASS, AND G. MILLER (1993). "Hierarchical 
Z-Buffer V~sibility", in proceedings of SIGCRAPH '93, 
Computer Graphics Proceedrngs, pp. 231 -238. 

HAERERLI, P. AND K. A K E L ~ Y  (1990). "The Accumulation 
Buffer: Hardware Support ior H~gh-Quality Render- 
ing", in proceedings of SIGGMPH '90, Cornp~rtcr Graph- 
ics. 24(4), pp. 309-318 

HAHN, J .  K. (1988). "Realistic Animation of Rigid Dodies", 
in proceedings of SIGGRAPH '88, Computer G~.nphics, 
22(4), p p  2W-308. 

HALL, R. A. AND D. P. GREENBERG (1983) "A Testbed for Re- 
a l i s t ~ ~  Image synthesis", IEI'E Computer Graphrsr and 
Applications, 3(8), pp. 10-20. 

HALL, R (1989). Illuminatior: Otr r i  Color irr Corrlpritrr &wt-r- 
atcd Imnxery, Springer-Verlag, New Ynrk. 



HANRAHAN, P (1982) "Creating Volume Models from 
Edge-Vertex Graphs", in proceedings of SIGGRAPH 
'82, Computer Gmphics, 16(3), pp. 77-84. 

HANRAHAN, P. A N D  1. LAWSON (1990). "A Language for 
Shading and Lighting Calculations", in proceedings of 
SIGGRAPH '90, Computer Graphics, 24(4), pp. 289-298. 

HART, J. C., D. J. SANDIN, AND L. H. KAUFFMAN (1989). "Ray 
Tracing Deterministic 3D Fractals", in proceedings ot 
SIGGRAPH '89, Computer Graphics, 23(3), pp. 289-296. 

HART, J C. ANT T. A. DEFANTI (1991). "Efficient Antialiasrd 
Rendering of S D  Linear Fractals", in proceedings of 
SIGGRAPH '91, Computer Graphics, 25(4), pp. 91 -100. 

HE, X. D., P.O. HEYNEN, R. L. PHILLIPS, FTAL. (1992). "A Fast 
and Accurate Light Reflection Model", in proceedings 
of SIGCRAPH '92, Computer Graphics, 26(2), pp. 
253-254. 

HEARN, D. AND P. BAKER (1991). "Scientific Visuabt icn :  
An Introduction", Eurographics '91 Technrcal Report 5- 
nes, Tutorial Lecture 6. 

HECKBERT, P. (1982) "Color lmage Quantization for Frame 
Buffer Display", in proceedings of SIGGRAPH '82, Corn 
puler Graphics, 16(3), pp. 297-307 

HECKBERT, P. AND P. HARAHAN (19W. '-am Tracing 
Polygonal Objects", in proceedings of SIGGRAPH '84. 
Cornpuler Gmphics, 18(3), pp. 119-127. 

HOPGOOD, F. R. A,, D. A. DUCE, J. R. GALLOP, ET AL. (1983) 
lntroducrlon to the Graphical Kernel System (GKSJ, Acadr 
rnic Press, London. 

HOPGC~D,  F. R. 'A. AND D. A. DUCE (1991). A Primer for 
PHIGS, John Wiley & Sons, Chichester, England. 

HOPPE, H., T. DEROSE, T. MCDONALD, FT AL. (1993). "Mesh 
Opt~rnizahon", in proceed~ngs of SICGRAPH '93, Con#- 
puter Gr~phics Proceedrngs, pp. 19-26. 

HOWARD, T L. J., W. T. H ~ w m ;  R.J. HUBBOLD, ET AL. (1991). 
A Practical lntroduclion to PHIGS and PHIGS Plits, Addi- 
son-Wesley, Wohingham, England. 

HUGHE, J .  F. (1992). "Scheduled Fourier Volume Morph. 
ing", In proceedings of S I G G M H  '92, Computer Gn~pfr- 
icz, 26(2L pp. 43-46. 

HUITRIC, H. AND M. NAHAS (1985). "0-Spline Surfaces. A 
Tool for Computer Painting", lEEE Comput~r Graphl~,.~ 
and Applications, S(3). pp. 39-47. 

IKEDO, T. (1984). "High-speed Techniques for a 3-D Color 
Graphics Terminal", l E E E  Computer Graphics and Applr 
cations, 4(5), pp. 46-58. 

IMMEL, D. S., M. F. COHEN, AND D. P. GREENBERG (1986). " A  
Radlosity Method for Non-Diffuse Environments", In 
proceedings of SIGGRAPH '86, Computer Graphic, 
20(4), pp 133-142. 

ISAACS, P. M. AND M. F. COHEN (1987). "Controlling D y  
namic Simulation with Kinematic Constraints, Behavior 
Functions, and Inverse Dynamics", in proceedings of 
SIGGRAPH '87, Computer Graphics, 21(4), pp. 215-224. 

JAKVIS, J. F., C. N.  JUDICE, AND W. H. NINKE (1976). "A Sur- 
vey of Techniques for the lmage Display of Continuous 
Tone Pictures on Bilevel Displays", Computer Grapllir.. 
a d  Imapcs Procc'ssir~~, 5(1), pp. 13-40. 

JOHNSON, S. A. (1982). "Clin~cal Var~focal Mirror Display 
System at the University of Utah", Pruceedings of SPIE, 
367, August 1982, pp. 145-148. 

KANA, 1. T. (1983) "New Techniques for Ray Tracing Pro- 
cedurally Defined Objects". ACM Transactions on Graph- 
ICS, 2(3), p p  161 -181. 

KAINA, J.  T. (1986). 'The Rendering Equation", in proceed- 
lngs of SIGGRAPH '%, Computer Graphics, 20(4), pp. 
143-150. 

KA~IYA, J. T. AND T. L. KA) (1989) "Rendering Fur w ~ t h  
Three-Dimensional Textures", in proceedings of SIG- 
GRAPH '89, Computer Graphics, 23(3), pp. 271-280. 

KAPPEL, M. R. (1985). "An Ellipse-Drawing Algorithm for 
Faster Displays", in Fundamental Algorithms for Conr- 
puter Graphics, Springer-Verlag, Berlin, pp. 257-280. 

KARASICK, M., D. LIEBER, AND L. R. NACKMAN (1991). "Effi- 
cient Delaunay Triangulahon Usmg Rational Arith- 
metic", ACM Transactions or1 Graphics, 10(1), pp. 71-91. 

KAss, M. (1992). "CONDOR: Constraint-Based Dataflow", 
In proceed~ngs of SIGGRAPH '92, Computer Graphics, 
26(2), pp. 321-330. 

KASSON, 1. M. AND W. P ~ o t i m  (1992). "An Analysis of Se- 
lected Computer Interchange Color Spaces", ACM 
Transactions on Graphrcs, 11(4), pp. 373-405. 

KAUFMAN, A. (1987). "Efficient Algorithms for 3D Scan- 
Conversion of Parametric Curves, Surfaces, and Vol- 
umes", in proceedings of SlGCRAPH '87, Computer 
Graphics, 21(4), pp. 171-179 

KAWAGUCHI, Y. (1982). "A Morphological Study of the Form 
of Nature", In proceedings of SIGCRAPH '82, Computer 
Graphics, 16(3), pp. 223-232. 

KAY, T. L. A N D  J T K A J I ~ A  (1986). "Ray Traclng Complex 
Scenes", in proceedings of SIGGRAPH '86. Computer 
Graphics, 20(4), pp. 269-27s. 

KAY, D. C, AKD 1. R. LEVIN (1992). Graphics File Formats, 
Windcrest/McCraw-Hill, New York. 

KBLLBY, A. D., M. C. MALI\. ANL) C. M. NIELWN (1988). 
"Terrain Simulaticm using a Model of Stream Erosion", 
in proceedings of SICCRAPH '88, Computv Grnphlcs, 
22(4), pp. 263-268. 

KEM, J. R., W. E. C ~ R L W N ,  AS[) R. E. PARE~T (1992) "Shape 
Transformation for Polyhedral Objects", in proceedings 
of SIGGRAPH '92, Complc.r Gmphics, 26(2), pp. 47-54. 

KIRK, D. AND 1. ARW (1991). "Unbiased Sampling Tech- 
niques for lmage Synthesis", in proceedings of SIG- 
GRAPH '91, Cu~nprtter Graphics. 25(4), pp. 153-156 

KIRK, D., ED. (1992). Graphi-5 Gnns 111, Academic Press, San 
Diego, CA 

KNLTH, D. E. (1987). "DigitA tk~lftones by Dot Diffusion", 
ACM Tmrrsctio~rs or1 Gmplrrrs, 6(4), pp 245-273. 

KCCHANEK, D. H. U. AND R. H. BARTELI; (1984). "lnterpolat- 
ing Splines with Local Tension, Continuity, and Bias 
Control", in proceedings of SIGCRAPH '84, Cuntp~rtrr 
Graphics, 18(3), pp. 33-11 



KOH, E.-K. AND D. HEARN (1992). "Fast Generat~on and Sur- 
face Structuring Methods for Terrain and Other Natural 
Phenomena", in. proceedings of Eurographs '92 Com- 
puter Graphics Forum, 11(3), p p  C-169-180. 

KORIEN, J. U. AND N. I. BADLER (1982). 'Techniques for Gen- 
erating the Goal-Directed Motion of Articulated Struc- 
tures", lEEE Computer Graphics and Applications, 2(9), pp. 
71-81.. 

KORIEN, J. U. AND N. I. BADLER (19%). 'Temporal antialias- 
ing in Computer-Generated Animation", in proceed- 
ings of SIGGRAPH '83, Computer Graphics, 17(3), pp. 
377-388. 

LASSETER, J. (1987). "Principles of Traditional Animation 
Applied to 3D Computer Animation", in proceedings oi 
SIGGRAPH '87, Computer Graphrs, 21(4), pp. 35-44. 

LAUR, D. AND P. HANRAHAN (1991). "Hierarchical Splatting: 
A Progressive Refinement Algorithm for Volume Ren- 
dering", in proceedings of SICGRAPH '91, Computer 
Graphics, 25(4), pp. 285-288. 

LAUREL, B. (1990). The Art of Human-Computer lnterfacr De. 
sign, Addision-Wesley, Reading, MA. 

LEE, M. E., R. A. REDNER, AND S. P. USELTON (1985). "Statisi- 
cally Optimized Sampling for Distributed Ray Tracing". 
in proceedings of SIGGRAPH '85, Computer Graphics. 
19(3), pp. 61-68. 

L ~ L ,  A. AND T. PORTER (19%). "CHAP - A SlMD 
Graphics Processor", in proceedings of SIGGRAPH '84. 
Computer Graphics, 18(3), pp. 77-82. 

LEVOY, M. (1988). "Display of Surfaces from Volume Data". 
IEEE Computer Graphics and Applications, 8(3), pp. 29-37 

LEVOY, M. (1990). "A :Hybrid Ray Tracer for Rendering 
Polygon and Volume Data", lEEE Computer Graphics or~d 
Applications, 10(2), pp. 33-40. 

LEWIS, J.-P. (1989). "Algorithms for Solid Noise Svnthesis". 
in proceedings of ~ K X A P H  '89, ~ o m ~ u t e ;  Graphics. 
23(3), pp. 263-270. . . 

LIANG, Y.-D. AND B. A. BARSKY (1983). "An Analysis and Al- 
gorithm for Polygon Clipping." CACM, 26(11), pp 
868-877. 

LIANG, Y.-D. ANU 8. A. B.~RSKY (1984). "A New Conce~t and 
Method for Line Clipping", ACM Transactions on kraph- 
ics, 3(1), pp. 1-22. . . 

LIEN, S.-L., M. SHANTZ, AND V. mil (1987). "Adaptwe For- 
ward Differencing for Rendering Curves and Surfaces". 
in proceedings of SlGGRAPH '87, Computer Graphrcs, 
21(4), pp. 111-118. 

LINDLEY, C. A. (1992). Pmctical RPy Tracing in C ,  John Wiley 
& Sons, New York. 

LISCHINSKI, D., E TAMPIERI, AND D. GREENBERG (1993). 
"Combining Hierarchical Radiosity and Discontinuity 
Meshing", in proceedings of SIGGRAPH '93, Cnmputrr 
Graphics, pp. 1 W-208. 

LTTWINOWICZ, P. C. (1991). "Inkwell: A 2 1/2-D Animation 
System", in proceedings of SlGGRAPH '91, Computer 
Graphics, 25(4), pp. 113-122. 

LODDINC, K. N. (1983). "Iconic Interfacing", lEEE Cornprrttv 
Graphics and Applicntions, 3(2), pp. 11-20. 

LOKE, T.-S., D. TAN, H.-S. SEAH, ET AL. (1992). "Rendering 
Fireworks Displays", lEEE Computer Graphics and Appli- 
cations, 12(3), pp. 33-43. 

LOOMIS, J., H. POWER, U. BELLUGI, ET AL. (1983). "Computer 
Graphic Modeling of American Sign Language", in pro- 
ceedings of SIGGRAPH '83, Computer Graphics, 17(3), 
pp. 105-114. 

LORENSON, W. E AND H. CLINE (1987). "Marching Cubes: A 
High-Resolution 3D Surface Construction Algorithm", 
in proceedings of SlGGRAPH '87, Computer Graphics, 
21(4), pp. 163-169. 

MACKINLAY, J. D., S. K. CARD, AND G. G. ROBER'ISON (1990). 
"Rapid Controlled Movement Through a Virtual 3D 
Workspace", SlGGRAPH 90, pp. 171-176. 

MACKINLAY, J. D., G. G. R O B E ~ N ,  AND S. K. CARD (1991). 
"The Perspective Wall: Detail and Context Smoothly In- 
tegrated", CHI '91, pp. 173-179. 

MAGWENAT-THALMANN, N. AND D. THALMANN (1985). Com- 
puter Anirnntion: Theoy and Practice, Springer-Verlag, 
Tokyo. 

MAGNENAT-THALVANN, N. AND D. THALMANN (1987). Image 
Synthesis, Springer-Verlag, Tokyo. 

MAGNENAT-THALVANN, N. AND D. THALMANN (1991). 
"Complex Models for Animating Synthetic Actors", 
lEEE Cornputer Graphics and Applications, 11(5), pp. 
32-45. 

MANDELBROT, B. B. (1977). Fractals: Form, Chance, and Di- 
mension, Freeman Press, 5an Francisco. 

MANDELBROT, B. B. (1982). The Fractal Geometry i ~ f  Nature, 
Freeman Press, New York. 

MANTYLA, M. (1988). An Introduction lo Solid Modelinx, 
Computer Science Press, Rockville, MD. 

MAX, N. L. AND D. M. LERNER (1985). "A Two-and-a-Half-D 
Motion Blur Algorithm", in proceedings of SIGGRAPH 
'85, Computer Graphics, 19(3), pp. 85-94. 

MAX, N. L. (1986). "Atmospheric Illumination and Shad- 
ows", in proceedings of SIGGRAPH '86, Computer 
Graphics, 20(4), pp. 117-124. 

MAX, N. L. (1990). "ConeSpheres", in proceedings of S C -  
GRAPH '90, Computer Graphics, 24(4), pp. 59-62. 

METAXAS, D. AND D. TERZOPOULCS (1992). "Dynamic Defor- 
mation of Solid Primitives with Constraints", in pro- 
ceedings of SIGGRAPH '92, Computer Graphics, 2k2). 
pp. 309-312. . . 

MEYER, G. W., M. E. RUSHMEIER, M. F. CVHEN, ET AL. (1986). 
"An Experimental Evaluation of Computer Graphics 
Imagery", ACM Transactrons or? Graphics, 6(1), pp. 30-50. 

MEYER, G. W. AND D. P. GREENBERC (1988). "Color-Defective 
Vision and Computer Graphics Displays", lEEE Com- 
puter Graphics and Appl&atrons, 8(5), pp. 28-40. 

MEYFRS, D., S. SKINNER, AND K.  SLOAN (1992). "Surfaces 
from Contours", ACM Transactions on Graphics, 11(3), 
pp. 228-258. 

MILLER, G. S. P. (1988). "The Motion Dynamics of Snakes 
and Worms", in proceedings of SlGGRAPH '88, Corn- 
puler Grnphics, 22(4), pp. 169-178. 



MILLER, I. V., D. E. BREEN, W. E. LORENSON, ET AL. (1991). 
"Geometrically Deformed Models: A Method for Ex- 
tracting Closed Geometric Models from Volume Data", 
in proceedings of SIGGRAPH '91, Computer Graph~cs, 
25(4), pp. 21 7-226. 

MITCHELL, D. P. (1991). "Spectrally Optimal Sampling for 
Distribution Ray Tracing", in proceedings of SIC;- 
GRAPH '91, Computer Graphics, 25(4), pp. 157-165. 

MITCHELL, D. P. AND P. HANRAHAN (1992). "Numination 
from Curved Reflectors", in proceedings of SIGGRAPH 
'92, Computer Graphics, 26(2), pp. 283-291. 

MWATA, K. (1990). "A Method of Generating Stone Wall 
Patterns", in proceedings of SIGGRAPH '90, Computcr 
Graphics, 24(4), pp. 387-394. 

MOWAR, S., J. En=, AND J.  POULTON (1992). "PielFlow: 
High-speed Rendering Using Image Composition", in 
proceedings of SIGGRAPH '92, Computer Graphics, 
26(2), pp. 231-240. 

MOON, F. C. (1992). Chaotic and Frnctnl Dynamics, John 
Wiley & Sons, New York. 

MWRE, M. AND J. WILHELMS (1988). "Collision Detection 
a-.d Response for Computer Animation", in proceed- 
ings of SIGGRAPH '88, Computer Graphics, 22(4), pp. 
289-298. 

MORTENSON, M. E. (1985). Geometric Modeling, John Wily & 
Sons, New York. 

MURAKI, S. (1991). "Volumetric Shape Description of Range 
Data Using the 'Blobby Model' ", in p d i n g s  of SIG- 
GRAPH '91, Computer Graphics, 25(4), pp. 227-235. 

- - 

MUSGRAVE, F. K., C. E. KOLB, AND R. S. MACE (1989). "The 
Synthesis and Rendering of Eroded Fractal Terrains", in 
proceedings of SIGGRAPH '89, Computer Graphics, 
23(3), pp. 41-50, 

MYERS, B. A. AND W. BUXTON (1986). "Creating High-Inter- 
active and Graphical User Interfaces by Demonstra- 
tion", in proceedings of SIGGRAPH '86, Computcr 
Graphics, 20(4), pp. 249-258. 

NAYLOR, B., J. AMANATIDES, AND W. THIBAULT (1990). "Merg- 
ing BSP Trees Yields Polyhedral Set Operations", in pro- 
ceedings of SIGGRAPH '90, Computer Graphics, 24(4), 
pp. 115-124. 

NEWMAN, W. H. (1968). "A Svstem for Interactive Gra~hi- 
cal Programming", SICC, ?hompson Books, ~ashinGon, 
D. C., pp. 47-54. 

NEWMAN, W. H. AND R. F. SPROULL (1979). Principles of Inter- 
nctive Computer Graphics, McGraw-Hdl, New York. 

NGO, J. T. A N D  J. MARKS (1993). "Spacetime Constraints Re- 
visited", in proceedings of SIGGRAPH '93, Computer 
Graphics, pp. 343-350. 

NICHOLL, T. M., D. T. LEE, AND R. A. NICHOLL (1987). "An 
Efficient New Algorithm for 2D Line Clipping: Its De 
velopment and Analysis", in proceedings of SIG.. 
GRAPH '87, Computer Graphics, 21(4), pp. 253-262. 

NIELSON, G .  M., B SHRIVER, AND L. ROSENBLUM, ED. (1990). 
Visunliwtion in Scientific Computing, IEEE Computer So- 
ciety Press, Los Alamitos, CA. 

NIELSON, G. M. (1993). "Scattered Data Modeling", I E E ~  
Computer Graphics and Applications, 13(1), p p  60-70. 

h'rs~1~vru. H. (1985). "Object Modeling by Distribution 
Function and a Method of Image Generation", Journal 
Electronics Comm. Conf. '85, J68(4), pp. 718-725. 

NISHITA, T. AND E. NAKAMAE (1986). "Continuous-Tone 
Representation of Three-Dimensional Objects IUurni- 
nated by Sky Light", in proceedings of SIGGRAPH '86, 
Computer Grnphics, 20(4), pp. 125-132. 

NISHITA, T., SIRAI, K. TADAMURA, FI AL. (1993). "Display of 
the Earth Taking into Account Atmospheric Scattering", 
in proceedings of SIGGRAPH '93, Computer Graphics 
ProcPedings, pp. 175-182. 

N o ~ N ,  A. (1982). "Generation and Display of Geometric 
Fractals in 3-D", in proceedings of SIGGRAPH '82, 
Computer Graphics, 16(3), pp. 61-67. 

NSF ~NV~~ATIONAL WORKSHOP (1992). "Research Direaions 
in Virtual Environments", Computer Graphics, 26(3), 
pp. 153-177. 

CUBE, H., H. IMAOKA, T. TOMIHA, ET AL. (1992). "Three 
Dimensional Apparel CAD System", in proceedings of 
SIGGRAPH '92, Computer Grnphics, 26(2), pp. 105-110. 

@wGL ARCHITECTURE REVIEW BOARD (1993). OpenGL Pro- 
gramming Guide, Addision-Wesley, Reading, MA. 

@PENHEWER, P. E. (1986). "Real-Tie Design and Anima- 
tion of Fractal Plants and Trees", in proceedings of SIG- 
GRAPH '86, Computer Grnphics, 20(4), pp. 55-64. 

OSFIMOTLF (1989). OSFIMotij Style Guide, Open Software 
Foundation, PrentieHall, Englewood Cliffs, NJ. 

P A ~ R ,  J. AND K. SWAN (1989). "Antialiased Ray Tracing 
by Adaptive Progressive Refinement", in proceedings 
of SIGGRAPH '89, Computer Graphics, 23(3), pp. 
281-288. 

PANG, A. T. (1990). "LineDrawing Algorithms for Parallel 
Machines", lEEE Computer Graphics nnd Applications. 
10(5), pp. 54-59. 

PAYLIDIS, T. (1982). Algorithms For Graphics and Image Pro- 
cessing, Computer Science Press, Rockville, MD. 

PAVLIDIS, T. (1983). "Curve Fitting with Conic Splines", 
ACM Transctions on Graphics, 2(1), pp. 1-31. 

PEACHEY, D. R. (1986). '%lodeling Waves and Surf", in p n  
ceedings of SIGGRAPH '86, Computer Graphics, 20(4), 
pp. 65-74. 

PEITCEN, H . 4 .  AND P. H. RICHTER (1986). The Beauty of Frac- 
tals, Springer-Verlag, Berlin. 

PEITCEN, H.-0. AND D. SAUPE, ED. (1988). The Science of Frac- 
tal Images, Springer-Verlag, Berlin. 

PEN~AND, A. AND J. WILLIAMS (1989). "Good Vibrations: 
Modal Dvnamics for Gra~hics and Animation", in DIU- 
ceedings'of SIGGRAPH "89, Computer ~ m ~ h i c s ,  2%3), 
pp. 215-222. 

PERLLN, K. A ~ J  E. M. HOFFERT (1989): "Hypertexture", in 
proceedings of SIGGRAPH '89, Computer Gmphics, 
23(3), pp. 253-262. 

PHILLIPS, R. L (l977l. "A Query Language for a Network 
Data Base with Graphical Entities", in proceedings of 
SIGGRAPH '77, Computer Grnphics, 11(2), pp. 179-185. 



PHONC, B. T. (1975). "llluminat~on for ComputerGenerated 
Images", CACM, 18(6), pp. 311-317. 

PINEDA, J. (1988). "A Parallel Algorithm for Polygon Ras- 
terization", in proceedings of SIGGRAPH '88, Computer 
Graphics, 22(4), pp. 17-20 

PIXWAY, M. L. V AND D. J. WATXINSON (1980). "Bresen- 
ham's Algorithm with Gray Scale", CACM, 23(11), pp 
625-626. 

PLATT, J. C. AND A. H. BARR (1988). "Constraint Methods for 
Flexible Models", in proceedings of SIGGRAPH '88, 
Computer Graphics, 22(4), pp. 279-288. 

PORTER, T. AND T. DVFF (1984). "Compositing Digital lm- 
ages", in proceedings of SIGGRAPH '84, Computer 
Graphics, 18(3), pp. 253-259. 

POTMESIL, M. AND I. C H A K R A V A ~  (1982). "Synthetic lmage 
Generation with a Lens and Aperture Camera Model" 
ACM Tranwtions on Graphics, 1(2), pp. 85-108. 

POTMESIL, M. AND I. C ~ A K R A V A ~  (1983). "Modeling Mo- 
tion Blur in ComputerGenerated Images", in proceed- 
ings of SIGGRAPH '83, Computer Graphics, i7(3), pp  
389-399. 

POTMESIL, M. AND E. M. H O ~ T  (1987). "FRAMES: Soft- 
ware Tools for Modeling, Rendering and Animation of 
3D Scenes", in proceedings of SIGGRAPH '87, Computer 
Graphics, 21(4), pp. 85-93. 

PGTMESIL, M. AND E. M. H o m w  (1989). "The Pixel Ma- 
chine: A Parallel lmage Computef', in proceedings of 
SIGGMPH '89, Computer Gmphics, 23(3), pp. 69-78. 

PRATT, W. K. (1). Digital lmage Processing, John Wiley & 
Sons, New York. 

PREI'ARATA, F P. AND M I. SHAMOS (1985). C ~ m p ~ t ~ f l ~ n d  
Geomefry, Springer-Verlag, New York. 

PRESS, W. H .  S. A. TEWOLSKY, W. T. VETTUILLNG, ET AL. 
(1992). Numerical Recipes in C ,  Cambridge University 
Press, Cambridge, England. 

PRUSINKIEWICZ, p, M. S. HAMMEL, AND E. MIOSSS (1993). 
"Animation of Plant Development", in proceedings uf 
SIGGRAPH '93, Computer Graphics Proceedings. pp. 
351-360. 

PRUYN, P. W. AND D. I? GREENBERG (1993). "Exploring 3D 
Computer Graphics in Cockpit Avionics", IEEE Conl- 
puter Graphics and Applications, 13(3), pp. 28-35. 

QJEK, L.-H. AND D. HEAM (1988). "Efficient Space-Subdi- 
vision Methods in Ray-Tracing Algorithms", Univer- 
sity of Illinois, Department of Computer Science Report 
UIUCDCS K-88-1468. 

RAIBERT, M. H. AND J. K. HODGINS (1991). "Animation of Dy- 
namic Legged Locomotion", in proceedings of SlG- 
GRAPH '91, Computer Graphics, 23(4), pp. 349-358. 

REEVES, W. T (1983). "Particle Systems: A Technique for 
Modelmg a Class of Fuzzy Objects", ACM Transactions 
an Graphics, 2(2), pp. 91 -108. 

REEVES, W. T. (1983). "Particle Systems-A Technique for 
Modeling a Class of Fuzzy Objects", in proceedings uf 
SIGGRAPH '83. Computer Graphics, 17(3), pp 359-376. 

REEVES, W. T. AM) R. BLAU (1985). "Approximate and Prob- 
abilistic Algorithms for Shading and Rendering Struc- 
tured Particle Systems", in proceedings of SICGRAPH 
'85, Computer Graphics, 19(3), pp. 313-321. 

REEVES, W. T., D. H. SALESIN, AND R. L. COOK (1987). "Ren- 
dering Antiaiiased Shadows with Depth Maps", In pro- 
ceedings of SIGGRAPH '87. Computn Graphics, 21(4), 
pp. 283-291. 

REQUICHA, A. A. G. AND J .  R. ROISIGNAC (1992). "Solid Mod- 
eling and Beyond", lEEE Computer Graphics and Applica- 
tinns, 12(5), pp. 31-44. 

REYNOLDS, C. W. (1982). "Computer Animation with Scripts 
and Aaors", in proceedings of SIGGRAPH '82, Com- 
puter Graphics, 16(3), pp. 289--2%. 

REYNOLDS, C. W. (1987). "Flocks, Herds, and Schools: A 
Ihhibuted Behavioral Model", in proceedings of SIG- 
GRAPH '87, Computer Graphics, 21 (4), pp. 25-34. 

RIESENFELD, R. F. (1981). "Homogeneous Coord i~ te s  and 
PmpcCive Planes in Computer Graphics", lEEE Com- 
puter Graphics and Applicntions, 1(1), pp. 50-55. 

R o s u m o ~ ,  P. K. (1988). 'Viualizing Color Gamuts: A User 
Interface for the Effective Use of Perceptual Color 
Spaces in Data Displays", IEEE Computer Graphics and 
Applications, 8(5), pp. 50-64. 

ROBERTSON, G. G., J. D. ~ ~ A C K L N L A Y  AND S. K. CARD (1991). 
"Cone Trees: Animated 3D Visualiiitions of Hierarchi- 
cal Information", CHI '91, p ~ .  189-194. 

ROCERS, D. F. AND R. A. EARNSHAW, ED. (1987). Techniques for 
Computer Gmphics, Springer-Verlag, New York. 

ROCERS, D. F. AND J .  A. ADA& (1990). Mathematical Elements 
for Computer Graphics, McGraw-Hill, New York. 

R ~ E N T H A L ,  D. S. H., ET AL. (1982). "The Detailed Semantics 
of Graphics Input Devices", in proceedings of SIG- 
GRAPH '82, Computer Graphics, 16(3), pp. 33-38. 

RUBINE, D. (1991). "Specifying Gestures by Example", in 
proceedings of SIGGRAPH '91, Computer Graphics, 
25(4), pp. 329-337. 

RLISHMEIER, H. AND K. TORRANCE (1987). "The Zonal 
Method for Calculating Light Intensities in the Presence 
of a Participating Medium", in proceedings of SIG- 
GRAPH '87, Computer Graphics, 21(4), pp. 293-302. 

RUSHMEIER, H. E. AND K. E. TOKRANCE (1990). "Extending 
the Radiosity Method to Include SpecularIy Reflecting 
and Translucent Materials", ACM Transactions on Graph- 
ics, 9(1), pp. 1-27. 

SABELLA, P. (1988). "A Rendering Algorithm for Visualizing 
3D Scalar Fields", in proceedings of SIGGRAPH '88, 
Ccrnputer Graphics, 22(4), pp 51-58. 

SABlh, M. A. (1985). "Contouring: The State of the Art", in 
Flrndamer~tal Algorithms for Computer Graphics, R. A. 
Earnshaw, ed, Springer-Verlag, Berlin, pp. 41 1-182. 

SAI PSIN, D. AND R. BARZEL (1993). "Adjustable Tools: An 
Object-Oriented ln!eraction Metaphor", ACM Transoc- 
tions on Graphics, 12(1), pp. 103-107. 

SAMET, H. AND R. E. WEBBER (1985). "Sorting a Collect~on of 
Polygons using Quadtrees". ACM Trflnsactions on Graph- 
rcs, 4(3), pp. 182-222. 



SAMET, H. AND M. TWINEN (1985). "Bintrees, CSG TWS, 
and lime", in proceedings of SIGGRAPH '85, Computer 
Graphics, 19(3), pp. 121-130. 

SAMET, H. AND R. E. WDBER (1988). "Hierarchical Data 
Structwes and Algorithms for Computer Graphics: Part 
I", IEEE Computn Graphics and Applications, 8(4), pp. 
59-75. 

S m ,  H. AND R. E. W ~ E R  (1%). "Hieraxhical Data 
Structures and Algorithms for Computer Graphics: Part 
2", IEEE Computer Graphics and Applications, 8(3), pp- 
48-68. 

~HEIFLER, R. W. AND J. G m  (1986). 'The X Window Sys- 
tem", ACM Transactions on Graphics, 5(2), pp. 79-109. 

SCHOEW, C., J. DORSEY, 8. S M ~ ,  ET AL. (1993). "Global 
Illumination", in proceedings of SIGGRAPH '93, Com- 
puter Graphics Proceedings, pp. 143-146. 

~HRODER, P. AND P. HMWAH,W (1993). "On the Form Fac- 
tor Between Two Polygons", in proceedings of SlG- 
GRAPH '93, Computn Graphis Proceedings, pp. 163-164. 

S~HWARTZ, M. W., W. 8. COWAN, AND J. C. BEATW (1987). 
"An Experimental Comparison of RGB, YlQ LAB, HSV, 
and Opponent Color Models", ACM Transactions on 
Graphics, 6(2), pp. 123-158. 

SEDERBERG, T. W. AND E. GREENWOOD (1992). "A Physically 
Based Approached to 2-D Shape Bending', in proceed- 
ings of SIGGRAPH '92, Cornplttn Graphics, 26(2), pp. 
25-34. 

SEDERBERG, T. W., P. GAO, G. WANC, ET AL. (1993). "2D Shape 
Blending: An Intrinsic Solution to the Vertex Path Prob- 
lem", in proceedings of SIGGRAPH '93, Computn 
Grnph~cs Proceedings, pp. 15-18. 

SEGAL, M. (1990). "Using Tolerances to Guarantee Valid 
Polyhedral Modeling Results", in proceedings of SIC;- 
GRAPH '90, Computer Graphics, 24(4), pp. 105-114. 

SEGAL, M., C. KOROBKIN, R. VAN WIDENFELT, ET AL. (1992). 
"Fast Shadows and Lighting Effects Using Texture Map- 
ping", in proceedings of SIGGRAPH '92, Computrr 
Graphics, 26(2), pp. 249-252. 

SEQUIN, C. H. AND E. K. SMYRL (1989). "Parameterized Ray- 
Tracing", in proceedings of SIGGRAPH '89, Computt.r 
Graphics, 23(3), pp. 307-314. 

SHERR, S. (iy93). Electronic Displays, John Wiley & Sons, 
New York. 

SHILLING, A. AND W. SMER (1993). "EXACT: Algorithm 
and Hardware Architecture for an Improved A-Buffer", 
in proceedings of SIGGRAPH '93, Computm Graphics 
Proceedings, pp. 85-92. 

SHIRLEY, I? (1990). "A Ray Tracing Method for llluminativn 
Calculation in Diffuse-Specular Scenes", Graphin Inter- 
face '90, pp. 205-212. 

SHNEIDERMAN, B. (1986). Designing the User Interface, Addi- 
son-Wesley, Reading, MA. 

SHOEMAKE, K. (1985). "Animating Rotation with Quater- 
nion Curves", in proceedings of SIGGRAPH '85, Covr- 
puter Graphics, 19(3), pp. 245-254. 

SIBERT, J. L., W. D. HURLEY, AND T. W. BLESER (1986). "An Ob- 
ject-Oriented User Interface Management System", in 
proceedings of SIGGRAPH 'M, Computn Graphics, 
20(4), p p  t59-268. 

SILLION, F. X. AND C. PUKH (1989). "A General Two-Pass 
Method Integrating Specular and Diffuse Reflection", in 
proceedings of SIGGRAPH '89, Computer Graphics, 
23(3), pp. 335-344. 

SILUON, F. X., 1. R .  ARVO, S. H. WESTIN, ET AL. (1991). "A 
Global Illumination Solution for General Reflectance 
Distributions", in proceedmgs of SIGGRAPH '91, Com- 
putn Graphics, 25(4), pp. 187-196. 

Sws, K. (1990). "Particle Animation and Rendering Using 
Data Parallel Computation", in proceedings of SIG- 
GRAPH '90, Computn Graphics, 24(4), pp. 405-413. 

SIMS, K. (1991). "Artificial Evolution for Computer Graph- 
ics", in pmeedings of SIGGRAPH '91, Computn Graph- 
ics, 25(4), pp. 319-326. 

SINGH, B., J. C. B E A ~ ,  K S. BOOTH, n AL. (1983). "A Graph- 
i n  Editor for Benesh Movement Notation", in proceed- 
ings of SIGGRAPH '83, Computer Graphics, 17(3), pp. 
51-62. 

SMTH, A. R. (1978). "Color Gamut Transform Pain", Com- 
puter Graphics, 12(3), pp. 12-19 

Surm, A. R. (1979). '71nt Fill", Computer Graphics, 13(2), 
pp. 276-283. 

S m ,  A. R. (1984). "Plants, Fractals, and FormaI Lam 
pages", in proceedings of SIGGRAPH '84, Computer 
Graphics, 18(3), pp. 1-10. 

Swm, R. B. (1987). "Experiences with the Alternate Reality 
Kit: An Example of the Tension Between Literalism and 
Magic", IEEE Computer Graphics and Applications, 7(9), 
pp. 42-50. 

S M ~ ,  A. R. (1987). "Planar 2-Pass Texture Mapping and 
Warping", in proceedings of SIGGRAPH '87, Computer 
Graphics, 21 ( 4 ,  pp. 203-272. 

SMITS, B. E., J. R. ARVO, AXD D. H. SALESIN (1992). "An Im- 
portanceDriven Radiosity Algorithm", in proceedings 
of SlGGRAPH '92, Computer Graphics, 26(2), pp- 
273-282. 

SNYDER, J. M. AND J. T. KAIIYA (1992). "Generative Model- 
ing: A Symbolic System for Geometric Modeling", in 
proceedings of SlGGRAPH '92, Computer Graphics, 
26(2), pp. 369-378. 

SNYDER, J. M., A. R. WOODBURY, K. 'FLEIXHER, m AL. (1993). 
"interval Method for Multi-Point Collisions between 
Time-Dependent Curved Surlacw", in proceedings of 
SlGGRAPH '93, Compuler Graphics, pp. 321334. 

SPROULL, R. F. AND I. E. SUTHERLAND (1968). "A Clipping Di- 
videt", AFlPS Fall Joint Computer Conference. 

STAM, J. AND E. FIUME (1993) "Turbulent Wind Fields for 
Gaseous Phenomena". in proceedings of SIGGRAPH 
'95, Computer Grapl~ks Proseed~n~s, pp. 369-376. 

STETTNER, A, AND D. P GREENBERG (1989). "Computer 
Graphics Visualization for Acoustic Simulation", in pro- 
ceedings of SIGGRAPH 89, Computer Graphics, 23(3), 
pp. 195-206. 



STRASSMANN, S. (1986). "Hairy Brushes", in proceedings of  
SICGRAPH '86, Computer Graphics, 20(4), pp. 225-232. 

STRAUSS, l? S. AND R. CAREY (1992). "An Object-Oriented 3D 
Graphics Toolkit", in proceedings of SIGGRAPH '92. 
Computer Graphics, 26(2), pp. 341-349. 

SUNG, H. C. K., G. ROGERS, AND W. J .  KuBrrz (1990). "A Crit- 
ical Evaluation of PEX", IEEE Computer Graphrcs and Ap. 
plications, 10(6), pp. 65-75. 

SUTHERLAND, I. E. (1963). "Sketchpad: A Man-Machine 
Graphical Communication System", AFIPS Spring Joint 
Computer Conference, 23 pp. 329-346. 

S ~ R L A N D ,  1. E., R. F. SPROLKL, AND R. SCHUMACKEII 
(1974). "A Characterization of Ten Hidden Surface Al- 
gorithms", ACM Computing Surveys, 6(1), pp. 1-55. 

S ~ E R L A N D ,  I. E. AND G. W. HODGMAN (1974). "Reentranr 
Polygon Clipping", CACM, 17(1), pp. 32-42. 

SWEZEY, R. W. AND E. G. DAVIS (1983). "A Case Study 01 
Human Factors Guidelines in Computer Graphics", 
IEEE Computer Graphics and Applications, 3(8), pp. 21-30 

TAKALA, T. A ~ V  1. HAHN (1992). "Sound Rcndenng", In pro- 
ceedings of SICGRAPH '92, Computer Graphics, 26(2), 
pp. 211-220. 

TANNAS, I., LAWRENCE E., ED. (1985). Flat-Panel Displays arid 
CRTs, Van Nostrand Reinhold Company, New York. 

TELLER, S. AND P. HANRAHAN (1993). "Global Visibility Al- 
gorithms for Illumination Cornputahons", in p r o c d -  
ings of SIGGRAPH '93, Computer Graphics Proceedings, 
pp. 239-246. 

TERZOPOULOS, D., J .  PLATT, A. H. BARR, ET AL. (1987). "Elasti- 
cally Deformable Models", in proceedings of SIG- 
GRAPH '87, Compuler Graphics, 21(4), pp. 205-214. 

THALMANN, D., ED. (1990). Scientific Visualizntimr and Graph- 
ics Simulalion, John Wiey &Sons, Chichester, England. 

THIBAULT, W. C. AND 8. F. NAYLOR (1987). "Set Operations 
on Polvhedra using Binarv Space Partitionine Trees", in 

of S~CRA'PH '87, ~ o m ~ u t r ;  Crqhics, 
21(4), pp. 153-162. 

TORBERG, 1. G. (1987). "AParallel Processor Architecture for 
Graphics Arithmetic Operations", in proceedings of 
SlGGRAPH '87, Computer Graphlcs, 21(4), pp. 197-204. 

TORRANCE, K. E. AND E. M. SPARROW (1967). 'Theory for 
Off-Specular Reflection from Roughened Surfaces", 1. 
Optical Society of America, 57(9), pp. 1105-1114. 

TRAVIS, D. (1991). El(ecti.ve Color Displays, Academic Press, 
London.' 

TUFTE, E. R. (1983). The Visual Displuy of Quantitative Infor- 
mation, Graphics Press, Cheshire, CN. 

Tu~i-E, E. R. (1990). Envisioning Information, Graphics Press, 
Cheshire, CN. 

TURKOWSKI, K. (1982). "Antialiasing Thmugh the Llse ol 
Coordinate Transformations", ACM Transactions on 
Graphics, 1(3), pp. 215-234. 

UIWN,  C. AND M. KEELER (1988). "VBUFFER: Visible Vol- 
ume Rendering", in proceedings of SIGGRAPH '88, 
Computer Graphics, 22(4), pp. 59-64. 

UPSON, C., T. FAULHABER JR., D. KAMINE~ m AL. (1989). "The 
Application Visualization System: A Computational En- 
vironment for Scientific Visualization", lEEE Computer 
Graphics and Applications, 9(4), p p  30-42. 

UWILL, S. (1990). The RenderMan Companion, Addison- 
Wesley, Reading, MA. 

VAN DE PANNE, M A N D  E. FIUME (1993). "Sensor-Actuator 
Networks", in proceedings oi  SIGGRAPH '93, Computer 
Graphics Proceedings, pp. 335-342. 

VAN WIJK, J .  J .  (1991). "Spot Noise-Texture Synthesis for 
Data Visualization", in procredings of SIGGRAPH '91. 
Computer Graph~cs, 25(4), pp. 309-318. 

VEEMTRA, J .  AND N. AHUIA (1988). "Line Drawings of 
Octree-Represented Objects", ACM Transactions on 
Graphics, 7(1), pp. 61-75. 

VELHO, L. AND J..D. M. GOMES (1991). "Digital Halftoning 
with Space-Filing Curves", in proceedings of SIG- 
GRAPH '91, Computer Graphics, 25(4), pp 81-90. 

VON HERZEN, B., A. H. BARR, AND H. R ZATZ (1990). "Geo- 
metric Collis~ons for lime-Dependent Parametric Sur- 
faces", in proceedings of SIGGRAPH '90, Computer 
Graphics, 24(4), pp. 39-48. 

WALLACE, V. L. (1976). 'The %mantics of Graphic Input 
Devices", in proceedings of SIGGRAPH '76, Computer 
Grnphics, lO(l), pp. 61-65. 

WALLACE, J. R., K. A. ELMQUIST, AND E. A. HAINES (1989) "A 
Ray-Tracing Algorithm for  Progressive Radiosity*', in 
proceedings of SIGGR4PH '89, Computer Graplrics, 
23(3), pp. 315-324. 

WANCER, L. R., J .  A. FERWERDA, AND D. P. GREENBERG (1992). 
"Perceiving Spatial Relationships in ComputerCener- 
ated Images", IEEE Cornpuler Graphics and Applications, 
12(3), pp. 44-58. 

WARE, C. (1988). "Color Sequences for Univariate Maps: 
Theory, Experiments, and Principles", IEEE Computer 
Graphics and Applications, 86) .  pp. 41-49. 

WARN, D. R. (1983). "Light~ng Controls for Synthetic Im- 
ages", in proceedings of SIGGRAPH '83, Computer 
Grnpl~rcs, 17(3), pp. 13-21. 

WARNOCK, J .  AND D. K. WYAT (1982). "A Device-lndepen- 
dent Graphics Imaging Model for Use with Raster De- 
vices", in proceedings of SIGGRAPH '82, Compuler 
Graphics, 16(3), pp. 313-319. 

WATT, A. (1989). Fundamentals 3f Three-Ditnerrsional Corn- 
puler Graphics, Addison-Wesley, Wokingham, England. 

WATT, M. (1990). "Light-Water Interaction Using Backward 
Beam Tracing", in proceedings of SlGGRAPH '90, Coni- 
puler Graphics, 24(4), pp. 377-386. 

WATT, A. AND M. WATT (1992). Advar~ced Atrimatron and Ren- 
derrrrg Techniques, Addison-Wesley, Wokingham, Eng- 
land. 

WECHORST, H., C. HOOPER, ANtl  D. r. GREENBERG (1984). 
"Improved Computational Methods for Ray Tracing", 
ACM Transactions on Grnph~cs, 3(1J, pp. 52-69. 

WEIL, J .  (1986). 'The Synthesis of Cloth Objects", in pro- 
ceedings of SIGGRAPH '86, Cnnrputel. Grnplrics, 20(4), 
pp. 49-54. 



WEILER, K. AND P. ATHERTON (1977). "fidden-Surface Re- 
moval Using Polygon Area Sorting", in proceedings of 
SlGGRAPH '77, Computer Graphics, 11(2), pp. 214-222. 

WEILER, K. (1980). "Polygon Comparison Using a Graph 
Representation", in proceedings of SlGGRAPH '80, 
Computer Graph~cs, 14(3), pp. 10-18. 

WETIN, S. H., 1. R. ARVO, AND K. E. TORRANCE (I 992). "Pre- 
dicting Reflectance Functions from Complex Surfaces", 
in proceedings of SIGGRAPH '92, Computer Graphics, 
26(2), p p  255-264. 

WESTOVER, L. (1990). "Footprint Evaluation for Volume 
Rendering", in proceedings of SIGGRAPH '90, Com- 
puler Graph~cs, 24(4), pp. 367-376. 

W H ~ E D ,  T. (1980). "An Improved Illumination Model for 
Shaded Display", CACM, 23(6), pp. 343-349. 

WHITED, T. AND D. M. WEIMER (1982). "ASoftware Testbed 
for the Development of 3D Raster Graphics Systems", 
ACM Transactions on Graphics, 1 (I), pp. 43-58. 

WHITTW, T (1983). "Antialiased Line Drawing Using 
Brush Extrusion", in proceedings of SlGGRAPH '83, 
Computer Graphics, 17(3), pp. 151-156. 

WILHELMS, J. (1987). "Toward Automatic Motlon Control", 
IEEE Computer Graphics and Applications, 7(4), pp. 11-22. 

WILI~ELMS, J. AND A. V. GELDER (1991). "A Coherent Projec- 
tion Approach for Direct Volume Rendering", in pro- 
ceedings of SIGGRAPH '91, Computer Grnphks, 25(4), 
pp. 275-284. 

WILHELMS, J. AND A. VAN GELDER (1992). "Octi-ees for Faster 
lsosurface Generation", ACM Transaclions on Grnphics, 
ll(31, pp. 201 -227. 

WILLIAMS, L. (1990). "PerformanceDriven Facial Anima- 
tion", in proceedings of SIGGWPH '90, Completer 
Graphics, 24(4), pp. 235-242. 

WILLIAMS, P. L. (1992). "Visibility Ordering Meshed Polyhe 
dra", ACM Transactions on Graphics, 11(2), pp. 103-126. 

WITKIN, A. AND W. WELCH (1990). "Fast Animation and 
Control of Nonrigid Structures", in proceedings of SIG- 
GRAPH '90, Coinputer Graphics, 24(4), pp. 243-252. 

WITKIN, A. AND M KASS (1991). "Reaction-Diffusion Tex- 
tures", in proceedings of SIGGRAPH '91, Cornytrrrr 
Graphics, 25(4), pp. 299-308. 

WOLFRAM, S. (1991). Mathetnntica, Addison-Wesley, Read- 
ing, MA. 

WOO, A., P. P o u u ~ ,  AND A. FOURNIER (1990). "A SulVey of 
Shadow Algorithms", IEEE Computer Gmphics and Ap- 
plications, 10(6), pp. 13-32. 

WRIGHT, W. E. (1990). "Parallelization of Bresenham's Lme 
and Circle Algorithms", IEEE Computer Graphics and Ap- 
plications, 10(5), pp. 60-67. 

Wu, X. (1991). "An Efficient Antialiasing Technique", in 
proceedings of SIGGRAPH '91, Computer Graphics, 
25(4), pp. 143-152. 

WYUECKI, G. WD W. S. Srrws (1982). Color Science, john 
Wiley & Sons, New York. 

WWILL, G., 8. WWILL, AND C. MCPHEE~ERS (1987). "Sdid 
Texturing of Soft Objects", IEEE Computer Graphics and 
Applicaf~ons, 7(12), pp. 20-26. 

YAEGER, L., C. UPSON, AND R. MYERS (1986). " C o m b i n g  
Physical and Visual Simulation: Creation of the Planet 
Iupiter for the Film "201V"', in proceedings of SIG- 
GRAPH '86, Computer Graphics, 20(4), pp. 85-94. 

YACEL, R., D. COHEN, AND A. KAUFMAN (1992). "Discrete 
Ray Tracing", IEEE Computer Graphics and Applications, 
12(5), pp. 19-28. 

YAMAGUCHI, K., T. L. Kmn, AND ~ J I M U R A  (1984). "Octree- 
Related Data Structures and Algorithms", IEEE Com- 
puter Graphics and Applications, 4(1), pp. 53-59. 

YOUNG, D. A. (1990). T k  X Window System - Progrnmmlng 
and Applications with Xt, OSF/Motif Edition, Prentice- 
Hall, Englewood Cliffs, NJ. 

ZELEZNICK, R. C., D. 8. CONNER, M. M. WLOKA, ET AL. (1991). 
"An Object-Oriented Framework for the Integration of 
Interactive Animation Techniques", in proceedings of 
SIGGRAPH 91, Computer Graphics, 25(4), pp. 105-112. 

ZELTZER, D. (1982). "Motor Contml Techniques for Figure 
Animat~on", IEEE Computer Graphics and Applications, 
2(9), pp 53-60. 

ZHANG, Y. AND R. E. WEBBER (1993). "Space Diffusion: An 
Improved Parallel Halftoning Technique Using Space 
Filling Curves", in proceedings of SlGGRAPH '93, Com- 
puter Gmphics Proceedings, pp. 305-312. 





Subject Index 

Absolute cwrdmnale. % 
A-bder  algorithm, 47576 
Amustic digit*. 66 -67 
Adive edge b t ,  I 22, m 
A c t i v e m a h  LCD, 47 
Adaptive sampling. 53-40 
Adaphve spahdl subdinsmn: 

BSP me, 362 
ray t r a h g ,  536-3 

Additive culor model. 569.572 
f f i e  hamformation, 203 
Alias+ 171 
Alignment (text), 166 
Amb~ent light, 497 (see al?o Illwninationmodels) 
Amhent reflemon crreftioent, 499 
Amencan h'ahonal Standards LNtitute (ANSI), 78 
Angle: 

dlrrdion (vector), 606 
inndence, 499 
phase, 595 
refradim, 509 
rotation, I86 
sperular-reflemon. 501 

Angshm, Hh 
Anrmahon, 584 

acrelerations. 591-94 
action specficatiom, 587 
applicahons, 5-7, 17-18.19-24 
cels, 588 
color-table, 586 87 
d i m 1  mohon spec~ficdnon, 594-95 
double buff&& 55 
dynamics, 595-W 
hamehy-hame, 5R5 
funmons, 586 
goaldvectd.  595 
~n-behwem. 58.5 
Inverse dynamics, 5% 
Inverse bncmatics. 596 
key hame, 585 
hy-frame svstern. 587 
kinematics, 588.595-% 
Kochanrk.Barteb splme., 325-27 
languages, 597 
rnorphing 18.588-91 
mohon specification, 594-96 
obpct definitions, 585 
paramehimi syslm, 587 
phys~caUy h a d  modehng. 393-95.588 5% 
raster methods, 586-87 
real-time, 55. 585, 586 
scene dexriphon, 587 
scripting system, .W 
storyhoard. 585 

ANSI (Amencan National Standards Institute), 78 

Antilliasmg: 
area boundlria, 176-78 
area samph& IR ,  174,539 
tiltenng, 174-75 
lines. 172-76 
Nyquist sunplurg mterual, 171 
PlmRvay-Wa-n, 177-78 
pixel phasinb 172,175 
pnxel-wetghbng musks, 174.5% 
prefilkrin& I72 
p t f i l t m g  172 
m ray tracing, 5%-43 
stochastic sampling, 540-43 
supersampling. 172-74538-40 
surface boundaries, 538-43 
in t e x t m  mapping, 354-56 

AppLcahon icon. 273 
AppLcahonr (m (;raplucsapphcaho~) 
Approximation s p h ,  316 
Area dipping 237-44 
Area tilling: (sm also Fil: a m )  

anhd hsmg,  176-78 
boundary-fill algorithm, 127-30 
bundled attributes., 169 
curved boundanes. 126-30 
f l d - f i l l  algorithm, 1X1 
fundons, 131 
hatch, 158, 161 
nonzero rnndmg number ~ l e ,  125-26 
oddpven rule. 125 
scan-line algorithm, 117-27 
soft fill. 162-63 
tint fill, 162 
unbundled attributes, 168 

Area sampling. 172,174,539 
Aspect ratio, 40 
Aspeci sourn h e ,  168 
Areasubdmv~slon vs~bility algorithm, 482-85 
Arlifinal reality ( u r  Virtual reality) 
Anentuation funchon. 506 
Attnbute, 77 

awa-fill, 158-h1, 169 
bundled. 168-63 
brush, 149-52 
characier, 163-tg, 169-70 
color, 154-5: 
curve, 152-54 
graywale, 157 
individual, I68 
lnqumry funci~ons. I70 
~ntcnsiiy level. 3 5  (wal.w Color Intensity 

levels) 
h e  color, 149 -W, 168-69 
hne type. 141-%, 168-69 
line width, 146- 49, 168-69 
mark-, 167-68,1711 
parameter. 144 
p n ,  149-52 

Smlctyl.2, -54 
syrtcm iat, IU 
table, 306 
kxi, 163-68.169-70 
unbundled. 168 

Axis 
reBection, 201 
&tion. 186,41bM 
shear, m 

hdsvertor (nulation), 414-15 
.h i9  vectors mid, 609 
Axonanrtric propciion, 440 

Back-facedetection 471-72 
Back plane (clipping) 447 
Backgmund (ambient) light, 497 
Bardrart, 11-12.137-3 
Barn d m m  (light conhol), 5W 
Bascline (charader], 164 
BasevPdor, 609 (rcr also Bawd 
Bass: 

coordinate vectors, 609 
normal, 609 
orthogonal, 603 
orthonormal. 609 

Bask functions, 319 (YP also Blending 
functions) 

Basis matrix (sphe), 320 
Lhm-penebation CRT, 42-43 (seealso 

Cathoderay tube1 
h t e m  polynomiak, 327 
Betaparameter, 345 
W-spline, 345-47 
Bevel jom. 149 
Mzier: 

blending hmmons, 327-28 
&plinr convemons, 350 
c l o d  curve. 330 
N ~ I C  Curve, 331 -33 
mwa, 327-33 
dmgn  lechnique, 330-31 
matrix, ,333 
pmpemes, 329-30 
surfaces, 333-34 

Bias parameter (splime). 325,,346 
Binary spce-partitioning I&, 362 (.ur 

BSP hve) 
Bind~ng (language), 78 
Biserhon mot finding. 622 
BitBlt bit-block transfax), 210 
Bit map, W (ser alm Frame buffer1 
Bitmap font, 132-33 
Blerdurg lundions, 319 

BP7.iax, 327-28 
Bspline. 335 



Subject Index 

Blading MON (ccnf.) 
cardirul. 325 
Hamite, 323 

B W  h.ndcr. 210 
Blotby o+-t, 314 

=Xmcter ,  164 
nonrigid, 393 
rigd. l a 5  1% 

b l e a n  oporahonr: 
a ~ - f i l l ,  161 
mur ~ f o r r m t i o r u ,  210 

Bowduy cunditms ~ ~ ) .  317,318-19 
Boundary-fill algorithm% 

Bmnn~atd rrgion, 127 
Cconmc(ed q o n ,  127-30 

Boundary representation, 305 
Bounding: 

b x .  161 
re-ngk, 94.161 
volume, 9.5 

Box covering, 366 
Box tiller, 174-75 
Box dimension, 246 
&rep (boundary rrpnerentalion), 305 
Breyrrham's algorithm: 

cirde, 98 
h e ,  88-92 

Baightna (light), 566 
Bmwnian motion. 3i7 
BN-h and pen attributes. 149-52 
BSP. 

ray hacing. 536 
me, 362 
visibility algonthm. 481-82 

B-spline: 
Beaer conversicm. 350 
blending fvnctioru, 335 
CoxdeBmr mcunion formulas 335 
cuhic. 3.79-41 
curves. 3 - 4 4  
knot vector, 335 
loral control, 335,336 
matnx. 341 
nonunlform. 336.343-44 
n o n u ~ f o m  rational (NURB), 3 7  
open. 36,341-44 
penod~c, 337-41 
pmpemes, 335-36 
quadratic,338-39,342-44 
rahonal, 347 
surfaces. 344-45 
tenslon parameter, 341 
uniform. 336-44 

Buffer, 40 (set elso Frame buffer) 
Bump function, 5% (maim Frame mapping) 
Bump mapping, 558-59 
Bundled attrihutq 168-69 
Bundle table, 16.3 
Busmess vsualization. 25, 395 (scealso Data 

visualuahon) 
Butt h e c a p ,  147 
Button box, 6 I .  279 

C 
Cabinet pro*ton. 443 
CAD,4-11 
Calligraphic (valor) d~splay, 41 
Camera wewing. 433-36 
Camera lens effects. 541 
Caphne (character) lh4 
Cardlnal spl~ne, 323-25 

Cud*, 159-10 
~ E m r d i r u t a . 6 0 0 - 6 0 1 , 6 0 2  
G l h o d m y  lube, 36-40 (set & Vidm monitm) 

mpcr ratio, 40 
tam intarily, 28 
tam pemmtion. 42-43 
cola, U-B 
cumpame. 37-38 
el- guh 37 
de l t addh  dudow c w k ,  43 
elecbatllic beam deflcrtion, 28-39 

-A,, 
Xe urn--, 44 
mgnclic baun dulcction. 37. J8 
persistare. 39 
phosphor, 37-39 

rate, 40-41 
r e c l o ~ n , 3 9 4  
RCB, 45 
shadow-& 43-44 

G m d - R o m  spline, 325 
G v a k  proimion. 443 
Cell a m ,  131 
cell&% 
Cek, 588 
Center d projection, 4.33 
Cenlrdl structwe stom (US), 251 
CGI (Compltcr Craphicr Interface), 79 
CGM (Computn Graphics M e m e ) ,  79 
C h a r a d r  

a~tnbu@, 163-63 
baselme, 164 
body, 164 
bottom line 164 
capline, l M  
color, 164 
descender, 164 
fonts, 132, 163 
function$, 163-168 
genention, 132-34 
grid, 55-56,132-U 
height, I64 
italic, 163 
kern, 164 
outline fonts, 55-54,132 133 
text precision. 166-1U 
tophe,  164 
t y p e f a ,  132-33, I63 
up vector. I65 
width. 164-65 

Charaaeristic polygon, 31 h 
o u r t  

bar. 11-12, 137-38 
pie. 11-12,138-40 
b e ,  11. 13 
Imp, 11, 136-37 

Choice input device, 276.279 
Chmmahcily, 567 

diagram, 569-71 
values, 569 

CIE (International Commirs~on on Illumination), 
w --- 

Circle equation 
CaResian, 97 
nonparametnc, 97,619 
parametric. 97,619 
polar, 97 

Circle-generating algonthms, 97-102 
Bresenham, 98 
midpoint, 98- 102 
m~dpoint function, 96-98 
m~dpoint d a ~ s i o n  parameters. 99 

C i l e  symmehy, 47-98 
ClioDinn: 

&s,"237-44 
Cohm-Sutherland line algorithm, 22640,232 
curves, 244 
Cvrus-h-k line alnorithm. 239 
&or, 245,246 
hardware unphentat ion,  463-64 
b homogene& caardirutar, 461-63 
Lung-Be&y line algorithm, 230-32 
Liang-Barsky polygon algorithm, 243 
Nichol-Lae-Nichol line algonthm. 233-35 
no-rrgutsr window, 235 
in normahzed cwrdr~tes, 224,458-61 
parallel methods, 239 
parametric, 230-32 
planes, 447-93,456-63 
pOmb, 225 
polygons, 237-43 
region ccdes, 227,460 
stralght h e  segments, 225-37,456.M-61 
Sutherland.Hodgman polygon algorithm, 

W3-42 
text, 244,145 
thrrP-dunmsional, 456-0 
twc-d~rncnsional, 224-45 
view volumes, 447-50,456-63 
Weiler-Alherton polygon algorithm, 242-43 
window, 224 
in world coordinates, 224 

CMY color model, 574-75 
Codes (ray tranng), 541 
Coeffinenl: 

ambient-~fla-tion 499 
diffuse-reflection, 498 
matrix, 6M 
specukr-reflecbon, 501-2 
hmSpaI'tIIV, 510 

Cohen-Suthnland line-clipping algorithm, 
226-30.232 

Cohemnce. 119-24. 471 
Color 

chmmathty. 567 
chmmaticlty diagram, 569-71 
chmmatic~ty values, 569 
coding, 25,396 
romplementa~,  569.570 
cube, 572-73 (wralso Cobr models) 
dominant frequency, 566 
dom~nant m,avelength, 566,569-70 
fill, I%-@ 
gamut, 568.570 -7; 
hue. 566.575.5i9 
~lluminal C, 570 
in iUum~ndt~on models, 507-8 
inhntive concepts. 571-72 
l i g h t n e  (HLS parameter), 579 
line, 149-52.168-69 
lookup table. 155-56 
marker, 168, I70 
matching iunctrons, 56R 
model, 565. -568 
monitor, 42-45 (serelr, hdeo mon~tor) 
nonspectral, 571 
percephon, 566-67 
primarips, 568 
pure, 567,569 
punty, 567 
purple line, 570 
RGB, 155-9 
saturation 567. 575,579 
reledon cons~derat~ons, 580-81 
shades, 571, 577 



Sub~ect  Index 

spearum (electromagnetic), 565 
standad CIE priman-, 568-69 
Cable. 155-56 
text, 164,169 
lints, 571,577 
tones, 571,577 
rristimulus vision theory. 572 
value (HSV parameter), 575 

Color model, 565.568 
additive, 569,572 
CMY, 571-75 
m, 579-80 
HSB (srr HSV model) 
HSV, 575-77 
HSV-RGB conversion, 578-79 
RGB, 572-73 
RGBCMY mnvenion, 575 
XYZ, 569 
na 574 

Color-tableanimation, 586-87 
Column veclor, 611 
Command imn, 273 
Commission Internationale d e  l'tchuage (( 

568 
Complemrntary colon, 558,570 
Complex number 

absolutevalue, 616 
conjugate, 616 
Euler's formula, 61 7 
unaginary prI, 615 
length (rnodulusl, 616 
modulus, 616 
ordered-pair representation. 615 
polar represenlation, 616-17 
pure imaginary, 615 
real part. 615 
mts, 617 

Complex plane. 615 
Composite I ~ o N ~ o ~ ,  44-45 
Composition (mstru), 191 
Computed tomography (CT), 32 
Computer-aided design (CAD), 4-11 
Computer-aided surgery.33 
Computer art. 13-18 
Computa Graphics Interface (CGI), 79 
Computer Graphio Metafile (CGM), 79 
Concatenahon (matrix), 191,612-13 
Concave polygon splitting, 235-37 
Cone filter, 174, 175 
Cone receptors, 572 
Cone hadng. 500 (xr alsoRay tracing) 
Conic curver, 110-12.348-49 
Conjugate lmmplexl, 616 
Consbnt- inlai ty shading, 522-23 
Constramb, 288-89 
Constructire solid gmmeby (133,356 

mass calculations, 359 
octree methods, 361-62 
rayiat ing methods, 357-59 
volume calmlations, 358-59 

Continuity mnditions (spline): 
geometric. 318-19 
parametric, 317-18 

Continuity parameter, 325 
Continuous-tone Images. 515.516 (m alw 

Halftone) 
Contour (intensity border), 515.518 
Contour plots: 

applications, 11, 12. 25 
surface lmes, 489-90 
threedimensional (isosurfaces), 398 
twodlmensional (wlines). 3%-97 

Contrachon (tensor), 402 

Control graph. 316 
Control icon, 273 
Control operahons. 78 
Control point isplme), 316 
Control polygon, 316 
Conbd  surfacp ltena~nl, 376-77 
Convex hull, 31b 
Coordinate-axis rotations, 409-13 
Coordinate-ax~s vmors (bask). 609 
Coordinate extents, 94 
Coordinate point. 602,605,612 
Coordinates. 

absolute. 96 
current posihoo, 96 
homogeneous. 189 
relative, % 
screen, 114 

Coodinate system. 
Cartewan, 600-601, 602 
curvilinear, 602 
r).lindrical. 603 -4 
dewce, 76 

XE), left-handed, 435,602 
local. 76,265 
master, 76, 2b5 
modelin& 76,265,426-29 
normalized device, 76 
normal~zed prupchon, 458 
orthogonal. 603 
polar 60-2  
nght:handeb% 
sown,  54.76, I I4 
spherical, 6C4 
threed~mewonal, 602-4 
translomatlon of, 205-7,2219-20.426-29 
h-mdimens~onal. 600-602 
irrrl, 435-38 
viewing, 218,219-20.432-36 
world, 76 

Copy function. 213 
CoxdeBoor m r s i o n  lormuhs, 335 
Cramer's rule, 621 
Cross hatch fili, 1% 159 
Cross pmdua  l tmorl .  608 -9 
CRT. 36-00 (wdw Cathode-rav tube) 
CSG. 3% tur niw Con5trudive geomeu 
CT (Computed Tomography) scan, 32 
Cubicspline, 112, 319 

beta. 346-47 
Mzier, 331 -33 
8-spline, 33?-41 
interpolation, 320-27 

Current event m ~ ~ r d ,  286 
Current position. 96 
Curve 

atmbutes. 152-54 
beta spline, 345 -46 
Mzier splinc, 3Z7 
8-spline, 334-35 
cardinal spline, 323-24 
cadioid, 130-40 
Cahnull-Rom spllne, 325 
circle, 97, l l i  
con~c sedion, 110-12.348-49 
ellipr, 102-3 
fractal, 362-M ( w t  alsoFractal cun-es) 
generalized tunction, 113 
Hermite spline. 322 
hvperbola, Ill, I12 
Koch (fractal). 367 
Kochanek-Bartels splme, 325 
Iimacon, 139-40 
natural spl~re,  221 

Overhauser spline. 325 
parabola. 112 
paralklalgorithms. 112-13 
parade rcpmentatlons, 112.619 
pieceu~~constn~ct ion.  315-16 
polynomial, 112 
spiral, 139-40 
spline. 112.315-20 (see also Spl~ne curve) 
superquadric. 312- 13 
s)mmetrycons~derat~ons, 97-98, 103,112 

Curved swlace, 
ellipwid. 311 
pdramerric representations, 619-20 
qwdric. 310-12 
rendenng (xe Surface rendenng) 
sphere 31 1 
spline, 316 (scea1roSpline surface) 
superquadric, 312-13 
tarus, 311.12 
wtb~hh :  487-90. (set alco Visible-surface 

detemon) 
CurviLnear coord~nates. 602 
Cutaway :lW% 305,302 
Cylindrical candwe,tez., 603-4 
C y w - k k  line-chpping aigor~thm, 230 

Damping constant, 595 
Dilshed be, 14-46 
Data glove. 64.65.292-93 (sernlsoV~rtual realttyi 
Data tablet 64 (secrlso Dig~tlzer) 
Data nsuaiuahon: 

appl~cat~onr. 25-31 
contour plots, 3%-97 
field bes. 400 
glyps. 103 
isollns, 396-97 
irosurfaces, 398 
mulhvanate fields, 402-3 
peudocolor methods, 396 
mla r  fields, 395-99 
streamlines, 400 
tensor fields, 001-2 

7' vector hrlds, 4M)-401 
volume rendenng, 399 

DDA lim dlgurithm, 87-88 
Deflection coils. 37.38 (sn nlm Cathode-rav tubel 
Deltadelta shadox-mask CRT, 43 
Densiv function (blobby object). 314 
Depth-butler algorithm, 472-75 
Depth cuelnn. 299-300 
~/ph.rorhng algorithm, 478-81 
k e n d e r  (character), 164 
Detrctabhtv filter, 284-85 
Detcrmindnt, 613-14 
Devlce &a, 281-82 
Device cardinates. 76 
Differential valinp, 188 
Diffuse relledion, 497-500 
Diffuse mraction. X9 
Digitizec h$ 

aauac!. 65 .6  
acoustic. 66-67 
appl~catiom, 13-15 
electromagnetic. 65-66 
locator dev~ce. 277 
r~lolution, 65.66 
smtc, bh 
stmkedr\.~ce, 27? 
thrpcdl-nensional. 67 
valuator devrce, 278 



-, 
E n r q y  propagahon ( rad~c*~n, t  .544 
Environment array, 552 
Environment mappmg. 52 
Error-diffus~on algorithm, YO-22 
Euler'b lormula, 617 (rcrulu Cornplcr numbcnl 
Even-odd polygon-filling mlr .  I25 
Event, 285 

input mode, 281,285- 37 
queue. 285 

False-ps:t~on root hnd~np.  r C ?  
Far planekhpp~ng), 447 
Fast Phong shad~np,, 526-:- 
Feedback 275-761 
Ficld Itnes, 4W 
FjII. 

algorithms is-c' Area filhn;? 
area, 77, 117 
attributrs, 1%-67 twrulw Area hlhng) 
color, 158 
hatch, 159. I61 
p a l t r m ,  159-62 
soft, 162-63 
sIyle. 158 
hnf, 162 

Filter. 
box, 174 175 
cone, 1?4, 175 
function, 174 
Gauss~an, 174 -75 
structure, 253-54.284 - 8' 
worbtatlon ptck detrnabdln, 284-85 

Flxed p l h o n  (ualmg). IN1 193, 421 
Raps  (light controll, 504 
Rat-pane. displav, 45 

emissive. 45 
g a s d ~ u h a r g r .  45 
Itght-enulhngdiode (LEE), 4h-47 
I tqu~d-wsta l  (LCD), 47- 48 
nonemksive, 45 
pawvcmntri\ .  47 
plasma 45-46 
Ihin-film electmlun~mmct~nl. 46 

Rat s h a d m ~ .  522 
Fltghl simulators. 21-24 
Rod-fillalgorithm, 130 
flood gun, 45 
F a u s  potnl (ell~pwl, 102 
Font, 132 (wr nIw Typlacal 

bttmap, 152-33 
cache, 133 
outline, 132. 133 
pmportionall!. spact'd. I h.1 

Forcr constant. 393 
Form lactars (radiosty). FJt 
Forward d~ffercnr~s ,  351 -5.1 
4-connccld reglnn, 127-2'1 
Fractal: 

afhne o n s h u m o n s ,  3 7 2 - 3  
box-ro\.ering method% b b  
Bmwnlan mohon, 372-78 
characteristics. U2-6.3 
clas~ficalion, .%+I 
dimension, 363.364-67 
generalton pmcaiur?s. .%I - M 
generator, 6 7  

geomelnc constructtons, 367-71 
gmmclry, 3 2  
~nlhalor,  36i 
~nvariant set. .W 
random m1ap1n141splarement melhodr, 

373-78 
self-affine, 3M 
=If-~nversr, 361 
self-inversion methods, 385-87 
self-srmilar W 
self-sirnilari~y, Mi 
self-squaring. 364 
self-squanng methods, 376-85 
s~milarity dimcnslon, 365 
subdiwsion rnethds .  373-78 
topolo~tcal mverinp, metllods, 365-66 

Fractal curve. 
Brownian mohon 372 
d m e n s ~ o n ,  % 
fractional B r o w n ~ . ~ n  motion, 372-73 
g m m e h i r  ronsl~urtions, 367-68 
Invarmnl. 379- 8: 
rnversion consmciion methods, 385-87 
Julia set, 370 
Kwh. 367 
Mandelbml set k u n d a r y ,  38-84 
m i d w i n t  dimlacunent.  373-75 
reark ,  366 ' 
self-affine, 372-75 
self-inverse, 385-87 
rdl.smllar,  ,367-71 
sell~squanng, ,379 -ffl 
snowflake, 267-b(i 

Fractal soltd, 36h 
Fractal surfacr 

Brownian. 372-711 
d~mension.  3M 
lour-dmenr~onal.  ,W-85 

self-sim~lar, 364-71 
,elf-squaring, 364 -85 
surfacc rendering 376 
terrain, 372-78 

Frart~onal Brownldn motion,372-7e 
Frachonal dimemton. 366 
Frame (anmation). 585 
Frame buffer. 10, &4 

h~t-block tran4efi. 210 
copy funclton. 210 
loading intens~ty values, 94-95 
lookup table. 195- 56,513 
raster l ~ a n s f s r m a l m q  210-11 
read funmon. 210 
resolution. 40 
write funrltun, 2111 

Frame mapping, 559-60 
Fresnel reflection laws, 501 
Frequency spectrum (electromagnetic). 565 
Front plane (cltppmg), 447 
Full-color system. 4:. 
Frustum. 447 
Funchons, 77-78 (sw also Functmn Index) 

Gamma corrcmon. 513-15 
Gamut (color), 5h6, 570-71 
G a s d i x h a r g e  dlapl.~ys, 45 
Causstan bump, 314 
Gaussian density function, 3 i 4  15 
G a u s i a n  dim~naholr.  621 



Subject lndex 

Gaussian hlter, 174-75 
Gauss-Scidel method, 621 
Genaator (fractal). 367 
Geometric continuity (spline). 318-19 
Geometric models, 261 
Ceometric&jed proprrtico, 114-17 
Ceometnc production ~ 1 e .  387-89 
Geometric table, 306-7 
Geometric banrfomutions, 77,184,108 
GKS (Graphical Kernel System), 78 
GL (Graphia Ltbrary), 76,251,264,327.432, M. 

435,439.458 
Gbhal lighting effects, 497,527,544 
Glyph, 403 
Cml-dirPded mobon, 595 
Gouraud sbding model, 523-25 
Grahal, %9 
Graph~cal u m  interface: 

applicahonr. 34 
backup and error handlmg, 274-75 
componentr. 272-76 
feedback, 275-76 
help facilitin, 274 
icons, 34,273 
inleramve techques,  288-93 
menus, 34,273 
model, 2 n  
user dialogue, 272-73 
user's model, 272 
mndows, 34.273 

G~aphics applications, 
advettisin& 8.17-18 
agrirulture, 27,28 
an~mations, 5-7.17-18.1'3-24 
archlteclure, 10. 11 
art, 13-18 
astronomv. 25 
bus~ness.il-13. 17-18. ~ 5 . 3 1  
CAD. 4-11 
rdrlugraplrv, I1 
education, 21-24 
engineering. 4-9 
entertanwnent. 18-21 
facllity planning. 9, 10 
flight sunulators, 21-24 
geology 32 
graphs and charts. 11-13 
Image processing. 32-33 
manufacturing. 8-9 
mathemaha, 14-17.25-27 
med~c~ne. 32-33 
modeling and smulations, 4-8,21-25.25-31 
physical soences. 25-31,32 
pub l~shq ,  17 
scientific veualualion, 25-31 
simulations, 5-10.21-31 
simulators. 21-25 
training. 21-24 
user ~nterfaces. 34 
v~rlual reality, 5-8.466-67 

~rmhics  controller. 55.56 
C,raphm funrt~ons. 77.78 L .  olu, F u n d ~ o i  Index) 
C r a ~ h l n  munllon. 36-52 (w J I M  Vldw monitors) 
Graphics mf!ware packages 

h a w  fundrons. 77-78 
GKS. 78 
GL, 76,251, 2M. 327,432,434,435,439,458 
PHIGS. 78 
PHIGSt, 70 
standards, 78-79 
threedmensionai, 302-3 

Graph ploltmg. 1.X-39 (w;~nl.oCharls) 
Ciraphrcs tahlet. J.3- 15. 64-67 ~ s r ~ r l s o  Dig~nzerl 

Gnvitational acceleration, 111 
Gnvity field, 2% 
Gnyscak, 157 
Grids 

charmer, 55-56.132-33 
In interdctive wnshuctions. 283-90 

Halhone, 516 
apptuximatiom, 516-19 
color methods, 519 
d~therink 519-22 
patlems, 516 

Hslhvav vector. M3 
~ a r d - & ~ ~  drvlccr. T-75 
Hatch fill, 159.161 
Hadorlf-f-Besicovitch dimension, 3t4 
Had-mounted disply, 6-7 lur also Virtual 

reality) 
Hemicube (radiority), 518-49 
liermitc aplme, 322-23 
Hexcone (HSV), 576 
Hidden-he eliminabon, 490 
Hidden3urface elimination, 470 lscedw Visible 

surface detection) 
Hierarchical modehng, 266-68 
HighAefinilion monitor, 40 
H~ghAghting. 

as depthecueing techn~que, 299-500 
pnmhves, 287 
rpeollar reflections. 497,500-504 
shuctures, 253-54.287 

HLS color model. 579-80 
Homogenms coordinates, 189 
Hooke's law. 393 
HonZOntaI retrace, 41 
Homer's polynom~al fadonng method, 351 
HSB color model (xc HSV model) 
HSV color model, 575-77 
Hue. 566.575.579 

Imn. 34.273 
Ideal reflector. 498 
Illuminant C. 570 
lllumination model, 495 

ambient light, 497 
altentuation funcnon. 506 
basrc components, 497-51 1 
color considerations. 507-8 
comblned diffuse-speculr, 324 
difhire reflection, 497-500 
flaps. 504 
ideal reflector, 498 
intensity attcntuation, 505-6 
hght sources, 496-97 
multiple light sources, 504 
opaotv factor, 510 
Phong, 501-4 
mhamon, 508.10 
shadows. 511 
Snell's law, 509 
specular reflectmn. 500-504 
spotlights, 504 
lransm~ssion vectrr. 510 
transparen?, !:-I1 
Warn, 504 

Image-order scanning, 554 

Image procasing, 32-33 
& m a g  scanners, 67.68 
lmage-space methods (visibility detmon),  470 
ImagLvry number, 615 
lmpaci printer. 72 
Implicit mprepenlaHon. 618 
In-behueens, 585 
lndex of refrardon, .W 
I~hIliabor (fT0dal). 367 
Ink-kt ~rinler. 72-73 
~nne'r p;oduicvaor) ,  607 
In-11ne shadow-mask CRT, 43 
Input dences 

bunon bax, 61,279 
choice, 276,279 
d a b  glove, 64,65,292-93 
dials, 61.62 
dig&, 64-67.277-80 
graph- lablet, 64 
inihalizing, 287-88 
pyst ict  63-64.2TI-80 
keyboard. 61,277-80 
tight pn, 70.71 
locator, 276,277 
lozical classification, 276 
m & c ,  61 -62,277-80 
pick. 276, 279-80 
scanner, 67.68 
spaceball. 63 
siring, 276.277 
strcke. 276, 277 
switches. 61,62 
threedimensional sonu digitizers, 67 
touch panel, 68-70 
trackball, 63 
valuator, 276,277-78 
voire system, 70-71 

lnput functions, 78,281-87 
lnput modes: 

concurrent use, 287 
event, 281,285-87 
request. 281.282-b5 
sample, 281.285 

lnput priority, 283 
Inquiry functions, 170 
Insideoutside test: 

polygon odd-ven rule, 125 
polygon nomerowinding number rule, 125-26 
spa t~d  planesurface, M8 

Inside polygon face. M8 
Instanm, 261 (see also Modeling) 
Integral quat ion solving: 

rectangle approximations, 622 
Simpson's rule, 623 
trapezo~d rule, 623 
Monte Carlo methods, 623-24 

Intew~ly 
attcntuanon, 505-6 
dep!h cuelng, 299-500 
~nterpolation shadlng Gouraud), 523 
modeling. 495-97 (see alvr lllumination 

models) 
radioslty model, 54-51 

Inlmsily level, 
adjustrng (srr Antialiasing) 
assign~ng 512-13 
color lookup tables, 155-56 
conlours (borders) 515. 518 
frame-buffer storage, 240 
gamma correction, 513-15 
ratio. 512 
RGB, 507- R 
vidm lookup table. l55.513 



Subiect Index 

Interactive P~C~UR construction techniques, 
L88-92 

Intcrlacmg scan Lnes. 41 
Intmtionsl  Commls~on on fllumwhon (CIE). 

568 
Interpolahon splur, 316 
Inverw geomehic hnslormations, 190,409,413, 

421-22 
I n v m  dynamics, 5% 
Inverse kmematics, 5% 
Inverse matrix, 614 
l n v m  quatemlon, 618 
Inverse wanning. 554 
lSO(1ntemational StandardsOrganluhon). 78 
Isohnes, 3%-97 
lromchic pystick, 64 
isometric p m p c m .  440-41 
Isosurfaces, 398 

Jaggies, 85 (we alsa Anhallaslng; AntLliasing) 
Jittering. 541 
Joyrhck: 

as localordevice. 277 
movable, 63-64 
as plck device. 279 
pmsure sensitive (wrnetric), 63, 64 
as stroke dev~ce, 277 
ar valuator device. 278 

Julia set, 379 

Kern, 164 
Keyboard, bl 

as choice device. 279 
a, locator devm. 277 
a5 pick dev~ce, 280 
as stnng devlce, 277 
ar valuator device. 278 

Key frame, 585 
Key-hamc system. 587 
Kinematics, 588.595-% tw rrlso A~mat ion)  
Knot vector, 335 
Kochanek-Rartels spline. 325-27 
Koch curve, 367 

Larnkrtian nfleclor. 498 
L~mbr t ' s  coslne law, 498 
Language binding. 78 
Law printer, 72 
LCD (I~quid-crystal d~splay), 47-48 
Least-uluares data f i t l in~.  625 

Legble typeface. 132 
Length 

mmplex number, 616 
veaor, 605 

L-grammar. N19 
Liang-Barsky chpping. 

polygons. 243 
two.d~mens~ond Imrs. 23-32 

L18hl. 
arnhwnl, 497 
angle of lncrdence, 4W 
chromatic~ty, H 7  

644 

chromatlnty diagram, 569-7l 
diffuse reflection, 497-500 
dl- refraction, 50) 
hqumcy band, 565 
hue, 566 
ideal relrctDr, 498 
i n d a  of refraction, 339 
illurninant C. 570 
illumination model 495 (ur .Ira llhrmmtion 

modek) 
intensity-level assignment, 512-13 
L a m k d s  cosine law. 498 
Phong rpecuhr model, 501-4 
pmp;;ti'es. 5-55-13 
pur ig  5b7 
reflemon mfficientr. 4W-502 
rehadion angle, 509 
mhlration, 567,575,574 
spechum. 565 
specular rrllection, 5a)-YW 
sperukr rehadion, 509 
speed. 566 
hansparenq coefficient, 510 
wavelength. 566 
white. 567.570 

Light b k e r  iny tracing), 537 
Light-mitting diode (LED), 46-47 
Lighting model, 495 Cut also illumination model) 
Liclhlness (HLS ~aranwer) .  579 
~ i i h t  pen, 70.7i 
Lieht source. 

dommant frequency, 5h6 
dominant wavelenah. 566 
energy distributio<567 
frequency distribuhon. 545 
lum~nance, 5b6 
multiple, Y)4 
polnt, 4% 

Lima~on, 139.140 
Line. 

bundled attributes. lbF-69 
chart. 11.136-37 
clippmg, 225-37 (rr alw L~ne  clipping) 
color, 149-50 
contour, 11, 12,25,3%--97 
dashed. 14-46 
function, 95-% 
parametric representatton. 230.444 
pen and brush options. 149,154 
samplmg. 87, RR-84 
s!opelntercept equahon. 86 
type. 15.1-46 
width, 146-49 

Linear congruenlial generator, 624 
Linear equation rolvlng: 

Cramer's mle. 621 
Gaussian elimination, b21 
Gauss-bidel, 621 

Line caps, 147 
Lme clipping. 
Cohen.Sutherlsnd.226-30.232 
Cym-Beck, 230 
Liang-Barsky, 230 -32 
N~chd-Lee-N~chol, 232-35 
nonrectangular chp wmdow. 235 
parallel methods. 239 
paramctnc, 230-32 
i h d i m e n s ~ o n a l ,  4 5 0  

Linedrawng algorithms, 8b-95 

DDA, 87-80 
h a m c b u h  Imding, 94-95 
parallel, 92-94 

L iqu idqsb l  display (LCD). 47-48 
Local c m r d ~ ~ l s ,  76,265 
k a l  mnhol ( s p h ) ,  332,335,336 
Lna l  transformation matrix. 266 
Lmator m p t  dev~ce, 276,277 
@cal input device, 276 
Imk.at point, 434 
Lookup lable, 155-56.513 
Luminance. 544. % 

Mach band 525 
Mandelbml set, 381-84 
Marclung cubes algorithm (sw Isosurlaces) 
Marker, 133-34 
Marker ahibutes 167-68.170 
Mask, 146,517 (xc  also Pixel. mask) 
Mass calahhonr (CX;), 359 
Master cmrd~nalrs, 76,265 
Matnx, 611 

addihon612 
b s i s  (spline), 320 
Wzier, 333 
B-splme.341 
cardinal. 325 
cwffiomt, 620 
column, 61 1 
concatenahon, 191,612-13 
de te rmi~n t .  613-14 
dither, 5m 
Hermite, 323 
identity. 614 
Inverse, 614 
multiplication. 612-13 
nons~ngular. 614 
reflection, 201-3.422 
~ow.611 
rotahon, 1%. 1%. 193,410-12,418-20 
scalar multiplication. 612 
scaling, 187. 193. 192. 421 
shear, 203-4.423 
smgular.614 
splme charxter~zatson. 320 
square, 611 
translation. 18:. 190.40R 
transpaw, 613 

Medical applicat~ons. 32-33 
Menu. 34,273 
Mesh (polygon), Mh. 309-10 
M&ball mdel. 315 
Metafile. 79 
Metnc tenmr, 6W-ll 
Midpoint clrcle algorithm. 98-102 
M~dpoinldisplacement fractal generation. 373-78 
Midpoint elhpse dlgorithm, 103-10 
Mrter pin, 148-49 
Mode (input dev~ce), 281 
Model. 261 
Modelidg, ?61 (see nlw Graphlcs applicat~ons. 

Ob* repmmtalions: Illunrm.at~on models) 
basic concepts. 240-64 
coordinates 76.265.426-29 
display prowdures. 261. ZM, 
geonretrlr. 261 
hierarchical 2h2-63 
Instance. 261 
local coordmates, 265 
master mrdinates. 265 



Subject Index 

modules, 262 
paclges.  263-b4 
physically based, 393-95.588.596 
reprrsenlations. 261 -62 
srmctwv hieramhie. 266-68 
sy mbol. 261 
aymbol hierarchies, 26243 
hansfonnahons. 77.26S-68, U6-29 

Modules, 262 
Modulus (complex), 616 
Monte Carb methods, 623-24 
Monitor, 36-52 l s n  a1.w Video monitor) 
Monltor response curve, 513 
Morphing, 18.588-91 
Motion blur, 541,542-43 
Motion spenhcation, 594-96 
Mouse, 61-63 

as choicedevice. 279 
as locator device, 277 
as pick device, 279 
as strokedevm, 277 

Multivariatr data visual~zation, 402-3 

National Televis~on System Committee (MSC ), 
514,573,574 

Natural splme, 321 
Near plane (clipping), 447 
Newton-Raphson mo(-finding 621-22 
Newton's second law of motion. 5% 
N~choll-Lee-Nicholl Ime-chpping, 233-35 
Notse (dither), 519-20 
Nonemmiw d ~ p l a y s .  45 
Nonemitter, 45 
Nonlinearquahon solving: 

bisedion, 622 
false-posiliun. 622 
Newton-Raphson, 621-22 

Nonparammc reprrsentahons, 618-19 
Nonrigid obpct, 393 
Nonsingular mamx. 614 
Nonspectral color, 571 
Nonundonn B-splines, 336.343444 
Nonuniform (d~lferentwl) scaling 188 
Nonuniform rational &spline ORIRB), 347 
Nonzero winding numberrule, 125-26 
Normal haw. 609 
Normalized device coordinates, 76 
Normalized propchon coordinates, 458 
Normalized n e w  volumes. 458 (see aka Clipping) 
Normal vector: 

average (polygon mesh), 523 
curved surface, 558 
mterpolat~on (Phong shading). 525 
plane surface. 308-9 
new-plane, 434-36 

NTSC (National Television System Committee), 
514,573,574 

Numerical rne+hods: 
bisedion method, 622 
Cramer's rule, 621 
fakeposition method, 622 
Gaussidn elunination, 621 
Gus-Seidel  method, 621 
integral evaluations, 622-24 
least-squares data filling 625 
Imear equallons, 620-21 
Monte Carlo methods, 623-24 
Newton-Raybn method. 621-22 
nonlinear equations, 621-22 
root nndlnk 621.22 

Simpson's rule, h23 
trapezoid rule. 6U 

NURB (Nonuniform rational &spline), 347 
Nyquist samplurg ~nterval, 171 

O b p t :  
nonngid (flexible). 393 
as pidurecompnenl. 77. 251 
rigid, 185.1%-97 

Obpa  geometry 114-17 
O b p t  repyentanan 

beta bphne. 34547 
Bezier splines. 327-34 
boundary (5rep1, 305 
&splines, U4-45 
BSP trees, 362 
blobby surfaces, 314-15 
CSG methods. 356-59 
rubic s ~ h n e  interwlalion. 320-27 
data v6ualiwho~. 395-403 
explicit, 618 
Ira& curvesand surfaces, X2-87 
implrit. 618 
nonparamelnc, 618-19 
Mm,  359-62 
parametnc. 619-20 
panicle systems. 390-92 ; 
physically b a s d  mdehng. 193-95 
polygon, 305-10 
quadric surfaces, 310-12 
rational spline. 37-49 
shape grammars, -37-89 
space-parhtiormg methods, 395 
superquadrics, 312-14 
Sweep COnStNCtlON, 355-56 

C++?clspace methods (visibillh deledon), 470 
Gblique projectinn 439.441-43. 447-50.452.43 
Octrpe. 359 

CSG operations, .MI-62 
generation, 360-hl 
visibility detedlon, 362.485-87 
volumeelenwnt, 360 
voxcl, 360 

Odd-even polygon-filling rule, 125 
U n e p n t  perspertive projection. 146 
Opanty factor. 510 
Order (spline curve contmulty). 317-19 
Ordered dither, 520 
Onhogoral hasr, 609 
Orthogonal coordinates. 603 
Orthographic pmjertions, 439,441,447--48 
Onhonomul bas~s. 6C4 
Odtllnt font, 132 133 
Cutput primitives, 77 

cell array, 131, 13; 
circle, 97-102 
character, 131-34 
conic M i o n ,  110 -I2 
ellipse, 102-10 
fill area, 117-30 
m a r k ,  133-34 
point, 84-86 
polynomtal, 112 
spline, 112 
straight line segrnmt, 85.86 -94 
text. 131-33 

Outside polygon face, M8 
Overhauser spline. 325 

P 
Pamtbrush programs, 13-16.291-92 
Painter's algorithm (depth sorting), 478 
Pannmg. 219 
Parahala. 11 2 
Parallel algorithms: 

area-hlling, 120-21 
curvedrawing, 112-13 
line-draw~ng, 92-94 

Parallel projection, 298-99. 438 
axonemetric, 440 
cahnet, 443 
cavaher, 443 
elevahon view. 440 
isometric, 440-41 
obl:que, 439,441-43,447-50.452-53 
orthographic. 439,441,447-48 
plan view, 440 
prinopal axes, 440 
shear transf0rmat:on. 442,453 
view volume. 447-50 

Paramcmc continuity (spline), 317-18 
Parametric representations, 619-20 

circle, 97. 619 
curve. 111-12.619 
ellipse. 103 
ellipsoid. 311-12 
sphere, 311.620 
splme. 112.315-16 
malghl line, 210.444 
surlare, 619-20 
torus. 311-12 

Parametrized svstem, 587 
Parity (odd-even) rule, 125 
Panic!e systems, 390-92 
Path (text). 166 
Passivematrix LCD, 47 
Pattern hll, 159-61 

index, 159 
reference point, 13-60 
representstion. 159 
size, 159 
tiling. 160 

Pallern mapping 554 
Pattern recognition, 277 
Peanocum, 3& 
Pel. 4 
Penand brush attributes. 149,150,154 
Penumbra shadow. 3 2  
Pcrfeci refledor, 498 
Persistence. 39 
Perspecrive projectm. 299.4.33 

frustum. 447 
onepoint, 446 
pnnopal vantshing p m t ,  446 
reference point, 438 
shear hastonnation, 454-56 
thwpoint ,  446 
twopomt. 446 
vanah~ng point, 446 
view volume, 447-49 

PET (Pos~tlon enussion tomography) 32-33 
Phase angle, 595 
PHIGS, 78 (st? nlw, Function Index) 

attnbutes, 145, 146,149,1%,lY)-59,164-70 
Input, 281.-87,')02 
mdeling, 26748,427 
output primitives, 95-%.113,131,133.302 
structures, 251-60 
threed~menrional transformations, 425-26 
th-dima~nsional viewing. 464-t6 

6 



PHICS (con! I 
fvc-dlmenslondl transforlnallons. 208-9 
bvo.d~rnens~onaI wrwmy;. 222-23 
vorkslal~on, 79 

PHIGS+, 78 
Phong specular rdectlon m d 4 ,  i01-4 
Phong shadmg. 515-27 
Phospher. 37-19 
Photoreallsm. 495 
Phgs~ally based rnodel~ng, 393-95. 588, 596 
Pick 

dastance, 279-80,28R 
hlter, 2E-l-85 
ldenhfier. 284 
mput ~ C V I C P ,  276 279-X0 
wmdow, 280 

P~ckab~l~ty (slmcture), 254 
Picbng: 284 
P~clure elemenl (pixel). 40 
Piecew~se approximal~on (splme:. 715 - I6 
Piechart, 11-12. 138-40 
P~tteway-Watk~nsanlialias~ng, li7-78 
Plvol polnl, IW, 
PixBll, 210 
Plxrl. 40 

a d d r e s n n ~  1\4-17 
gnd. 114 
mask. 146, 144-51. 152,517 
patterns (halftone), 516 
phasmg. 172 
rap 528-29 
uclghling mnsk. 174, 555 

P~xrl-order scanning. 554-55 
Pixrnap, 40 
Plane- 

clippng. 456-63 
coeffic~enls, 308 
comple~, 615 
equallons, 30X-r( 
far iclipp~ng). 447 
~nslde-ourslde laces. 30ti 
near (chpplng), 447 
nwmal vector. XYI-9 

Plan vmw, 440 
Plasma-panel d~splay, 45-46 
Plollers (ucnlso Pnnlrrs) 

hllhed, 74 
color, 73.74 
drum, 74 
Ilatbed. 74.75 
mk-jet, 72-73.74 
laser. 72. 73.74 
pcn, 74.75 
rollfeed, 74.75 

Point. 
chpplng. 225 
control (spline). 316 
coordinate, 602.605.61 2 
piollm& 84-86 
sarnpllng. 87 
as unit of character sue. 164 

Poinl hghl source. 496 
Polar cmrdinales, MI-2. b04 
Polar form (complex number),61n-17 
Polygon. 

activc edge Ilsl, 122, 477 
charaderi~hc, 316 
control. 316 
Age vector, 126 
fill. 117-27 ( u r  olw Area (ill~ng) 
inside face, 308 
~nslde-mlnde tats. 125 -26 (wnlxl  Plane) 
mesh, 306.309-10 

646 

normal vector, 3 0 8 ~  
ouls~de fdce, 308 
planeequallon, 307-9 
rendering (shadmg) 522-27 
ray Intersectton, 531-34 
sorted edge table, 121 
splithn& 23-37 
surface, KL5-6 
surfacedela~l, 557-54 
tables, 121-22,306-7.4"b-77 

Polygon clrpp~ng. 
parallel methods, 239 
~arametric methods. 243 

Weila-Atherton. 242- 4 1 
Polylme. 95-96 
Polyllne connecllons. 14b -49 
Polynomial curve, 1 I0 
Pos~lion emlssion tornngraphy (PET) 32-33 
Pos~lionlng methods. 286 
Pmthltmng. 172 (we olw Antialiaslng) 
Posting (structures), 252 
Precision (Iexl). 166-67 
Prefiltering, I R  (.we also Anllallasmgl 
Pmntat ion graphics. 11 13 
Pressure-wnsillve pvsttch, 63, 64 
Primar).colors. 568, 549 
Primitives, 77 (sc~olsoOu~pul primitwe) 
Princ~pal a m ,  440 
Principal vanishing ~ ' 1 1 7 1  446 
Prinlm- 

dot-mamx, 72 
electrolhennal, 73 
impact, 72 
laser, 72,73.74 
nonlmpart, 72 
~lwlrostalic, 73. 74 
ink-jd. 72-73.74 

Pnority 
strudurp. 252 
view-transfonnal~on Input. 283 

Procedural o b ~ l  represenlatton, 362-92 
Procedural lexture mapplng. 556-57 
Production rules. 387-80 
Progrcsrive refinement (rad~os~ty) ,  549-50 
Pmiectinp. squdre Ilne can 14: 
h o b t i o x .  ' 

axonomcln', 440 
cabinet, 443 
cavalier, 443 
center of. 438 
ftmtum, 447 
isometric, 440-41 
obbque. 439.441-43.447-M. 452-53 
orthographic, 439,441.447-48 
parallel. 298, 419-43. 452-54 
perswive.  299,439.453-47.454-56 
plane, 433 
reference pant ,  4.W 
veclos 450,452-53 
view volume. 447 
window, 447 

Pseudcwolor methods. 3% 
Pure color, 567.569 
Purity (light), 567 
Purple h e .  570 

Quadriccurve. 310 
Quadncsurfares, 310-1: 

Quadrilateral mt-h, 309-13 
Quadtree, 359 
Quatemlon, 61 7 

add~ l~on .  618 
In fractal ronstrucllons. 384-85 
Inverse, 618 
magn~lu$, olti 
mulf!plrahon. 518 
ordered-par resresenlahon. 419,618 
rolatlons.4 19-20 
scalar mulhplxation, 618 
scalar part, 419. 617 
veclor pan, 419.618 

Radtanl energy (Radiand, 544 
Radmsily model, %-51 

energy transpon equalton, 546 
lorm factors, 546 
hemlabe. 518-49 
lummance, 544 
progressive refinemenl, 545-50 
refleclivity laclor, 546 
surface mclosurc. 546 

Randomdither (nose), 520-21 
Random m~dpomr-displacemenl methods, 373-78 
Random-scan monitor, 41-42 

color, 42 
refresh d~splay hle, 42 

Rat~dom.xm bvsrrm: 
d~splay fi!e, 42. 56 
graphics controller, 56 
processing unll 56 

Random walk. 371 
Raster animahon, 3%-87 
Raster ops. ? 10 
Raster-scan monitor. 40-41 

b~level. 4C' 
b~tmap, 40 
color, 42-45 
frame buffer, 4C 
horizontal retraie, 41 
interlac~ng. 4 I 
pixel, 40 
plxmap, 40 
relrcsh bulfer, 40 
verhcal retrace. 41 

Raster-scan ,?stem 
cell encoding. 5 i  
display processax, 55 
run-length eniodlng, 55 
scan convPrslon. 55 
vldeo contolle~ 53-55 

Raster translormahons. 210-11 
Rattonal spltnc. M i  49 
Ray casling. 

constructwe sol~d Reomehy, 357-59 
visible-surface detertlon. 487-88 

Ray tracing, 527 
a d a p ~ v e  rarnpl~ng, 53-40 
adaptive subdlr ~sion, 53-38 
anldiastng, 59-43 
area samplmg. 539 
basic algorithm. 528-31 
bundles, 538 
cdmera-kns e l l a s ,  541 
cell haversal 536-37 
codes.541 
cone tracing, 5411 
d~slr lbutd,  .%I -43 
eve rav (kt, p~xel ray) 



Subject Index 

equation, 531 
intascccion calculations, 53-35 
pm& w 
Irght-bufh method, 537 
motion blur, 541.542-43 
pixel (primry) ray. 528-23 
polygon interrection. 533-34 
in radimity model, 550 
&&ion ray, 529.530-31 
rehadon ray, 529,530 
serondary ray, 529 
shadow ray, 529-30 
space subdivision. 535-3 
sphem intersection, 532-33 
stcchartic sampling, 510 
s u p e m p l m k  533-40 
m, 529 
uluform s u t d i v ~ ~ o n ,  536 

Read function, 210 
Readahle typeface, 132 
Real-time animation, 55.585.586 
Reference point (viewing), 218,219,438 
Reflection: 

angle of irmdence, 499 
an, 201 
meffinenb, 497-502 
diffuse, 497-YX) 
Fresnel lam, 501 
halfway vector, 503 
Lambertian, 498 
mapplnk 552 
plane, 422 
ray, 529 
specular, 500-504.530 
vector, 501-3.530 

Reflpchon transformation, 201-3.423 
Reflechvity, 498 
Reflectivity factor (mdiosity), 446 
Rehadion: 

angle, 503 
diffuse. 509 
mdex, 509 
ray, 529, 539 
Snell's law.503 
r ~ l a r ,  509 
transmission vector, 5lO,530-31 
transparency coefficient, 510 
-or, 510.5JO-31 

Rehesh buffer, 40 (ser alx, Frame buffer) 
Refmh CRT.37-45 (we nlsoCathoderay I 
Refresh display file. 42 
R e b h  rate (CUT). 40-41 
Repon codes (clipping). 

three-dimennonal, 460 
hvodinwnsional, 227 

Relahve cmrdinates, 96 
Rendering (ru Surface rendering) 
Rquest input nwdr. 281,282-85 
Resolution. 

&play device, 39-40 
halhone appronmations.518 

Retrace (electron beam), 41 
REYES, 475 
RCB chromalic~ty coord~nales. 573 
RCB color model. 572-73 
RCB monltor, 45 (W a1.w Video monitor) 
Right-hand cmrd~nate system, 602 
R&-hand rule, b08 
Ripd-hdy  transformat~on, 185, 1%-97 
Rlg~d motlon, 1% 
Roots: 

nonlinear equatluns, 621-22 
complex numbers. 617 

Rcu tion: 
angle, 186 
axis, 186,413-M 
axis vector, 414.15 
mmpolihon, 191 
inverse, 1W. 413 
numx repreuntation, 190,192-93.410-12. 

4I8-l9,&!O 
pivot point, 1% 
quaternion, 419-20 
raster methods. 211 
three-dimensional. -09-20 
Iwcxlimensional. 186-87, 190, 191. 392-93 
1ad.411-12 
y axis, 412 
: ax=, 409-11 

Rotahonal polygon-splitting method, 737 
Rmmd pin, 148-149 
Round line cap. 147 
Row vector, 61 1 
Rubber-band methbis, 290,291 
Run-length encodin& 56 

Sample lnput mode, 281,285 
Sampling: 

adapbve, 538-40 
area, 172,174 
h e .  87.88-89 
Nyquist interval 171 
point, 87 
ruperjampling. 172-74.538-40 
wnghted, 174 

Sans serif typeface. 132 
Saturation (light). 567 
Scalar data-field visualization, 395-99 
Scalar input methods, 277-78 
Scalar p d u d  of two vmors, 607-8 
Scaling. 

m arbitrary directions, 193-94 
composition, 192 
curved ob'+5,1SX 
differenhal. 188 
lactors, 187,421 
fixed point, 188, E l  
Inverse. 190,421-22 
mamx rppresentatlon. 190,421 
nonuniform (differential), 188,421 
parameters (factorsl, 187,421 
raster methods, 211 
threedmensional, 420-22 
rwcxlimensional, 187-88,1W,192,19>94 
uniform, 187-80.421 

Scan converaon, 55 
areas. 117-30 
charaden. 132-33 
orcles, 98-102 
ruwed-boundaryamas, 126-30 
mwed lines, 110-13 
ellipses, 103-10 
patterned hll, 159-63 
pants, 84.85-M 
polygons. 117-27 
straight lines. Rb-94 ( u r  air, Linedrawing 

algorithms) 
s t r u a u d ~ s t  travwsal, 252 

Scan line. 40 
S c a n h e  ~nterlaang, 4 I 
Scan-line algorithms 

area hll~ng, 117-27. I.%-&? 
v~sible-surface detection, 476-78 

Scanner, 67. f4 
S Q ~ i n g :  

imagcorder, 554 
invene. 5% 
vixel-order. 554-55 
texture, 5% 

Scientific virurlization. 25,395 ( s w a b  Data 
vinulization) 

Scrren coordinates, 54.76.11 4 k r  also Cmrdinate 
system, devlcr) 

Scripting system (animation), 588 
Secondary ray, 529 
Segment, 77. 251 
Self-she ha&, 364,372-78 
Self-inverse fractals, 364,385-87 
Self-similar hactaLs, 364,367-71 
S e l f w g  fradaLs.364.378-85 
Serif typfaoe, 132 
Shades(color), 571,5i7 
Shading algorithm: (ur Swfam rendmng) 
S h a d i i  rncdel, 495 (u( also Illumination model) 
Shadow mask. 43 
Shadow ray, 529-30 
Shadow: 

modeling 51 1,52940,542 
wenumbra. 542 
Lmbra. 542 

Shape grammars. 387-W 
Shear: 

axrs. 203 
manix, 423 
in pmpc(8on mapping. 442,453.454-56 
three-dimensional, 423 
hvodunens~onaI. 223-5 
r - d m o n .  203 
y-dirpbion, 204 
zdirection. 423 

Shift vector, I84 (wc also Translation) 
Similarity dimenslon, 365 
Simpson's rule, 623 
Simulstions, 5-10.21-31 (pcrahoGrrphics 

applicahons) 
Simulators. 2 1-25 
Simultaneous linear equation solvink 620-21 
Singular matnx, 611 
Sketch~ng, 13-16.291-92 
Snell's law. 503 
Snowlkke (fractal), .%7-68 
Soft fill. 162-63 
Software standards. 78-79 . - 
Solid an&, 5-44-45. &X 
Sohd m d l i n g :  (see aisoSurlace; Curved 

surface) 
applications, 4. 5.8.9 
constructive solid geometry, 356-59 
sweep constructiom, 355-56 

Solid texture, 556 
Sonic digitizer, 66 
Sorted edgc table, 17.1 
Spacebll, b3 
SpaceCraph system, 19 
Space-part~tioning m h o d s  (ray tracing): 

adaptwe. 536-38 
light buffer, 537 
ray bundles, 5.3 
uniform. 536 

Space-partitiontng representations, 305 
Specular reflechon, 497, YX)-504.530 

angle, 501 
coelfirient 50-2 
Fresnel labs, 501 
halfway vrctor, 503 
parameter Wl 



Subiect Index 

SprvLr rrlkctpn (mt . )  
Pkmg lnndel. 501-4 
vector, 5014.530 

SpcvLr &&n, 509 
S p c d  oi light, 5% 
Sphere, 3lI.62O 
Sphmol  mordinltes. 604 
Spinl, 139-40 
splinccurvc. 112.315-16 

approxinution, 316 
barir h n c l i o ~ ,  319 
hw matrix. 320 
bcbsplim. 345-47 
BCus, 327-3 
h s  pnmeter, 325,346 
b l d m g  functionr, 319 
kpl ine,  W-44 
cardirul, 3P-25 
Ghnull-Rom, 325 
chKwoairtic polygon, 316 
ronhuity corditionr, 317-19 
conhnruty paramner, 325 
control graph, 316 
m n h d  poink. 316 
conversion% 39-50 
convex hull, 316 
cuhc interpahon. 320-27 
dispbying. 351-55 
Hermite, 322-23 
mterpolation, 316 
knot vedor, 335 
KahanekBMe!s, 325-17 
lad conml, 332,335, 536 
msmx representation, 320 
natural, 321 
NURB, 347 
DvRhauser, 325 
rah0~1,347-(9 
tuuion panmeter, 324,325,341,346 

Spline generation: 
Homer's methad, 351 
f o n v a r d d i f f ~ m c ~  methud. 31-53 
subdivision methods, 353-55 

Spline surface. 316 
EQner, 333-34 
&pline, 344-45 

Splrtting concave polygons 
rotational method. 237 
vector method. 2.36 

Spotlnghk, 504 
Spnng constant, 393 
Spnng network (nonrigid body]. 393 
Square matrix. 611 
Stalrstep effect, 85 
Strrad~an, 56-45. b01 
St ereoscop~c 

glasses, 51 
headsets, 52 
news, 6.7.50-52.292.293.300-301 
vrmal-realily applications, 5-7.50-52 

Stffhashc Sampling, 540 
Storyboard. 585 
Strearnlrnes. 403 
String mput device, 276.277 
String precision (text). 146, 167 
Stroke lnput device. 276. 277 
Stroke precision (text) 166-67 
Strok~wnting display. 41 (sn nllo Video mcmmrs, 

random-scan) 
Stnrcture, 77.251 

s t t n b u t ~ .  253-54 
b r i c  functions, 251-54 
central structure store (CSS), 251 

648 

ronccpb, 251-52 
copying. 260 
mation, 251-52 
deJection 253.260 
displaying (posh@. 252 
editing, 254-60 
dement, zss 
element pointer, 2% 
f l t m ,  253,281-85 
h i m h y ,  266-68 
highhghling filter, 253-54 
h, 252 
nwtofile, 79 
pichbrlity, W 
posting. 252 
prionty, 252 
rekbelul& 253 
trauerd, 252 
unpring. 25253 
visibility, 253 
workstation filter& 254,284-85 

Sutdivis~on mcchodr: 
adaptive ray ad&& 536-38 
RSP bee, 562 
fradal gmeraticn.373-78 
-, 359-62 
spline generatian. 353-55 
unilorm ray m a n &  536 

Subhadive color model (CMY), 574-75 
Superquadnc, 312-14 
Supemmphg,  172-74.53-40 
Surfam 

blotby, 314-1 5 
awed, 310 Cur alw Curved surfam) 
h a d ,  366,369-85 
paramebic repmsentltion. 619-20 
plane, 305-9 
quadric. 310-12 
spline, 316 (sealsospline surfacel 
supquadric ,  312-14 
weighting. 174 

Surface d d ,  53-60 
bump mapping, 558-59 
environment mappmg, 552 
hame mapping. 55940 
imageorder scanrung, 554 
inveme wnnmg. 554 
p t t m  mappink 54( 
p~xel-order scan&& 554 
wlrpon mesh, 553-54 
p&ural texturing. 556-57 
solid texture mappma 556 
texture mspPb&55&% 
texture scanning. 5% 

Surfam enclosure (radiority), 546 
Surfam normal vector, 308-9.523.558 
Surface rendering. 297-98.495 

antialiasma U8-43 
bump mapping. 59-59 
constant-intemity shading, 522-23 
environment mappmg 552 
fast Phong shading. 526-27 
flat shadmg, 522 
kame mapping. 559-MI 
Couraud shading, 523-25 
i n f m s ~ y  interpoltian, 523 
Mach bands. 525 
normal-vector mterpolstion, 525 
Phong shadmg. 525-27 
polygon methods, 522-27 
polygon surfam detml, 553.-W 
procedural tertunnk 556-57 
rad~antv. 544-50 

ray-hating, 527-43 
@mu. nupping. 59-56 

Surface shding (sa Sudace rendering) 
SutherLnd-Hodgeman polygon4ipping. 238-42 
Sweep rrpmentations, 355-54 
Symbol, 26 1 

hmardws, 262-63 
instance. 261 
in modelin& 26144 

sr"metr,: 
circle, 97-98 
in curvednwmg algorithms, 97-96,103,112 
ellipse. 103 

Table (polygon) 
attribute, 306 
edge, 121-22.306-7.476-77 
glomcmc, 306-7 
sorted d g e  table, 121 
vertex, 306-7 

Tablet, M-67 (wr also [)lgituer) 
Task pkruung, 13 
Tension parameter (spline), 324,325,341,346 
Tensor, 610 

contradiOn 402 
data-field visualization, 40-2 
mebic, 610-11 

Terram (fractal), 372-78 
T-b W surface, 506 
Text: (so a h  Cluradd 

alignment. 1 6  
attributes, 163-67,169-70 
dippink 244.245 
generation, 132-33 
path lbb 
p d i o n ,  166-67 

Texture, 553 (set also Surface rendering) 
mapping, 554-56 
procedural m h o d s .  556-57 
scanning, 554 
sohd, 556 
spam, 55% 556-57 

Thin-film elechulumincscent display, 46 
Threepoint perspective pmpction. 446 
Tling. 160,306 
Tmcdrert. 11, 13 
Tmr ~mlor), 571.577 
l int  fill, 162 
Tone (color), 571,577 
Topline (character), 164 
Topological covering. 365-66 
Touch panel, 68-70 
TracbU,  63 
Transformation- 

a fine, 26) 
basic gcamdric, 1%-203, W - 2 2  
commutative, 194-95 
composite. 191-200.42335 
cornputabonal efficien? 195-97 
coordinate system, 205 -7,426-29 
functiom, 208-9.425-26 
geometric. 77.184 
irumncr, 265- 
laal .  265-68 
m&x representations, 188-90 
modeling. 77,265-68.426-29 
noncommutativr, W-95 
mrallel Dmralon. 298-99.438 
&p&ve'pmpdion, 299.438 
raster methods 210-11 



Sublect Index 

reflection, 201-3,422 
rotation, 186-87.190-93. UX-20 
scaling, 187-88,190,192-94,420-22 
shear, 2034.423 
three-dimensional geometric, 408-22 
threedimmr~onal viewing, 432-56 
translation, llU-85,190,191,408-9 
two-dimensional geometric, 184-205 
huo-dimenslonal newing, 217-22 
viewing, 77,217-22.432-56 
mndow-to-newport, 217,220-22 
workrtation, 221-22.456 
world-to-viewmg coordinate, 218-20.437-3 

Translation, 
:omposihon. 191 
curved o w .  185 
d~slances. 184.408 
invene, 190,409 
m a 6  representation, IW 408 
raster methods, 210 
thrredimenstonal, 408-9 
two-dimemio~l. 184-B5.190.191 
vector, 1ffl.408 

Transrnkron vector (draction), 510.53041 
Tramparency (sn nlsa Rehadion, Ray tradng) 

coefficient, 510 
modeling Y18-ll 
opac~ty factor, 510 
vector. 510.550-31 

Tram- (mbix), 613 
Trawwld rule, 623 
~ r a b e w l  state List, 252 
Triangle smp, 309 
Tristimulus vuion theory, 572 
T~e-cdor  system. 45 
TWW angle, 434 
Two-point perrpmive prok;hon, 446 
Typeface. 131-33 (rrr also Font) 

legible, 132 
readable, 132 
sans mil. 132 
senf, 132 

Umbra shadow, 542 
Unbundled attributes, 168 
Uniform & p h ,  336-44 
Unifonn xalmg, 187-88.421 
Uniform smtial subdivision. 

atree, i59-62 
ray tranng. 536 

Unit cub (clippmg), 458 
Up vector Ccharacter). 165 
Uur  dia!ogue, ZR-73 
User help fadline, 274 
User ~nterfacp. 34,272-76,288-93 (ore also 

Graphical user interface) 
User model, 2R 
U M  coordinate system, 435-38 
u17 plane, 435 

Valuator input device, 276,2T-78 
Value (HSV parameter), 575 
Vanishing point, 446 
Varllucal mtnor, 49 
Vector, 605,611-12 

additnon. 607 

h i s .  609 
calumn, 61 1 
components, 605 
cnas product, t08-.9 
dab-held visualization, 400-401 
dlmction angle. 606 
direction cosines. 606 
dot (inner) product. 607-8 
knot, 1-5 
magnitude (Iengthl, 605 
polygon edge, 126 
produh m - 9  
pmjfftion, 450.452-53 
in quaternion rtpmntatbon, 419,618 
reflection, 501-3, 5:M 
rolahon.414-15 
row, 611 
scalar multiplicahon, 607 
sa la r  (do0 p rdur t ,  607-8 
spa-. 609 
rpecular rdemon  500-504.h 
surface normal. 203-9,523,558 
bamtni?sion (mhact~on). 510,530-31 
m m l a t i a ,  164 408 

Vecior method (polygon splitting), 2% 
Vector monitor, 41 
Vertex table, 306-7 
Vehcal m a c e ,  41 
Wdeomnboller, 53-55 
Vrdm lookup table. 155,!33 
Vldeo monitor (sn d m  Cathoderay rube) 

calligraphic, 41 
color CRT, 42-45 
romposite, 44-45 
direc-vrew storage mbe (DVSl3.45 
emissive, 45 
flat-panel, 45 
tullalor, 45 
gardischarge. IS 
LCD &quid c+ystal device), 47-18 
LED (I~ght-emimng diode). a - 4 7  
non-ive, 45 
pksma panel, 45- 46 
random-scan, 41-42 
rasterran, 40-4 1 
rrtresh CRT, 37-45 
resoluhon, 39-00 
RGB, 45 
stereoscopic, 50-5: 
chin-film electroluminexent. 46 
~ W i m e n s i o n a l ,  49 
truemlor, 45 
vector, 41 

view 
look-at point. 434 
reference point, 218, 214,439 
up vector. 219,434 
nvist angle, 434 

Viewing: 
rtereorcopic. 6, 7, 50-52.292, 293, m-MI 
threedimensional. 297 
two-dimensional, 217-45 

Viewing coordmater: 
Ieftft-handed, 435 
thrPe-dimensiona:, 433-34 
two-dunensionai. 218.219-20 

Viewmg transformation: 
back (far) chpp in~  plane, 447 
clipping 224-45.156-63 
honl Inear) clipping pbne. 447 
fnutum, 447 
hnctrons. 222-23.464-66 
hardware implrrncntatlon, 463 .W 

Input priority, 283 
norrnal~zed propamn coordinate. 4% 
normalize3 view volume, 458-61 
plpel~ne, 217-19,432-33 
lhreedimemional, 432-33 
twodmensional, 217-22 
viewport, 217.458-60 
view volume. 447 
window. 217,447 
workstation mapping, 221-22. W 

Viewing table, 223,465 
V i m  pldne, 433-34 

normal vector, 434 
position. 434-35 
window. 447 

Viewport. 
chppmg. 24.460-bl 
fundion. 222-23 
pnonn: 283 
three-dunensional ( x c  View volume) 
huc-dimmsional, 117 
workstation, 222 

View reference point,218,219,434 
V~ew-up vector, 219,434 
View volume, 447 

unit mbe, 458 
normalized, 458 
perspeai\ e. 447-49 
parallel, 447-50 

View window, 447 
VirNal rrality. 

applications. 5-8.466-67 
display devices, 51-52 
input devms, 64 
environments. 292-93 

Visible structure, 253 
V~sihleline detection. 490 (seealso Depth cuc in~)  
Vinblesurface detedon. 470 

A h H e r  method. 475-76 
algorithm c lau ihnon ,  470-71 
area-subdivision method, 482-85 
hack-face detection, 471-72 
BSP-hee method, 481-82 
comparlrm of algorithms, 491,92 
curved surfaces, 487-90 
depth-butfer 12-buffer) method, 472-75 
depth-sorting method, 478-81 
fundion, 490-91 
imagesp~ce methods, 470 
objec-space methods, 470 
ochee methods, 485-87 
painter's algorithm (depth sorting), 478 
rayiastlng method, 487-88 
scan-lme method, 476-78 
surface contour plots. 489-90 
w~rpfram+ methods. 490 

Vision (trist~mulus theory), 572 
Visualizat~on: 

applications, 25-31 
methods, 395-403 (urrrlso Data visualization) 

Voicesystems, 70-71 
Volume cal"ulations (CSG), 39-59 
Volume element, W 
Volume rendering 399 
Voxel, 360 

Warn lightmg model, W - 5  
Wavelength (light), %6 
Weighted siimplmg, 174,555 
Weighting surface, 174 



Sublea Index 

Wriler-Athenon polygondipping algorithm, 
242-43 

 ti Ilght. 567, 570 
Winding numbet. 125 
Wmdow: 

fun&ons, 222-23,465 
manager, 34.273 
nomctangulu, 21 7 
pick, 280 
p r o e o n ,  447 
rotated, 218,219-20 
thrPrdimemio~l viewing 432-56 
nvodimensiorul newin& 217 
user-rnlerlace, 34,273 
new-plane, 433-34 
worhtahon, 221-22.465 

Wmdowmg tramforma Son, 21 7 
panning. 219 
m m i n g  218-19 

Wndow-teviewpon mapping, 217.220-22 
Wveframe, 4.5.298 
Wveframe visibilily algorithms 493 

Worlrrlalion 
in graphim applications, 57-b0 
idenhfier, 79 
PHIGS, 79 
pick filter. 784-85 
stnrcture filters, 254, 284-85 
transformation, 221-22,466 
window, 221-22, 4b5 
viewpon. 222.465 

World coordinates, 76 
World-reviewing caodinate Iranclormation, 218, 

219-20,437-36 
Wnte function. 210 

x-axis rotation, 411-12 
I-dimthon shear, 203 
X window System, 272 
XYZmlor model, 569 

y-axls rotahon, 412 
ydrrP*ion shear. 204 
YlQ mlor n~cdel .  574 

z.axis rotahon, 40?-11 
I-buffer algonthm. 472 (snalso Depih-buffer 

algonthm) 
zdimtian shear. 423 
2 mouse, 62-63 
morning, 218-15. 



Function Index 

generdired[XamngPnmlllve, 113 
getchoice. 286 
ge:Lncator, 2% 
getkator3,302 
getl'lck, 286 
getpixel. 86 
gedtring 286 
gedtroke, 286 
getvaluator, 286 

label. 258 

sampleChoace, 285 
samplelocator, 285 
samplePick. 285 
sampleStrin&. 285 
sampleSmk<-, 285 
~rnplcValuator, 285 
scale, 208 
scale), 425 
~st~tCharacterExpa~ionFaclor. 165 
~CharactcrHcight. I 6 4  
setCharacter5pacmg. 165 
setCharacterUpVertor, 165 
seChoiceMnde, 281 
setColourReyrmnlalion, 156 
setEdtIMode, 2% 
sctElementR)~nler, 255 
.wIElemenlP~~~nlerAtLabrI, 259 
selH~ghlighltngF~lter, 241 
selHLHSbdentifier, 491 
sctlndiv~dualASF, 168 
sellntenoC~11ourlndex. 158 
sellntenorlndcx, 169 
scllnleriorRc.prcscnlat~on, 169 
sellntenodtvle, 158 
sellnler1or5Wlelndcx. 159 
setlnvbib~l~tvF!ller, 253 
setLincPmc. 145 



Function Index 

setTtxtRrprrsenlation 169 setW~tationWmdow3,466 
BetWuatorMcde, 281 
BetViewLndex, 2l3.M 
BctV~ewRepresentabon. W T 
setMewReprrrmtahon3,465 
setV~ewTraruformatianInpulPnbrity, 283 text, 133 
sefWorbtalionViewpac, 223 IexlJ, 302 
w W o r k s t a 6 o n V i ~ ,  466 hanSformPoint, 209 
se:WorlcstationWmdow, 22 t r a ~ f d o i n t 3 . 4 2 6  


	Computer Graphics, C Version (2nd Ed.)
	Contents
	Ch1 A Survey of Computer Graphics
	1.1 Computer-Aided Design
	1.2 Presentation Graphics
	1.3 Computer Art
	1.4 Entertainment
	1.5 Education & Training
	1.6 Visualization
	1.7 Image Processing
	1.8 Graphical User Interfaces

	Ch2 Overview of Graphics Systems
	2.1 Video Display Devices
	2.2 Raster-Scan Systems
	2.3 Random-Scan Systems
	2.4 Graphics Monitors & Workstations
	2.5 Input Devices
	2.6 Hard-Copy Devices
	2.7 Graphics Software
	Summary
	References
	Exercise

	Ch3 Output Primitives
	3.1 Points & Lines 
	3.2 Line-Drawing Algorithms
	3.3 Loading the Frame Buffer
	3.4 Line Function
	3.5 Circle-Generating Algorithms
	3.6 Ellipse-Generating Algorithms
	3.7 Other Curves
	3.8 Parallel Curve Algorithms
	3.9 Curve Functions
	3.10 Pixel Addressing & Object Geometry
	3.11 Filled-Area Primitives
	3.12 Fill-Area Functions
	3.13 Cell Array
	3.14 Character Generation
	Summary
	Applications
	References
	Exercises

	Ch4 Attributes of Output Primitives
	4.1 Line Attributes
	4.2 Curve Attributes
	4.3 Color & Grayscale Levels
	4.4 Area-Fill Attributes
	4.5 Character Attributes
	4.6 Bundled Attributes
	4.7 Inquiry Functions
	4.8 Antialiasing
	Summary
	References
	Exercises

	Ch5 Two-Dimensional Geometric Transformations
	5.1 Basic Transformations
	5.2 Matrix Representations & Homogeneous Coordinates
	5.3 Composite Transformations
	5.4 Other Transformations
	5.5 Transformations between Coordinate Systems
	5.6 Affine Transformations
	5.7 Tansformation Functions
	5.8 Raster Methods for Transformations
	Summary
	References
	Exercises

	Ch6 Two-Dimensional Viewing
	6.1 The Viewing Pipeline
	6.2 Viewing Coordinate Reference Frame
	6.3 Window-to-Viewport Coordinate Transformation
	6.4 Two-Dimensional Viewing Functions
	6.5 Clipping Operations
	6.6 Point Clipping
	6.7 Line Clipping
	6.8 Polygon Clipping
	6.9 Curve Clipping
	6.10 Text Clipping
	6.11 Exterior Clipping
	Summary
	References
	Exercises

	Ch7 Structures & Hierarchical Modeling
	7.1 Structure Concepts
	7.2 Editing Structures
	7.3 Basic Modeling Concepts
	7.4 Hierarchical Modeling with Structures
	Summary
	References
	Exercises

	Ch8 Graphic User Interfaces & Interactive Input Methods
	8.1 The User Dialogue
	8.2 Input of Graphical Data
	8.3 Input Functions
	8.4 Initial Values for Input-Device Parameters
	8.5 Interactive Picture-Construction Techniques
	8.6 Virtual-Reality Environments
	Summary
	References
	Exercises

	Ch9 Three Dimensional Concepts
	9.1 Three-Dimensional Display Methods
	9.2 Three-Dimensional Graphics Packages

	Ch10 Three-Dimensional Object Representations 
	10.1 Polygon Surfaces
	10.2 Curved Lines & Surfaces
	10.3 Quadric Surfaces
	10.4 Superquadrics
	10.5 Blobby Objects
	10.6 Spline Representations
	10.7 Cubic Spline Interpolation Methods
	10.8 Bezier Curves & Surfaces
	10.9 B-Spline Curves & Surfaces
	10.10 Beta-Splines
	10.11 Rational Splines
	10.12 Conversion between Spline Representations
	10.13 Displaying Spline Curves & Surfaces
	10.14 Sweep Representaions
	10.15 Constructive Solid-Geometry Methods
	10.16 Octrees
	10.17 BSP Trees
	10.18 Fractal-Geometry Methods
	10.19 Shape Grammars & Other Procedural Methods
	10.20 Particle Systems
	10.21 Physically Based Modeling
	10.22 Visualization of Data Sets
	Summary
	References
	Exercises

	Ch11 Three-Dimensional Geometric & Modeling Transformations
	11.1 Translation
	11.2 Rotation
	11.3 Scaling
	11.4 Other Transformations
	11.5 Composite Transformations
	11.6 Three-Dimensional Transformation Functions
	11.7 Modeling & Coordinate Transformations
	Summary
	References
	Exercises

	Ch12 Three-Dimensional Viewing
	12.1 Viewing Pipeline
	12.2 Viewing Coordinates
	12.3 Projections
	12.4 View Volumes & General Projection Transformations
	12.5 Clipping
	12.6 Hardware Implementations
	12.7 Three-Dimensional Viewing Functions
	Summary
	References
	Exercises

	Ch13 Visible-Surface Detection methods
	13.1 Classification of Visible-Surface Detection Algorithms
	13.2 Back-Face Detection
	13.3 Depth-Buffer Method
	13.4 A-Buffer Method
	13.5 Scan-Line Method
	13.6 Depth-Sorting Method
	13.7 BSP-Tree Method
	13.8 Area-Subdivision Method
	13.9 Octree Methods
	13.10 Ray-Casting Method
	13.11 Curved Surfaces
	13.12 Wireframe Methods 
	13.13 Visibility-Detection Functions
	Summary
	References
	Exercises

	Ch14 Illustration Models & Surface-Rendering Methods
	14.1 Light Sources
	14.2 Basic Illumination Models
	14.3 Displaying Light Intensities
	14.4 Halftone Patterns & Dithering Techniques
	14.5 Polygon-Rendering Methods
	14.6 Ray-Tracing Methods
	14.7 Radiosity Lighting Model
	14.8 Environment Mapping
	14.9 Adding Surface Detail
	Summary
	References
	Exercises

	Ch15 Color Models & Color Applications
	15.1 Properties of Light
	15.2 Standard Primaries & Chromaticity Diagram
	15.3 Intuitive Color Concepts
	15.4 RGB Color Model
	15.5 YIQ Color Model
	15.6 CMY Color Model
	15.7 HSV Color model
	15.8 Conversion between HSV & RGB Models
	15.9 HLS Color Model
	15.10 Color Selection & Applications
	Summary
	References
	Exercises

	Ch16 Computer Animation
	16.1 Design of Animation Sequences
	16.2 General Computer-Animation Functions
	16.3 Raster Animations
	16.4 Computer-Animation Languages
	16.5 Key-Frame Systems
	16.6 Motion Specifications
	Summary
	References
	Exercises

	AppA Mathematics for Computer Graphics
	A.1 Coordinate Reference Frames
	A.2 Points & Vectors
	A.3 Basis Vectors & the Metric Tensor
	A.4 Matrices
	A.5 Complex Numbers
	A.6 Quaternions
	A.7 Nonparametric Representations
	A.8 Parametric Representations
	A.9 Numerical Methods

	Bibliography
	Subject Index
	Function Index




